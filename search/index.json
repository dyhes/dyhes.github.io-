[{"content":"TypeScript 在 TypeScript 中，struct 并不是一个内置的关键字或概念。TypeScript 主要基于 JavaScript，而 JavaScript 是一种动态类型的语言，没有像 C 或 C++ 中的 struct 这样的结构体类型。\n不过，TypeScript 提供了几种方式来定义和操作复杂的数据结构，类似于 struct 的功能：\n1. 使用 interface interface 是 TypeScript 中定义对象形状的主要方式。你可以使用 interface 来定义一个结构化的数据类型。\ninterface Person { name: string; age: number; isStudent: boolean; } const person: Person = { name: \u0026#34;Alice\u0026#34;, age: 25, isStudent: false }; 2. 使用 type type 是另一种定义复杂数据类型的方式，功能与 interface 类似。\ntype Person = { name: string; age: number; isStudent: boolean; }; const person: Person = { name: \u0026#34;Bob\u0026#34;, age: 30, isStudent: true }; 3. 使用 class 如果你需要更复杂的行为（如方法），可以使用 class 来定义一个结构。\nclass Person { name: string; age: number; isStudent: boolean; constructor(name: string, age: number, isStudent: boolean) { this.name = name; this.age = age; this.isStudent = isStudent; } greet() { console.log(`Hello, my name is ${this.name}`); } } const person = new Person(\u0026#34;Charlie\u0026#34;, 28, false); person.greet(); 4. 使用 type 和 interface 的组合 你可以将 type 和 interface 结合使用，以创建更复杂的数据结构。\ninterface Address { street: string; city: string; zipCode: string; } type Person = { name: string; age: number; address: Address; }; const person: Person = { name: \u0026#34;David\u0026#34;, age: 35, address: { street: \u0026#34;123 Main St\u0026#34;, city: \u0026#34;Anytown\u0026#34;, zipCode: \u0026#34;12345\u0026#34; } }; 5. 使用 readonly 和 optional 属性 你还可以使用 readonly 和 ? 来定义只读和可选属性。\ninterface Person { readonly id: number; name: string; age?: number; // 可选属性 } const person: Person = { id: 1, name: \u0026#34;Eve\u0026#34; }; 总结 虽然 TypeScript 没有直接的 struct 概念，但通过 interface、type 和 class，你可以轻松定义和使用复杂的数据结构。这些工具提供了比传统 struct 更强大和灵活的功能。\n在 ArkTS（ArkUI TypeScript）中，struct 是一种用于定义自定义组件的核心数据结构。它类似于 TypeScript 中的 class 或 interface，但具有特定的用途和限制，主要用于声明式 UI 开发。以下是关于 ArkTS 中 struct 的详细说明：\n1. struct 的基本概念 定义方式：struct 是 ArkTS 中用于定义自定义组件的基础结构，通常与装饰器（如 @Component 和 @Entry）结合使用。\n@Component struct MyComponent { // 成员变量和方法 build() { // UI 描述 } } 作用：struct 用于封装组件的状态、属性和 UI 描述，使其成为一个可复用的独立单元。\n与 class 和 interface 的区别：\nstruct 主要用于定义数据模型和 UI 组件，通常与 UI 绑定紧密相关。 struct 不支持继承，也不能扩展或实现其他类或接口。 struct 通常设计为可序列化，便于在应用程序中传递数据。 2. struct 的核心特性 （1）@Component 装饰器 @Component 是 ArkTS 中用于修饰 struct 的装饰器，使其具备组件化的能力。 被 @Component 装饰的 struct 必须实现 build() 方法，用于定义组件的 UI 描述。 @Component struct MyComponent { build() { Column() { Text(\u0026#39;Hello, ArkTS!\u0026#39;) } } } （2）@Entry 装饰器 @Entry 用于标记页面的入口组件，每个页面只能有一个 @Entry 装饰的组件。 @Entry @Component struct MainPage { build() { Column() { Text(\u0026#39;This is the main page\u0026#39;) } } } （3）build() 方法 build() 方法是自定义组件的核心，用于声明组件的 UI 结构。 必须遵循以下规则： 根节点唯一且必要，通常为容器组件（如 Column、Row）。 不允许声明本地变量或创建本地作用域。 不允许直接调用未使用 @Builder 装饰的方法。 3. struct 的成员变量与函数 （1）成员变量 成员变量用于存储组件的状态和数据，通常使用装饰器（如 @State、@Prop、@Link）进行修饰。 成员变量是私有的，不支持静态变量。 @Component struct MyComponent { @State message: string = \u0026#39;Hello, ArkTS!\u0026#39;; build() { Column() { Text(this.message) } } } （2）成员函数 成员函数用于定义组件的逻辑，通常用于事件处理或数据操作。 成员函数是私有的，不支持静态函数。 @Component struct MyComponent { @State count: number = 0; private increaseCount() { this.count += 1; } build() { Column() { Text(`Count: ${this.count}`) Button(\u0026#39;Increase\u0026#39;).onClick(() =\u0026gt; this.increaseCount()) } } } 4. struct 的使用场景 （1）自定义组件 struct 是 ArkTS 中定义自定义组件的基础，开发者可以通过组合系统组件（如 Text、Button）和自定义逻辑来创建复杂的 UI 组件。 （2）状态管理 通过装饰器（如 @State、@Prop、@Link），struct 可以实现组件内部状态的管理，并驱动 UI 更新。 （3）模块化开发 使用 export 关键字导出 struct，可以在其他文件中导入并复用自定义组件。 // MyComponent.ets @Component export struct MyComponent { build() { Column() { Text(\u0026#39;This is a reusable component\u0026#39;) } } } // MainPage.ets import { MyComponent } from \u0026#39;./MyComponent\u0026#39;; @Entry @Component struct MainPage { build() { Column() { MyComponent() } } } 5. struct 的限制 不支持继承：struct 不能继承其他类或接口。 实例化：struct 的实例化可以省略 new 关键字。 命名规则：自定义组件名、类名、函数名不能与系统组件名相同。 总结 在 ArkTS 中，struct 是一种用于定义自定义组件的核心数据结构，具有以下特点：\n与 @Component 和 @Entry 装饰器结合使用，用于声明式 UI 开发。 支持成员变量和函数，用于管理组件状态和逻辑。 不支持继承，但可以通过模块化实现代码复用。 主要用于构建可复用的 UI 组件，并支持状态驱动的 UI 更新。 通过合理使用 struct，开发者可以高效地构建复杂的用户界面，并实现灵活的状态管理。\n","date":"2025-01-13T00:00:00Z","permalink":"https://dyhes.github.io/p/arktsstruct/","title":"【ArkTs】struct"},{"content":"期末评教 失败版\nfor (var i of document.querySelectorAll(\u0026#39;.ant-form-item-control\u0026#39;)) { var group = i.querySelector(\u0026#39;.ant-radio-group\u0026#39;); if (group) { i.className = \u0026#39;ant-form-item-control has-success\u0026#39;; var label = group.querySelectorAll(\u0026#39;.ant-radio-wrapper\u0026#39;)[0]; if (label) { label.className = \u0026#39;ant-radio-wrapper ant-radio-wrapper-checked\u0026#39;; var radio = label.querySelector(\u0026#39;input\u0026#39;); if (radio) { radio.checked = true; // 触发 change 事件 var event = new Event(\u0026#39;change\u0026#39;, { \u0026#39;bubbles\u0026#39;: true }); radio.dispatchEvent(event); } } } } 千问的帮助下：\nfor (var i of document.querySelectorAll(\u0026#39;.ant-radio-group\u0026#39;)) { var radio = i.childNodes[0].querySelector(\u0026#39;input\u0026#39;); radio.click(); var event = new Event(\u0026#39;change\u0026#39;, { \u0026#39;bubbles\u0026#39;: true }); radio.dispatchEvent(event); } ","date":"2025-01-13T00:00:00Z","permalink":"https://dyhes.github.io/p/code%E6%9C%9F%E6%9C%AB%E8%AF%84%E6%95%99/","title":"【Code】期末评教"},{"content":"Introduction 我们对应用框架、应用服务、系统、媒体、图形等领域的重点开放能力提供了开发指导，助力开发者快速完成应用的开发。\n应用框架相关Kit开放能力：Ability Kit（程序框架服务）、ArkUI（方舟UI框架）等。 应用服务相关Kit开放能力：Calendar Kit（日历服务）、Location Kit（位置服务）等。 系统相关Kit开放能力：Network Kit（网络服务）、Universal Keystore Kit（密钥管理服务）等。 媒体相关Kit开放能力：Audio Kit（音频服务）、Media Library Kit（媒体文件管理服务）等。 图形相关Kit开放能力：ArkGraphics 2D（方舟2D图形服务）等。 UI 框架 OpenHarmony提供了一套UI开发框架，即方舟开发框架（ArkUI框架）。方舟开发框架可为开发者提供应用UI开发所必需的能力，比如多种组件、布局计算、动画能力、UI交互、绘制等。 方舟开发框架针对不同目的和技术背景的开发者提供了两种开发范式，分别是基于ArkTS的声明式开发范式（简称“声明式开发范式”）和兼容JS的类Web开发范式（简称“类Web开发范式”）。以下是两种开发范式的简单对比。\n开发范式名称 语言生态 UI更新方式 适用场景 适用人群 声明式开发范式 ArkTS语言 数据驱动更新 复杂度较大、团队合作度较高的程序 移动系统应用开发人员、系统应用开发人员 类Web开发范式 JS语言 数据驱动更新 界面较为简单的程序应用和卡片 Web前端开发人员 应用模型 应用模型是OpenHarmony为开发者提供的应用程序所需能力的抽象提炼，它提供了应用程序必备的组件和运行机制。有了应用模型，开发者可以基于一套统一的模型进行应用开发，使应用开发更简单、高效。请见应用模型的构成要素。 随着系统的演进发展，OpenHarmony先后提供了两种应用模型：\nStage模型： OpenHarmony API 9开始新增的模型，是目前主推且会长期演进的模型。在该模型中，由于提供了AbilityStage、WindowStage等类作为应用组件和Window窗口的“舞台”，因此称这种应用模型为Stage模型。Stage模型开发可见Stage模型开发概述。快速入门以此为例提供开发指导。 FA（Feature Ability）模型： OpenHarmony API 7开始支持的模型，已经不再主推。FA模型开发可见FA模型开发概述。快速入门章节不再对此展开提供开发指导。 ⠀FA模型和Stage模型的整体架构和设计思想等更多区别，请见应用模型解读。 快速入门提供了一个含有两个页面的开发实例，并基于Stage模型构建第一个ArkTS应用，以便开发者理解以上基本概念及应用开发流程。 ArkTS工程目录结构（Stage模型） AppScope \u0026gt; app.json5：应用的全局配置信息，详见app.json5配置文件。 entry：OpenHarmony工程模块，编译构建生成一个HAP包。 src \u0026gt; main \u0026gt; ets：用于存放ArkTS源码。 src \u0026gt; main \u0026gt; ets \u0026gt; entryability：应用/服务的入口。 src \u0026gt; main \u0026gt; ets \u0026gt; pages：应用/服务包含的页面。 src \u0026gt; main \u0026gt; resources：用于存放应用/服务所用到的资源文件，如图形、多媒体、字符串、布局文件等。关于资源文件，详见资源文件的分类。 src \u0026gt; main \u0026gt; module.json5：模块配置文件。主要包含HAP包的配置信息、应用/服务在具体设备上的配置信息以及应用/服务的全局配置信息。具体的配置文件说明，详见module.json5配置文件。 build-profile.json5：当前的模块信息 、编译信息配置项，包括buildOption、targets配置等。 hvigorfile.ts：模块级编译构建任务脚本，开发者可以自定义相关任务和代码实现。 obfuscation-rules.txt：混淆规则文件。混淆开启后，在使用Release模式进行编译时，会对代码进行编译、混淆及压缩处理，保护代码资产。 oh_modules：用于存放三方库依赖信息。 build-profile.json5：应用级配置信息，包括签名signingConfigs、产品配置products等。 hvigorfile.ts：应用级编译构建任务脚本。 应用程序包 用户应用程序泛指运行在设备的操作系统之上，为用户提供特定服务的程序，简称“应用”。一个应用所对应的软件包文件，称为“应用程序包”。 当前系统提供了应用程序包开发、安装、查询、更新、卸载的管理机制，便于开发者开发和管理应用。同时，系统还屏蔽了不同的芯片平台的差异（包括x86/ARM，32位/64位等），应用程序包在不同的芯片平台都能够安装运行，这使得开发者可以聚焦于应用的功能实现。\nMulti Module 支持模块化开发： 一个应用通常会包含多种功能，将不同的功能特性按模块来划分和管理是一种良好的设计方式。在开发过程中，我们可以将每个功能模块作为一个独立的Module进行开发，Module中可以包含源代码、资源文件、第三方库、配置文件等，每一个Module可以独立编译，实现特定的功能。这种模块化、松耦合的应用管理方式有助于应用的开发、维护与扩展。 支持多设备适配： 一个应用往往需要适配多种设备类型，在采用多Module设计的应用中，每个Module都会标注所支持的设备类型。有些Module支持全部类型的设备，有些Module只支持某一种或几种类型的设备（比如平板），那么在应用市场分发应用包时，也能够根据设备类型做精准的筛选和匹配，从而将不同的包合理的组合和部署到对应的设备上。 Module Type Module按照使用场景可以分为两种类型：\nAbility类型的Module： 用于实现应用的功能和特性。每一个Ability类型的Module编译后，会生成一个以.hap为后缀的文件，我们称其为HAP（Harmony Ability Package）包。HAP包可以独立安装和运行，是应用安装的基本单位，一个应用中可以包含一个或多个HAP包，具体包含如下两种类型。 entry类型的Module：应用的主模块，包含应用的入口界面、入口图标和主功能特性，编译后生成entry类型的HAP。每一个应用分发到同一类型的设备上的应用程序包，只能包含唯一一个entry类型的HAP。 feature类型的Module：应用的动态特性模块，编译后生成feature类型的HAP。一个应用中可以包含一个或多个feature类型的HAP，也可以不包含。 Library类型的Module： 用于实现代码和资源的共享。同一个Library类型的Module可以被其他的Module多次引用，合理地使用该类型的Module，能够降低开发和维护成本。Library类型的Module分为Static和Shared两种类型，编译后会生成共享包。 Static Library：静态共享库。编译后会生成一个以.har为后缀的文件，即静态共享包HAR（Harmony Archive）。 Shared Library：动态共享库。编译后会生成一个以.hsp为后缀的文件，即动态共享包HSP（Harmony Shared Package）。 **说明：**实际上，Shared Library编译后除了会生成一个.hsp文件，还会生成一个.har文件。这个.har文件中包含了HSP对外导出的接口，应用中的其他模块需要通过.har文件来引用HSP的功能。为了表述方便，我们通常认为Shared Library编译后生成HSP。 HAR与HSP两种共享包的主要区别体现在：\n共享包类型 编译和运行方式 发布和引用方式 HAR HAR中的代码和资源跟随使用方编译，如果有多个使用方，它们的编译产物中会存在多份相同拷贝。\n注意：编译HAR时，建议开启混淆能力，保护代码资产。 HAR除了支持应用内引用，还可以独立打包发布，供其他应用引用。 HSP HSP中的代码和资源可以独立编译，运行时在一个进程中代码也只会存在一份。 HSP一般随应用进行打包，当前支持应用内和集成态HSP。应用内HSP只支持应用内引用，集成态HSP支持发布到ohpm私仓和跨应用引用。 ⠀Stage模型包结构 分别对开发态、编译态、发布态的应用程序结构展开介绍。\n开发态 工程结构主要包含的文件类型及用途如下：\n说明：\nAppScope目录由DevEco Studio自动生成，不可更改。 Module目录名称可以由DevEco Studio自动生成（比如entry、library等），也可以自定义。为了便于说明，下表中统一采用Module_name表示。 文件类型 说明 配置文件 包括应用级配置信息、以及Module级配置信息：\n- AppScope \u0026gt; app.json5：app.json5配置文件，用于声明应用的全局配置信息，比如应用Bundle名称、应用名称、应用图标、应用版本号等。\n- Module_name \u0026gt; src \u0026gt; main \u0026gt; module.json5：module.json5配置文件，用于声明Module基本信息、支持的设备类型、所含的组件信息、运行所需申请的权限等。 ArkTS源码文件 Module_name \u0026gt; src \u0026gt; main \u0026gt; ets：用于存放Module的ArkTS源码文件（.ets文件）。 资源文件 包括应用级资源文件、以及Module级资源文件，支持图形、多媒体、字符串、布局文件等，详见资源分类与访问。\n- AppScope \u0026gt; resources ：用于存放应用需要用到的资源文件。\n- Module_name \u0026gt; src \u0026gt; main \u0026gt; resources ：用于存放该Module需要用到的资源文件。 其他配置文件 用于编译构建，包括构建配置文件、编译构建任务脚本、混淆规则文件、依赖的共享包信息等。\n- build-profile.json5：工程级或Module级的构建配置文件，包括应用签名、产品配置等。\n- hvigorfile.ts：应用级或Module级的编译构建任务脚本，开发者可以自定义编译构建工具版本、控制构建行为的配置参数。\n- obfuscation-rules.txt：混淆规则文件。混淆开启后，在使用Release模式进行编译时，会对代码进行编译、混淆及压缩处理，保护代码资产。\n- oh-package.json5：用于存放依赖库的信息，包括所依赖的三方库和共享包。 编译态 从开发态到编译态，Module中的文件会发生如下变更：\nets目录：ArkTS源码编译生成.abc文件。 resources目录：AppScope目录下的资源文件会合入到Module下面资源目录中，如果两个目录下存在重名文件，编译打包后只会保留AppScope目录下的资源文件。 module配置文件：AppScope目录下的app.json5文件字段会合入到Module下面的module.json5文件之中，编译后生成HAP或HSP最终的module.json文件。 ⠀说明： 在编译HAP和HSP时，会把他们所依赖的HAR直接编译到HAP和HSP中。 发布态 每个应用中至少包含一个.hap文件，可能包含若干个.hsp文件、也可能不含，一个应用中的所有.hap与.hsp文件合在一起称为Bundle，其对应的bundleName是应用的唯一标识（详见app.json5配置文件中的bundleName标签）。 当应用发布上架到应用市场时，需要将Bundle打包为一个.app后缀的文件用于上架，这个.app文件称为App Pack（Application Package），与此同时，DevEco Studio工具自动会生成一个pack.info文件。pack.info文件描述了App Pack中每个HAP和HSP的属性，包含APP中的bundleName和versionCode信息、以及Module中的name、type和abilities等信息。\n说明：\nApp Pack是发布上架到应用市场的基本单元，但是不能在设备上直接安装和运行。 在应用签名、云端分发、端侧安装时，都是以HAP/HSP为单位进行签名、分发和安装的。 包选择 Module类型 包类型 说明 Ability HAP 应用的功能模块，可以独立安装和运行，必须包含一个entry类型的HAP，可选包含一个或多个feature类型的HAP。 Static Library HAR 静态共享包，编译态复用。\n- 支持应用内共享，也可以发布后供其他应用使用。\n- 作为二方库，发布到OHPM私仓，供公司内部其他应用使用。\n- 作为三方库，发布到OHPM中心仓，供其他应用使用。\n- 多包（HAP/HSP）引用相同的HAR时，会造成多包间代码和资源的重复拷贝，从而导致应用包膨大。\n- 注意：编译HAR时，建议开启混淆能力，保护代码资产。 Shared Library HSP 动态共享包，运行时复用。\n- 当多包（HAP/HSP）同时引用同一个共享包时，采用HSP替代HAR，可以避免HAR造成的多包间代码和资源的重复拷贝，从而减小应用包大小。 规格 HAP HAR HSP 支持在配置文件中声明UIAbility组件与ExtensionAbility组件 √ × × 支持在配置文件中声明pages页面 √ × √ 支持在设备上独立安装运行 √ × × 说明：\nHAR虽然不支持在配置文件中声明pages页面，但是可以包含pages页面，并通过命名路由的方式进行跳转。 由于HSP仅支持应用内共享，如果HAR依赖了HSP，则该HAR文件仅支持应用内共享，不支持发布到二方仓或三方仓供其他应用使用，否则会导致编译失败。 HAR和HSP均不支持循环依赖，也不支持依赖传递。 HAP HAP（Harmony Ability Package）是应用安装和运行的基本单元。HAP包是由代码、资源、第三方库、配置文件等打包生成的模块包，其主要分为两种类型：entry和feature。\nentry：应用的主模块，作为应用的入口，提供了应用的基础功能。 feature：应用的动态特性模块，作为应用能力的扩展，可以根据用户的需求和设备类型进行选择性安装。 ⠀应用程序包可以只包含一个基础的entry包，也可以包含一个基础的entry包和多个功能性的feature包。 使用场景 单HAP场景：如果只包含UIAbility组件，无需使用ExtensionAbility组件，优先采用单HAP（即一个entry包）来实现应用开发。虽然一个HAP中可以包含一个或多个UIAbility组件，为了避免不必要的资源加载，推荐采用“一个UIAbility+多个页面”的方式。 多HAP场景：如果应用的功能比较复杂，需要使用ExtensionAbility组件，可以采用多HAP（即一个entry包+多个feature包）来实现应用开发，每个HAP中包含一个UIAbility组件或者一个ExtensionAbility组件。在这种场景下，可能会存在多个HAP引用相同的库文件，导致重复打包的问题。 ⠀约束限制 不支持导出接口和ArkUI组件，给其他模块使用。 多HAP场景下，App Pack包中同一设备类型的所有HAP中必须有且只有一个Entry类型的HAP，Feature类型的HAP可以有一个或者多个，也可以没有。 多HAP场景下，同一应用中的所有HAP的配置文件中的bundleName、versionCode、versionName、minCompatibleVersionCode、debug、minAPIVersion、targetAPIVersion、apiReleaseType相同，同一设备类型的所有HAP对应的moduleName标签必须唯一。HAP打包生成App Pack包时，会对上述参数配置进行校验。 多HAP场景下，同一应用的所有HAP、HSP的签名证书要保持一致。上架应用市场是以App Pack形式上架，应用市场分发时会将所有HAP从App Pack中拆分出来，同时对其中的所有HAP进行重签名，这样保证了所有HAP签名证书的一致性。在调试阶段，开发者通过命令行或DevEco Studio将HAP安装到设备上时，要保证所有HAP签名证书一致，否则会出现安装失败的问题。 应用配置文件 每个应用项目的代码目录下必须包含应用配置文件，这些配置文件会向编译工具、操作系统和应用市场提供应用的基本信息。 在基于Stage模型开发的应用项目代码下，都存在一个app.json5配置文件、以及一个或多个module.json5配置文件。 app.json5配置文件主要包含以下内容：\n应用的全局配置信息，包含应用的Bundle名称、开发厂商、版本号等基本信息。 特定设备类型的配置信息。 ⠀module.json5配置文件主要包含以下内容：\nModule的基本配置信息，包含Module名称、类型、描述、支持的设备类型等基本信息。 应用组件信息，包含UIAbility组件和ExtensionAbility组件的描述信息。 应用运行过程中所需的权限信息。 app.json5 https://docs.openharmony.cn/pages/v5.0/zh-cn/application-dev/quick-start/app-configuration-file.md\nmodule.json5 https://docs.openharmony.cn/pages/v5.0/zh-cn/application-dev/quick-start/module-configuration-file.md\nArkTs OpenAtom OpenHarmony\nSupplement Module 在 OpenHarmony 中，Module（模块）是应用开发中的一个重要概念，用于组织和划分应用的功能单元。它是 OpenHarmony 应用的基本组成部分，每个模块可以独立开发、编译和部署，最终组合成一个完整的应用。\nModule 的核心概念 模块化设计 OpenHarmony 采用模块化设计，允许开发者将应用拆分为多个模块，每个模块负责特定的功能或业务逻辑。 这种设计提高了代码的可维护性和复用性，同时也支持动态加载和按需安装。 模块类型 Entry Module（主模块）：应用的入口模块，包含应用的核心功能和资源，是应用安装和运行的起点。 Feature Module（功能模块）：应用的扩展模块，包含特定的功能或业务逻辑，可以按需下载和安装，支持应用的动态扩展。 模块结构\n每个模块通常包含以下内容： 代码文件：包括 ArkTS、JavaScript、C/C++ 等代码，用于实现模块的功能。 资源文件：包括图片、字符串、布局文件等，用于支持模块的 UI 和功能。 配置文件：module.json 文件，定义了模块的元数据、依赖关系、Ability 信息等。 Module 的配置文件 每个模块都有一个 module.json 文件，用于描述模块的基本信息和配置。其主要内容包括：\n模块名称：模块的唯一标识符。 模块类型：指明模块是 Entry Module 还是 Feature Module。 Ability 信息：定义模块中包含的 Ability（功能单元），如 Page Ability 和 Service Ability。 依赖关系：声明模块依赖的其他模块或库。 资源路径：指定模块的资源文件存放位置。 示例 module.json 文件： { \u0026#34;module\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;entry\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;entry\u0026#34;, \u0026#34;abilities\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;.MainAbility\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;$media:icon\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;$string:app_name\u0026#34;, \u0026#34;launchType\u0026#34;: \u0026#34;standard\u0026#34; } ], \u0026#34;dependencies\u0026#34;: [ { \u0026#34;bundleName\u0026#34;: \u0026#34;com.example.feature\u0026#34;, \u0026#34;moduleName\u0026#34;: \u0026#34;feature\u0026#34; } ] } } Module 的开发与构建 开发工具 使用 DevEco Studio 开发工具，开发者可以创建和管理模块，编写代码和配置文件。 构建与打包 每个模块可以独立编译和打包，生成对应的 HAP（HarmonyOS Ability Package）文件。 最终，多个模块的 HAP 文件组合成一个完整的应用。 动态加载 Feature Module 支持动态加载，可以在应用运行时按需下载和安装，减少初始安装包的大小。 Module 的应用场景 大型应用开发 对于功能复杂的大型应用，可以将应用拆分为多个模块，由不同团队并行开发，提高开发效率。 动态功能扩展 通过 Feature Module，可以实现应用的动态功能扩展，用户可以根据需要下载和安装特定功能。 代码复用 将通用功能封装为独立的模块，可以在多个应用中复用，减少重复开发。 总结 在 OpenHarmony 中，Module 是应用开发的基本单元，支持模块化设计和动态加载。通过将应用拆分为多个模块，开发者可以提高代码的可维护性、复用性和扩展性，同时为用户提供更灵活的应用体验。\n.json5 JSON5 是 JSON（JavaScript Object Notation）的扩展格式，旨在解决 JSON 格式的一些局限性，使其更易于人类阅读和编写。JSON5 在保留 JSON 简单性和兼容性的基础上，增加了一些灵活的语法特性。\nJSON5 的主要改进 支持注释 JSON5 允许在数据中添加单行注释（//）和多行注释（/* ... */），方便开发者添加说明。 { // 这是一个单行注释 \u0026#34;name\u0026#34;: \u0026#34;JSON5\u0026#34;, /* 这是一个 多行注释 */ \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; } 键名可以不加引号 JSON5 允许键名不使用引号（前提是键名是有效的标识符），简化了书写。 { name: \u0026#34;JSON5\u0026#34;, // 键名不加引号 version: \u0026#34;1.0.0\u0026#34; } 支持尾随逗号 JSON5 允许在对象和数组的最后一个元素后添加逗号，避免修改数据时频繁调整格式。 { \u0026#34;name\u0026#34;: \u0026#34;JSON5\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, // 尾随逗号 } 字符串支持多行和单引号 JSON5 允许字符串使用单引号（'）和多行形式（用反引号或换行符），方便处理复杂文本。 { description: \u0026#39;这是一个使用单引号的字符串\u0026#39;, multiLine: `这是一个 多行字符串` } 数字支持更多格式 JSON5 支持十六进制（0x）、浮点数（.）、正负号（+/-）以及特殊值（如Infinity和NaN）。 { hex: 0xFF, // 十六进制 float: 3.14, // 浮点数 positive: +100, // 正数 negative: -50, // 负数 infinity: Infinity, // 无穷大 notANumber: NaN // 非数字 } JSON5 的使用场景 配置文件 JSON5 的注释和灵活语法使其非常适合用于配置文件，开发者可以更清晰地描述配置项。 开发工具 许多开发工具和框架支持 JSON5，例如 Babel、ESLint 等，方便开发者编写和维护配置。 数据交换 虽然 JSON5 的灵活性更适合人类阅读，但在数据交换场景中，仍建议使用标准的 JSON 格式以确保兼容性。 JSON5 的解析与生成 JavaScript 解析 可以使用 json5 库来解析 JSON5 数据。 const JSON5 = require(\u0026#39;json5\u0026#39;); const data = JSON5.parse(\u0026#39;{name: \u0026#34;JSON5\u0026#34;}\u0026#39;); console.log(data.name); // 输出: JSON5 生成 JSON5 可以使用 json5 库将 JavaScript 对象转换为 JSON5 字符串。 const JSON5 = require(\u0026#39;json5\u0026#39;); const obj = {name: \u0026#34;JSON5\u0026#34;, version: \u0026#34;1.0.0\u0026#34;}; const json5Str = JSON5.stringify(obj); console.log(json5Str); // 输出: {name:\u0026#34;JSON5\u0026#34;,version:\u0026#34;1.0.0\u0026#34;} 总结 JSON5 是 JSON 的扩展格式，通过支持注释、无引号键名、尾随逗号、多行字符串等特性，使其更易于人类阅读和编写。它非常适合用于配置文件和开发工具，但在数据交换场景中仍需注意兼容性问题。\n.so .so文件是共享对象文件（Shared Object File）的缩写，是一种在Linux和类Unix系统（包括Android和HarmonyOS）中使用的动态链接库文件格式。它类似于Windows系统中的.dll文件（动态链接库），用于存储可被多个程序共享的代码和资源。\n.so文件的核心特点 动态链接 .so文件在程序运行时被加载，而不是在编译时静态链接到程序中。 这种机制可以减少程序的内存占用，并支持代码的复用和更新。 共享性 多个程序可以同时使用同一个.so文件，避免重复加载相同的代码，节省系统资源。 平台依赖性 .so文件是平台相关的，通常针对特定的CPU架构（如ARM、x86）和操作系统编译，不能跨平台直接使用。 .so文件的使用场景 跨语言调用 在HarmonyOS或Android中，.so文件通常用于实现C/C++代码与Java/ArkTS等高级语言之间的跨语言调用。 例如，通过JNI（Java Native Interface）或Node-API调用.so文件中的函数。 性能优化 对于计算密集型任务（如图像处理、物理模拟等），使用C/C++编写的.so文件可以提供更高的性能。 复用已有代码 开发者可以将已有的C/C++库编译为.so文件，在HarmonyOS或Android应用中复用，减少开发成本。 .so文件的生成 编译工具 使用GCC、Clang等编译器，将C/C++代码编译为.so文件。 在HarmonyOS中，可以使用NDK（Native Development Kit）提供的工具链进行编译。 编译命令示例 gcc -shared -o libexample.so example.c -shared选项表示生成共享库文件。 -o选项指定输出文件名。 .so文件的加载与使用 动态加载 在程序运行时，可以通过dlopen函数动态加载.so文件，并通过dlsym函数获取其中的函数指针。 示例代码 #include \u0026lt;dlfcn.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main() { void *handle = dlopen(\u0026#34;./libexample.so\u0026#34;, RTLD_LAZY); if (!handle) { fprintf(stderr, \u0026#34;%s\\n\u0026#34;, dlerror()); return 1; } void (*func)() = dlsym(handle, \u0026#34;example_function\u0026#34;); if (func) { func(); } dlclose(handle); return 0; } .so文件在HarmonyOS中的应用 在HarmonyOS中，.so文件通常用于：\n实现高性能的底层功能（如图形渲染、音频处理）。 支持跨语言调用（如ArkTS/JS与C/C++之间的交互）。 复用已有的C/C++库，降低开发成本。 总结 .so文件是一种动态链接库文件，广泛用于Linux、Android和HarmonyOS等系统中。它支持代码的共享和动态加载，适用于跨语言调用、性能优化和代码复用等场景。在HarmonyOS开发中，.so文件是实现高性能和跨语言能力的重要工具。\nHAP HAP（HarmonyOS Ability Package）是HarmonyOS应用的基本部署单元，类似于Android中的APK（Android Package）。它是HarmonyOS应用的安装包格式，包含了应用的所有资源、代码和配置文件，用于在HarmonyOS设备上安装和运行应用。\nHAP包的核心组成 Ability Ability是HarmonyOS应用的基本组成单元，代表应用的一个功能模块。 分为两种类型： Page Ability：用于提供用户界面（UI）的Ability，类似于Android中的Activity。 Service Ability：用于在后台执行任务的Ability，类似于Android中的Service。 资源文件 包括图片、字符串、布局文件等，用于支持应用的UI和功能。 资源文件通常存放在resources目录下，支持多语言、多分辨率适配。 配置文件 config.json：应用的全局配置文件，定义了应用的包名、版本号、Ability信息、权限声明等。 module.json：模块级配置文件，定义了模块的依赖关系、资源路径等。 代码文件 包括ArkTS（HarmonyOS推荐的前端开发语言）、JavaScript、C/C++等代码文件，用于实现应用的逻辑功能。 库文件 包括动态库（.so文件）和静态库（.a文件），用于支持应用的底层功能。 HAP包的类型 Entry HAP 主模块包，包含应用的核心功能和资源，是应用安装和运行的入口。 Feature HAP 功能模块包，包含应用的扩展功能，可以按需下载和安装，支持应用的动态扩展。 HAP包的特点 模块化设计 HAP包支持模块化开发，开发者可以将应用拆分为多个HAP包，实现功能的动态加载和按需安装。 分布式能力 HAP包支持HarmonyOS的分布式特性，可以在多个设备上无缝协同运行。 安全性 HAP包通过签名机制确保应用的完整性和安全性，防止篡改和恶意攻击。 HAP包的生成与安装 生成 使用DevEco Studio开发工具，开发者可以编译和打包应用，生成HAP包。 打包过程中会自动处理资源压缩、代码混淆等优化操作。 安装 HAP包可以通过应用市场、ADB命令行或OTA方式安装到HarmonyOS设备上。 安装过程中会进行签名验证和权限检查，确保应用的安全性。 总结 HAP包是HarmonyOS应用的安装包格式，包含了应用的代码、资源和配置文件，支持模块化设计和分布式能力。通过HAP包，开发者可以高效地开发、部署和运行HarmonyOS应用，为用户提供无缝的智能体验。\nhvigor hvigor 是 OpenHarmony 和 HarmonyOS 生态中的一种构建工具，专为 HarmonyOS 应用开发设计。它基于 Gradle 构建系统，但针对 HarmonyOS 的特点进行了优化和扩展，旨在提高开发效率和构建性能。\nhvigor 的核心功能 项目构建 hvigor 支持 HarmonyOS 应用的编译、打包和构建，生成 HAP（HarmonyOS Ability Package）文件。 它能够处理多模块项目的依赖关系，确保构建过程的正确性和高效性。 任务管理 hvigor 提供了丰富的构建任务（如编译、打包、清理等），开发者可以通过命令行或 IDE（如 DevEco Studio）执行这些任务。 支持自定义任务，满足特定项目的构建需求。 依赖管理 hvigor 支持对项目依赖的管理，包括本地依赖和远程依赖。 通过配置文件（如 build-profile.json5），开发者可以声明项目的依赖关系。 插件支持 hvigor 支持插件机制，开发者可以通过插件扩展构建功能。 HarmonyOS 提供了多种官方插件，如代码混淆插件、资源压缩插件等。 性能优化 hvigor 针对 HarmonyOS 应用的构建过程进行了优化，减少了构建时间，提高了构建效率。 支持增量编译，只编译发生变化的代码和资源，进一步提升构建速度。 hvigor 的配置文件 hvigor 使用 JSON5 格式的配置文件来定义项目的构建规则和参数。主要配置文件包括：\nbuild-profile.json5 定义项目的构建配置，如编译选项、依赖关系、插件配置等。\n示例： { \u0026#34;app\u0026#34;: { \u0026#34;signingConfigs\u0026#34;: { \u0026#34;release\u0026#34;: { \u0026#34;storeFile\u0026#34;: \u0026#34;release.keystore\u0026#34;, \u0026#34;storePassword\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;keyAlias\u0026#34;: \u0026#34;key\u0026#34;, \u0026#34;keyPassword\u0026#34;: \u0026#34;password\u0026#34; } }, \u0026#34;buildTypes\u0026#34;: { \u0026#34;release\u0026#34;: { \u0026#34;signingConfig\u0026#34;: \u0026#34;release\u0026#34; } } } } module.json5 定义模块的元数据和配置，如模块名称、Ability 信息、资源路径等。\n示例： { \u0026#34;module\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;entry\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;entry\u0026#34;, \u0026#34;abilities\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;.MainAbility\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;$media:icon\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;$string:app_name\u0026#34;, \u0026#34;launchType\u0026#34;: \u0026#34;standard\u0026#34; } ] } } hvigor 的使用 命令行使用 开发者可以通过命令行执行 hvigor 任务，例如： hvigor clean # 清理构建目录 hvigor build # 构建项目 集成到 DevEco Studio DevEco Studio 内置了 hvigor 支持，开发者可以通过 IDE 的图形界面执行构建任务，查看构建日志和结果。 hvigor 的优势 专为 HarmonyOS 优化 hvigor 针对 HarmonyOS 应用的构建需求进行了深度优化，提供了更好的性能和兼容性。 简化构建流程 通过配置文件和插件机制，hvigor 简化了构建流程，降低了开发者的学习成本。 高效开发体验 支持增量编译和任务并行执行，显著提升了开发效率。 总结 hvigor 是 HarmonyOS 生态中的核心构建工具，基于 Gradle 构建系统并针对 HarmonyOS 进行了优化。它提供了高效的构建能力、灵活的配置选项和丰富的插件支持，帮助开发者快速构建和部署 HarmonyOS 应用。通过 hvigor，开发者可以更专注于应用功能的实现，而无需过多关注构建细节。\n","date":"2025-01-13T00:00:00Z","permalink":"https://dyhes.github.io/p/openharmonybasics/","title":"【OpenHarmony】Basics"},{"content":"The .venv directory is typically a folder that contains a virtual environment for Python projects. A virtual environment is a self-contained directory that holds a specific version of Python and any packages or libraries your project requires. This helps isolate your project’s dependencies from the global Python environment on your system, ensuring that each project has its own environment and dependencies without interfering with other projects. When you execute a Python script in VS Code, if it’s using a .venv directory, it means that the script is running within this virtual environment. The .venv directory is often created by using the python -m venv .venv command or through VS Code’s environment setup. Key points about .venv:\nIt’s not a Python package itself but a directory. It helps in managing dependencies for a specific project. You can activate it using source .venv/bin/activate (Linux/macOS) or .venv\\Scripts\\activate (Windows) in the terminal. ⠀VS Code typically detects virtual environments and allows you to select the appropriate interpreter for your project, ensuring that Python uses the version and packages from the virtual environment rather than the system-wide installation.\n","date":"2025-01-13T00:00:00Z","permalink":"https://dyhes.github.io/p/python.venv/","title":"【Python】.venv"},{"content":"OverView 软件价值观 行为价值:让程序完成指定的功能 大多数程序员认为这是主要工作——完成功能、发现并修正bug 架构价值:让程序保持易于修改 容易被忽视，但这种价值更符合“软件”的存在逻辑 软件之所以存在并发展，就是因为它比硬件“软” 用户的需求在变、股东的方向在变 历史 在20世纪90年代，有一个协调一致的努力来定义和编纂该学科的基本方面，研究工作集中在体系结构风格(模式)、体系结构描述语言、体系结构文档和形式化方法上。 卡内基梅隆大学的Mary Shaw和David Garlan在1996年写了一本名为《软件架构:新兴学科的视角》的书，这本书促进了诸如组件、连接器和风格等软件架构概念。 定义 Wikipedia\nSoftware architecture is the set of structures required for analyzing a software system and the field of creating such structures and systems. Each structure consists of software elements, the relationships between them, as well as the properties of both the elements and the relationships. IEEE 1471-2000 Software architecture is the fundamental organization of a system, embodied in its components, their relationships to each other and the environment, and the principles governing its design and evolution. Programming Paradigms 结构化程序设计 (Structured Programming) 面向对象程序设计 (Object-Oriented Programming) 函数式程序设计 (Functional Programming) 结构化程序设计 Dijkstra是发现程序设计这个学科、并将其发展成一门科学的人之一 对于’不管多简单的程序，只要程序员稍稍忽视一点细节，程序就会看似正确，却以令人诧异的方式出错‘的问题，Dijkstra 的方案：程序的形式化证明\n用欧式几何的思路：公理、引理、定理\u0026hellip;\u0026hellip; 程序员可以用可证明正确的结构，搭建出更大的程序 顺序、分支、循环、调用、递归\u0026hellip;\u0026hellip; 结构化程序设计由此诞生了 结构化程序设计取得了巨大的成功 因为它使得我们能够将任务层层分解（直到无穷） 程序的形式化证明至今未能广泛应用（太累了！） 取而代之的是更“科学”的方法 面向对象程序设计 面向对象\n数据与函数的组合 封装、继承、多态 对于软件架构师来说，面向对象是一种通过使用多态获得的，对系统中每个源代码依赖的绝对控制能力 允许软件架构师创建插件架构，让高级策略模块独立于低级细节模块 低级细节被下放到插件模块中，可以独立于高级策略模块进行部署和开发 函数式程序设计 函数式程序设计是以函数为核心的程序设计范式 所有的并发问题都是由于变量(内存里的值)可变引起的 函数式程序设计中，变量“不变” 将尽量多的功能实现在不可变组件中，尽量少的实现在可变组件中 总结 结构化程序设计 剥夺了程序员使用跳转语句的自由，赋予了软件模块化的能力 面向对象程序设计 剥夺了程序员使用函数指针的自由，赋予了软件插件化的能力 函数式程序设计 剥夺了程序员使用赋值语句的自由，赋予了软件并行化的能力 Principals 设计原则 如何将函数和数据组织为类 将砖拼装为房间 组件原则 如何将类组织为组件和软件 将房间拼装为大楼 设计原则 Single Responsibility Principle 一个类应该只有一个引起它变化的原因。这意味着一个类应该只负责一个功能或职责，并且所有行为都与该职责紧密相关。 Open-Closed Principle 软件实体（类、模块、函数等）应该对扩展开放，对修改关闭。 Liskov Substitution Principle 子类对象应该能够替换其基类对象，并且不破坏程序的正确性。这个原则强调了继承关系中子类对基类行为的一致性。 Interface Segregation Principle 可能被不同模块调用的函数，应抽象为不同的接口 Dependency Inversion Principle 高层模块不应该依赖于低层模块，两者都应该依赖于抽象；抽象不应该依赖于细节，细节应该依赖于抽象。 组件原则 组件聚合原则：哪些类组织到哪个组件？ Reuse/Release Equivalence Principle 重用的粒度，就是发布的粒度 Common Closure Principle 把那些会因为相同原因而同时修改的类组成一个组件，把那些会因为不同原因而分别修改的类组分隔为不同组件。 组件层次的单一责任原则SRP。 可维护 \u0026gt; 可重用。 一次修改最好集中在一个组件，不希望横跨多个组件。 修改一个组件，不相关的组件无需重新编译部署。 Common Reuse Principle 不要强迫组件的用户依赖它不需要的东西(类、接口等) 组件层次的接口隔离原则ISP 组件耦合原则：组件之间如何交互？ Acyclic Dependencies Principle 组件依赖图中不能有环 如何消除环？ 使用依赖反转原则DIP 加入新组件管理权限 系统不是被自顶向下设计出来的，而是随着系统增长被演化出来的 Stable Dependencies Principle 依赖方向应从不稳定到稳定 不应让不稳定的组件被难于修改的组件依赖 不稳定性 I=Degree_out/(Degree_out + Degree_in) 依赖方向为𝐼值降序方向 Stable Abstractions Principle 组件的稳定性应与它的抽象程度对应 越抽象越稳定，越稳定越抽象 依赖方向应该为具体依赖抽象 抽象度 A=N_abstract_class/N_class 组件应尽量接近“主序线”: 𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒= |𝐴+𝐼-1| 组件历史 最初，程序所在的位置是固定的，后来，应用越来越大，编译时间越来越长，函数库诞生了。 由于位置是固定的，应用被限制在了函数库之前的位置，一旦超出预留的大小，应用程序就需要被切分。 人们终于意识到，程序需要能够重定位。如何实现程序的重定位？ 加载顺序 动态链接 可单独编译、可独立发布、可动态加载、可动态链接的软件模块 ——组件 组件风格 组件（Component）\n一组函数和数据的封装 组件间通过接口沟通 组件可被同样接口的组件替换 组件被充分测试，足够鲁棒 组件有足够的文档说明 组件风格 组件风格关注可复用的组件。 实际制造满足需要的组件非常困难。 组件模型为制造组件提供模板规范。 组件模型 Enterprise JavaBeans (EJB) 模型 Component Object Model (COM) 模型 .NET模型 X-MAN组件模型 Common Object Request Broker Architecture (CORBA) 组件模型 管道过滤器风格 过滤器 每个组件都有其输入和输出 组件的处理过程就是根据输入产生输出 管道 连接两个组件，将一个组件的输出变成另一个组件的输入 过滤器通过管道连接为有向无环图，形成复杂功能 对比： 组件风格关注组件提供/需要的接口 管道-过滤器风格强调数据流的衔接 优势 弱耦合性：各个过滤器相互独立，设计者可以将整个系统的输入、输出特性理解为各个过滤器功能的简单合成。 易于重用：任意两个过滤器只要相互间传输的数据格式一致，就可以连接在一起。 易于维护扩展：新过滤器可以很容易加入到系统中，旧的过滤器可以很容易被新的过滤器替代。 易于分析测试：每个过滤器可以独立测试。 天然并发性：每个过滤器都可以独立运行，形成流水线pipeline。 缺点 交互性差：由于过滤器的传输特性，管道过滤器模式通常不适合于交互性很强的应用。尤其是在系统需要逐步显示数据流变化的过程时，因为增量显示和过滤器的输出数据差距太大。 维持通信困难：维持两个相对独立但存在某种关系的数据流之间的通信可能很困难。 通信效率低：设计者也需要在数据传输时被迫使用底层公共命名，导致过滤器必须对输入、输出管道中的数据进行解析或反解析的额外工作。 总结 管道-过滤器风格关注可连接的组件 将组件功能抽象为从输入数据到输出数据的过滤过程 通过管道，连接过滤器，形成复杂的数据处理功能 天然对分布式并发特性具有良好适应 管道中的传输过程有待改进 分层风格 优势 分层符合人类“分而治之”的思维方式。 结构化程序设计之所以能够取得巨大的成功，正是因为它使得我们能够将任务层层分解。 耦合性低。 重用性高。 可维护性高。 无环依赖原则（Acyclic Dependencies Principle，ADP） 组件依赖图中不能有环。 有向无环图一定能分层。 总结 分层风格关注组件间的偏序关系 将系统功能层层分解，降低耦合性，提升重用性 最常见的分层风格是三层架构 层与层之间隔离有利于快速部署 层与层之间协同有利于性能效率 事件驱动风格 以事件衔接程序之间的数据流 程序产生事件，触发其他程序的功能 事件驱动风格可以认为是广义的管道-过滤器风格 优缺点与管道-过滤器风格类似 事件驱动风格的程序框架通常是提供事件等待/触发机制的中间件 共享数据风格 黑板风格 黑板模块：全局存储空间 知识源模块：处理知识、更新黑板 控制模块：调度可用的知识源模块 总结 黑板风格：与其移动数据，不如共享数据！ 数据中心风格：与其移动数据，不如移动程序！ 客户端服务器风格 服务器：拥有资源，提供服务 客户端：建立会话，提出请求 总结 天然的两层分层风格 如何将三层风格变为两层风格？根据通信量切分 问题：随着资源规模和请求吞吐量需求增大，服务器成为瓶颈 解决方案 端到端风格 端-边-云架构 端到端风格 Peer-to-Peer, P2P\n去中心：所有参与者在网络中是平等的 每个参与者都提供一部分资源（计算、存储、带宽等） 有效解决了客户端-服务器风格中，服务器的资源瓶颈 核心优势 全局内容发现 高效的内容索引 端到端架构将参与者重新组织为逻辑网络\n非结构化网络 Gnutella、Gossip、Kazaa\u0026hellip;\u0026hellip; 易于组织，支持参与者随时加入离开 难以发现内容 结构化网络 分布式哈希表DHT 每个参与者、内容均与键值联系 易于全局内容发现，但存在均衡问题 面向服务风格 将软件分解为多个独立的服务 服务：独立的功能单元，可通过通信协议与之进行远程沟通 代表可重复的业务活动 具有相对完整的功能 对用户是黑盒，用户无需关心服务的内部实现 可以调用其他服务完成功能 对比 与组件风格对比 在组件风格中，通常支持多个组件在同一个进程中 服务更强调独立性，通信协议均为网络协议（支持跨进程通信） 与管道-过滤器风格对比 过滤器之间通过管道传递消息，不关心消息被用来做什么了 服务接受的是请求，并须要针对请求给出答复 与事件驱动风格也有类似的区别 与分层风格对比 只要服务与服务之间的“调用”关系不存在“递归”，就也属于分层风格 万一服务之间有递归呢？ 与客户端-服务器风格对比 客户端-服务器风格中，通常服务器是整个系统中的单个中心 面向服务风格中，服务可以有很多 近亲 糅合（Mashup） 将两种以上使用公共或者私有数据库的Web应用加在一起，形成一个整合应用 软件即服务（SaaS，Software as a Service） 让用户能够通过互联网连接来使用基于云的应用程序 云计算（Cloud Computing） 将来自用户的任务请求分解成数个小程序，利用网络“云”上多部服务器的资源进行并行处理，得到结果并返回给用户 组成 服务提供者 向服务中间人注册自己提供的服务，答复服务请求者的请求 服务请求者 要求服务中间人查找所需服务，向服务提供者提出请求 服务中间人（服务注册、服务检索） 注册服务提供者，给服务请求者反馈服务提供者信息 可以分为公有中间人、私有中间人 无共享风格 Shared-Nothing Architecture\n分布式系统风格 每个更新操作只由集群中的一个结点来完成 与Shared Everything相对，旨在消除数据竞争 数据库两大类应用 联机事务处理（OLTP，On-Line Transaction Processing） 联机分析处理（OLAP，On-Line Analytical Processing） 新兴数据库 特点 容量需求大 数据增加多 数据更新少 查询效率尽量快 数据仓库（Data Warehouse） 数据湖（Data Lake） ","date":"2025-01-13T00:00:00Z","permalink":"https://dyhes.github.io/p/software-architecturenotes/","title":"【Software Architecture】Notes"},{"content":"past 完成毕设 实习 完成课程 完成论文 now Postopia 博客 看书 reddit, twitter, BBC News, Tech Church, youtube future 英语口语 完成全部课程 准备实习 学习 CUDA 力扣 ","date":"2025-01-13T00:00:00Z","permalink":"https://dyhes.github.io/p/summary2024/","title":"【Summary】2024"},{"content":"使用的对象存储使用的是 Cloudinary 上传图片时文档不完善,通过将图像转为data url解决\n感觉突然像是卡住了一样，进入了倦怠期，也许是因为某个问题遇到了两三次还是没记住，让我有些灰心了（10.21）\n进度：\nupdate avatar\nget user info\nshow email\nget user\u0026rsquo;s posts and comments\nDependency Injection The recommended way of dependency injection in Spring Boot is constructor injection. Here\u0026rsquo;s why it\u0026rsquo;s preferred and how to implement it:\nConstructor Injection: This is the most recommended approach. Here\u0026rsquo;s how it works:\n@Service public class UserService { private final UserRepository userRepository; private final EmailService emailService; @Autowired // Optional in newer Spring versions public UserService(UserRepository userRepository, EmailService emailService) { this.userRepository = userRepository; this.emailService = emailService; } // Service methods... } Advantages:\nPromotes immutability (dependencies can be final) Makes dependencies explicit Ensures the bean is fully initialized after construction Easier to test (you can easily provide mock dependencies) Prevents circular dependencies at compile-time Setter Injection: While not as favored as constructor injection, setter injection can be useful in certain scenarios, particularly when dealing with optional dependencies:\n@Service public class UserService { private UserRepository userRepository; @Autowired public void setUserRepository(UserRepository userRepository) { this.userRepository = userRepository; } // Service methods... } Setter injection is less common but can be useful for optional dependencies or when you need to change dependencies at runtime.\nField Injection (Not Recommended): As discussed earlier, this method is generally discouraged:\n@Service public class UserService { @Autowired private UserRepository userRepository; // Service methods... } Best Practices:\nUse constructor injection for required dependencies. Consider setter injection for optional dependencies. Avoid field injection in most cases. Use the @Autowired annotation on constructors (optional in newer Spring versions). Keep your classes focused and try to limit the number of dependencies. Would you like me to elaborate on any specific aspect of dependency injection or provide a more complex example?\nField Injection (not recommended) Field injection in Spring Boot is generally not recommended for several reasons:\nImmutability: Field injection makes it difficult to create immutable objects, as the fields need to be non-final to allow injection.\nTestability: Classes using field injection are harder to unit test, as you can\u0026rsquo;t easily inject mock dependencies without using a Spring container or reflection.\nDependency hiding: Field injection can hide dependencies, making it less obvious what a class requires to function properly.\nCircular dependencies: Field injection can more easily lead to circular dependency issues, which are harder to detect and resolve.\nInversion of Control (IoC) principle: Field injection violates the IoC principle to some extent, as the class is directly asking for its dependencies rather than having them provided.\nReflection use: Spring uses reflection to inject dependencies into private fields, which can have a minor performance impact and bypasses encapsulation.\nInstead of field injection, constructor injection is typically recommended as it addresses these issues. It allows for immutable objects, makes dependencies explicit, and facilitates easier testing and better design.\nWould you like me to elaborate on any of these points or provide an example of constructor injection?\nUser Id Exposure Exposing a backend user ID in frontend code or APIs is generally not a good practice for several reasons, especially in public-facing applications. Here\u0026rsquo;s why:\nSecurity Risks Enumeration Attacks: If user IDs are predictable (like auto-incrementing integers), attackers can guess and try to access other users\u0026rsquo; data, leading to potential data breaches. User Impersonation: Exposing a backend ID might allow an attacker to impersonate another user if your application doesn\u0026rsquo;t properly authenticate requests. Privacy Concerns Sensitive Information: In some cases, a user ID may be linked to personal data. Exposing these identifiers could lead to privacy violations, depending on the nature of the application. Business Logic Exposure Insights into Structure: If your backend user IDs reveal too much about your data structure, an attacker could gain insights into how your system is designed, which may make it easier to exploit vulnerabilities. Alternatives and Best Practices Use UUIDs or Random Identifiers: Instead of exposing sequential user IDs, consider using universally unique identifiers (UUIDs) or other random tokens that are harder to predict and manipulate. Tokenization: Use session tokens or API tokens that map to a user ID on the backend but don\u0026rsquo;t directly expose the ID itself. Limit Information: Only expose what is absolutely necessary for the frontend or API consumer. If there\u0026rsquo;s no need to show the user ID, don\u0026rsquo;t include it. Access Control: Ensure proper access control and authentication mechanisms so that even if a user ID is exposed, unauthorized users cannot exploit it. In summary, it’s a best practice to avoid exposing backend user IDs unless necessary and only if adequate security measures are in place to mitigate risks.\n","date":"2024-10-11T00:00:00Z","permalink":"https://dyhes.github.io/p/postopia-dev-logweek-6/","title":"【Postopia Dev Log】Week 6"},{"content":"Base64 Encoding Base64 is a binary-to-text encoding scheme that converts binary data into a set of 64 characters. Here\u0026rsquo;s a brief overview:\nPurpose: It\u0026rsquo;s used to represent binary data using a limited set of printable ASCII characters, making it safer to transmit or store data in environments that only support text.\nCharacter set: Base64 uses A-Z, a-z, 0-9, and typically \u0026lsquo;+\u0026rsquo; and \u0026lsquo;/\u0026rsquo; (with \u0026lsquo;=\u0026rsquo; for padding).\nProcess:\nTakes 3 bytes (24 bits) of binary data Divides it into 4 groups of 6 bits each Converts each group to its corresponding Base64 character Common uses:\nEncoding binary data in email attachments Storing complex data in JSON Embedding image data in CSS or HTML Advantages:\nEnsures data integrity during transfer Allows binary data to be treated as text Disadvantages:\nIncreases data size by about 33% (3byte to 4byte) Not designed for encryption (it\u0026rsquo;s an encoding, not encryption) Would you like me to elaborate on any specific aspect of Base64 encoding?\nWhy it is preferred ASCII compatibility:\nASCII defines 128 characters, but only 95 of them are printable. Base64 uses 64 characters, which are all printable and common across different systems. 2^6 efficiency:\n64 is 2^6, which aligns perfectly with binary data (multiples of 8 bits 6x4=3x8). This allows efficient conversion between binary and Base64 without wasting bits. Avoiding special characters:\nBase64 mostly uses alphanumeric characters, which are safe in most contexts. Higher bases would require more special characters, increasing the risk of conflicts. URL and filename safety:\nThe standard Base64 character set is mostly safe for URLs and filenames. Minor variants (like Base64URL) make it completely safe for these uses. Historical reasons:\nBase64 emerged when 6-bit character codes were common in early computing. It became a standard and has remained widely supported. Tradeoffs with higher bases:\nBase128 would require 7 bits per character, not aligning well with 8-bit bytes. It would also require using many more special characters or non-printable ASCII characters. Lower bases comparison:\nBase32 exists but increases size more than Base64. Base16 (hexadecimal) is simple but doubles the data size. While Base64 does increase data size, it strikes a balance between efficiency, compatibility, and safety that has made it a lasting standard in data encoding.\nComparison With Base64URL Base64 and Base64URL (also known as URL-safe Base64) are very similar encoding schemes, but they have a few key differences designed to make Base64URL safe for use in URLs and filenames. Here\u0026rsquo;s a comparison:\nCharacter set:\nBase64: Uses A-Z, a-z, 0-9, +, and / Base64URL: Uses A-Z, a-z, 0-9, -, and _ Padding:\nBase64: Uses = for padding Base64URL: Often omits padding, though some implementations may keep it Main differences:\nBase64URL replaces + with - Base64URL replaces / with _ Base64URL often omits the = padding Purpose:\nBase64: General-purpose encoding for binary data Base64URL: Specifically designed for use in URLs and filenames URL safety:\nBase64: Can cause issues in URLs due to +, /, and = characters Base64URL: Safe for use in URLs without escaping Example: Original: Hello World! Base64: SGVsbG8gV29ybGQh Base64URL: SGVsbG8gV29ybGQh (in this case, they\u0026rsquo;re the same)\nOriginal: Subject? Base64: U3ViamVjdD8= Base64URL: U3ViamVjdD8 (note: padding removed)\nCompatibility:\nMost Base64 decoders can handle Base64URL input The reverse is not always true Usage contexts:\nBase64: Email attachments, general data encoding Base64URL: URL parameters, JWT tokens, filenames Conversion: To convert Base64 to Base64URL:\nReplace + with - Replace / with _ Remove = padding (optional, depending on implementation) While the differences are small, they\u0026rsquo;re crucial for ensuring that encoded data can be safely used in URL contexts without requiring additional escaping or causing parsing issues. In most other respects, the two encodings function identically.\nData URL Data URLs are a way to embed small files directly into web pages or CSS, and Base64 encoding plays a crucial role in this. Here\u0026rsquo;s an overview of Base64 usage in data URLs:\nPurpose:\nData URLs allow embedding file data directly in web pages or stylesheets. They eliminate the need for separate HTTP requests for small resources. Structure:\nFormat: data:[\u0026lt;media type\u0026gt;][;base64],\u0026lt;encoded data\u0026gt; Example: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAACklEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg== Base64 role:\nBinary data (like images) is encoded to Base64 to fit in the URL-safe text format. This allows binary content to be included directly in HTML or CSS. Common uses:\nSmall images (icons, logos) Custom fonts Small audio or video files Any small file where reducing HTTP requests is beneficial Advantages:\nReduces HTTP requests, potentially improving load times Useful for resources that change frequently Simplifies deployment by bundling resources with code Disadvantages:\nIncreases file size due to Base64 encoding overhead Not cached separately by the browser Can make HTML or CSS files larger and harder to read Best practices:\nGenerally recommended for very small files (usually \u0026lt; 1-2 KB) For larger files, conventional URLs are usually more efficient Example usage in HTML:\n\u0026lt;img src=\u0026#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAACklEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==\u0026#34; alt=\u0026#34;Small dot\u0026#34;\u0026gt; Example usage in CSS:\n.icon { background-image: url(\u0026#39;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAACklEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==\u0026#39;); } Data URLs with Base64 encoding provide a way to embed resources directly in web documents, offering a trade-off between reducing HTTP requests and increasing document size. They\u0026rsquo;re particularly useful for small, frequently changing resources in web development.\nWhy not Base64URL Data URLs are already part of a URL structure, so URL-safety is not a concern. The data portion is treated as an opaque blob, not as part of the URL syntax. ","date":"2024-09-30T00:00:00Z","permalink":"https://dyhes.github.io/p/base64encoding/","title":"【Base64】Encoding"},{"content":"如何处理以前没有遇到过的新需求（如给用户发送邮件），需要积累解决问题的能力（实习的时候可以边问边查，做自己的项目只能自己去查，那么如何高效检索并善用AI）,并形成方法论，以便提高能力\n发现 Claude 之前教的在Service Layer 中使用 @AuthenticatedPrincipal 的方法不正确，当时没有及时测试导致现在有若干地方需要修改！！！使用但不要盲目相信\nidea无法热加载， 根据\nPerperlexity\nIn IntelliJ IDEA, ensure that the project is set to build automatically. Go to File -\u0026gt; Settings -\u0026gt; Build, Execution, Deployment -\u0026gt; Compiler, and check the option Build project automatically. This setting allows the IDE to compile changes without needing to manually trigger a build each time\n以及\nSpring Boot DevTools Live Reload Not Working\n两者结合解决\n进度：\nchange user nickname\nbind email\n本周写论文(实验室横向)，无其他进展\nto do:\nquery comment (recursive query, complex)\nSpace in application.properties value Leading and trailing spaces: By default, leading and trailing spaces in property values are trimmed. If you need to preserve them, you can use quotation marks.\nSpaces within the value: Spaces within the value are preserved normally.\nEscaping: If you want to be explicit about including spaces, you can escape them with a backslash.\nHere are some examples:\n# Space in the middle (works fine) app.name=My Application # Preserving leading/trailing spaces with quotes app.description=\u0026#34; This is a description with spaces \u0026#34; # Escaping spaces (not necessary, but valid) app.path=C:\\Program\\ Files\\My\\ App # Multiline values app.multiline=This is a long \\ property value that \\ spans multiple lines If you\u0026rsquo;re having issues with spaces in your application.properties file, you might want to double-check how you\u0026rsquo;re reading the properties in your application. Some methods of reading properties may handle spaces differently.\nStore Credentials in spring boot Basic approach (not recommended for production): database.username=myuser database.password=mypassword Using environment variables: Instead of hardcoding credentials, you can reference environment variables: database.username=${DB_USERNAME} database.password=${DB_PASSWORD} Then set these environment variables on your system or in your deployment environment.\nUsing Spring Boot\u0026rsquo;s config server: For more secure credential management, especially in production, consider using Spring Cloud Config Server.\nEncryption: Spring Boot supports property encryption. You can use tools like Jasypt to encrypt sensitive properties:\ndatabase.username=ENC(encrypted_username) database.password=ENC(encrypted_password) Externalized configuration: Store sensitive data in external files and reference them: spring.config.import=file:/path/to/secure/credentials.properties For production environments, I recommend using a secrets management system like HashiCorp Vault, AWS Secrets Manager, or Azure Key Vault. These provide more secure ways to manage and access sensitive information.\nWould you like me to elaborate on any of these approaches or provide a more detailed example of implementing one of them in a Spring Boot application?\n.env in spring boot Using environment variables in a Spring Boot application is a good practice for managing sensitive information like credentials. There are a few ways to work with environment variables, and creating a .env file is one approach, though it requires some additional setup. Let\u0026rsquo;s go through the process:\nUsing system environment variables:\nThis is the most straightforward method. You set the variables in your operating system or deployment environment, and Spring Boot can access them directly.\nFor example, you might set variables like this:\nexport DB_USERNAME=myuser export DB_PASSWORD=mypassword Then in your application.properties:\nspring.datasource.username=${DB_USERNAME} spring.datasource.password=${DB_PASSWORD} Using a .env file:\nWhile Spring Boot doesn\u0026rsquo;t natively support .env files, you can use a library like dotenv-java to load variables from a .env file. Here\u0026rsquo;s how:\n// Add this dependency to your pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.cdimascio\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dotenv-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; // In your main application class import io.github.cdimascio.dotenv.Dotenv; @SpringBootApplication public class MyApplication { public static void main(String[] args) { Dotenv dotenv = Dotenv.load(); System.setProperty(\u0026#34;DB_USERNAME\u0026#34;, dotenv.get(\u0026#34;DB_USERNAME\u0026#34;)); System.setProperty(\u0026#34;DB_PASSWORD\u0026#34;, dotenv.get(\u0026#34;DB_PASSWORD\u0026#34;)); SpringApplication.run(MyApplication.class, args); } } // Create a .env file in your project root DB_USERNAME=myuser DB_PASSWORD=mypassword // In your application.properties spring.datasource.username=${DB_USERNAME} spring.datasource.password=${DB_PASSWORD} ​\tRemember to add .env to your .gitignore file to prevent committing sensitive information to version control.\nCommand-line arguments: You can also pass environment variables as command-line arguments when running your application:\njava -jar myapp.jar --spring.datasource.username=myuser --spring.datasource.password=mypassword Remember, while using a .env file can be convenient for development, it\u0026rsquo;s generally not recommended for production environments. In production, it\u0026rsquo;s better to use system environment variables, secret management systems, or cloud provider-specific solutions.\nOne Time Password @PathVariable To use path parameters with a POST mapping in Spring Boot, you can use the @PathVariable annotation along with @PostMapping. Here\u0026rsquo;s a brief example:\n@RestController @RequestMapping(\u0026#34;/api\u0026#34;) public class MyController { @PostMapping(\u0026#34;/users/{id}\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; createUser(@PathVariable Long id, @RequestBody User user) { // Process the user creation with the given id return ResponseEntity.ok(\u0026#34;User created with ID: \u0026#34; + id); } } In this example:\nThe @PostMapping annotation defines the endpoint for POST requests. The {id} in the path is a path variable. The @PathVariable Long id parameter in the method signature captures the value of the {id} path variable. You can also include a request body using @RequestBody if needed. To call this endpoint, you would send a POST request to a URL like /api/users/123, where 123 is the ID.\nWould you like me to explain any part of this in more detail or show you how to handle multiple path variables?\nReferences Guide to Spring Email\n","date":"2024-09-30T00:00:00Z","permalink":"https://dyhes.github.io/p/postopia-dev-logweek-5/","title":"【Postopia Dev Log】Week 5"},{"content":"Perperlexity\nIn IntelliJ IDEA, ensure that the project is set to build automatically. Go to File -\u0026gt; Settings -\u0026gt; Build, Execution, Deployment -\u0026gt; Compiler, and check the option Build project automatically. This setting allows the IDE to compile changes without needing to manually trigger a build each time\n以及\n","date":"2024-09-23T00:00:00Z","permalink":"https://dyhes.github.io/p/idealive-reload/","title":"【IDEA】live reload"},{"content":"String[] parts = str.split(\u0026#34;\\\\.\u0026#34;); 而非\nString[] parts = str.split(\u0026#34;\\.\u0026#34;); ","date":"2024-09-23T00:00:00Z","permalink":"https://dyhes.github.io/p/javastring-split/","title":"【Java】String split"},{"content":"If you\u0026rsquo;re specifically looking for number sequence-based authentication codes (like OTPs, often consisting of 6 digits), the following options are most suitable:\n1. TOTP (Time-based One-Time Password) using twilio-otp or java-totp TOTP (Time-based One-Time Password) generates numeric sequences that change periodically (e.g., every 30 seconds). It is a commonly used mechanism for two-factor authentication (2FA), and it generates a sequence of numbers, typically 6 digits.\nMaven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.warrenstrange\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;googleauth\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Example usage (with java-totp):\nimport com.warrenstrange.googleauth.GoogleAuthenticator; GoogleAuthenticator gAuth = new GoogleAuthenticator(); int otp = gAuth.getTotpPassword(\u0026#34;your-secret-key\u0026#34;); This will generate a 6-digit numeric OTP based on a time-window.\n2. HOTP (HMAC-based One-Time Password) using commons-codec HOTP generates numeric sequences based on a counter (not time-based like TOTP). It is also a number-based sequence and typically results in a 6-digit or longer OTP.\nMaven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-codec\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-codec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.15\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Example usage:\nimport org.apache.commons.codec.digest.HmacUtils; import org.apache.commons.codec.binary.Base32; byte[] key = new Base32().decode(\u0026#34;your-secret-key\u0026#34;); long counter = 12345L; // A counter you maintain int otp = HmacUtils.hmacSha1(key, counter); This approach generates a numeric sequence based on a secret key and counter value.\n3. Custom Number Generation using SecureRandom If you just need a random sequence of numbers (e.g., for simple email verification codes), you can use SecureRandom to generate a numeric sequence.\nExample usage: import java.security.SecureRandom; SecureRandom random = new SecureRandom(); int otp = 100000 + random.nextInt(900000); // Generates a 6-digit number This will generate a 6-digit numeric OTP, which is simple and effective for most email verification needs. Summary: TOTP and HOTP are ideal if you need more secure, standardized OTPs with time-based or counter-based mechanisms (usually for 2FA). Custom random number generation using SecureRandom is the simplest method for generating a numeric sequence, typically for less security-critical scenarios like email verification. If you need a secure numeric OTP, TOTP or HOTP is the better choice, especially for systems requiring re-verification over time or multiple verification attempts.\n","date":"2024-09-23T00:00:00Z","permalink":"https://dyhes.github.io/p/optsolutions/","title":"【OPT】Solutions"},{"content":"dependency \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; application.properties spring.data.redis.host=localhost spring.data.redis.port=6379 compose.yaml redis: # 与 ’redis:latest\u0026#39; subtle differences image: redis:latest container_name: postopia-redis ports: - \u0026#34;6379:6379\u0026#34; Config @Configuration public class RedisConfig { @Bean RedisConnectionFactory redisConnectionFactory() { return new LettuceConnectionFactory(); } @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory connectionFactory) { RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); template.setConnectionFactory(connectionFactory); // Use String serializers for keys template.setKeySerializer(new StringRedisSerializer()); template.setValueSerializer(new StringRedisSerializer()); return template; } } Service @Service public class RedisService { @Autowired private RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate; public void setByMinute(String key, Object value, int minute) { redisTemplate.opsForValue().set(key, value, minute, TimeUnit.MINUTES); } public String get(String key) { return (String) redisTemplate.opsForValue().get(key); } public void delete(String key) { redisTemplate.delete(key); } } ","date":"2024-09-23T00:00:00Z","permalink":"https://dyhes.github.io/p/redisin-spring-boot/","title":"【Redis】In Spring Boot"},{"content":"之前都是在mac上进行开发，现在换到Windows上，跑不起来\n原来是因为自定义query方法里的字段设置不对（应与entity中字段而非表字段保持一致 ）\n之前在mac上用code跑项目，运行时并没有查出这个错误，现在在win上用idea跑检查出来了\n经过接近一个月（生活琐事与开学前的休息）再重新打开项目，忘了该从哪里继续项目，以后要注意记录开发进度\n进度：\nlike, dislike post, comment\nquery like status in post\nupdate user nickname\nto do:\nquery comment (recursive query, complex)\nspring boot application configure port resource/application.properties\nserver.port=8081 @AuthorizedPrincipal related i defined \u0026ldquo;void likePost(Long id, @AuthenticationPrincipal User user);\u0026rdquo; in an interface, should its implementation method signature be \u0026ldquo;void likePost(Long id, @AuthenticationPrincipal User user)\u0026rdquo; or \u0026ldquo;void likePost(Long id, User user);\u0026rdquo; is enough?\nThe implementation method signature should match the interface declaration exactly, including any annotations.\nThe @AuthenticationPrincipal annotation is significant and should be retained in the implementation for the following reasons:\nConsistency: The implementation should match the interface definition to maintain consistency and avoid confusion. Functionality: The @AuthenticationPrincipal annotation is typically used in Spring Security to automatically resolve the currently authenticated user. Removing it might affect how the user object is populated or handled by the framework. Clarity: Keeping the annotation in the implementation makes it clear to other developers that this method expects the authenticated user to be injected. idea auto implemenation doesn\u0026rsquo;t retain the @AuthenticationPrincipal annotation\nYou\u0026rsquo;re right to notice that discrepancy. IDEs like IntelliJ IDEA sometimes don\u0026rsquo;t automatically retain certain annotations when generating method implementations. This is a known limitation in some IDE auto-implementation features.\nUnderstand the issue: The IDE\u0026rsquo;s auto-implementation feature is dropping the @AuthenticationPrincipal annotation, which is not the desired behavior.\nCorrect approach: You should manually add the annotation back to the implementation method.\nCASE in sql Case statements in SQL allow you to add conditional logic to your queries. They\u0026rsquo;re useful for categorizing data or performing different calculations based on specific conditions.\nThe general syntax is:\nCASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 ... ELSE default_result END Key points:\nYou can have multiple WHEN-THEN pairs. The ELSE clause is optional. If omitted and no conditions are met, it returns NULL. CASE statements can be used in SELECT, WHERE, and ORDER BY clauses. Here\u0026rsquo;s a simple example:\nSELECT product_name, price, CASE WHEN price \u0026lt; 20 THEN \u0026#39;Low\u0026#39; WHEN price \u0026gt;= 20 AND price \u0026lt; 50 THEN \u0026#39;Medium\u0026#39; ELSE \u0026#39;High\u0026#39; END AS price_category FROM products; This query categorizes products based on their price.\nJOIN and LEFT JOIN JOIN and LEFT JOIN are not the same. They are different types of SQL join operations that produce different results. Let me explain the key differences:\nJOIN (or INNER JOIN):\nReturns only the rows that have matching values in both tables. If a row doesn\u0026rsquo;t have a match in the other table, it\u0026rsquo;s excluded from the result set. LEFT JOIN (or LEFT OUTER JOIN):\nReturns all rows from the left table, and the matched rows from the right table. If there\u0026rsquo;s no match in the right table, the result still contains the row from the left table, with NULL values for the right table\u0026rsquo;s columns. Here\u0026rsquo;s an example to illustrate:\nSuppose we have two tables:\nEmployees: Projects: ID Name ID ProjectName EmployeeID -- ---- -- ----------- ---------- 1 Alice 1 Alpha 1 2 Bob 2 Beta 1 3 Charlie 3 Gamma 2 4 Delta 4 Using INNER JOIN:\nSELECT e.Name, p.ProjectName FROM Employees e JOIN Projects p ON e.ID = p.EmployeeID; Result:\nName ProjectName ---- ----------- Alice Alpha Alice Beta Bob Gamma Using LEFT JOIN:\nSELECT e.Name, p.ProjectName FROM Employees e LEFT JOIN Projects p ON e.ID = p.EmployeeID; Result:\nName ProjectName ---- ----------- Alice Alpha Alice Beta Bob Gamma Charlie NULL Key differences in the results:\nINNER JOIN doesn\u0026rsquo;t include Charlie because he has no matching project. LEFT JOIN includes Charlie with a NULL project. Neither join includes the Delta project because it has no matching employee. In summary, use JOIN when you only want results with matches in both tables, and use LEFT JOIN when you want all rows from the left table, regardless of whether there\u0026rsquo;s a match in the right table.\nWould you like me to elaborate on any part of this explanation or provide more examples?\nextra query for user opinions will using an extra query to get user opinion information rather than querying it by above method be more efficient?\nThat\u0026rsquo;s an excellent question. The efficiency of using a single query versus multiple queries depends on several factors. Let\u0026rsquo;s analyze both approaches:\nSingle Query Approach (current method):\nPros: Reduces the number of database round trips Can be more efficient for smaller result sets Cons: More complex query Potential for larger result set due to the LEFT JOIN May be slower for large datasets due to the join operation Separate Queries Approach:\nPros: Simpler, more focused queries Can be more efficient for large datasets Allows for more granular caching strategies Cons: Multiple database round trips Potential for N+1 query problem if not implemented carefully To determine which approach is more efficient, consider the following factors:\nDataset size: For smaller datasets, the single query approach might be faster. For larger datasets, separate queries could perform better.\nQuery complexity: If the join operation significantly complicates the query or slows down execution, separate queries might be preferable.\nCaching strategies: Separate queries allow for more granular caching of post information and user opinions.\nNetwork latency: If network latency between your application and database is high, fewer round trips (single query) might be beneficial.\nDatabase indexes: Proper indexing can significantly impact the performance of both approaches.\nGiven these considerations, here\u0026rsquo;s a potential implementation using separate queries:\n@Service public class PostService { private final PostRepository postRepository; private final OpinionRepository opinionRepository; // Constructor injection... public Page\u0026lt;PostInfo\u0026gt; getPostsInSpace(Long spaceId, Long userId, int page, int size) { Pageable pageable = PageRequest.of(page, size); Page\u0026lt;PostInfo\u0026gt; posts = postRepository.findPostsInSpace(spaceId, pageable); List\u0026lt;Long\u0026gt; postIds = posts.getContent().stream() .map(PostInfo::getId) .collect(Collectors.toList()); Map\u0026lt;Long, UserOpinion\u0026gt; opinions = opinionRepository.findUserOpinionsByPostIds(userId, postIds); posts.getContent().forEach(post -\u0026gt; post.setUserOpinion(opinions.getOrDefault(post.getId(), UserOpinion.NO_OPINION)) ); return posts; } } @Repository public interface PostRepository extends JpaRepository\u0026lt;Post, Long\u0026gt; { @Query(\u0026#34;SELECT NEW com.heslin.postopia.dto.post.PostInfo(p.id, p.subject, p.content, p.positiveCount, p.negativeCount, p.commentCount, u.username, u.nickname, u.avatar) FROM Post p JOIN p.user u JOIN p.space s WHERE s.id = :spaceId ORDER BY p.createdAt DESC\u0026#34;) Page\u0026lt;PostInfo\u0026gt; findPostsInSpace(@Param(\u0026#34;spaceId\u0026#34;) Long spaceId, Pageable pageable); } @Repository public interface OpinionRepository extends JpaRepository\u0026lt;Opinion, Long\u0026gt; { @Query(\u0026#34;SELECT o.post.id as postId, CASE WHEN o.isPositive = true THEN \u0026#39;LIKE\u0026#39; WHEN o.isPositive = false THEN \u0026#39;DISLIKE\u0026#39; ELSE \u0026#39;NO_OPINION\u0026#39; END as opinion FROM Opinion o WHERE o.user.id = :userId AND o.post.id IN :postIds\u0026#34;) List\u0026lt;Object[]\u0026gt; findUserOpinionsByPostIds(@Param(\u0026#34;userId\u0026#34;) Long userId, @Param(\u0026#34;postIds\u0026#34;) List\u0026lt;Long\u0026gt; postIds); default Map\u0026lt;Long, UserOpinion\u0026gt; findUserOpinionsByPostIds(Long userId, List\u0026lt;Long\u0026gt; postIds) { return findUserOpinionsByPostIds(userId, postIds).stream() .collect(Collectors.toMap( arr -\u0026gt; (Long) arr[0], arr -\u0026gt; UserOpinion.valueOf((String) arr[1]) )); } } This approach allows you to:\nFetch posts without the complexity of joining with the opinions table. Retrieve user opinions for only the fetched posts in a separate query. Combine the results in memory. To determine which method is more efficient for your specific use case, I recommend:\nImplement both approaches. Create performance tests with various dataset sizes and query patterns. Profile the application to measure actual performance differences. Consider the maintainability and readability of each approach in your codebase. Remember, premature optimization can lead to unnecessary complexity. Start with the approach that\u0026rsquo;s most straightforward for your team to implement and maintain, then optimize if and when performance becomes an issue based on real-world usage patterns.\nCommon Table Expressions Common Table Expressions (CTEs) in SQL are temporary named result sets that you can reference within a SELECT, INSERT, UPDATE, DELETE, or MERGE statement. They are defined using a WITH clause at the beginning of a query. Here\u0026rsquo;s a brief explanation of CTEs:\nSyntax:\nWITH cte_name AS ( SELECT ... ) SELECT ... FROM cte_name; Key features:\nImprove readability by breaking complex queries into simpler, named parts Can be referenced multiple times within the main query Useful for recursive queries Example:\nWITH sales_summary AS ( SELECT product_id, SUM(quantity) as total_quantity, SUM(price * quantity) as total_revenue FROM sales GROUP BY product_id ) SELECT p.product_name, s.total_quantity, s.total_revenue FROM products p JOIN sales_summary s ON p.product_id = s.product_id; CTEs are particularly useful for complex queries, hierarchical data, and when you need to reference the same subquery multiple times.\nWould you like me to elaborate on any specific aspect of Common Table Expressions?\n","date":"2024-09-20T00:00:00Z","permalink":"https://dyhes.github.io/p/postopia-dev-logweek-4/","title":"【Postopia Dev Log】Week 4"},{"content":"Database Schema DBMSs utilize an internal schema, which represents the structure of the data as viewed by the DBMS, and an external schema, which represents various structures of the data as viewed by the end user. The conceptual schema represents the basic underlying structure of data as viewed by the enterprise as a whole.\n外模式\n外模式又称子模式或用户模式，对应于用户级。它是某个或某几个用户所看到的数据库的数据视图，是与某一应用有关的数据的逻辑表示。外模式是从模式导出的一个子集，包含模式中允许特定用户使用的那部分数据。用户可以通过外模式描述语言来描述、定义对应于用户的数据记录(外模式)，也可以利用数据操纵语言(Data Manipulation Language，DML)对这些数据记录进行操作。外模式反映了数据库系统的用户观。\n概念模式\n概念模式又称模式或逻辑模式，对应于概念级。它是由数据库设计者综合所有用户的数据，按照统一的观点构造的全局逻辑结构，是对数据库中全部数据的逻辑结构和特征的总体描述，是所有用户的公共数据视图(全局视图)。它是由数据库管理系统提供的数据模式描述语言(Data Description Language，DDL)来描述、定义的。概念模式反映了数据库系统的整体观。\n内模式\n内模式又称存储模式，对应于物理级。它是数据库中全体数据的内部表示或底层描述，是数据库最低一级的逻辑描述，它描述了数据在存储介质上的存储方式和物理结构，对应着实际存储在外存储介质上的数据库。内模式由内模式描述语言来描述、定义的。内模式反映了数据库系统的存储观。\n在一个数据库系统中，只有唯一的数据库， 因而作为定义 、描述数据库存储结构的内模式和定义、描述数据库逻辑结构的模式，也是唯一的，但建立在数据库系统之上的应用则是非常广泛、多样的，所以对应的外模式不是唯一的，也不可能是唯一的。\n","date":"2024-09-19T00:00:00Z","permalink":"https://dyhes.github.io/p/databaseschema/","title":"【Database】Schema"},{"content":"Postgres Docker Image\nContainer \u0026gt; Image\npsql -U {username} -d {database} ","date":"2024-09-19T00:00:00Z","permalink":"https://dyhes.github.io/p/dockerpostgres/","title":"【Docker】Postgres"},{"content":"Basic Using a .env file is a great practice for managing environment variables in a Node.js project.\nimport * as dotenv from \u0026#39;dotenv\u0026#39;; // Load environment variables from .env file dotenv.config(); // usage process.env.IMGBB_API_KEY Process Now, let\u0026rsquo;s go through the changes and additional steps to set up the .env file:\nFirst, install the dotenv package:\nnpm install dotenv @types/dotenv Create a .env file in the root of your project with the following content:\nIMGBB_API_KEY=your_actual_api_key_here MARKDOWN_DIRECTORY=./path/to/your/markdown/files In the script, we\u0026rsquo;ve made the following changes:\nImported the dotenv package: import * as dotenv from 'dotenv'; Added dotenv.config(); to load the environment variables. Replaced the hardcoded API key with process.env.IMGBB_API_KEY. Added error handling if the API key is not set. Used process.env.MARKDOWN_DIRECTORY for the directory path, with a fallback. Make sure to add .env to your .gitignore file to prevent committing sensitive information:\necho \u0026#34;.env\u0026#34; \u0026gt;\u0026gt; .gitignore ","date":"2024-09-12T00:00:00Z","permalink":"https://dyhes.github.io/p/node.env/","title":"【Node】.env"},{"content":"hugo新帖子的Tags添加失败，老帖子的Tags修改可以产生变化 通过 hugo \u0026ndash;gc \u0026ndash;minify 清除缓存解决问题\n","date":"2024-09-10T00:00:00Z","permalink":"https://dyhes.github.io/p/hugocache/","title":"【Hugo】Cache"},{"content":"永远对利益保持谨慎，事前充分了解机制\npublic record emailDto(Long id, String email){}; @PostMapping(\u0026ldquo;email/request\u0026rdquo;) public BasicApiResponseEntity updateemail(@RequestBody emailDto dto) { if (dto.id == null || dto.email == null) { throw new BadRequestException(); } userService.updateUseremail(dto.id, dto.email); return BasicApiResponseEntity.ok(\u0026ldquo;succeed!\u0026rdquo;); }\n","date":"2024-09-08T00:00:00Z","permalink":"https://dyhes.github.io/p/%E9%98%B4%E9%98%B3%E5%B8%88%E8%97%8F%E5%AE%9D%E9%98%81%E5%87%BA%E5%8F%B7%E6%84%9F%E6%82%9F/","title":"【阴阳师】藏宝阁出号感悟"},{"content":"Week3 写评论查询的时候遇到了递归查询和递归结构构建的问题，这样的问题是表层学习而不真正深入使用知识、框架时难以遇到并学习如何解决的。\nenum in @Query @Query(\u0026#34;select new com.heslin.postopia.dto.post.PostSummary(p.subject, p.positive_count, p.negative_count, p.comment_count, u.username, u.nickname, u.avatar) from Post p JOIN p.user u where p.space_id = :id and p.status != com.heslin.postopia.enums.PostStatus.DRAFT\u0026#34;) record Generics Records in Java can be combined with generics. Records, introduced in Java 14 as a preview feature and made stable in Java 16, provide a concise way to model immutable data. Generics allow types (classes and interfaces) to be parameters when defining classes, interfaces, and methods. Combining these two can lead to more flexible and reusable code.\nHere is an example of how you can define and use a generic record in Java:\n// Define a generic record public record Pair\u0026lt;K, V\u0026gt;(K key, V value) {} // Usage example public class Main { public static void main(String[] args) { Pair\u0026lt;String, Integer\u0026gt; pair = new Pair\u0026lt;\u0026gt;(\u0026#34;age\u0026#34;, 30); System.out.println(\u0026#34;Key: \u0026#34; + pair.key()); System.out.println(\u0026#34;Value: \u0026#34; + pair.value()); } } In this example:\nPair\u0026lt;K, V\u0026gt; is a generic record with two type parameters, K and V. The Pair record has two components, key and value, of types K and V respectively. When creating an instance of Pair, you specify the actual types to use, such as String and Integer in this case. You can then use the key() and value() methods to access the components of the record. This example illustrates how generics can be seamlessly integrated with records to create versatile and type-safe data structures.\nExtension No, records in Java cannot be extended. Records are implicitly final, meaning they cannot be subclassed. This design choice aligns with the primary purpose of records, which is to provide a simple and concise way to create immutable data carriers. Allowing inheritance would complicate this model and could undermine the immutability guarantees that records provide.\nHere\u0026rsquo;s a brief overview of some key characteristics of records:\nImmutability: Records are designed to be immutable. All fields in a record are implicitly private and final. Compact Syntax: Records provide a compact syntax for declaring classes that are primarily used to hold data. Final and Non-Extendable: Records are implicitly final, meaning you cannot extend a record class. If you need to share common functionality or data among multiple types, consider using composition or interfaces instead of inheritance. Here is an example using composition:\n// Common functionality encapsulated in a class class CommonData { private final String commonField; public CommonData(String commonField) { this.commonField = commonField; } public String getCommonField() { return commonField; } } // Record using composition public record SpecializedRecord(String specificField, CommonData commonData) {} public class Main { public static void main(String[] args) { CommonData commonData = new CommonData(\u0026#34;Shared Data\u0026#34;); SpecializedRecord record = new SpecializedRecord(\u0026#34;Specific Data\u0026#34;, commonData); System.out.println(\u0026#34;Specific Field: \u0026#34; + record.specificField()); System.out.println(\u0026#34;Common Field: \u0026#34; + record.commonData().getCommonField()); } } In this example, the CommonData class encapsulates the shared functionality, and the SpecializedRecord record uses composition to include this common data. This approach maintains the benefits of immutability and concise syntax while avoiding the pitfalls of inheritance.\nBreak circular dependencies In Spring Boot, it\u0026rsquo;s generally not advisable to have two services autowire each other as fields because it can create a circular dependency. Circular dependencies can make your application context fail to start and are generally considered a bad design practice.\nHowever, if you still need to achieve this, there are some workaround strategies. Here are a few approaches:\n1. Setter Injection This method uses setter methods instead of field injection to avoid circular dependencies.\n@Service public class ServiceA { private ServiceB serviceB; @Autowired public void setServiceB(ServiceB serviceB) { this.serviceB = serviceB; } public void methodA() { serviceB.methodB(); } } @Service public class ServiceB { private ServiceA serviceA; @Autowired public void setServiceA(ServiceA serviceA) { this.serviceA = serviceA; } public void methodB() { serviceA.methodA(); } } 2. @Lazy Annotation You can use the @Lazy annotation to delay the initialization of one of the services, breaking the circular dependency.\n@Service public class ServiceA { private final ServiceB serviceB; @Autowired public ServiceA(@Lazy ServiceB serviceB) { this.serviceB = serviceB; } public void methodA() { serviceB.methodB(); } } @Service public class ServiceB { private final ServiceA serviceA; @Autowired public ServiceB(@Lazy ServiceA serviceA) { this.serviceA = serviceA; } public void methodB() { serviceA.methodA(); } } 3. Redesign Your Services Often, circular dependencies are a sign that your service design needs to be refactored. Consider whether you can split responsibilities differently to avoid the circular dependency. For example, you could introduce a third service or a common interface.\n@Service public class ServiceA { private final CommonService commonService; @Autowired public ServiceA(CommonService commonService) { this.commonService = commonService; } public void methodA() { commonService.commonMethod(); } } @Service public class ServiceB { private final CommonService commonService; @Autowired public ServiceB(CommonService commonService) { this.commonService = commonService; } public void methodB() { commonService.commonMethod(); } } @Service public class CommonService { public void commonMethod() { // Common logic here } } 4. Application Context You can also manually fetch the beans from the application context, but this is generally not recommended and should be used as a last resort.\n@Service public class ServiceA implements ApplicationContextAware { private ServiceB serviceB; private ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.applicationContext = applicationContext; } @PostConstruct public void init() { this.serviceB = applicationContext.getBean(ServiceB.class); } public void methodA() { serviceB.methodB(); } } @Service public class ServiceB { private final ServiceA serviceA; @Autowired public ServiceB(ServiceA serviceA) { this.serviceA = serviceA; } public void methodB() { serviceA.methodA(); } } Conclusion While you can technically create circular dependencies using these methods, it\u0026rsquo;s usually better to refactor your code to avoid such designs. Circular dependencies can make your code harder to understand, test, and maintain. Consider whether there are better ways to structure your services to keep them decoupled.\nRecursive Model Query Check post with the same name for details\n","date":"2024-08-16T00:00:00Z","permalink":"https://dyhes.github.io/p/postopia-dev-logweek-3/","title":"【Postopia Dev Log】Week 3"},{"content":"// simplified @Data @Entity @Table(name = \u0026#34;comments\u0026#34;) @EntityListeners(AuditingEntityListener.class) public class Comment { @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Long id; @ManyToOne(fetch = FetchType.LAZY) @JoinColumn(name = \u0026#34;parent_id\u0026#34;) private Comment parent; @OneToMany(mappedBy = \u0026#34;parent\u0026#34;, orphanRemoval = true) private List\u0026lt;Comment\u0026gt; children = new ArrayList\u0026lt;\u0026gt;(); } FetchType.EAGER public Comment getCommentWithChildren(Long commentId) { return commentRepository.findById(commentId).orElse(null); } Derived Method public List\u0026lt;Comment\u0026gt; findSubcomments(Long commentId) { Comment rootComment = commentRepository.findById(commentId).orElse(null); if (rootComment == null) { return null; // or throw an exception if preferred } fetchChildren(rootComment); return List.of(rootComment); } private void fetchChildren(Comment parent) { List\u0026lt;Comment\u0026gt; children = commentRepository.findByParentId(parent.getId()); parent.setChildren(children); for (Comment child : children) { fetchChildren(child); } } Common Table Expressions (CTEs) Spring Data JPA does not support recursive queries out-of-the-box, but you can use native queries to achieve this. Below is an example of how you can perform a recursive query using a native query in Spring Data JPA.\n@Query(value = \u0026#34;WITH RECURSIVE Subcomments AS (\u0026#34; + \u0026#34; SELECT c.id, c.content, c.parent_id \u0026#34; + \u0026#34; FROM Comment c \u0026#34; + \u0026#34; WHERE c.id = :commentId \u0026#34; + \u0026#34; UNION ALL \u0026#34; + \u0026#34; SELECT c.id, c.content, c.parent_id \u0026#34; + \u0026#34; FROM Comment c \u0026#34; + \u0026#34; INNER JOIN Subcomments s ON c.parent_id = s.id\u0026#34; + \u0026#34;) \u0026#34; + \u0026#34;SELECT * FROM Subcomments\u0026#34;, nativeQuery = true) List\u0026lt;Comment\u0026gt; findSubcomments(@Param(\u0026#34;commentId\u0026#34;) Long commentId); The fetched result is a flatten list of Comment.\nPerformance Considerations CTEs Pros: Efficiency: Fetching the entire hierarchy in a single query is efficient and reduces the number of database round-trips. Control: You have fine-grained control over how the data is processed and structured in the application layer. Cons: Complexity: The reconstruction of the recursive structure in the application layer can be complex and error-prone. Memory Usage: Depending on the size of the dataset, holding the entire hierarchy in memory for reconstruction can be memory-intensive. Performance Overhead: The reconstruction process itself can introduce performance overhead, especially if the hierarchy is deep or has many nodes. Performance Comparison Native Query and CTEs with Reconstruction:\nPros: Efficiently fetches the entire hierarchy in a single query. Minimizes the number of database round-trips. Cons: Complexity in reconstructing the hierarchy in the application layer. Potential memory usage concerns if the dataset is large. Additional performance overhead due to the reconstruction process. FetchType.EAGER:\nPros: Simplifies data access by automatically fetching children. Avoids the need for manual reconstruction. Cons: Performance overhead due to eager fetching, especially with large datasets. Risk of the N+1 query problem. Increased memory usage due to loading entire collections. Derived Methods and Manually Assembling:\nPros: Flexibility and simplicity in using JPA abstractions. Allows for fine-grained control over data fetching and processing. Cons: Potential performance overhead due to multiple queries. Complexity in manually assembling the hierarchical structure. Risk of performance and consistency issues if not managed carefully. Recommendations For Large, Complex Hierarchies: Use native queries with CTEs for efficient data fetching. Be prepared to handle the complexity of reconstructing the hierarchy in the application layer. This approach provides the best performance but requires careful implementation. For Small to Medium Hierarchies: Use FetchType.EAGER for convenience and simplicity. Monitor performance and memory usage to ensure it remains within acceptable limits. For Flexible, Maintainable Code: Use derived methods and manually assemble the hierarchical structure if the performance impact is manageable. This approach offers a good balance between flexibility and simplicity. Conclusion Native Query and CTEs with Reconstruction: Best for performance but requires careful handling of the reconstruction phase. FetchType.EAGER: Convenient but can lead to performance and memory overhead. Derived Methods and Manual Assembly: Flexible and maintainable with acceptable performance for smaller datasets. Ultimately, the choice depends on your specific use case, dataset size, and performance requirements. Always profile and benchmark your application to make an informed decision based on actual performance metrics.\n","date":"2024-08-15T00:00:00Z","permalink":"https://dyhes.github.io/p/spring-data-jparecursive-model-query/","title":"【Spring Data JPA】Recursive Model Query"},{"content":"In Ant Design (AntD), a popular React UI library, the component is often used to create buttons in user interfaces. The htmlType attribute of the component is used to specify the native HTML type of the button element. This is analogous to the type attribute in a standard HTML element.\nWhen you set htmlType=\u0026ldquo;submit\u0026rdquo; in an AntD , it means that the button will function as a submit button within a form. Here’s a breakdown of what this does:\nSubmit Button: When the button is clicked, it will trigger the form submission. This is the same behavior you would expect from a native HTML . ","date":"2024-08-09T00:00:00Z","permalink":"https://dyhes.github.io/p/antdbutton-htmltype/","title":"【Antd】Button HtmlType"},{"content":"useForm and Antd Form Both useForm and the antd Form component are popular tools for handling form state and validation in React applications, but they come from different libraries and have different approaches and features.\nuseForm (from react-hook-form) useForm is a hook provided by the react-hook-form library, which focuses on providing a highly performant and flexible way to handle form state and validation in React.\nKey Features Performance: useForm minimizes re-renders, making it highly performant, especially for large forms. Minimal Re-renders: By default, form state is managed in a way that prevents unnecessary re-renders of the entire form, updating only the specific fields that need to be updated. Validation: Offers built-in validation and also supports schema validation using libraries like Yup. Ease of Use: Simple API that makes it easy to integrate and manage form state. Integration with UI Libraries: Can be easily integrated with various UI component libraries, although it doesn\u0026rsquo;t provide its own set of form components. Example Usage import React from \u0026#39;react\u0026#39;; import { useForm } from \u0026#39;react-hook-form\u0026#39;; function MyForm() { const { register, handleSubmit, formState: { errors } } = useForm(); const onSubmit = data =\u0026gt; { console.log(data); }; return ( \u0026lt;form onSubmit={handleSubmit(onSubmit)}\u0026gt; \u0026lt;input {...register(\u0026#39;firstName\u0026#39;, { required: true })} /\u0026gt; {errors.firstName \u0026amp;\u0026amp; \u0026lt;span\u0026gt;This field is required\u0026lt;/span\u0026gt;} \u0026lt;input type=\u0026#34;submit\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; ); } antd Form Component The antd Form component is part of the Ant Design library, which is a popular UI framework for React. The Form component provides a set of form management features that are tightly integrated with the Ant Design component library.\nKey Features Integrated with Ant Design: Comes with pre-styled form components that match the Ant Design aesthetic. Declarative Syntax: Uses a declarative syntax to define form fields and validation rules. Validation: Built-in support for validation rules and messages, including custom validation. Data Binding: Provides mechanisms for data binding and state management. Layout: Offers various layout options to organize form fields in a responsive manner. Example Usage import React from \u0026#39;react\u0026#39;; import { Form, Input, Button } from \u0026#39;antd\u0026#39;; import \u0026#39;antd/dist/antd.css\u0026#39;; const MyForm = () =\u0026gt; { const onFinish = (values) =\u0026gt; { console.log(\u0026#39;Success:\u0026#39;, values); }; const onFinishFailed = (errorInfo) =\u0026gt; { console.log(\u0026#39;Failed:\u0026#39;, errorInfo); }; return ( \u0026lt;Form name=\u0026#34;basic\u0026#34; initialValues={{ remember: true }} onFinish={onFinish} onFinishFailed={onFinishFailed} \u0026gt; \u0026lt;Form.Item label=\u0026#34;Username\u0026#34; name=\u0026#34;username\u0026#34; rules={[{ required: true, message: \u0026#39;Please input your username!\u0026#39; }]} \u0026gt; \u0026lt;Input /\u0026gt; \u0026lt;/Form.Item\u0026gt; \u0026lt;Form.Item label=\u0026#34;Password\u0026#34; name=\u0026#34;password\u0026#34; rules={[{ required: true, message: \u0026#39;Please input your password!\u0026#39; }]} \u0026gt; \u0026lt;Input.Password /\u0026gt; \u0026lt;/Form.Item\u0026gt; \u0026lt;Form.Item\u0026gt; \u0026lt;Button type=\u0026#34;primary\u0026#34; htmlType=\u0026#34;submit\u0026#34;\u0026gt; Submit \u0026lt;/Button\u0026gt; \u0026lt;/Form.Item\u0026gt; \u0026lt;/Form\u0026gt; ); }; export default MyForm; Summary Performance: useForm is generally more performant due to its minimal re-renders. UI Integration: antd Form is tightly integrated with the Ant Design component library, providing a consistent UI. Flexibility: useForm offers more flexibility for integrating with different UI libraries. Declarative vs. Imperative: antd Form uses a more declarative approach to define fields and validation, while useForm relies on hooks and a more imperative approach. Choosing between the two often depends on your specific needs, such as performance requirements, UI consistency, and the complexity of your form logic.\n","date":"2024-08-09T00:00:00Z","permalink":"https://dyhes.github.io/p/antdform/","title":"【Antd】Form"},{"content":"In URL encoding, reserved characters that have special meanings and unsafe characters might be altered during transmission or have special meanings in certain contexts in URLs are typically encoded using a percent sign (\u0026rsquo;%\u0026rsquo;) followed by two hexadecimal digits representing the ASCII value of the character. For example:\nThe ASCII value of \u0026lsquo;/\u0026rsquo; is 47 in decimal. 47 in hexadecimal is 2F. ⠀Thus, \u0026lsquo;/\u0026rsquo; becomes \u0026lsquo;%2F\u0026rsquo; when it is URL-encoded. This encoding is often necessary to ensure that URLs are transmitted correctly over the Internet, where certain characters might otherwise be misinterpreted. Any characters outside the ASCII range (0-127) should also be encoded, as they might not be correctly interpreted by all systems. These are typically encoded using UTF-8 and then percent-encoded. Path and Query Parameters usually need to be url-encoded. ","date":"2024-08-07T00:00:00Z","permalink":"https://dyhes.github.io/p/fronturl-encoding/","title":"【Front】URL encoding"},{"content":"Hadoop Hadoop is an open-source framework developed by the Apache Software Foundation used for storing and processing large datasets in a distributed computing environment. It is designed to scale up from a single server to thousands of machines, each offering local computation and storage. Hadoop is built to handle massive amounts of data in a fault-tolerant and efficient manner.\nCore Components Hadoop Distributed File System (HDFS): Purpose: HDFS is designed to store large datasets reliably and to stream those datasets to user applications at high bandwidth. Architecture: It follows a master-slave architecture. The HDFS cluster consists of a single NameNode (master) that manages the file system namespace and regulates access to files by clients. There are multiple DataNodes (slaves) that manage storage attached to the nodes they run on. Features: Fault Tolerance: Data is replicated across multiple nodes to ensure availability in case of hardware failure. Scalability: Can scale up to thousands of nodes. High Throughput: Optimized for high throughput of data access rather than low latency. MapReduce: Purpose: A programming model and execution engine that is used for processing and generating large datasets. Components: JobTracker: The master node that manages the jobs and resources in the cluster. TaskTracker: The slave nodes that execute the tasks as directed by the JobTracker. Phases: Map Phase: Processes input data and converts it into a set of key-value pairs. Reduce Phase: Processes the intermediate key-value pairs to generate the final output. 1 YARN (Yet Another Resource Negotiator): Purpose: Manages resources in the Hadoop cluster and schedules jobs. Components: ResourceManager: The master that controls and allocates resources. NodeManager: Manages resources and monitoring on the individual nodes. ApplicationMaster: Manages the lifecycle of applications running on YARN. Hadoop Common: Purpose: Provides common utilities and libraries that support the other Hadoop components. It includes the necessary Java libraries and files needed to start Hadoop. Ecosystem and Tools Hadoop has a rich ecosystem of tools and frameworks that enhance its functionality:\nHive: A data warehouse infrastructure that provides data summarization, query, and analysis. It uses a SQL-like language called HiveQL. Pig: A high-level platform for creating MapReduce programs using a language called Pig Latin. HBase: A distributed, scalable, big data store modeled after Google’s Bigtable and written in Java. Sqoop: A tool designed for efficiently transferring bulk data between Hadoop and structured data stores such as relational databases. Flume: A service for efficiently collecting, aggregating, and moving large amounts of log data. Oozie: A workflow scheduling system to manage Hadoop jobs. Zookeeper: A centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. Spark: A fast and general engine for large-scale data processing that can run on Hadoop clusters. Key Features Scalability: Can scale horizontally to handle more data by adding more nodes to the cluster. Cost-Effective: Uses commodity hardware, making it a cost-effective solution for big data processing. Flexibility: Can process structured, semi-structured, and unstructured data. Fault Tolerance: Automatically handles hardware failures by replicating data across multiple nodes. Distributed Computing: Processes data in parallel across multiple nodes, increasing processing speed and efficiency. ⠀Use Cases Data Warehousing: Companies use Hadoop for storing vast amounts of structured and unstructured data. Log Processing: Hadoop is used to analyze and process server logs for insights and troubleshooting. Recommendation Systems: Used by companies like Netflix and Amazon for building recommendation engines. Data Archival: Storing large volumes of historical data for compliance and analysis. Spark Apache Spark is an open-source, distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at UC Berkeley’s AMPLab, Spark offers a fast and general-purpose cluster-computing framework. It extends the MapReduce model to support more types of computations and optimizes performance by keeping data in memory whenever possible.\nKey Features Speed: Spark processes data in memory, which significantly reduces the time to read and write to disk. This makes Spark up to 100 times faster than Hadoop MapReduce for certain applications. The DAG (Directed Acyclic Graph) execution engine optimizes task execution by breaking down jobs into stages and tasks, which can be executed in parallel. Ease of Use: Spark provides high-level APIs in Java, Scala, Python, and R. This versatility allows developers to use the language they are most comfortable with. The interactive shell provided by Spark allows for real-time data analysis and debugging. Compatibility: Spark is compatible with Hadoop’s HDFS and YARN, allowing it to be deployed on existing Hadoop clusters. It also supports a wide range of data sources, including HBase, Cassandra, Kafka, and more. ","date":"2024-08-04T00:00:00Z","permalink":"https://dyhes.github.io/p/big-datahadoop/","title":"【Big Data】Hadoop"},{"content":"How to read faster Cut the Inner Monologue try reading without sounding out the words in your head Read With Your Finger our eyes kind of naturally jump around and it takes mental effort to keep our eyes focused on each line Stop reading content you don’t like skip entire book or chapter when you don’t like the contents, table of contents he Schedule Reading utilize scattered time Read more than one book simultaneously How to remember What you read Highlighting and note-taking is basically worthless. We remember what’s useful find ways to implement ideas you read into your life mental effort: how does this(things I read) apply to my own life? there are ways to kind of trick your brain into thinking that you were using the stuff you read talk about the ideas to somebody else (humans tend to see social interactions as the most useful and important things) Instead of remembering the details of a book, remember overview of the book, and look it up when actually in need. ","date":"2024-08-02T00:00:00Z","permalink":"https://dyhes.github.io/p/life-hacksread-faster/","title":"【Life Hacks】Read faster"},{"content":"可以使用 umi 微生成器\nnpx umi g tailwindcss 快速下载tailwind\n","date":"2024-07-31T00:00:00Z","permalink":"https://dyhes.github.io/p/umitailwind/","title":"【Umi】Tailwind"},{"content":"The object-fit CSS property is used to specify how the content of a replaced element, such as an or , should be resized to fit its container. Replaced elements are elements whose content is not managed by the CSS box model, like images, videos, or iframes. The object-fit property can take several values, which control how the replaced element’s content is resized:\nfill This is the default value. The content is resized to fill the container, ignoring its aspect ratio. The image or video may look distorted if the aspect ratio of the container differs from that of the content. contain The content is scaled to maintain its aspect ratio while fitting within the container. The entire content will be visible, but there may be empty space (letterboxing or pillarboxing) if the aspect ratios do not match. cover The content is scaled to maintain its aspect ratio while covering the entire container. Parts of the content might be cropped if the aspect ratio of the container and content do not match. none The content is not resized. The content is displayed at its original size, and it may overflow the container if it is larger than the container. scale-down The content is scaled down to fit within the container while maintaining its aspect ratio. This behaves like contain if the content is larger than the container, or like none if the content is smaller than the container. ","date":"2024-07-25T00:00:00Z","permalink":"https://dyhes.github.io/p/cssobject-fit/","title":"【CSS】Object-fit"},{"content":"ReactElement A ReactElement is a plain object describing a component instance or DOM node and its desired properties. It’s the return type of React.createElement(). It’s always an object with a type and props. It’s more specific and represents a single React element. const element: React.ReactElement = \u0026lt;div\u0026gt;Hello\u0026lt;/div\u0026gt;; ReactNode ReactNode is a more general type that can represent a React element, a string, a number, a boolean, null, undefined, or an array of ReactNodes. It’s a union type that includes ReactElement. It’s more flexible and can represent any valid value that can be rendered in React. const node1: React.ReactNode = \u0026#34;Hello\u0026#34;; const node2: React.ReactNode = 42; const node3: React.ReactNode = \u0026lt;div\u0026gt;World\u0026lt;/div\u0026gt;; const node4: React.ReactNode = [node1, node2, node3]; Key Differences Specificity: ReactElement is more specific and always represents a single React element. ReactNode is more general and can represent various types that React can render. Usage: ReactElement is typically used when you specifically need a React element. ReactNode is often used for component children or when you want to accept any renderable content. Type Hierarchy: ReactNode includes ReactElement, but not vice versa. All ReactElements are ReactNodes, but not all ReactNodes are ReactElements. Common Use Cases: ReactElement: When you’re working with specific React components or elements. ReactNode: When defining props that can accept various types of content, like children. import React, { ReactElement, ReactNode } from \u0026#39;react\u0026#39;; // This only accepts a single React element const ElementComponent: React.FC\u0026lt;{ content: ReactElement }\u0026gt; = ({ content }) =\u0026gt; { return \u0026lt;div\u0026gt;{content}\u0026lt;/div\u0026gt;; }; // This accepts any renderable content const NodeComponent: React.FC\u0026lt;{ content: ReactNode }\u0026gt; = ({ content }) =\u0026gt; { return \u0026lt;div\u0026gt;{content}\u0026lt;/div\u0026gt;; }; function App() { return ( \u0026lt;div\u0026gt; \u0026lt;ElementComponent content={\u0026lt;span\u0026gt;Hello\u0026lt;/span\u0026gt;} /\u0026gt; {/* Valid */} {/* \u0026lt;ElementComponent content=\u0026#34;Hello\u0026#34; /\u0026gt; */} {/* Invalid */} \u0026lt;NodeComponent content={\u0026lt;span\u0026gt;World\u0026lt;/span\u0026gt;} /\u0026gt; {/* Valid */} \u0026lt;NodeComponent content=\u0026#34;Hello\u0026#34; /\u0026gt; {/* Also Valid */} \u0026lt;/div\u0026gt; ); } ","date":"2024-07-25T00:00:00Z","permalink":"https://dyhes.github.io/p/reactreactnode-vs-reactelement/","title":"【React】ReactNode vs ReactElement"},{"content":"按部就班进行开发，发现自己可能会在一些自己认知范围内的性能优化点特别在意，又会觉得优化需要对语言底层有深刻的认知，接近于一个无穷无尽的过程，然后对自己的语言功底感到焦虑。\nDelete method Yes, derived delete methods in Spring Data JPA repositories can return a boolean value. When you define a derived delete method to return a boolean, it will indicate whether any records were actually deleted.\n@EntityListeners(AuditingEntityListener.class) 除\n@EnableJpaAuditting @CreateDate 还需要加上： @EntityListeners(AuditingEntityListener.class) Data Transfer Objects query only the needed fields in Spring Data JPA for several reasons:\nPerformance optimization: By querying only the required fields, you reduce the amount of data transferred between the database and your application, which can improve performance, especially for large datasets. Reduced network traffic: Transferring only the necessary data minimizes network usage, which is particularly beneficial in distributed systems or when dealing with remote clients. Separation of concerns: DTOs help separate the data representation used in your application’s business logic from the underlying database entities, providing a clear boundary between layers. Flexibility and maintainability: DTOs allow you to shape the data according to specific use cases, making it easier to evolve your API without affecting the underlying data model. Security: By using DTOs, you can control which data is exposed to the client, reducing the risk of accidentally exposing sensitive information. Avoid lazy loading issues: When working with JPA entities, lazy-loaded associations can cause performance problems or errors if accessed outside of a transaction. DTOs help avoid these issues by containing only the required data. Var keyword In Java, the var keyword was introduced in Java 10 as part of the Local Variable Type Inference feature. It allows you to declare local variables without explicitly specifying their type. The compiler infers the type based on the initialization expression. Here are some key points about var in Java:\nusage var variableName = initialValue; Scope: var can only be used for local variables inside methods or loops. It cannot be used for fields, method parameters, or return types. Initialization: Variables declared with var must be initialized in the same statement. Type Inference: The compiler infers the type based on the right-hand side of the assignment. Limitations: Cannot be used with null literals Cannot be used with array initializers Cannot be used in a compound declaration (declaring multiple variables in one statement) Examples // Basic usage var name = \u0026#34;John\u0026#34;; // inferred as String var age = 30; // inferred as int var list = new ArrayList\u0026lt;String\u0026gt;(); // inferred as ArrayList\u0026lt;String\u0026gt; // In a for loop for (var i = 0; i \u0026lt; 10; i++) { System.out.println(i); } // With lambda expressions var comparator = (String s1, String s2) -\u0026gt; s1.compareTo(s2); // With try-with-resources try (var reader = new BufferedReader(new FileReader(\u0026#34;file.txt\u0026#34;))) { // Use reader } Benefits of using var: Reduces boilerplate code, especially with long type names. Improves readability in some cases, particularly with generic types. Allows the type to change without changing the variable declaration. Considerations: Use var judiciously. Sometimes explicit type declarations can improve code clarity. var doesn’t make Java a dynamically-typed language. The type is still determined at compile-time. IDEs can usually show you the inferred type, which helps with code understanding. Other Local Variable Type Inference feature Diamond Operator (\u0026lt;\u0026gt;): Introduced in Java 7, it allows type inference for generic class instantiation. List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); // Instead of new ArrayList\u0026lt;String\u0026gt;() Method References: Introduced in Java 8, method references work with type inference. List\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;); names.forEach(System.out::println); Try-with-resources try (var input = new FileInputStream(\u0026#34;file.txt\u0026#34;)) { // Use input } Repository Layer Referencing One @Repository and Multiple @Services Pros Promotes separation of concerns Follows the Single Responsibility Principle Can lead to better code organization and maintainability Easier to manage transactions across multiple services Cons May require more classes and interfaces Can potentially lead to more complex dependency injection Multiple @Repositories Pros Can simplify the service layer if operations frequently require data from multiple repositories Reduces the number of classes and interfaces May be more straightforward for simpler applications ⠀Cons Can lead to a “fat” service class with too many responsibilities May make it harder to maintain and test the service class Could potentially violate the Single Responsibility Principle Considerations Generally speaking, it’s often considered a good practice to follow the first approach (one @Repository and multiple @Services) for the following reasons:\nBetter separation of concerns: Each service class can focus on a specific domain or set of related operations. Improved maintainability: Smaller, more focused classes are usually easier to understand and maintain. Easier testing: With more granular services, it’s easier to write unit tests and mock dependencies. Flexibility: It’s easier to reuse and combine services in different ways. Scalability: As your application grows, it’s easier to add new features or modify existing ones without affecting other parts of the system. However, the best approach depends on your specific use case. If your application is small and simple, or if a particular service genuinely needs to work with multiple repositories directly, then referencing multiple repositories in a service class might be appropriate.\nIn conclusion, while referencing one @Repository and multiple @Services in a @Service class is often a good practice, the most important thing is to choose an approach that makes sense for your application’s architecture and maintainability goals.\nLong or long In Java, it’s generally recommended to use the primitive type long instead of the wrapper class Long in method signatures, unless you have a specific reason to use the object wrapper. Here’s why:\nPerformance: Primitive types are more efficient in terms of memory usage and performance. They don’t require object creation or autoboxing/unboxing. Simplicity: Using primitives makes the code simpler and more straightforward. Null safety: Primitive types can’t be null, which can prevent null pointer exceptions. However, there are cases where using Long might be preferable:\nWhen null values are meaningful: If you need to represent the absence of a value, Long can be null while long cannot. In collections: Java collections (like ArrayList or HashMap) require object types, so you’d use Long in these cases. When working with generics: Generics in Java don’t work with primitive types, so you’d need to use Long in these situations. In certain APIs or frameworks: Some APIs or frameworks might require the use of wrapper classes. // Preferred for most cases public void methodWithLong(long value) { // ... } // Use when null is a meaningful value public void methodWithLongObject(Long value) { if (value == null) { // Handle null case } else { // Handle non-null case } } In summary, use long by default for better performance and simplicity, but don’t hesitate to use Long when the situation calls for it, such as when dealing with nullable values or working with Java collections and generics.\nCheck Or Not Checking existence before update if (repository.existsById(id)) { repository.save(entity); } else { throw new EntityNotFoundException(\u0026#34;Entity not found\u0026#34;); } Pros Provides more control over the flow of your application. Allows you to give more specific feedback to the user (e.g., “User not found” instead of a generic error). Can be more efficient if you have additional logic that depends on the entity’s existence. ⠀Cons Requires an additional database query, which can impact performance, especially in high-traffic scenarios. There’s a small chance of a race condition between the check and the update (though this is rare in most applications). Handling the update exception directly try { repository.save(entity); } catch (EntityNotFoundException e) { // Handle the case where the entity doesn\u0026#39;t exist } Pros More straightforward code, following the “it’s easier to ask forgiveness than permission” principle. Potentially more efficient as it avoids an extra database query. Eliminates the possibility of race conditions between checking and updating. ⠀Cons Less control over the specific error message or handling. May catch other unexpected exceptions that you didn’t intend to handle in the same way. Recommendation For most simple CRUD operations, handling the exception directly is often sufficient and more efficient. Consider checking existence first if: You need to provide very specific feedback to the user. You have complex logic that depends on whether the entity exists or not. You’re in a scenario where the extra database query doesn’t significantly impact performance. In high-performance systems with many updates, handling the exception might be preferable to avoid the extra query. If you’re using optimistic locking (e.g., with @Version), you might want to handle OptimisticLockingFailureException separately, as this indicates a concurrent modification rather than a non-existent entity. Consider your specific use case: if updates to non-existent entities are rare, handling the exception might be cleaner. If they’re common, checking first could provide a better user experience. Transactional on Repository Layer By default, Spring Data JPA repository methods are transactional, but only for read-only operations. For modifying operations (like updates), you need to explicitly mark the method or the surrounding service method with @Transactional.\nDerived Query Method In Spring Data JPA, derived query methods are primarily used for retrieving data rather than updating it. While derived query methods are very convenient for simple read operations, they do not support update operations directly.\nMalicious Use When developing and deploying a REST API, it’s essential to consider both valid scenarios and potential malicious use cases. Security: Ignoring potential malicious use can leave your API vulnerable to attacks such as SQL injection, Cross-Site Scripting (XSS), Distributed Denial of Service (DDoS), and others. Stability: Malicious uses can lead to unexpected behavior, crashes, or resource exhaustion, affecting the availability and reliability of your service. Compliance: Many industries have regulations requiring secure handling of data, which includes protecting against malicious use. User Trust: Users expect their data to be secure. Failing to protect against malicious use can damage your reputation and erode user trust. @AuthenticatedPrincipal Usage Passing the principal from the controller Pros Makes the flow of data more explicit. Easier to unit test the service layer, as you can pass any principal object directly. Keeps the service layer completely decoupled from Spring Security. Cons Requires passing the principal through potentially many layers if you have a deep call hierarchy. May lead to repetitive code in controllers if many endpoints need to pass the principal. Using@AuthenticatedPrincipal in the service layer Pros Keeps the service method signature clean and focused on business logic parameters. Reduces coupling between the controller and service layer. Allows the service to be used in different contexts (e.g., scheduled tasks, message listeners) where a controller might not be involved. Cons May make unit testing the service layer slightly more complex, as you need to mock the security context. Could be seen as mixing concerns if you consider authentication a cross-cutting concern. Recommendation In general, using @AuthenticatedPrincipal in the service layer is often preferred because:\nIt adheres to the Don’t Repeat Yourself (DRY) principle, avoiding the need to pass the principal in every controller method. It makes the service methods self-contained and more reusable in different contexts. It leverages Spring Security’s built-in capabilities, which are well-tested and maintained. ","date":"2024-07-19T00:00:00Z","permalink":"https://dyhes.github.io/p/postopia-dev-logweek-2/","title":"【Postopia Dev Log】Week 2"},{"content":"EmptyResultDataAccessException deleteById(ID id) Using deleteById(ID id) with @Transactional:\u2028If the method is annotated with @Transactional, it might throw an EmptyResultDataAccessException if the entity doesn’t exist. Using deleteById(ID id):\u2028This method doesn’t throw an exception if the entity doesn’t exist. It silently does nothing. ","date":"2024-07-18T00:00:00Z","permalink":"https://dyhes.github.io/p/spring-data-jpadatabase-exception/","title":"【Spring Data JPA】Database Exception"},{"content":"benefits:\nIt’s more efficient, as it doesn’t require an additional database query to fetch the Space object. It’s particularly useful when you’re dealing with large volumes of data or when the related entity (Space in this case) has a lot of fields that you don’t need. However, there are a few things to keep in mind:\nMake sure the Space with the given ID actually exists in the database. If it doesn’t, you’ll get a EntityNotFoundException when the entity is first accessed. This approach only works for setting the relationship. If you need to access properties of the Space object beyond its ID, you’ll need to fetch the full object from the database. Be aware of any cascade operations or other JPA settings that might affect this behavior. entityManager.getReference @Autowired private EntityManager entityManager; // Post newPost = new Post(); newPost.setTitle(\u0026#34;New Post Title\u0026#34;); newPost.setContent(\u0026#34;Post content...\u0026#34;); Space spaceReference = entityManager.getReference(Space.class, knownSpaceId); newPost.setSpace(spaceReference); postRepository.save(newPost); Pros Type-safe: It ensures the referenced entity type is correct. JPA-compliant: It’s the standard JPA way of creating entity references. Lazy-loading: The reference is a proxy that can lazy-load the full entity when needed. Consistency: It maintains consistency with how JPA handles entity relationships. Cons Dependency: It requires injecting EntityManager, which might not always be desirable or available. Complexity: It adds a bit more complexity to your code. Simple Reference Object Post newPost = new Post(); newPost.setTitle(\u0026#34;New Post Title\u0026#34;); newPost.setContent(\u0026#34;Post content...\u0026#34;); Space spaceReference = new Space(); spaceReference.setId(knownSpaceId); newPost.setSpace(spaceReference); postRepository.save(newPost); Pros 1 Simplicity: It’s straightforward and easy to understand. 2 No additional dependencies: It doesn’t require EntityManager. 3 Lightweight: It’s a very lightweight operation. Cons Not type-safe: You need to ensure you’re using the correct entity class and ID type. Not a true JPA proxy: It won’t lazy-load additional entity details if accessed. Potential for inconsistency: If not used carefully, it could lead to inconsistencies with how JPA manages entities. Guidelines If you’re working within a JPA/Hibernate context and have easy access to EntityManager, using EntityManager.getReference() is generally the preferred method. It’s more aligned with JPA standards and provides better integration with JPA’s entity management. If you’re working in a simpler context, perhaps in a service layer where you don’t want to introduce a dependency on EntityManager, using a simple reference object can be appropriate. This is especially true if you’re sure you won’t need to access any properties of the referenced entity beyond its ID. If you’re working on a large-scale application where performance is critical, and you’re dealing with a high volume of entities, the simple reference object might be slightly more efficient as it doesn’t create a proxy object. If you’re using Spring Data JPA repositories, you can also use the getOne() method (deprecated in newer versions) or getReferenceById() method, which internally use EntityManager.getReference(). This approach combines the benefits of using EntityManager.getReference() with the convenience of working with repositories. Space spaceReference = spaceRepository.getReferenceById(knownSpaceId); newPost.setSpace(spaceReference); This approach combines the benefits of using EntityManager.getReference() with the convenience of working with repositories. In general, if you’re working within a JPA environment, using EntityManager.getReference() or the repository’s getReferenceById() method is often the most robust and recommended approach. However, in simpler scenarios or where JPA is not heavily used, a simple reference object can be perfectly adequate. Lazy-loaded Reference When using a lazy-loaded reference (like those created by EntityManager.getReference() or JpaRepository.getReferenceById()), the full entity will be lazy-loaded in the following scenarios:\nAccessing Non-ID Properties When you try to access any property of the entity other than its ID, the full entity will be loaded from the database.\nSpace spaceReference = entityManager.getReference(Space.class, spaceId); String spaceName = spaceReference.getName(); // This will trigger lazy loading Invoking Methods If you call any method on the proxy object that requires data from the entity, it will trigger lazy loading.\nSpace spaceReference = entityManager.getReference(Space.class, spaceId); spaceReference.someMethod(); // This will trigger lazy loading if the method uses any non-ID fields Relationship Navigation If you navigate from the proxy to related entities (that are not already loaded), it will trigger lazy loading.\nSpace spaceReference = entityManager.getReference(Space.class, spaceId); List\u0026lt;Post\u0026gt; posts = spaceReference.getPosts(); // This will trigger lazy loading Explicit Initialization If you explicitly initialize the proxy using Hibernate.initialize() or similar methods.\nSpace spaceReference = entityManager.getReference(Space.class, spaceId); Hibernate.initialize(spaceReference); // This will trigger lazy loading toString(), equals(), or hashCode() If these methods are overridden in your entity class to use non-ID fields, calling them will trigger lazy loading.\nSpace spaceReference = entityManager.getReference(Space.class, spaceId); System.out.println(spaceReference); // This might trigger lazy loading if toString() is overridden Serialization If you try to serialize the proxy object, it will typically trigger lazy loading to ensure all data is available for serialization.\nImportant points Lazy loading will only work within an active persistence context (typically, within a transaction). If you try to access a lazy-loaded property outside of a persistence context, you might get a LazyInitializationException. The ID of the entity is always available without triggering a load, as it’s used to create the proxy. If the referenced entity doesn’t exist in the database, accessing it will throw an EntityNotFoundException when lazy loading is triggered. ","date":"2024-07-18T00:00:00Z","permalink":"https://dyhes.github.io/p/spring-data-jpareference-object/","title":"【Spring Data JPA】Reference Object"},{"content":"Java enums are a special type of class used to define collections of constants. They provide a way to represent a fixed set of values, which can be useful for categorizing data and ensuring type safety.\nKey Points Implicitly final and static: Enum constants are implicitly public, static, and final. values() method: Every enum has a static values() method that returns an array of all enum constants. valueOf(String) method: This static method returns the enum constant with the specified name. The name() method returns the name of the enum constant as declared in its enum declaration. It\u0026rsquo;s an instance method available on all enum constants. valueOf(String) and name() provide a way to convert between the Enum and String. ordinal() method: Returns the position of the enum constant (zero-based). Constructors, Fields, and Methods: Enums can have constructors, fields, and methods, allowing you to associate data and behavior with each constant. Implementing Interfaces: Enums can implement interfaces, providing a way to define behavior for each constant. EnumSet and EnumMap: These are specialized Set and Map implementations for use with enum types, offering better performance than their general-purpose counterparts. Constant-specific method implementation: You can override methods for specific enum constants, allowing for different behavior per constant. Abstract methods in enums: You can define abstract methods in an enum, forcing each constant to provide its own implementation. example // Basic enum definition public enum Day { MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY } // Enum with constructor, fields, and methods public enum Planet { MERCURY(3.303e+23, 2.4397e6), VENUS(4.869e+24, 6.0518e6), EARTH(5.976e+24, 6.37814e6); private final double mass; // in kilograms private final double radius; // in meters Planet(double mass, double radius) { this.mass = mass; this.radius = radius; } public double getMass() { return mass; } public double getRadius() { return radius; } // Enum constant-specific method implementation public double surfaceGravity() { return 6.67300E-11 * mass / (radius * radius); } } // Usage examples public class EnumExample { public static void main(String[] args) { // Basic usage Day today = Day.MONDAY; System.out.println(\u0026#34;Today is \u0026#34; + today); // Switch statement with enum switch (today) { case MONDAY: System.out.println(\u0026#34;Start of the work week\u0026#34;); break; case FRIDAY: System.out.println(\u0026#34;TGIF!\u0026#34;); break; default: System.out.println(\u0026#34;Midweek\u0026#34;); } // Using enum methods Planet earth = Planet.EARTH; System.out.println(\u0026#34;Earth\u0026#39;s mass: \u0026#34; + earth.getMass()); System.out.println(\u0026#34;Earth\u0026#39;s surface gravity: \u0026#34; + earth.surfaceGravity()); // Iterating over enum values for (Planet planet : Planet.values()) { System.out.println(planet.name() + \u0026#34;: \u0026#34; + planet.surfaceGravity()); } } } Usage with Pageable public enum UserSortField { ID(\u0026#34;id\u0026#34;), FIRST_NAME(\u0026#34;firstName\u0026#34;), LAST_NAME(\u0026#34;lastName\u0026#34;), EMAIL(\u0026#34;email\u0026#34;), CREATED_DATE(\u0026#34;createdDate\u0026#34;); private final String fieldName; UserSortField(String fiel\u001bdName) { this.fieldName = fieldName; } public String getFieldName() { return fieldName; } } @RestController @RequestMapping(\u0026#34;/api/users\u0026#34;) public class UserController { @Autowired private UserService userService; @GetMapping public ResponseEntity\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; getAllUsers( @RequestParam(defaultValue = \u0026#34;0\u0026#34;) int page, @RequestParam(defaultValue = \u0026#34;10\u0026#34;) int size, @RequestParam(defaultValue = \u0026#34;ID\u0026#34;) UserSortField sortBy) { Page\u0026lt;User\u0026gt; userPage = userService.getAllUsers(page, size, sortBy); // ... rest of the method remains the same } } @Service public class UserService { @Autowired private UserRepository userRepository; public Page\u0026lt;User\u0026gt; getAllUsers(int page, int size, UserSortField sortBy) { Pageable pageable = PageRequest.of(page, size, Sort.by(sortBy.getFieldName())); return userRepository.findAll(pageable); } } Usage with Spring Data JPA public enum Status { ACTIVE, INACTIVE, PENDING } import javax.persistence.*; @Entity public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; @Enumerated(EnumType.STRING) private Status status; // getters and setters } The @Enumerated annotation is used to specify how the Enum should be persisted in the database. There are two options:\nEnumType.ORDINAL: Stores the Enum as an integer (the ordinal value of the Enum constant). EnumType.STRING: Stores the Enum as a string (the name of the Enum constant).\nEnumType.ORDINAL Pros Database efficiency: Stores enums as integers, which typically use less storage space than strings. Potentially faster queries: Integer comparisons are generally faster than string comparisons. Simpler database representation: The database column is a simple integer type. Cons Fragility to enum order changes: If you add, remove, or reorder enum constants, the ordinal values change, which can corrupt existing data. Less readable in raw database queries: You see numbers instead of meaningful names. Potential for invalid states: If the database contains an integer that doesn’t correspond to any enum constant, it can lead to runtime errors. EnumType.STRING Pros Readability: The database stores the actual names of the enum constants, making raw database queries more understandable. Resilience to enum order changes: Adding or reordering enum constants doesn’t affect existing data. Self-documenting: The database schema itself documents the possible enum values. Safety: It’s harder to accidentally introduce invalid states, as any string not matching an enum constant will be rejected. Cons Less efficient storage: Strings typically use more storage space than integers. Potentially slower queries: String comparisons can be slower than integer comparisons, especially for large datasets. Case sensitivity: By default, the comparison is case-sensitive, which might lead to issues if not handled carefully. Recommendation In most cases, EnumType.STRING is the safer and more maintainable choice, despite the slight performance trade-off. The benefits of readability, safety, and resilience to changes usually outweigh the minor efficiency gains of EnumType.ORDINAL.\nHowever, if you’re dealing with a very large dataset where performance and storage efficiency are critical, and you can guarantee that the enum order will never change, EnumType.ORDINAL might be considered.\nBest Practices Default to EnumType.STRING unless you have a compelling reason not to. If using EnumType.ORDINAL, document it clearly and warn about the dangers of changing the enum order. Consider using a custom UserType for more complex enum persistence scenarios. If using EnumType.STRING, be aware of case sensitivity in your database queries.\n","date":"2024-07-14T00:00:00Z","image":"https://dyhes.github.io/covers/cover19.png","permalink":"https://dyhes.github.io/p/javaenum/","title":"【Java】Enum"},{"content":"@Query @Query is an annotation provided by Spring Data JPA. It allows you to define custom queries using either JPQL (Java Persistence Query Language) or native SQL. This annotation is typically used on repository method declarations to specify the query that should be executed when the method is called.\nNative Query While @Query typically uses JPQL, you can also use native SQL by setting the nativeQuery attribute to true.\n@Query(value = \u0026#34;SELECT * FROM users WHERE status = ?1\u0026#34;, nativeQuery = true) List\u0026lt;User\u0026gt; findUsersByStatus(int status); Positional Parameters public interface UserRepository extends JpaRepository\u0026lt;User, Long\u0026gt; { @Query(\u0026#34;SELECT u FROM User u WHERE u.email = ?1\u0026#34;) User findByEmail(String email); } Named Parameters Instead of positional parameters (?1, ?2), you can use named parameters:\n@Query(\u0026#34;SELECT u FROM User u WHERE u.status = :status AND u.name LIKE :nameLike\u0026#34;) List\u0026lt;User\u0026gt; findUsersByStatusAndNameLike(@Param(\u0026#34;status\u0026#34;) int status, @Param(\u0026#34;nameLike\u0026#34;) String nameLike); Modifying Queries For update or delete operations, use the @Modifying annotation along with @Query (mandatory)\n@Modifying @Query(\u0026#34;UPDATE User u SET u.status = :status WHERE u.lastLoginDate \u0026lt; :date\u0026#34;) int updateUserStatusByLastLoginDate(@Param(\u0026#34;status\u0026#34;) int status, @Param(\u0026#34;date\u0026#34;) LocalDate date); JPQL (Java Persistence Query Language) JPQL is a query language similar to SQL, but it operates on JPA entity objects rather than database tables. It\u0026rsquo;s used to define queries against entities to search for and retrieve entity objects. JPQL is database-independent, which means you can write queries that work across different database systems.\nJOIN In database terms, a JOIN is an operation that combines rows from two or more tables based on a related column between them. In the context of JPQL, a JOIN allows you to fetch associated entities based on their relationships.\nType Entity Definition @Entity public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String email; private String name; @OneToMany(mappedBy = \u0026#34;user\u0026#34;) private List\u0026lt;Order\u0026gt; orders; // getters and setters } @Entity public class Order { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String product; private Integer quantity; @ManyToOne @JoinColumn(name = \u0026#34;user_id\u0026#34;) private User user; // getters and setters } Inner Join Retrieves records that have matching values in both entities.\nIn JPQL (Java Persistence Query Language), the keyword JOIN by itself defaults to an INNER JOIN.\npublic interface UserRepository extends JpaRepository\u0026lt;User, Long\u0026gt; { @Query(\u0026#34;SELECT u FROM User u INNER JOIN u.orders o WHERE o.product = :product\u0026#34;) // equals @Query(\u0026#34;SELECT u FROM User u JOIN u.orders o WHERE o.product = :product\u0026#34;) List\u0026lt;User\u0026gt; findUsersByProduct(@Param(\u0026#34;product\u0026#34;) String product); } Left (Outer) Join Retrieves all records from the left entity and the matched records from the right entity. If no match is found, NULL values are returned for columns from the right entity.\npublic interface UserRepository extends JpaRepository\u0026lt;User, Long\u0026gt; { @Query(\u0026#34;SELECT u FROM User u LEFT JOIN u.orders o WHERE u.name = :name\u0026#34;) List\u0026lt;User\u0026gt; findUsersWithOrders(@Param(\u0026#34;name\u0026#34;) String name); } Fetch Join An optimization technique to eagerly fetch associated entities in a single query.\npublic interface UserRepository extends JpaRepository\u0026lt;User, Long\u0026gt; { @Query(\u0026#34;SELECT u FROM User u LEFT JOIN FETCH u.orders WHERE u.name = :name\u0026#34;) List\u0026lt;User\u0026gt; findUsersWithOrdersFetch(@Param(\u0026#34;name\u0026#34;) String name); } Right (Outer) Join Retrieves all records from the right entity and the matched records from the left entity. If no match is found, NULL values are returned for columns from the left entity. (Less commonly used in JPQL)\n@Query(\u0026#34;SELECT o FROM Order o RIGHT JOIN o.user u\u0026#34;) List\u0026lt;Order\u0026gt; findAllOrdersWithUsers(); Full (Outer) Join Retrieves records when there is a match in one of the entities. (Not supported in JPQL; you would typically use a combination of left and right joins to achieve this effect)\nImplicit Join In JPQL, you can use implicit joins by simply referencing related entities in the WHERE clause.\n@Query(\u0026#34;SELECT u FROM User u WHERE u.address.city = :city\u0026#34;) List\u0026lt;User\u0026gt; findUsersByCity(@Param(\u0026#34;city\u0026#34;) String city); @Query(\u0026#34;SELECT u FROM User u JOIN u.address a WHERE a.city = :city\u0026#34;) List\u0026lt;User\u0026gt; findUsersByCity(@Param(\u0026#34;city\u0026#34;) String city); Differences In general, implicit joins do not inherently have performance issues compared to explicit joins. The database optimizer typically treats them the same way. However, there are some important considerations:\nQuery Optimization: Modern database management systems (DBMS) are quite sophisticated in their query optimization. They often transform implicit joins into explicit joins during the query execution plan generation. This means that in many cases, there\u0026rsquo;s no performance difference between implicit and explicit joins. Readability and Maintainability: While not strictly a performance issue, explicit joins are often considered more readable and maintainable. This can indirectly affect performance if unclear queries lead to misunderstandings and suboptimal database usage. Complex Queries: In more complex scenarios, especially involving multiple joins or outer joins, explicit joins can be clearer and might help the optimizer make better decisions. This is more about query complexity than implicit vs. explicit syntax. Potential Pitfalls of implicit join: N+1 Query Problem: This can occur with lazy loading and implicit joins, but it\u0026rsquo;s not due to the implicit join itself. It\u0026rsquo;s more about how the ORM (Object-Relational Mapping) handles lazy loading. Cartesian Products: In rare cases, implicit joins might lead to unintended cartesian products, especially in complex queries. Explicit joins make the intentions clearer. Performance Considerations JOINs can be expensive operations, especially on large tables. Proper indexing on join columns is crucial for performance. Be cautious with JOIN FETCHes on collections, as they can lead to cartesian products and duplicate results. Use DISTINCT when necessary. ","date":"2024-07-14T00:00:00Z","image":"https://dyhes.github.io/covers/cover19.png","permalink":"https://dyhes.github.io/p/spring-data-jpa@query-and-jpql/","title":"【Spring Data JPA】@Query and JPQL"},{"content":"Pageable Pageable is an interface in Spring Data JPA that\u0026rsquo;s used for pagination and sorting when working with the PagingAndSortingRepository. It allows you to retrieve data from a database in smaller chunks (pages) and specify sorting criteria.\nCreation Creating Pageable objects:\nUse PageRequest.of() static factory methods public Page\u0026lt;User\u0026gt; getUsersPage(int page, int size) { Pageable pageable = PageRequest.of(page, size, Sort.by(\u0026#34;lastName\u0026#34;).ascending()); return userRepository.findAll(pageable); } Performance Considerations Pagination can significantly improve performance for large datasets. Be cautious with offset-based pagination (which Pageable uses) for very large datasets, as it can become inefficient. For large datasets, consider using indexed columns for sorting and filtering. Be cautious with complex joins in paginated queries, as they can impact performance. Method Signature The method should return a Page object. Include a Pageable parameter in the method signature. Sorting The Pageable object includes sorting information. Spring Data adds ORDER BY clauses based on the Sort specification in the Pageable object. The sorting specified in the Pageable object overrides any ORDER BY clause in your query. Native Queries The query can be in JPQL (Java Persistence Query Language) or native SQL. When using native SQL queries, you need to provide a separate count query for pagination. Use the countQuery attribute of the @Query annotation for this. @Query(value = \u0026#34;SELECT * FROM users u WHERE u.created_date \u0026gt; :date\u0026#34;, countQuery = \u0026#34;SELECT count(*) FROM users u WHERE u.created_date \u0026gt; :date\u0026#34;, nativeQuery = true) Page\u0026lt;User\u0026gt; findUsersCreatedAfter(@Param(\u0026#34;date\u0026#34;) Date date, Pageable pageable); Sort Sort is a class in Spring Data that represents sorting parameters. It\u0026rsquo;s used to specify the order in which data should be returned from a query.\nCreation Use static factory methods like Sort.by() to create Sort instances.\nSort sortByName = Sort.by(\u0026#34;name\u0026#34;); Direction Default direction is ascending. Use Direction.ASC or Direction.DESC to specify direction. Sort sortByAgeDesc = Sort.by(Direction.DESC, \u0026#34;age\u0026#34;); Order The Order class represents a single sort criterion (property + direction). Multiple Order objects can be combined in a Sort. Sort complexSort = Sort.by( Order.asc(\u0026#34;department\u0026#34;), Order.desc(\u0026#34;salary\u0026#34;) ); Method Chaining Sort provides a fluent API for method chaining. Use and() to combine multiple sort criteria. Sort chainedSort = Sort.by(\u0026#34;department\u0026#34;).ascending() .and(Sort.by(\u0026#34;salary\u0026#34;).descending()); Null Handling Null handling:\nYou can specify null handling with nullsFirst() or nullsLast(). Sort sort = Sort.by( Order.asc(\u0026#34;department\u0026#34;).nullsLast(), Order.desc(\u0026#34;salary\u0026#34;).nullsFirst() ); Ignoring case Use ignoreCase() for case-insensitive sorting. ⠀Typed Sort For type-safe sorting, use Sort.sort(Class). Example import org.springframework.data.domain.Sort; import org.springframework.data.domain.Sort.Direction; import org.springframework.data.domain.Sort.Order; public class SortExample { public void demonstrateSort() { // Sorting by multiple properties Sort sortByLastNameThenFirstName =Sort.by(\u0026#34;lastName\u0026#34;).and(Sort.by(\u0026#34;firstName\u0026#34;)); // Combining sorts Sort combinedSort = sortByName.and(sortByAgeDesc); } } ","date":"2024-07-14T00:00:00Z","permalink":"https://dyhes.github.io/p/spring-data-jpapageable-and-sort/","title":"【Spring Data JPA】Pageable and Sort"},{"content":"git filter-branch --env-filter \u0026#39; CORRECT_NAME=\u0026#34;dyes\u0026#34; CORRECT_EMAIL=\u0026#34;1325574784@qq.com\u0026#34; if [ \u0026#34;$GIT_COMMITTER_EMAIL\u0026#34; != \u0026#34;$CORRECT_EMAIL\u0026#34; ] then export GIT_COMMITTER_NAME=\u0026#34;$CORRECT_NAME\u0026#34; export GIT_COMMITTER_EMAIL=\u0026#34;$CORRECT_EMAIL\u0026#34; fi if [ \u0026#34;$GIT_AUTHOR_EMAIL\u0026#34; != \u0026#34;$CORRECT_EMAIL\u0026#34; ] then export GIT_AUTHOR_NAME=\u0026#34;$CORRECT_NAME\u0026#34; export GIT_AUTHOR_EMAIL=\u0026#34;$CORRECT_EMAIL\u0026#34; fi \u0026#39; --tag-name-filter cat -- --branches --tags 然后执行\ngit push --force --tags origin \u0026#39;refs/heads/*\u0026#39; ","date":"2024-07-13T00:00:00Z","permalink":"https://dyhes.github.io/p/gitupdate-author-information/","title":"【Git】Update Author Information"},{"content":"example import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.*; import org.springframework.web.multipart.MultipartFile; import java.io.IOException; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; import java.util.UUID; @RestController @RequestMapping(\u0026#34;/api/users\u0026#34;) public class UserController { private static final String AVATAR_UPLOAD_DIR = \u0026#34;uploads/avatars/\u0026#34;; @PostMapping(\u0026#34;/register\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; registerUser( @RequestPart(\u0026#34;userInfo\u0026#34;) UserRegistrationDto userInfo, @RequestParam(value = \u0026#34;avatar\u0026#34;, required = false) MultipartFile avatarFile) { // Generate a unique user ID String userId = UUID.randomUUID().toString(); // Process user information System.out.println(\u0026#34;Registering user: \u0026#34; + userInfo.getUsername()); System.out.println(\u0026#34;Email: \u0026#34; + userInfo.getEmail()); // Here you would typically save the user information to a database // Process avatar if provided String avatarUrl = null; if (avatarFile != null \u0026amp;\u0026amp; !avatarFile.isEmpty()) { avatarUrl = saveAvatar(avatarFile, userId); } // Construct response String response = \u0026#34;User registered successfully. \u0026#34; + \u0026#34;Username: \u0026#34; + userInfo.getUsername() + \u0026#34;, Email: \u0026#34; + userInfo.getEmail(); if (avatarUrl != null) { response += \u0026#34;, Avatar URL: \u0026#34; + avatarUrl; } return ResponseEntity.ok(response); } private String saveAvatar(MultipartFile file, String userId) { try { String fileName = userId + \u0026#34;_\u0026#34; + file.getOriginalFilename(); Path path = Paths.get(AVATAR_UPLOAD_DIR + fileName); Files.write(path, file.getBytes()); return \u0026#34;/avatars/\u0026#34; + fileName; // Return the relative URL } catch (IOException e) { e.printStackTrace(); return null; } } } class UserRegistrationDto { private String username; private String email; private String password; // Note: In a real application, never store passwords in plain text // Getters and setters } We use @RequestPart for userInfo because it\u0026rsquo;s a complex object (JSON data) that needs to be deserialized into a UserRegistrationDto object. We use @RequestParam for avatarFile because it\u0026rsquo;s a simple file upload. While we could use @RequestPart here as well, @RequestParam is sufficient and clearly indicates that it\u0026rsquo;s an optional parameter.\n@RequestPart @RequestParam Typically used for simple values like strings, numbers, or booleans. By default, parameters are required. You can make them optional by setting required = false.\nFor GET requests, it looks at query parameters. For requests with application/x-www-form-urlencoded, it looks at the form data in the request body. For requests with multipart/form-data, it can look at both the URL and the parts of the multipart request. It can even extract data from the query string for POST requests, which is occasionally useful. compare with @PathVariable @PathVariable Used to handle template variables in the request URI. Part of the path itself, not a query string. Typically used for mandatory parameters that identify a resource. Cannot have default values (if the value is not in the URL, the route won\u0026rsquo;t match). @GetMapping(\u0026#34;/users/{id}\u0026#34;) public User getUser(@PathVariable Long id) { // Implementation } @Request Body For application/json or application/xml\napplication/x-www-form-urlencoded and multipart/form-data application/x-www-form-urlencoded This is the default content type for HTML form submissions and is widely used for sending simple data.\nKey characteristics Data is encoded in key-value pairs Keys and values are URL-encoded (spaces become \u0026lsquo;+\u0026rsquo; or \u0026lsquo;%20\u0026rsquo;, special characters are percent-encoded) Pairs are separated by \u0026lsquo;\u0026amp;\u0026rsquo; Pros Simple and widely supported Efficient for small amounts of text data Easy to generate and parse ⠀Cons Not suitable for sending large amounts of data Not efficient for sending binary data (like files) Can become unwieldy with complex data structures multipart/form-data This content type is used when the form includes files, non-ASCII data, or when you need to send binary data.\nKey characteristics The body is divided into separate parts, each representing a form field or file Each part can have its own content type Parts are separated by a boundary delimiter example --boundary Content-Disposition: form-data; name=\u0026#34;name\u0026#34; John Doe --boundary Content-Disposition: form-data; name=\u0026#34;file\u0026#34;; filename=\u0026#34;example.txt\u0026#34; Content-Type: text/plain [File content goes here] --boundary-- Pros Can handle file uploads efficiently Supports mixed data types (text fields and files) in a single request Better for sending large amounts of data or binary data Each part can have its own content type ⠀Cons More complex to parse and generate Slightly larger overhead due to boundaries and headers Get Method HTTP Specification According to the HTTP/1.1 specification (RFC 7231), a GET request can have a body. However, the specification also states that a GET request with a body has no defined semantics, meaning servers are not required to process or even acknowledge the body.\nReal-world Practice In practice, while it\u0026rsquo;s technically possible to send a body with a GET request, it\u0026rsquo;s generally discouraged and often not supported for several reasons:\nMany servers, clients, and proxies ignore the body of a GET request. Some implementations might reject GET requests with a body. It goes against the intended use of GET as defined in the HTTP specification. RESTful API Design In RESTful API design, GET requests are meant to be safe and idempotent. They should retrieve resources without modifying server state. Including a body in a GET request could imply that the request is intended to modify something, which contradicts these principles.\n","date":"2024-07-13T00:00:00Z","permalink":"https://dyhes.github.io/p/spring-bootuploaded-image-processing/","title":"【Spring Boot】Uploaded Image Processing"},{"content":"Inheritance type InheritanceType.SINGLE_TABLE Pros Simplest to implement and typically offers the best performance for polymorphic queries. No join operations needed when querying the base class. Easy to add new subclasses without schema changes. Cons All columns for all subclasses are in one table, which can lead to many nullable columns. The table can become very wide if there are many subclasses or attributes. Potential for wasted space in the database. InheritanceType.TABLE_PER_CLASS Pros Each entity has its own table, which can be more intuitive. Easy to add new attributes to subclasses without affecting other classes. Good for cases where subclasses have many unique attributes. Cons Polymorphic queries can be inefficient, often requiring UNION operations. Redundant storage of common attributes across tables. Not well supported by all JPA providers. InheritanceType.JOINED Pros Normalized database design with minimal data redundancy. Flexible for adding new subclasses or attributes. Good balance between normalization and performance. Cons Requires joins for polymorphic queries, which can impact performance. Inserts and updates may affect multiple tables. More complex queries compared to SINGLE_TABLE. General consideration SINGLE_TABLE is often the default choice due to its simplicity and performance. JOINED is a good choice when you need a more normalized database structure. TABLE_PER_CLASS can be useful in specific scenarios but is often avoided due to performance concerns with polymorphic queries. The choice often depends on your specific use case, the number of subclasses, how often you perform polymorphic queries, and your database design preferences. ","date":"2024-07-13T00:00:00Z","permalink":"https://dyhes.github.io/p/spring-data-jpapolymophic-query/","title":"【Spring Data JPA】Polymophic Query"},{"content":"java.util.Date You\u0026rsquo;re right to ask about the Date class. The pre-Java 8 java.util.Date class is indeed still available, but it\u0026rsquo;s generally considered outdated for modern Java projects.\nIssues Mutable (not thread-safe) Poorly designed API Doesn\u0026rsquo;t handle time zones well Only millisecond precision Confusing method names (e.g., getYear() returns year since 1900) Modern alternatives Java 8+ java.time package (recommended) Third-party libraries like Joda-Time (for pre-Java 8 projects) Migration java.time provides methods to convert between old and new date/time classes java.time java.time introduced in Java 8, provides a comprehensive and much-improved API for handling dates, times, and durations.\nLocalDate Represents a date without time or time zone.\nLocalTime Represents a time without date or time zone.\nLocalDateTime Combines date and time, without a time zone.\nZonedDateTime Date and time with a time zone.\nZoneId represents a time zone\nInstant Instant represents a point in time on the timeline, typically in UTC (Coordinated Universal Time). It\u0026rsquo;s essentially a timestamp with nanosecond precision.\nPeriod Represents a date-based amount of time.\nDuration Represents a time-based amount of time.\nDateTimeFormatter is used for parsing and formatting date-time objects.\nUsage in Database Recommended classes LocalDate: For date-only fields (e.g., birthdate) LocalDateTime: For date and time without time zone Instant: For timestamps (e.g., created_at, updated_at) ZonedDateTime: If you need to store time zone information Considerations Instant is often preferred for timestamps because it\u0026rsquo;s always in UTC and avoids time zone ambiguities. If using an older version of Hibernate (pre-5.0), you might need additional configuration or converters. Some databases might require specific column definitions. For example, PostgreSQL might need @Column(columnDefinition = \u0026ldquo;TIMESTAMP WITH TIME ZONE\u0026rdquo;) for Instant. Database-specific notes MySQL: LocalDateTime is typically stored as DATETIME, Instant as TIMESTAMP. PostgreSQL: Supports all types well, but ensure your JDBC driver is up to date. Oracle: May require additional configuration for LocalDate and LocalDateTime. Best practices Use Instant for audit fields (created_at, updated_at). Use LocalDateTime for user-entered date-times if time zone isn\u0026rsquo;t important. Always consider time zone implications in your application logic. You can set the following property to ensure proper handling of JDBC time zones: spring.jpa.properties.hibernate.jdbc.time_zone=UTC example import javax.persistence.*; import java.time.Instant; import java.time.LocalDate; import java.time.LocalDateTime; @Entity public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private LocalDate birthDate; private LocalDateTime lastLogin; @Column(columnDefinition = \u0026#34;TIMESTAMP\u0026#34;) private Instant createdAt; // getters and setters } Instant usage Use Instant for:\nSystem-generated timestamps (created_at, updated_at) Event occurrences (lastLogin, orderPlacedAt) Any time you need to record a specific moment and might deal with different time zones Audit field created_at: When a record was created updated_at: When a record was last modified sometimes deleted_at: When a record was soft-deleted Benifits Querying\nEasy to query for records created or updated within a specific time range. Sorting\nStraightforward to sort records by creation or update time. Time zone handling\nWhen displaying to users, you can easily convert Instant to their local time zone. Potential pitfall to avoid\nDon\u0026rsquo;t use LocalDateTime for audit fields unless you\u0026rsquo;re absolutely sure all your servers and databases will always be in the same time zone. Converting for display\nWhen you need to display these times to users ZoneId userZone = ZoneId.of(\u0026#34;America/New_York\u0026#34;); Instant createdAt = entity.getCreatedAt(); ZonedDateTime userTime = ZonedDateTime.ofInstant(createdAt, userZone); Database considerations For MySQL: Use TIMESTAMP column type For PostgreSQL: Use TIMESTAMP WITH TIME ZONE JPA Auditting If you want to use Spring Data JPA\u0026rsquo;s automatic auditing features, such as:\n@CreatedDate @LastModifiedDate @CreatedBy @LastModifiedBy enable JPA Auditing. manual Auditting @Entity public class MyEntity { // other fields @Column(updatable = false) private Instant createdAt; private Instant updatedAt; @PrePersist protected void onCreate() { createdAt = Instant.now(); updatedAt = Instant.now(); } @PreUpdate protected void onUpdate() { updatedAt = Instant.now(); } } example import org.springframework.data.annotation.CreatedDate; import org.springframework.data.annotation.LastModifiedDate; import org.springframework.data.jpa.domain.support.AuditingEntityListener; import javax.persistence.*; import java.time.Instant; @Entity @EntityListeners(AuditingEntityListener.class) public class AuditedEntity { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; @CreatedDate @Column(updatable = false) private Instant createdAt; @LastModifiedDate private Instant updatedAt; // getters and setters } @Configuration @EnableJpaAuditing public class JpaConfig { // other configurations } Other usage It\u0026rsquo;s often a good practice to store times as Instant and convert to appropriate local times when presenting to users.\nThis ensures consistent storage and easier querying, while still allowing flexible display options.\nWhile Instant is great for many scenarios, there are cases where other types might be more appropriate:\na) LocalDate: When you only need the date without time (e.g., birthDate, holidayDate). b) LocalDateTime: When you need date and time, but the time zone is implicit or unnecessary. c) ZonedDateTime: When you need to preserve the specific time zone information. ","date":"2024-07-12T00:00:00Z","image":"https://dyhes.github.io/covers/cover19.png","permalink":"https://dyhes.github.io/p/javadates-and-times/","title":"【Java】Dates and Times"},{"content":"Oracle has moved to a new release model since Java 9:\nThere are new Java releases every six months. Every two (three before java17) years, the September release is designated as an LTS release. Non-LTS releases are supported only until the next release. Java 1.0 (1996) First public release Java 1.1 (1997) Inner classes JavaBeans JDBC RMI (Remote Method Invocation) Java 1.2 (1998) Collections framework Swing GUI JIT (Just-In-Time) compiler Java 1.3 (2000) HotSpot JVM JNDI (Java Naming and Directory Interface) Java 1.4 (2002) Assert keyword Regular expressions Logging API NIO (New Input/Output) Java 5 (2004) Generics Annotations Autoboxing/Unboxing Enumerations Varargs Enhanced for loop Java 6 (2006) Scripting language support JDBC 4.0 Java Compiler API Java 7 (2011) Try-with-resources Diamond operator Switch with Strings NIO 2.0 Java 8 (2014) - LTS Lambda expressions Stream API Optional class Default methods in interfaces New Date and Time API Java 9 (2017) Module system (Project Jigsaw) JShell (REPL) Private methods in interfaces Java 10 (2018) Local variable type inference (var keyword) Java 11 (2018) - LTS HTTP Client API Launch single-file source-code programs Java 12 (2019) Switch expressions (preview) Java 13 (2019) Text blocks (preview) Java 14 (2020) Records (preview) Pattern matching for instanceof (preview) Java 15 (2020) Sealed classes (preview) Hidden classes Java 16 (2021) Pattern matching for instanceof (finalized) Records (finalized) Java 17 (2021) - LTS Sealed classes (finalized) Pattern matching for switch (preview) Java 18 (2022) Simple web server Code snippets in Java API documentation Java 19 (2022) Virtual threads (preview) Pattern matching for switch (second preview) Java 20 (2023) Scoped values (incubator) Record patterns (second preview) Java 21(2023) - LTS Virtual Threads (finalized) Pattern Matching for Switch (finalized) Record Patterns (finalized) Sequenced Collections String Templates (preview) Unnamed Classes and Instance Main Methods (preview) Unnamed Patterns and Variables (preview) Scoped Values (preview) Foreign Function \u0026amp; Memory API (preview) Structured Concurrency (preview) Vector API (incubator) ","date":"2024-07-12T00:00:00Z","image":"https://dyhes.github.io/covers/cover9.png","permalink":"https://dyhes.github.io/p/javareleases/","title":"【Java】Releases"},{"content":"深入了解了@ManyToMany, @ManyToOne, @OneToMany, @OneToOne 这几个注解以及java中的日期时间及其在数据库里的使用\n定义了Comment, Space 等 model\n@Id The @Id annotation in JPA (Java Persistence API) is used to mark a field as the primary key of an entity.\nBy default:\nNullable: @Id fields are generally not nullable. Primary keys are used to uniquely identify each record in a database table, so they must have a value. However, the exact behavior can depend on the underlying database and the specific JPA implementation. Updatable: @Id fields are typically not updatable after they are set. Once an entity is persisted with a primary key value, that value should not change for the lifetime of the entity. Changing the ID of an existing entity can lead to data integrity issues and is generally discouraged. It’s important to note:\nWhile the @Id itself doesn’t have properties for nullable or updatable, you can use additional annotations or configurations to modify the behavior: You can use @Column(nullable = false) to explicitly specify that the field cannot be null (though this is usually the default for ID fields). You can use @Column(updatable = false) to explicitly specify that the field cannot be updated after initial persistence. The exact behavior can also depend on how the ID is generated. For example, if you’re using @GeneratedValue for automatic ID generation, the field will initially be null until the entity is persisted, at which point the database or JPA provider will generate and set the ID value. Some JPA implementations might allow updates to ID fields in certain circumstances, but this is not standard behavior and should be avoided in most cases. Set or List When deciding whether to use a Set or a List for @ManyToMany and @OneToMany relationships in JPA, there are several factors to consider, including the uniqueness requirement, ordering, and performance. Here is a breakdown of when to use each:\nWhen to Use Set Uniqueness Set does not allow duplicate elements. If you want to ensure that the collection of related entities is unique, a Set is appropriate. No Ordering Set does not maintain the order of elements. If the order of the related entities does not matter, you can use a Set. Performance In general, Set operations like add, remove, and contains are faster than List operations because Set uses hashing or other mechanisms to enforce uniqueness. When to Use List Ordering List maintains the order of elements. If the order of the related entities matters, or you need to preserve the order in which elements are inserted, you should use a List. Duplicates List allows duplicate elements. If there is a scenario where duplicates are meaningful or required, then List is the appropriate choice. Indexed Access List allows indexed access to elements. If you need to frequently access elements by their index, a List would be more suitable. Join Table Entity Check the corresponding Independent post\nDates in Java Check the corresponding Independent post\nConfusion about FetchType.LAZY Is the primary key of the referenced entity can be easily accessed even when the fetch is LAZY in @ManyToOne and @OneToOne of spring data jpa?\nYes, the primary key of the referenced entity can still be easily accessed even when the fetch type is set to FetchType.LAZY in @ManyToOne and @OneToOne relationships in Spring Data JPA.\nWhen you configure a relationship with FetchType.LAZY, it means that the related entity is not immediately loaded from the database when the parent entity is fetched. Instead, it is loaded on demand when you actually try to access the relationship.\nHowever, accessing the primary key of the referenced entity does not require fetching the entire entity. The primary key value is typically stored as a foreign key in the parent entity, and you can access this foreign key directly without triggering a lazy load of the related entity.\nIs it the same for @ManyToMany and @OneToMany?\nGPT-4o:\nNo.\nIn a @ManyToMany relationship, a join table is typically used to maintain the associations between the entities. When the fetch type is LAZY, the list of related entities is not loaded until you explicitly access it.\nIn a @ManyToMany relationship, The foreign key parent_id is stored in the Child entity and not directly accessible from the Parent entity without loading the children.\nIn both cases, there isn’t a straightforward way to access the primary keys of the related entities without triggering a lazy load.\nEntity Object Extension import javax.persistence.MappedSuperclass; import javax.persistence.Id; import javax.persistence.GeneratedValue; import javax.persistence.GenerationType; import javax.persistence.Column; @MappedSuperclass public abstract class BaseEntity { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(name = \u0026#34;created_at\u0026#34;) private LocalDateTime createdAt; @Column(name = \u0026#34;updated_at\u0026#34;) private LocalDateTime updatedAt; // Getters and Setters } Confusions about database In most cases, a simple COUNT() will be more efficient than retrieving detailed columns. the columns queried can have a significant impact on query time. SELECT * is often slower than selecting specific columns, especially in large tables. Smaller data types (e.g., INT) are typically faster to retrieve than larger ones (e.g., TEXT or BLOB). ","date":"2024-07-12T00:00:00Z","permalink":"https://dyhes.github.io/p/postopia-dev-logday-5/","title":"【Postopia Dev Log】Day 5"},{"content":"实现了创建空间和加入空间\n调试接口的时候报：\nfailed to lazily initialize a collection of role: com.heslin.postopia.model.User.spaces: could not initialize proxy - no Session\n检查之后发现 UsernamePasswordAuthenticationToken 在处理过程中会调用principal 的 toString方法，导致了这个错误，自己重载User.toString 之后恢复正常。\nPolymorphic Query Check the corresponding post\nJava Enum Check the corresponding post\nPageable and Sort Check the corresponding post\n@Query and JPQL Check the corresponding post\n","date":"2024-07-12T00:00:00Z","permalink":"https://dyhes.github.io/p/postopia-dev-logday-6/","title":"【Postopia Dev Log】Day 6"},{"content":"Types ALL: Applies all cascade types PERSIST: Cascades the persist (create) operation MERGE: Cascades the merge (update) operation REMOVE: Cascades the remove (delete) operation REFRESH: Cascades the refresh operation DETACH: Cascades the detach operation Common use cases CascadeType.PERSIST: When you save a parent entity, all its child entities are also saved. CascadeType.REMOVE: When you delete a parent entity, all its child entities are also deleted. CascadeType.ALL: Applies all cascade types, useful when you want full propagation of operations. Benefits Simplifies entity management by automating related entity operations Helps maintain data integrity and consistency Reduces boilerplate code for handling related entities Best practices Avoid using CascadeType.ALL by default\nDon\u0026rsquo;t automatically apply CascadeType.ALL to every @OneToMany relationship Can lead to unexpected behavior and potential issues Use specific cascade types Choose cascade types that match your specific needs: CascadeType.PERSIST: For saving related entities CascadeType.REMOVE: For deleting related entities CascadeType.MERGE: For updating related entities Be cautious with CascadeType.REMOVE Can lead to unintended deletions Consider using orphanRemoval=true instead for more controlled deletions Avoid cascades for shared references Don\u0026rsquo;t use cascades when child entities can exist independently or be shared Example: User (parent) and Address (child) where addresses might be shared Consider performance implications Cascading operations can impact performance, especially with large datasets Use judiciously and test thoroughly Conversation with Claude 3.5 Can i understand cascade operations like this: it is only necessary when i need to update entity from the non-owning part (namely the collection field holder, the one part in a @OneToMany relationship)?\nYour understanding is on the right track, but it\u0026rsquo;s not entirely accurate.\nCascade operations are not only necessary when updating an entity from the non-owning side (the collection field holder in a @OneToMany relationship). They are useful in various scenarios, regardless of which side of the relationship you\u0026rsquo;re operating on.\nCascade from parent to children: In a @OneToMany relationship, you might want to cascade operations from the parent (the \u0026ldquo;one\u0026rdquo; side) to the children (the \u0026ldquo;many\u0026rdquo; side). This is common when the child entities\u0026rsquo; lifecycle is dependent on the parent.\nCascade from child to parent: In some cases, you might want to cascade operations from the child to the parent. This is less common but can be useful in certain scenarios.\nBidirectional relationships: In bidirectional relationships, you can define cascades on both sides, although this requires careful consideration to avoid unintended consequences.\n","date":"2024-07-12T00:00:00Z","image":"https://dyhes.github.io/covers/cover7.png","permalink":"https://dyhes.github.io/p/spring-data-jpacascade-operations/","title":"【Spring Data JPA】Cascade Operations"},{"content":"Composite key @Entity public class BookAuthor { @EmbeddedId private BookAuthorId id; @ManyToOne @MapsId(\u0026#34;bookId\u0026#34;) private Book book; @ManyToOne @MapsId(\u0026#34;authorId\u0026#34;) private Author author; // Other fields, getters, setters... } @Embeddable public class BookAuthorId implements Serializable { private Long bookId; private Long authorId; // Constructors, equals, hashCode... } Pros Naturally represents the relationship between entities. Ensures data integrity at the database level. Can be more space-efficient in some cases. Useful when the combination of fields has a natural, real-world uniqueness. Cons Can be more complex to work with in queries and code. May lead to longer primary keys, which can impact performance in large tables. Less flexible if the relationship structure changes. Separate primary key @Entity public class BookAuthor { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @ManyToOne private Book book; @ManyToOne private Author author; // Other fields, getters, setters... } Pros Simpler to work with in code and queries. More flexible if relationships change. Can improve performance in some scenarios, especially with large datasets. Easier to reference from other tables. Cons Requires an additional column in the table. May not represent the natural relationship as clearly. Consideration Consider using a Composite Key The combination of foreign keys naturally and uniquely identifies the relationship. You want to enforce referential integrity at the database level. The relationship is stable and unlikely to change. You’re working with a relatively small dataset. Consider using a Separate Primary Key 1 You need more flexibility in your data model. 2 You’re working with large datasets where query performance is crucial. 3 You anticipate changes in the relationship structure. 4 You want to simplify your code and queries. 5 You need to reference this join table from other entities. Additional Considerations Database Performance: Test both approaches with your expected data volume to see which performs better. ORM Tool: Some ORM tools work better with one approach over the other. Team Preference: Consider what your team is more comfortable working with. Existing Patterns: If your project already uses one approach consistently, it might be best to stick with it for consistency. In many modern applications, using a separate primary key is often preferred due to its simplicity and flexibility. However, there are still valid use cases for composite keys, especially in systems where the relationship itself has significant business meaning.\n","date":"2024-07-12T00:00:00Z","image":"https://dyhes.github.io/covers/cover13.png","permalink":"https://dyhes.github.io/p/spring-data-jpajoin-table-entity/","title":"【Spring Data JPA】Join Table Entity"},{"content":"@OneToMany This annotation is used to define a one-to-many relationship between two entities. It’s typically used on the “one” side of the relationship. public class Department { @OneToMany(mappedBy = \u0026#34;department\u0026#34;, cascade = CascadeType.ALL ) private List\u0026lt;Employee\u0026gt; employees; } @ManyToOne This annotation is used to define a many-to-one relationship between two entities. It’s typically used on the “many” side of the relationship. public class Employee { @ManyToOne @JoinColumn(name = \u0026#34;department_id\u0026#34;) private Department department; } Often, @OneToMany and @ManyToOne are used together to create a bidirectional relationship. The @OneToMany side uses mappedBy to indicate the field that owns the relationship. The @ManyToOne side use @JoinColumn to specify the foreign key column.\nCascade operation Cascade operations in ORM frameworks like Hibernate and JPA allow you to automatically apply operations performed on a parent entity to its associated child entities. This feature is particularly useful in managing relationships between entities and can significantly simplify database operations.\nTypes PERSIST: Saves the child entity when the parent is saved. MERGE: Updates the child entity when the parent is updated. REMOVE: Deletes the child entity when the parent is deleted. REFRESH: Refreshes the child entity when the parent is refreshed. DETACH: Detaches the child entity when the parent is detached (from the persistence context). ALL: Applies all cascade types (PERSIST, MERGE, REMOVE, REFRESH, DETACH). Usage You can specify cascade operations in the @OneToMany, @ManyToOne, @OneToOne, or @ManyToMany annotations:\n@OneToMany(cascade = CascadeType.ALL) private List\u0026lt;ChildEntity\u0026gt; children; @OneToMany(cascade = {CascadeType.PERSIST, CascadeType.MERGE}) private List\u0026lt;ChildEntity\u0026gt; children; Considerations Orphan Removal: Used in conjunction with cascading, orphanRemoval = true will remove child entities that are no longer referenced by the parent. Bi-directional Relationships: Be cautious with cascade operations in bi-directional relationships to avoid unintended side effects. Performance: Cascading can impact performance, especially with large datasets. Use it judiciously ","date":"2024-07-11T00:00:00Z","image":"https://dyhes.github.io/covers/cover19.png","permalink":"https://dyhes.github.io/p/many-one-relationship/","title":"Many-One Relationship"},{"content":"A one-to-one relationship means that each instance of an entity is associated with precisely one instance of another entity. This relationship is bidirectional or unidirectional:\nUnidirectional One-to-One: One entity has a reference to another entity, but not vice versa. Bidirectional One-to-One: Both entities have references to each other. In the context of object-relational mapping (ORM) in Java, specifically when using Java Persistence API (JPA) or Hibernate, the @OneToOne annotation is used to define a one-to-one relationship between two entities. // unidirectional @Entity public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String username; @OneToOne(cascade = CascadeType.ALL) @JoinColumn(name = \u0026#34;profile_id\u0026#34;, referencedColumnName = \u0026#34;id\u0026#34;) private UserProfile userProfile; // Getters and setters } @Entity public class UserProfile { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String address; // Getters and setters } //bidirectional @Entity public class User { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String username; @OneToOne(cascade = CascadeType.ALL, mappedBy = \u0026#34;user\u0026#34;) private UserProfile userProfile; // Getters and setters } @Entity public class UserProfile { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String address; @OneToOne @JoinColumn(name = \u0026#34;user_id\u0026#34;) private User user; // Getters and setters } ","date":"2024-07-11T00:00:00Z","image":"https://dyhes.github.io/covers/cover21.jpg","permalink":"https://dyhes.github.io/p/one-one-relationship/","title":"One-One Relationship"},{"content":"提代码的时候发现change里有两个名为.DB_Store文件，查了一下是什么东西，记录一下。\nContent generated by GPT-4o:\nThe .DS_Store file is a hidden system file created by macOS (formerly Mac OS X) to store custom attributes of a folder, such as the position of icons, the choice of a background image, and other folder view options. The name stands for “Desktop Services Store.”\nIt is generally a good practice to add .DS_Store to your .gitignore file in Git repositories.\n","date":"2024-07-10T00:00:00Z","image":"https://dyhes.github.io/covers/cover20.jpg","permalink":"https://dyhes.github.io/p/mac.db_store-file/","title":"【Mac】.DB_Store file"},{"content":"忙于构建个人博客，没有什么进展\nKnowledge Java Record Java records were introduced in Java 14 as a preview feature and became a standard feature in Java 16. They provide a compact syntax for declaring classes that are used primarily to store data. Records are designed to reduce boilerplate code and make it easier to create simple, immutable data carriers. Syntax:\npublic record Person(String name, int age) { } Automatic generation:\nA constructor with parameters for all components\nAccessor methods for each component (e.g., name() and age())\nequals() and hashCode() methods\ntoString() method Immutability:\nRecords are implicitly final and cannot be extended.\nThe fields (components) of a record are final by default. Constructors:\nYou can define custom constructors, including compact constructors.\nA compact constructor doesn’t need to explicitly list all fields:\npublic record Person(String name, int age) { public Person { if (age \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;Age cannot be negative\u0026#34;); } } } Additional methods:You can add other methods to a record, just like in a regular class. Limitations:\nRecords cannot extend other classes (they implicitly extend java.lang.Record). They cannot declare instance fields other than the private final fields for the components of the state description. They cannot be abstract. Internationalization Internationalization (i18n) is typically handled on both the front end and the back end, but the approach and responsibilities can vary depending on the specific requirements of your application.\nIn many modern web applications, the front end handles a significant portion of i18n, especially for single-page applications (SPAs) and client-heavy architectures. This approach allows for more responsive user experiences and reduces server load.\nHowever, some aspects of i18n are better suited for the back end, particularly when dealing with sensitive data, complex business logic, or when you need to generate localized content server-side.\nBest practices\nBest practices often involve a combination of both:\nUse front-end i18n libraries for UI elements and client-side formatting. Implement back-end i18n for database content, emails, and API responses. Use content delivery networks (CDNs) or static file hosting for language resource files. Consider using a translation management system (TMS) to streamline the translation process.\n","date":"2024-07-10T00:00:00Z","image":"https://dyhes.github.io/covers/cover6.png","permalink":"https://dyhes.github.io/p/postopia-dev-logday-4/","title":"【Postopia Dev Log】Day 4"},{"content":"@ManyToMany The @ManyToMany annotation is used in Java persistence frameworks, particularly in Java Persistence API (JPA) and Object-Relational Mapping (ORM) tools like Hibernate. It’s used to define a many-to-many relationship between two entities.\nRelationship: It represents a relationship where multiple instances of one entity can be associated with multiple instances of another entity. Database representation: In a relational database, this is typically implemented using a join table that contains foreign keys to both entities. Bidirectional vs Unidirectional: The relationship can be bidirectional (defined on both entities) or unidirectional (defined on only one entity, namely the owning one). Usage: It’s typically used on a collection field in an entity class. Join Table: By default, JPA will create a join table, but you can customize this using the @JoinTable annotation. @Entity @Table(\u0026#34;students\u0026#34;) public class Student { @Id @GeneratedValue private Long id; private String name; @ManyToMany @JoinTable( name = \u0026#34;student_course\u0026#34;, joinColumns = @JoinColumn(name = \u0026#34;student_id\u0026#34;), inverseJoinColumns = @JoinColumn(name = \u0026#34;course_id\u0026#34;) ) private Set\u0026lt;Course\u0026gt; signedCourses; // getters and setters } @Entity @Table(\u0026#34;courses\u0026#34;) public class Course { @Id @GeneratedValue private Long id; private String name; @ManyToMany(mappedBy = \u0026#34;signedCourses\u0026#34;) private Set\u0026lt;Student\u0026gt; students; // getters and setters } In this example:\nA student can enroll in multiple courses, and a course can have multiple students. The @JoinTable annotation specifies the details of the join table. The mappedBy attribute in the Course entity indicates that Student is the owning side of the relationship. When using @ManyToMany, consider these best practices:\nUse Set instead of List to avoid duplicate entries. Be cautious with bidirectional relationships, as they can lead to performance issues if not managed properly. Consider using lazy loading (fetch = FetchType.LAZY) to improve performance. In some cases, it might be better to model the relationship as two one-to-many relationships with an intermediate entity, especially if you need to store additional information about the relationship. mappedBy The mappedBy attribute in a @ManyToMany relationship is used to indicate the non-owning side of a bidirectional relationship. In a @ManyToMany relationship, one side needs to be the owning side, and the other is the non-owning (inverse) side. The owning side is where the @JoinTable is specified (if using a custom join table). The non-owning side uses mappedBy to refer to the property on the owning side. Key Points Only the owning side of the relationship is responsible for updating the join table. Changes made to the non-owning side won’t be reflected in the database unless the owning side is also updated. Benefits Prevents duplicate join tables. Clarifies which side of the relationship is responsible for managing the association. Common Mistake: Forgetting to specify mappedBy on one side, which can lead to two separate join tables being created. Bidirectional Relationship Management: Even though mappedBy is specified, you typically need to update both sides of the relationship in your Java code for consistency. student.getCourses().add(course); course.getStudents().add(student); Database Perspective:\nThe database structure is the same regardless of which side is the owning side. The choice affects how JPA manages the relationship, not the underlying database schema. lazy loading Lazy loading means that an object doesn’t load all of its associated data from the database when it’s first retrieved. Instead, it loads only the data it needs immediately and loads other related data only when it’s specifically requested. In ORM Context: When you fetch an entity from the database, lazy loading allows you to retrieve the entity without immediately loading all of its associated entities or collections. The associated data is loaded only when you try to access it. When to use: Use lazy loading for associations that are not always needed. Use eager loading for associations that are almost always needed with the main entity. Implementation: In JPA, lazy loading is often the default for collection associations (@OneToMany, @ManyToMany). For single-valued associations (@ManyToOne, @OneToOne), eager loading is usually the default. Best Practices: Use lazy loading as the default strategy. Switch to eager loading only when you’re certain that the related data is always needed. ","date":"2024-07-10T00:00:00Z","image":"https://dyhes.github.io/covers/cover20.jpg","permalink":"https://dyhes.github.io/p/many-many-relationship/","title":"Many-Many Relationship"},{"content":"遇到一个好像很有用的网站 Programming \u0026amp; DevOps news, tutorials \u0026amp; tools\n实现了一个BadRequestExceptionHandler\nSpring Security 有点复杂，没找到好的参考资料\n在claude 3.5的帮助下初步实现了基于简单jwt的Authentication\n测试filter的时候不小心调用了两次chain.doFilter导致返回的json数据是重复的两份 Duplicate Json in Spring Boot Response\nKnowledge Keycloak Keycloak is an open-source Identity and Access Management (IAM) solution developed by Red Hat. It provides a comprehensive set of features for managing authentication and authorization in modern applications.\nKey Features 1 Single Sign-On (SSO): Keycloak allows users to authenticate once and access multiple applications without re-entering credentials.\n2 Identity Brokering: It can integrate with external identity providers like Google, Facebook, or other SAML 2.0 or OpenID Connect providers.\n3 User Federation: Keycloak can sync with existing LDAP or Active Directory servers.\n4 Customizable Authentication: Supports various authentication mechanisms including password, OTP, and custom authenticators.\n5 Fine-grained Authorization: Provides role-based access control (RBAC) and attribute-based access control (ABAC).\n6 Admin Console: Offers a user-friendly interface for managing users, roles, clients, and realm settings.\n7 Standard Protocols: Supports OAuth 2.0, OpenID Connect, and SAML 2.0.\n8 Client Adapters: Provides adapters for easy integration with various application stacks.\n9 Themes and Internationalization: Allows customization of login pages and supports multiple languages.\n10 Social Login: Enables users to log in using their social media accounts.\nOauth2 Involved Actors i) Resource Server: The server hosting user-owned resources that are protected by OAuth2. The resource server validates the access-token and serves the protected resources.\nii) Resource Owner: Typically, the user of the application is the resource owner. The resource owner has the ability to grant or deny access to their own data hosted on the resource server.\niii) Authorization Server: The authorization server gets consent from the resource owner and issues access tokens to clients for accessing protected resources hosted by a resource server.\niv) Client: An application making API requests to perform actions on protected resources on behalf of the resource owner. Before it may do so, it must be authorized by the resource owner, and the authorization must be validated by resource server/authorization server. OAuth2 defines two client types, based on their ability to authenticate securely with the authorization server (i.e., ability to maintain the confidentiality of their client credentials):\na)Confidential: Clients capable of maintaining the confidentiality of their credentials. Confidential clients are implemented on a secure server with restricted access to the client credentials (e.g., a web application running on a web server).\nb) Public: Clients incapable of maintaining the confidentiality of their credentials (e.g. an installed native application or a web browser-based application), and incapable of secure client authentication via any other means.\nAuthorization Grant Types To get the access token, the client obtains authorization from the resource owner. The authorization is expressed in the form of an authorization grant, which the client uses to request the access token. OAuth2 defines four standard grant types: authorization code, implicit, resource owner password credentials, and client credentials. It also provides an extension mechanism for defining additional grant types.\ni) Authorization Code Grant: this grant type is optimized for confidential clients (web application server). The authorization code flow does not expose the access token to the resource owner’s browser. Instead, authorization is accomplished using an intermediary “authorization code” that is passed through the browser. This code must be exchanged for an access token before calls can be made to protected APIs.\nii) Implicit Grant: this grant type is suitable for public clients. The implicit grant flow does not accommodate refresh tokens. If the authorization server expires access tokens regularly, your application will need to run through the authorization flow whenever it needs access. In this flow, an access token is immediately returned to the client after a user grants the requested authorization. An intermediate authorization code is not required as it is in the authorization code grant.\niii) Resource Owner Password Credentials: the resource owner password credentials grant type is suitable in cases where the resource owner has a trust relationship with the client, and the resource owner agrees to share hise/her credentials (username, password) with the client. Then the client can use resources from the owner\u0026rsquo;s credentials to get the access token from the authorization server.\niv) Client Credentials: this grant type is suitable when the client itself owns the data and does not need delegated access from a resource owner, or delegated access has already been granted to the application outside of a typical OAuth flow. In this flow, user consent is not involved. The client exchanges his client credentials to get an access token.\nTransactions A database transaction is a sequence of one or more database operations that are treated as a single unit of work. Transactions adhere to the ACID properties:\n1 Atomicity: All operations in a transaction either complete successfully or fail altogether. There\u0026rsquo;s no partial completion.\n2 Consistency: A transaction brings the database from one valid state to another. All rules and constraints must be satisfied.\n3 Isolation: Concurrent execution of transactions results in a state that would be obtained if transactions were executed sequentially.\n4 Durability: Once a transaction has been committed, it will remain so, even in the event of power loss, crashes, or errors.\nIsolation level Isolation levels define how transaction integrity is visible to other users and systems. They provide a trade-off between consistency and performance.\n1. READ UNCOMMITTED Lowest isolation level One transaction may read not-yet-committed changes made by other transactions (\u0026ldquo;dirty reads\u0026rdquo;) Transaction 1: UPDATE Account SET balance = balance - 100 WHERE account_id = \u0026#39;A\u0026#39;; Transaction 2: SELECT balance FROM Account WHERE account_id = \u0026#39;A\u0026#39;; -- May read the updated (uncommitted) balance Transaction 1: ROLLBACK; 2. READ COMMITTED A transaction only sees data committed before the transaction began Prevents dirty reads, but \u0026ldquo;non-repeatable reads\u0026rdquo; can occur Transaction 1: SELECT balance FROM Account WHERE account_id = \u0026#39;A\u0026#39;; -- Reads original balance Transaction 2: UPDATE Account SET balance = balance - 100 WHERE account_id = \u0026#39;A\u0026#39;; Transaction 2: COMMIT; Transaction 1: SELECT balance FROM Account WHERE account_id = \u0026#39;A\u0026#39;; -- Reads new balance 3. REPEATABLE READ Ensures that if a transaction reads a row, it will consistently see the same data for that row until the transaction completes Prevents non-repeatable reads, but \u0026ldquo;phantom reads\u0026rdquo; can occur Transaction 1: SELECT * FROM Account WHERE balance \u0026gt; 1000; Transaction 2: INSERT INTO Account (account_id, balance) VALUES (\u0026#39;C\u0026#39;, 1500); Transaction 2: COMMIT; Transaction 1: SELECT * FROM Account WHERE balance \u0026gt; 1000; -- May see the new account (phantom read) 4. SERIALIZABLE Highest isolation level Transactions are completely isolated from one another Prevents dirty reads, non-repeatable reads, and phantom reads Transaction 1: SELECT * FROM Account WHERE balance \u0026gt; 1000; Transaction 2: INSERT INTO Account (account_id, balance) VALUES (\u0026#39;C\u0026#39;, 1500); -- This will be blocked until Transaction 1 completes Transaction 1: SELECT * FROM Account WHERE balance \u0026gt; 1000; -- Will not see any changes made by other transactions Concurrency Issues 1 Dirty Read: Reading uncommitted data from another transaction.\n2 Non-Repeatable Read: Getting different results when reading the same row twice in the same transaction.\n3 Phantom Read: Getting different results when querying for a range of rows twice in the same transaction.\n⠀Isolation Level Comparison\nIsolation Level Dirty Read Non-Repeatable Read Phantom Read READ UNCOMMITTED Possible Possible Possible READ COMMITTED Prevented Possible Possible REPEATABLE READ Prevented Prevented Possible SERIALIZABLE Prevented Prevented Prevented Higher isolation levels provide more consistency but may reduce concurrency and impact performance.\nMany applications use READ COMMITTED as a default, as it provides a good balance between consistency and performance.\n@Transactional When you annotate a method with @Transactional, Spring will start a transaction before the method executes and commit it after the method completes. If an exception is thrown, the transaction will be rolled back.\nYou can place @Transactional on a method or a class. If placed on a class, all public methods of the class become transactional.(@Transactional only works on public methods) By default, transactions roll back on runtime exceptions.\nYou can customize\n@Transactional(rollbackFor = {CustomException.class}) In Spring Boot applications, transaction management is automatically enabled. In other Spring applications, you need to explicitly enable it with @EnableTransactionManagement. scenarios:\nMultiple Database Operations Read Operations Requiring Consistency … Service Layer Methods It\u0026rsquo;s often a good practice to put @Transactional on service-layer methods rather than repository methods or controllers. Don\u0026rsquo;t use @Transactional for single, simple database operations - the underlying repository methods are often already transactional. Articles Spring REST API + OAuth2 + Angular\nKeycloak Embedded in a Spring Boot Application\nSecure Spring REST With Spring Security and OAuth2\nOAuth 2.0 Beginner\u0026rsquo;s Guide\nOAuth Patterns and Anti-Patterns\n","date":"2024-07-09T00:00:00Z","image":"https://dyhes.github.io/covers/cover14.png","permalink":"https://dyhes.github.io/p/postopia-dev-logday-3/","title":"【Postopia Dev Log】Day 3"},{"content":"考虑到不确定能把系统做到什么程度，暂时不考虑分布式相关事宜\n运行./gradlew bootRun 热加载不知道为什么没有生效，找来找去没找到原因\n想直接用dokcer实现，不熟悉docker和docker compose困难重重\n根据 SpringBoot DevTools Auto Restart and Live Reload 一顿操作之后控制台有反应了，但是没有效果\n根据 LiveReload does not detect changes\n”For a change to be noticeable it has to be compiled. You can learn more about this in the reference documentation. We generally recommend using an IDE to both to run the application and to make changes. It will then compile the changes so that they can be noticed by DevTools.“\n成功解决问题\n进度： 完成注册，热加载\nknowledge UUID, Long ID comparison UUID:\nGlobally unique 128-bit value String representation No sequential ordering Suitable for distributed systems Can be generated without DB interaction Larger storage size (typically 36 characters) ⠀Long ID: Locally unique (per table) 64-bit integer Numeric representation Sequential (if auto-incremented) Simpler and more human-readable Smaller storage size (8 bytes) Typically faster for indexing and joining @GeneratedValue The @GeneratedValue annotation in Spring Boot is used to specify how the primary key should be generated for an entity. Here\u0026rsquo;s a breakdown of its key aspects: Common strategies:\nGenerationType.AUTO (default) GenerationType.IDENTITY Pros: Simple, efficient for single-node systems Cons: Can cause issues in distributed systems or data migrations GenerationType.SEQUENCE Pros: Efficient, works well in high-concurrency environments Cons: Not supported by all databases (e.g., MySQL before 8.0) GenerationType.TABLE Pros: Database-independent, works everywhere Cons: Potentially slower, requires extra table management Lombok Constructor related annotation @NoArgsConstructor @AllArgsConstructor @RequiredArgsConstructor all final and @NonNull fields @Builder used to generate a builder pattern for your class MyClass myClass = MyClass.builder() .name(\u0026#34;John Doe\u0026#34;) .age(30) .address(\u0026#34;123 Main St\u0026#34;) .build(); ./gradlew This part of the command is invoking the Gradle Wrapper (gradlew or gradlew.bat), which is a script that comes bundled with the Gradle build tool. The Gradle Wrapper ensures that the correct version of Gradle is used for the project, regardless of whether or not the user has Gradle installed on their system.\nDockerfile A Dockerfile is a script that contains a series of instructions on how to build a Docker image. Each instruction in a Dockerfile creates a layer in the image, and when you build the image, Docker executes these instructions step by step to produce the final image. FROM Specifies the base image to use for the Docker image you’re building. Every Dockerfile must start with a FROM instruction. FROM ubuntu:20.04\nLABEL Adds metadata to the image, such as a maintainer name or a version. LABEL maintainer=\u0026ldquo;example@example.com\u0026rdquo;\nRUN Executes a command in the shell. It’s often used for installing software packages. RUN apt-get update \u0026amp;\u0026amp; apt-get install -y nginx\nCOPY Copies files or directories from your local filesystem into the Docker image. COPY ./localfile /containerfile\nADD Similar to COPY, but also supports URL sources and automatic unpacking of compressed files. ADD https://example.com/file.tar.gz /tmp/\nWORKDIR Sets the working directory for any subsequent RUN, CMD, ENTRYPOINT, COPY, and ADD instructions. WORKDIR /app\nCMD Provides a command that will be executed when a container is started from the image. Only one CMD instruction is allowed per Dockerfile; if multiple CMD instructions are specified, only the last one takes effect. CMD [\u0026ldquo;nginx\u0026rdquo;, \u0026ldquo;-g\u0026rdquo;, \u0026ldquo;daemon off;\u0026rdquo;]\nENTRYPOINT Sets the default application to be used every time a container is created from the image. Unlike CMD, ENTRYPOINT won’t be overridden by command-line arguments. ENTRYPOINT [\u0026ldquo;nginx\u0026rdquo;, \u0026ldquo;-g\u0026rdquo;, \u0026ldquo;daemon off;\u0026rdquo;]\nENV Sets environment variables. ENV MY_ENV_VAR=my_value\nEXPOSE Informs Docker that the container listens on the specified network ports at runtime. This doesn’t actually publish the port; it’s a form of documentation. EXPOSE 80\nVOLUME Creates a mount point with the specified path and marks it as holding externally mounted volumes from the native host or other containers. VOLUME [\u0026quot;/data\u0026quot;]\nUSER Sets the user name or UID to use when running the image and for any RUN, CMD, and ENTRYPOINT instructions that follow it. USER nginx\na project can have multiple Dockerfiles. e.g. ├── Dockerfile ## Default Dockerfile for production ├── Dockerfile.dev ## Dockerfile for development ├── Dockerfile.test ## Dockerfile for testing\nArticles Using Docker Compose with Spring Boot and PostgreSQL\nSpringBoot DevTools Auto Restart and Live Reload\n","date":"2024-07-07T00:00:00Z","image":"https://dyhes.github.io/covers/cover11.png","permalink":"https://dyhes.github.io/p/postopia-dev-logday-2/","title":"【Postopia Dev Log】Day 2"},{"content":"为了提高自己的全栈开发能力，根据Reddit上某帖子(找不到原贴了)里关于提高代码能力的建议：Reinvent The Wheel, 决定自己开发一个类似Reddit的论坛系统，在 Claude3.5的建议下取名 Postopia。\n考虑用umi max还是umi，感觉umi max虽然功能齐全，但是有点臃肿。最后考虑到配置各种依赖的复杂程度，还是选择umi max。\n用claude 3.5生成了一些代码，但感觉质量不是很高，同时只能提供最基础的html,css,js三件套，利用不了更高层的框架能力，还是仅供参考\nPostopia Claude 3.5:\nHere are some name suggestions for a Reddit alternative:\nReadit Threaddit Postopia Forumly Discussr Postopia is indeed an interesting name choice. Let me break it down for you:\nEtymology:\n\u0026ldquo;Post\u0026rdquo; refers to the act of submitting content or messages, which is a core feature of Reddit-like platforms. \u0026ldquo;topia\u0026rdquo; comes from the Greek word \u0026ldquo;topos\u0026rdquo; meaning place, and is often used to describe an ideal or imaginary place.\nMeaning:\nTogether, Postopia suggests a \u0026ldquo;world of posts\u0026rdquo; or an \u0026ldquo;ideal place for posting.\u0026rdquo;\nConnotations:\nIt evokes a sense of a utopian community built around sharing and discussing content. The name implies a space where posting and engaging with posts is the primary focus.\nRelevance to Reddit alternative:\nLike Reddit, it emphasizes the user-generated content aspect. It suggests a potentially improved or idealized version of a posting platform.\nMemorability:\nThe combination of familiar elements (\u0026ldquo;post\u0026rdquo; and \u0026ldquo;topia\u0026rdquo;) creates a unique and catchy name. It\u0026rsquo;s relatively easy to spell and pronounce.\nDomain availability:\nAs of my last update, postopia.com was likely taken, but variations like getpostopia.com or postopia.io might be available.\nBranding potential:\nThe name lends itself well to visual branding, perhaps incorporating elements that suggest an ideal digital community.\n技术栈 前端 Umi max，node 18.20.3\n后端 Spring Boot 3.3，Java 17\n","date":"2024-07-05T00:00:00Z","image":"https://dyhes.github.io/covers/cover8.png","permalink":"https://dyhes.github.io/p/postopia-dev-logday-1/","title":"【Postopia Dev Log】Day 1"},{"content":"考虑隔离级别\nHard Core Spring Security 没有经验时的登录实现，文档杂乱，不知道如何寻找正确的实现方法\nInspiration w5 如何处理以前没有遇到过的新需求（如给用户发送邮件），需要积累解决问题的能力（实习的时候可以边问边查，做自己的项目只能自己去查，那么如何高效检索并善用AI）,并形成方法论，以便提高能力\nBest Practices 很多问题只有亲身经历才能体会，并寻求 best practices 比如:\n直接更新好，处理可能的异常好还是先查询存在再更新好 是否在Service Layer 使用@AuthenticatedPrincipal service引用多个repository或引用对应repository和其他service Performance Considerations 用户对于帖子或评论的态度（赞同，反对）是否用额外的query查询更好（w4） ","date":"2024-07-05T00:00:00Z","image":"https://dyhes.github.io/covers/cover18.png","permalink":"https://dyhes.github.io/p/postopia-dev-logsummary/","title":"【Postopia Dev Log】Summary"},{"content":"Link\nSpring boot Spring Boot is an open source extension of the Spring Framework designed to simplify the Spring application development.\nSpring Boot was introduced as a subproject under the Spring Framework to empower developers with a fast startup experience and exempt them from most of the configuration hazards.\nIt provides an additional layer between the Spring Framework for the user to simplify certain configuration aspects.\nA Spring Boot web application can be either Servlet-based or reactive type.\nComponents spring-boot\nThis is the primary Spring Boot component that provides support to other components. For example, it contains the SpringApplication class, which contains several static methods to create a standalone Spring Boot application. It also provides support for embedded web servers (e.g., Tomcat) and supports externalized application configurations (e.g., database details of your application), etc.\nspring-boot-autoconfigure\nSpring-boot-starters\nspring-boot-actuator\nIn the context of software, particularly within the Spring Boot framework, “Actuator” refers to a set of tools and features that provide insights into the application’s runtime behavior and health.\nspring-boot-actuator-autoconfigure\nspring-boot-loader\nThis component allows a Spring Boot application to be packaged as a single fat JAR file, including all dependencies and the embedded web servers that can be run standalone. You don’t use this module independently; instead, it is used along with Maven or Gradle plugins\nspring-boot-devtools\nLombok Lombok is a Java library that automatically generates the constructors, getter, setter, toString, and others based on the presence of a few annotations in the plain old Java object (POJO) class.\nPOJO A Plain Old Java Object (POJO) is a simple Java object that does not adhere to any special rules or conventions beyond those required by the Java Language Specification. The term is used to emphasize that these objects are free from the constraints typically imposed by more complex frameworks or libraries.\nRecord Java 14 has introduced the concept of records in the Java language. Records are immutable data classes that require you to specify only the type and name of the fields. The Java compiler can then generate the equals, hashCode, and toString methods. It also generates the private final fields, getter methods, and public constructor.\npom.xml file The pom.xml file is a fundamental component of a Maven-based Java project. Maven is a build automation tool used primarily for Java projects, and pom.xml (Project Object Model) is the configuration file that defines the project’s dependencies, build configuration, and other project-specific settings.\napplication.properties file The application.properties file is a key configuration file used in Spring Boot applications. It allows developers to define various settings and configurations for the application in a simple and readable format. WAR vs EAR In general, to run a web application, you build and package the application components in a WAR or EAR archive file and deploy it into a web (e.g., Apache Tomcat) or application server (e.g., Red Hat JBoss). Spring Boot simplifies this process to a certain degree. It does not enforce you to build a WAR or EAR file of your application. Instead, it lets you run the Spring Boot application like a regular Java application using a conventional main() method.\nJava Bean A JavaBean is a reusable software component that follows specific conventions defined by the JavaBeans specification. JavaBeans are used for encapsulating many objects into a single object (the bean), so they can be passed around as a single bean object.\n1 Default Constructor:\nA JavaBean must have a no-argument (default) constructor. This allows the bean to be easily instantiated.\n2 Properties:\nProperties are private fields that are accessed and modified through public getter and setter methods.\n3 Serializable:\nA JavaBean should implement the Serializable interface to allow its state to be saved and restored.\n4 Accessor Methods:\nGetter and setter methods follow a specific naming convention (getPropertyName for getters and setPropertyName for setters). The “Bean” part of the term is a metaphor borrowed from the agricultural sense of the word, implying something small, lightweight, and reusable.\n@SpringBootApplication @SpringBootApplication annotation is a convenient annotation that consists of three annotations:\n@EnableAutoConfiguration\nprovides the necessary support for Spring Boot to autoconfigure your application based on the JAR dependencies present in the application classpath\n@ComponentScan\nProvides support to scan the packages for Spring components in the application.\nSpring Component A component in Spring is a Java bean that is managed by Spring and annotated with the @Component, @Bean, or specialized component annotations. With the presence of @ComponentScan annotation, the Spring Boot application scans for all components present in the root package and sub-packages under it to manage their lifecycle. The key point to remember with ComponentScan is that the scan starts from a root package and continues to all child packages. Thus, if you have packages that are not in the root or its sub-package, none of those components will be scanned by the component scan.\n@SpringBootConfiguration This annotation indicates that the annotated class provides the Spring Boot application configuration. It is meta-annotated with Spring @Configuration annotation so that the configurations in the annotated class can be found automatically by Spring Boot. Thus, the beans defined in this main class can be autodetected and loaded by Spring. run() method Most of the time, you’ll use the static run() method of SpringApplication to bootstrap and launch your application. Spring Boot performs several activities while it executes the run() method:\n1 Creates an instance of an ApplicationContext based on the libraries present in the classpath.\n2 Registers a CommandLinePropertySource to expose command line arguments as Spring properties.\n3 Refreshes the ApplicationContext created at step 1 to load all singleton beans.\n4 Triggers the ApplicationRunners and CommandRunners configured in the application.\nMost Java applications you develop consist of objects. These objects interact with each other, and there are dependencies among them. To effectively manage object creation and interdependencies, Spring uses the principles of dependency injection (DI). This dependency injection or the inversion of control (IoC) approach lets Spring create the objects (or, more appropriately, the beans in Spring parlance) and inject the dependencies externally. The bean definitions are presented to Spring either through the XML bean definition files (e.g., applicationContext.xml) or through the annotation-based configurations (@Configuration annotation). Spring loads these bean definitions and keeps them available in the Spring IoC container. The ApplicationContext interface acts as the Spring IoC Container. Spring provides a plethora of ApplicationContext implementations based on the application type (Servlet or Reactive application), the bean definition configurations (e.g., to load from classpath or annotation), and so on.\nAlthough @EventListener works well in most circumstances, it does not work for events that are published very early in the application start-up, such as ApplicationStartingEvent and ApplicationEnvironmentPreparedEvent.\nBy default, Spring Boot reads the application.properties or application.yml file from the following locations:\n1 The classpath root\n2 The classpath /config package\n3 The current directory\n4 The /config subdirectory in the current directory\n5 Immediate child directories of the /config subdirectory\nYou can maintain the profile-specific property files with the application-{profile}.properties (or .yml) file. You can activate a profile (e.g., dev or test) using the spring.profiles.active Spring Boot property.\n1 The application properties (properties or the YAML file) packaged inside the application JAR 2 Profile-specific application properties packaged inside the application JAR 3 The application properties (properties or the YAML file) packaged outside the application JAR 4 Profile-specific application properties packaged outside the application JAR\nFollowing is the order in which properties get precedence. The higher sequence number overrides the properties of the lower sequence number: 1 SpringApplication 2 @PropertySource 3 Config data file 4 OS environment variable 5 Command line arguments\nBoth @Bean and @Component annotations let you instruct Spring to create instances of the annotated class, but their usage is slightly different. You typically use the @Bean annotation for the classes for which you don’t have access to the source code. Thus, you define a bean and return a new instance of the class. For the @Component annotation, as you have access to the source Java file, you can simply annotate the class with this annotation.Both @Bean and @Component annotations let you instruct Spring to create instances of the annotated class, but their usage is slightly different. You typically use the @Bean annotation for the classes for which you don’t have access to the source code. Thus, you define a bean and return a new instance of the class. For the @Component annotation, as you have access to the source Java file, you can simply annotate the class with this annotation.\nThe CommandLineRunner is a useful feature that is frequently used to perform several application initialization activities. In a CommandLineRunner implementation, you also have access to the command line arguments through the args parameter.\nBean Validation, part of the Java EE (Enterprise Edition) specification, is a framework that provides a standard way to enforce constraints on the properties of JavaBeans. These constraints ensure that the data within beans meets certain criteria before it is processed or persisted. Java annotations are a powerful feature introduced in Java 5 that provide a way to add metadata to Java code. They can be applied to various elements of the code, including classes, methods, fields, parameters, and more. Annotations do not change the action of the compiled program but can be used by the compiler and runtime tools to generate code, analyze code, and perform other tasks.\nSpring Data Spring Data lets you access data from a variety of data sources (e.g., relational and nonrelational databases, MapReduce databases, and cloud-based data services). It attempts to provide a uniform, easy-to-use, and familiar programming model through the Spring Framework.\nIt is an umbrella project under the Spring Framework that contains several sub-projects, each of which targets a specific database. For instance, the Spring Data JPA module is specific to relational databases (e.g., H2, MySQL, PostgreSQL). Similarly, Spring Data MongoDB aims to provide support for the MongoDB database.\nJava Database Connectivity (JDBC) is a standard API provided by Java that allows Java applications to interact with relational databases in a vendor-independent manner. JDBC provides a set of interfaces and classes that enable developers to connect to a database, execute SQL queries, and retrieve results. It serves as a bridge between Java applications and various database management systems (DBMS).\nJava Persistence API (JPA) is a specification for object-relational mapping (ORM) in Java, which allows developers to manage relational data in Java applications using an object-oriented paradigm. JPA provides a standard way to map Java objects to database tables and to manage persistent data in applications.\nJDBC: JDBC is a low-level API for interacting with relational databases. It provides a set of interfaces and classes for connecting to a database, executing SQL queries, and retrieving results. JDBC is database-centric and requires developers to write SQL queries and manage the mapping between Java objects and database tables manually. JPA: JPA is a high-level API for object-relational mapping (ORM). It abstracts the database interactions and allows developers to work with Java objects instead of SQL queries. JPA automatically maps Java objects to database tables and handles CRUD (Create, Read, Update, Delete) operations, making it easier to manage persistent data in an object-oriented way. JPA itself is not an implementation but a set of guidelines implemented by various ORM frameworks like Hibernate, EclipseLink, and Spring Data JPA. Hibernate is the default JPA provider in Spring Data JPA. MyBatis is a persistence framework that offers SQL mapping. It emphasizes direct SQL use, providing a higher degree of control over SQL queries and their execution. Unlike traditional ORM frameworks, MyBatis does not map Java objects to database tables but rather maps SQL statements to Java methods.\nOne of the core themes of Spring Data is to provide a consistent programming model to access various data sources.\nSpring Data provides a repository abstraction layer across the supported databases as a common programming model. The abstraction is contained in the Spring Data Commons module, and it provides several useful interfaces that let you perform the standard create, read, update, and delete (CRUD) operations as well as executing queries. This abstraction layer is the topmost layer and acts as the foundation for other Spring Data modules.\nAs part of the database configuration, Spring Boot automatically configures the HikariCP database connection pool. A database connection pool contains one or more database connections that are generally created at the time of application startup and available for use by the application. The benefit of a database connection pool is that a set of database connections are created at the application startup and available for use by the application. Thus, you don’t create a new connection each time you need a database connection and close it once done. The application can take a connection from the pool, use it, and return to the pool. Spring Boot uses HikariCP as the default database connection pool library. Spring Boot can load the SQL scripts from the classpath (e.g., the src/main/resources folder) or a preconfigured location. By default, you define the schema.sql file to provide all DDL scripts and define the data.sql file to include the DML scripts and place them inside the src/main/resources folder for Spring Boot to detect and execute these files. To begin with, if you are using a database other than an embedded (in-memory) database, you need to set spring.sql.init.mode to always in the application.properties file. In this schema initialization-based approach, Spring Boot re-creates the schema each time you restart the application. There is no database schema versioning done by Spring Boot.\nIn this schema initialization-based approach, Spring Boot re-creates the schema each time you restart the application. There is no database schema versioning done by Spring Boot. In addition to the schema.sql and data.sql files, Spring Boot also supports database-specific SQLs. For instance, if your application supports multiple database types, and there are SQL syntax differences, you can use schema-${platform}.sql and data-${platform}.sql files.\nSpring Data uses this marker interface Repository as the primary abstraction for a data source. In addition to the CrudRepository, Spring Data also provides a PagingAndSortingRepository, which extends the CrudRepository and provides additional support for pagination and sorting of the entities.\n@Entity marks a java class as a jpa entity @Table provides the database table details @Column provides mapping information between the Java fields and the associated column name in the table. @Repository marks a class as a spring repository\nSpring Security The following part comes from spring security official website Spring Security provides comprehensive support for authentication. Authentication is how we verify the identity of who is trying to access a particular resource.\nSpring Security’s PasswordEncoder interface is used to perform a one-way transformation of a password to let the password be stored securely.\nTo mitigate the effectiveness of Rainbow Tables, developers were encouraged to use salted passwords.\nAdaptive one-way functions: Can be adjusted to require more computational resources as hardware improves Examples of adaptive one-way functions that should be used include bcrypt, PBKDF2, scrypt, and argon2.\nPasswordEncoder passwordEncoder = PasswordEncoderFactories.createDelegatingPasswordEncoder(); DelegatingPasswordEncoder Storage Format\n{id}encodedPassword e.g. {bcrypt}$2a$10$dXJ3SW6G7P50lGmMkkmwe.20cQQubK3.HZWzG3YB1tlRy.fqvM/BG UserDetails user = User.withDefaultPasswordEncoder() .username(\u0026#34;user\u0026#34;) .password(\u0026#34;password\u0026#34;) .roles(\u0026#34;user\u0026#34;) .build(); System.out.println(user.getPassword()); // {bcrypt}$2a$10$dXJ3SW6G7P50lGmMkkmwe.20cQQubK3.HZWzG3YB1tlRy.fqvM/BG Spring Security provides comprehensive support for authorization. Authorization is determining who is allowed to access a particular resource. Spring Security provides defense in depth by allowing for request based authorization and method based authorization.\nPrevent CSRF (cross site request forgery)\nSynchronizer token request Samesite attribute Spring Security does not directly control the creation of the session cookie, so it does not provide support for the SameSite attribute. Spring Session provides support for the SameSite attribute in servlet-based applications. Safe Methods Must be Read-only For either protection against CSRF to work, the application must ensure that \u0026ldquo;safe\u0026rdquo; HTTP methods are read-only. This means that requests with the HTTP GET, HEAD, OPTIONS, and TRACE methods should not change the state of the application. Default Security Headers\nCache-Control: no-cache, no-store, max-age=0, must-revalidate Pragma: no-cache Expires: 0 X-Content-Type-Options: nosniff Strict-Transport-Security: max-age=31536000 ; includeSubDomains(https only) X-Frame-Options: DENY X-XSS-Protection: 0 Spring Security provides Spring Data integration that allows referring to the current user within your queries. It is not only useful but necessary to include the user in the queries to support paged results since filtering the results afterwards would not scale.\nIn most environments, Security is stored on a per Thread basis. This means that when work is done on a new Thread, the SecurityContext is lost. Spring Security provides some infrastructure to help make this much easier for users. Spring Security provides low level abstractions for working with Spring Security in multi-threaded environments. The default arrangement of Spring Boot and Spring Security affords the following behaviors at runtime:\nRequires an authenticated user for any endpoint (including Boot’s /error endpoint) Registers a default user with a generated password at startup (the password is logged to the console; in the preceding example, the password is 8e557245-73e2-4286-969a-ff57fe326336) Protects password storage with BCrypt as well as others Provides form-based login and logout flows Authenticates form-based login as well as HTTP Basic Provides content negotiation; for web requests, redirects to the login page; for service requests, returns a 401 Unauthorized Mitigates CSRF attacks Mitigates Session Fixation attacks Writes Strict-Transport-Security to ensure HTTPS Writes X-Content-Type-Options to mitigate sniffing attacks Writes Cache Control headers that protect authenticated resources Writes X-Frame-Options to mitigate Clickjacking Integrates with HttpServletRequest\u0026rsquo;s authentication methods Publishes authentication success and failure events FilterChain Spring Security’s Servlet support is based on Servlet Filters. The following image shows the typical layering of the handlers for a single HTTP request. The client sends a request to the application, and the container creates a FilterChain, which contains the Filter instances and Servlet that should process the HttpServletRequest, based on the path of the request URI. In a Spring MVC application, the Servlet is an instance of DispatcherServlet. At most, one Servlet can handle a single HttpServletRequest and HttpServletResponse. more than one Filter can be used to:\nPrevent downstream Filter instances or the Servlet from being invoked. In this case, the Filter typically writes the HttpServletResponse. Modify the HttpServletRequest or HttpServletResponse used by the downstream Filter instances and the Servlet. public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) { // do something before the rest of the application chain.doFilter(request, response); // invoke the rest of the application // do something after the rest of the application } DelegatingFilterProxy Spring provides a Filter implementation named DelegatingFilterProxy that allows bridging between the Servlet container’s lifecycle and Spring’s ApplicationContext. The Servlet container allows registering Filter instances by using its own standards, but it is not aware of Spring-defined Beans. You can register DelegatingFilterProxy through the standard Servlet container mechanisms but delegate all the work to a Spring Bean that implements Filter.\nHere is a picture of how DelegatingFilterProxy fits into the Filter instances and the FilterChain.\nDelegatingFilterProxy looks up Bean Filter 0 from the ApplicationContext and then invokes Bean Filter0. FilterChainProxy SecurityFilterChain is used by FilterChainProxy to determine which Spring Security Filter instances should be invoked for the current request. The Security Filters in SecurityFilterChain are typically Beans, but they are registered with FilterChainProxy instead of DelegatingFilterProxy. FilterChainProxy provides a number of advantages to registering directly with the Servlet container or DelegatingFilterProxy. First, it provides a starting point for all of Spring Security’s Servlet support. For that reason\nFilterChainProxy is central to Spring Security usage, it can perform tasks that are not viewed as optional. For example, it clears out the SecurityContext to avoid memory leaks. It also applies Spring Security’s HttpFirewall to protect applications against certain types of attacks. In addition, it provides more flexibility in determining when a SecurityFilterChain should be invoked. In a Servlet container, Filter instances are invoked based upon the URL alone. However, FilterChainProxy can determine invocation based upon anything in the HttpServletRequest by using the RequestMatcher interface. Security Filters Most of the time, the default security filters are enough to provide security to your application. However, there might be times that you want to add a custom Filter to the security filter chain.\npublic class TenantFilter implements Filter { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; HttpServletResponse response = (HttpServletResponse) servletResponse; String tenantId = request.getHeader(\u0026#34;X-Tenant-Id\u0026#34;); boolean hasAccess = isUserAllowed(tenantId); if (hasAccess) { filterChain.doFilter(request, response); return; } throw new AccessDeniedException(\u0026#34;Access denied\u0026#34;); } } @Configuration @EnableWebSecurity public class SecurityConfig { @Bean public SecurityFilterChain filterChain(HttpSecurity http) throws Exception { http .csrf(Customizer.withDefaults()) .authorizeHttpRequests(authorize -\u0026gt; authorize .anyRequest().authenticated() ) .httpBasic(Customizer.withDefaults()) .formLogin(Customizer.withDefaults()); .addFilterBefore(new TenantFilter(), AuthorizationFilter.class); return http.build(); } } Be careful when you declare your filter as a Spring bean, either by annotating it with @Component or by declaring it as a bean in your configuration, because Spring Boot will automatically register it with the embedded container. That may cause the filter to be invoked twice, once by the container and once by Spring Security and in a different order. Exception Handing ExceptionTranslationFilter is inserted into the FilterChainProxy as one of the Security Filters. The ExceptionTranslationFilter allows translation of AccessDeniedException and AuthenticationException into HTTP responses. If the user is not authenticated or it is an AuthenticationException, then Start Authentication. The SecurityContextHolder is cleared out. The HttpServletRequest is saved so that it can be used to replay the original request once authentication is successful. The AuthenticationEntryPoint is used to request credentials from the client. For example, it might redirect to a log in page or send a WWW-Authenticate header. If it is an AccessDeniedException, then Access Denied. The AccessDeniedHandler is invoked to handle access denied. If the application does not throw an AccessDeniedException or an AuthenticationException, then ExceptionTranslationFilter does not do anything.\nAuthentication At the heart of Spring Security’s authentication model is the SecurityContextHolder. It contains the SecurityContext. The SecurityContextHolder is where Spring Security stores the details of who is authenticated. Spring Security does not care how the SecurityContextHolder is populated. If it contains a value, it is used as the currently authenticated user.\nGet currently authenticated user\nSecurityContext context = SecurityContextHolder.getContext(); Authentication authentication = context.getAuthentication(); String username = authentication.getName(); Object principal = authentication.getPrincipal(); Collection\u0026lt;? extends GrantedAuthority\u0026gt; authorities = authentication.getAuthorities(); By default, SecurityContextHolder uses a ThreadLocal to store these details, which means that the SecurityContext is always available to methods in the same thread, even if the SecurityContext is not explicitly passed around as an argument to those methods. Using a ThreadLocal in this way is quite safe if you take care to clear the thread after the present principal’s request is processed. Spring Security’s FilterChainProxy ensures that the SecurityContext is always cleared. Authentication ⠀The Authentication contains:\nprincipal: Identifies the user. When authenticating with a username/password this is often an instance of UserDetails. credentials: Often a password. In many cases, this is cleared after the user is authenticated, to ensure that it is not leaked. authorities: The GrantedAuthority instances are high-level permissions the user is granted. Two examples are roles and scopes. AuthenticationManager is the API that defines how Spring Security’s Filters perform authentication. The Authentication that is returned is then set on the SecurityContextHolder by the controller (that is, by Spring Security’s Filters instances) that invoked the AuthenticationManager. While the implementation of AuthenticationManager could be anything, the most common implementation is ProviderManager. ProviderManager delegates to a List of AuthenticationProvider instances. Each AuthenticationProvider has an opportunity to indicate that authentication should be successful, fail, or indicate it cannot make a decision and allow a downstream AuthenticationProvider to decide. If none of the configured AuthenticationProvider instances can authenticate, authentication fails with a ProviderNotFoundException, which is a special AuthenticationException that indicates that the ProviderManager was not configured to support the type of Authentication that was passed into it. In practice each AuthenticationProvider knows how to perform a specific type of authentication. For example, one AuthenticationProvider might be able to validate a username/password, while another might be able to authenticate a SAML assertion. This lets each AuthenticationProvider do a very specific type of authentication while supporting multiple types of authentication and expose only a single AuthenticationManager bean. ProviderManager also allows configuring an optional parent AuthenticationManager, which is consulted in the event that no AuthenticationProvider can perform authentication. The parent can be any type of AuthenticationManager, but it is often an instance of ProviderManager. In fact, multiple ProviderManager instances might share the same parent AuthenticationManager. This is somewhat common in scenarios where there are multiple SecurityFilterChain instances that have some authentication in common (the shared parent AuthenticationManager), but also different authentication mechanisms (the different ProviderManager instances). When a client makes an unauthenticated request to a resource that they are not authorized to access. In this case, an implementation of AuthenticationEntryPoint is used to request credentials from the client. The AuthenticationEntryPoint implementation might perform a redirect to a log in page, respond with an WWW-Authenticate header, or take other action. AbstractAuthenticationProcessingFilter AbstractAuthenticationProcessingFilter is used as a base Filter for authenticating a user’s credentials. When the user submits their credentials, the AbstractAuthenticationProcessingFilter creates an Authentication from the HttpServletRequest to be authenticated. The type of Authentication created depends on the subclass of AbstractAuthenticationProcessingFilter. For example, UsernamePasswordAuthenticationFilter creates a UsernamePasswordAuthenticationToken from a username and password that are submitted in the HttpServletRequest. Next, the Authentication is passed into the AuthenticationManager to be authenticated. If authentication fails, then Failure.\nThe SecurityContextHolder is cleared out. RememberMeServices.loginFail is invoked. If remember me is not configured, this is a no-op. See the rememberme package. AuthenticationFailureHandler is invoked. See the AuthenticationFailureHandler interface. If authentication is successful, then Success. SessionAuthenticationStrategy is notified of a new login. See the SessionAuthenticationStrategy interface. The Authentication is set on the SecurityContextHolder. Later, if you need to save the SecurityContext so that it can be automatically set on future requests, SecurityContextRepository#saveContext must be explicitly invoked. See the SecurityContextHolderFilter class. RememberMeServices.loginSuccess is invoked. If remember me is not configured, this is a no-op. See the rememberme package. ApplicationEventPublisher publishes an InteractiveAuthenticationSuccessEvent. AuthenticationSuccessHandler is invoked. See the AuthenticationSuccessHandler interface. ⠀Username/Password Authentication authenticate users via a REST API @Configuration @EnableWebSecurity public class SecurityConfig { @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { http .authorizeHttpRequests((authorize) -\u0026gt; authorize .requestMatchers(\u0026#34;/login\u0026#34;).permitAll() .anyRequest().authenticated() ); return http.build(); } @Bean public AuthenticationManager authenticationManager( UserDetailsService userDetailsService, PasswordEncoder passwordEncoder) { DaoAuthenticationProvider authenticationProvider = new DaoAuthenticationProvider(); authenticationProvider.setUserDetailsService(userDetailsService); authenticationProvider.setPasswordEncoder(passwordEncoder); return new ProviderManager(authenticationProvider); } @Bean public UserDetailsService userDetailsService() { UserDetails userDetails = User.withDefaultPasswordEncoder() .username(\u0026#34;user\u0026#34;) .password(\u0026#34;password\u0026#34;) .roles(\u0026#34;USER\u0026#34;) .build(); return new InMemoryUserDetailsManager(userDetails); } @Bean public PasswordEncoder passwordEncoder() { return PasswordEncoderFactories.createDelegatingPasswordEncoder(); } } @RestController public class LoginController { private final AuthenticationManager authenticationManager; public LoginController(AuthenticationManager authenticationManager) { this.authenticationManager = authenticationManager; } @PostMapping(\u0026#34;/login\u0026#34;) public ResponseEntity\u0026lt;Void\u0026gt; login(@RequestBody LoginRequest loginRequest) { Authentication authenticationRequest = UsernamePasswordAuthenticationToken.unauthenticated(loginRequest.username(), loginRequest.password()); Authentication authenticationResponse = this.authenticationManager.authenticate(authenticationRequest); // ... } public record LoginRequest(String username, String password) { } } Built-in mechanism for reading a username:\nForm Basic WWW-Authenticate header Digest You should not use Digest Authentication in modern applications, because it is not considered to be secure. DaoAuthenticationProvider\nDaoAuthenticationProvider is an AuthenticationProvider implementation that uses a UserDetailsService and PasswordEncoder to authenticate a username and password. Persistence in Spring Security the association of the user to future requests is made using SecurityContextRepository. The default implementation of SecurityContextRepository is DelegatingSecurityContextRepository which delegates to the following:\nHttpSessionSecurityContextRepository RequestAttributeSecurityContextRepository NullSecurityContextRepository If it is not desirable to associate the SecurityContext to an HttpSession (i.e. when authenticating with OAuth) the NullSecurityContextRepository is an implementation of SecurityContextRepository that does nothing.\nTheSecurityContextPersistenceFilter is responsible for persisting the SecurityContext between requests using the SecurityContextRepository. SecurityContextHolderFilter The SecurityContextHolderFilter is responsible for loading the SecurityContext between requests using the SecurityContextRepository. Unlike,SecurityContextPersistenceFilter, SecurityContextHolderFilter only loads the SecurityContext it does not save the SecurityContext. This means that when using SecurityContextHolderFilter, it is required that the SecurityContext is explicitly saved.\nRestful API @RestController\n@Service\n@Reposity\n","date":"2024-07-02T00:00:00Z","permalink":"https://dyhes.github.io/p/spring-boot-in-practicenotes/","title":"【Spring Boot in practice】Notes"},{"content":"1-Momenta react通信\n父组件向子组件传递props，callback 子组件通过event向父组件发送消息 context 兄弟：将state提升到父组件 Global Variables(localStorage) 完全不相关：observer pattern vs. Vue\nprovide / inject react diff算法\n算法\neventloop\n同步 call stack\n异步 callback queues\n当call stack为空时，\n一个tick\ndeque and run the oldest task from macrotask queue (dom event, ajax, setTimeOut, setInterval) execute all microtasks (Promises aync/await) render changes if any wait if macrotask queue is empty 编译原理\n闲聊etc.\n2-ByteDance 项目\nSSR渲染\nrendering web pages on the server and sending fully rendered HTML pages to the client for display\ncsr: web pages are constructed in the browser using javascript\nbenefit:\nseo: search engine optimization performance eventloop\nvar let const\narrow function 和 普通函数的区别\n不能Generator函数，不能使用yeild关键字。 箭头函数不具有prototype 箭头函数不具有super 箭头函数不具有new.target，不能作为构造函数使用 箭头函数不会创建自己的this，call | apply | bind 无法改变箭头函数中this的指向 闭包及其运用\n内嵌的函数及它的lexical scoping\nprivate method\n继承（vs. Java）\nreact和vue的区别\nTCP三次握手\n3-好未来（学而思） 语义化\ninline和block\nscript defer和async\nArray的原型链\n构造函数 prototype 实例\n盒模型 IE和standard box-sizing\nflex实现水平垂直居中\nflex属性\nHTTP caching\nvue组件通信\nscope 闭包 外访问内\nOSI模型\nDNS\nHTTP缓存\nheuristic caching: as much as possible\nprivate cache\nshared cache\nproxy caches\nmanaged caches\ncdn等等\n// if both are available max-age is preferred Expires: Tue, 28 Feb 2022 22:22:22 GMT (HTTP/1.0) Cache-Control: max-age = 604800(sec = 1 week) (HTTP/1.1) Vary: Accept-Language\nrevalidation: transform a stale response into a fresh one by asking the origin server, done by conditional request\nIf-Modified-Since If-None-Match ETag HTTPS\nuse TLS(transport layer security) to encrypt all communication (ssl = secure sockets layer, old standard security technology, v3.0 Netscape 1996)\nTLS handshake\nTCP +\n1）客户端发起一个http请求，告诉服务器自己支持哪些hash算法。\n2）服务端把自己的信息以数字证书的形式返回给客户端（证书内容有密钥公钥，网站地址，证书颁发机构，失效日期等）。证书中有一个公钥来加密信息，私钥由服务器持有。\n3）验证证书的合法性\n客户端收到服务器的响应后会先验证证书的合法性（证书中包含的地址与正在访问的地址是否一致，证书是否过期）。\n4）生成随机密码（RSA签名）\n如果验证通过，或用户接受了不受信任的证书，浏览器就会生成一个随机的对称密钥（session key）（随机数）并用公钥加密，让服务端用私钥解密，解密后就用这个对称密钥进行传输了，并且能够说明服务端确实是私钥的持有者。（服务端并不是真的加密这个字符串，而是把字符串进行hash计算后再进行加密后发送给客户端。客户端收到后再解密这个hash值与原来字符串的hash值对比，从而确定对方是否持有私钥。）\n5）生成对称加密算法\n验证完服务端身份后，客户端生成一个对称加密的算法和对应密钥，以公钥加密之后发送给服务端。此时被黑客截获也没用，因为只有服务端的私钥才可以对其进行解密。之后客户端与服务端可以用这个对称加密算法来加密和解密通信内容了。\n跨域解决方法\n同源：协议/主机/端口\nCORS\nPreflight request: 一个 CORS 预检请求是用于检查服务器是否支持 CORS 即跨域资源共享。\n它一般是用了以下几个 HTTP 请求首部的 OPTIONS 请求：Access-Control-Request-Method 和 Access-Control-Request-Headers，以及一个 Origin 首部\nAccess-Control-Allow-Origin Access-Control-Allow-Methods Access-Control-Allow-Headers Access-Control-Allow-Credentials = true Access-Control-Expose-Headers 允许服务器指示那些响应标头可以暴露给浏览器中运行的脚本，以响应跨源请求。 Access-Control-Max-Age indicates how long the results of a preflight request can be cached By default, in cross-origin XMLHttpRequest or Fetch invocations, browsers will not send credentials. CORS-preflight requests must never include credentials. When responding to a credentialed request: The server must not specify the \u0026ldquo;*\u0026rdquo; wildcard for other related headers\njsonp\n利用了 script 不受同源策略的限制 （脚本的来源取决于脚本所嵌入的资源的来源，比如说访问A主机的当前HTML文件中有一个script标签，这个script标签的src属性请求了一个js脚本，因为这个脚本是由A主机的HTML文件的嵌入的script标签发起请求获取的，因此这个脚本的来源是属于A主机的。)\n通过动态添加\u0026lt;script\u0026gt;元素，向服务器发出请求\n缺点：只能 get 方式，易受到 XSS攻击\n代理跨域请求(Nginx)\n前端向发送请求，经过代理，请求需要的服务器资源\n缺点：需要额外的代理服务器\nHtml5 window.postMessage() 方法\nsafely enables cross-origin communication between window objects\n允许来自不同源的脚本采用异步方式进行有限的通信，可以实现跨文本、多窗口、跨域消息传递\n基于 Html5 websocket 协议\nwebsocket 是 Html5 一种新的协议，基于该协议可以做到浏览器与服务器全双工通信，允许跨域请求\n修改 document.domain 跨子域\n相同主域名下的不同子域名资源，设置 document.domain 为 相同的一级域名\n-比特大陆 水平垂直居中\n{ display: flex, align-items: center, justify-content: center } 绝对定位 + margin: auto\n绝对定位 + 负margin（宽高一半）\n{ position: absolute, top: 50%, left: 50%, margin-left: - width/2 margin-top: - height/2 } 绝对定位 + transform\n{ position: absolute, top: 50%, left: 50%, transform: translate(-50%, -50%) } ES6新特性\nconst let arrow function module template string default paramter spead/rest destructuring class symbol react和vue区别\nreact class component和function component区别\nclass组件是有状态的组件，可以定义state状态。 函数组件是无状态的。 class组件有生命周期。 函数组件没有生命周期，函数组件使用的是Hooks。 class组件是有this对象的。\nreact diff算法\nreact和vue diff算法的区别\nscope 闭包\nreact hooks\nuseEffect\nuseReducer\ncss flex\n虚拟列表\n深拷贝、浅拷贝区别 实现\n跳出forEach\nthrow Exception\n5-滴滴 虚拟DOM树的本质和作用\n本质：JS和dom之间的映射缓存\n作用：实现DOM元素的高效更新\nHTTP缓存 Header\nCache-Control\n防抖、节流 概念实现\n防抖（debounce)：n 秒后在执行该事件，若在 n 秒内被重复触发，则重新计时\n节流（throttle）: n 秒内只运行一次，若在 n 秒内重复触发，只有一次生效\n想象每天上班大厦底下的电梯。把电梯完成一次运送，类比为一次函数的执行和响应\n假设电梯有两种运行策略 debounce 和 throttle，超时设定为15秒，不考虑容量限制\n电梯第一个人进来后，15秒后准时运送一次，这是节流\n电梯第一个人进来后，等待15秒。如果过程中又有人进来，15秒等待重新计时，直到15秒后开始运送，这是防抖\nMap的使用\n这场面试一上来就是几个当时一点都不会的问题，直接大破防放弃面试，但是面试官人很好，还给了我“克服腼腆，提高沟通交流能力“的建议\n6-ThunderBit 消息订阅模式\n初创公司，创始人都有美国大厂背景，挺有人格魅力的，可惜没去成\n7-鳄梨科技 笔试\n层叠上下文（stacking context) 文档根元素（\u0026lt;html\u0026gt;）； position 值为 absolute（绝对定位）或 relative（相对定位）且 z-index 值不为 auto 的元素； position 值为 fixed（固定定位）或 sticky（粘滞定位）的元素（沾滞定位适配所有移动设备上的浏览器，但老的桌面浏览器不支持）； flex (flex) 容器的子元素，且 z-index 值不为 auto； grid (grid) 容器的子元素，且 z-index 值不为 auto； opacity 属性值小于 1 的元素（参见 the specification for opacity）； mix-blend-mode 属性值不为 normal 的元素； 以下任意属性值不为none的元素： transform filter backdrop-filter perspective clip-path mask / mask-image / mask-border isolation 属性值为 isolate 的元素； will-change 值设定了任一属性而该属性在 non-initial 值时会创建层叠上下文的元素（参考这篇文章）； contain 属性值为 layout、paint 或包含它们其中之一的合成值（比如 contain: strict、contain: content）的元素。 content, padding, border, margin 8-网易 强缓存与协商缓存的区别\n强缓存：浏览器不与服务端协商直接取浏览器缓存\n协商缓存：浏览器会先向服务器确认资源的有效性后才决定是从缓存中取资源还是重新获取资源 (304 Not Modified)\nreact生命周期\n挂载\nconstructor getDerivedStateFromProps render componentDidMount 更新\ngetDerivedStateFromProps shouldComponentUpdate render getSnapshotBeforeUpdate componentDidUpdate 卸载\ncomponentWillUnmount http响应码\n信息响应 (100–199) 成功响应 (200–299) 重定向消息 (300–399) 客户端错误响应 (400–499) 服务端错误响应 (500–599) 201 Created\n202 Accepted\n204 No Content\n301 Moved Permanently\n307 Temporary Redirect\n308 Permanent Redirect\n400 Bad Request\n403 Forbidden\n404 Not Found\n405 Method Not Allowed\n408 Request Timeout\n500 Internal Server Error\n501 Not Implemented\n502 Bad Gateway\n503 Service Unavailable\n504 Gateway Timeout\n深拷贝浅拷贝\n深拷贝实现\n堆上新建了一份对象\n递归 序列化与反序列化 structuredClone(value, options) 浅拷贝实现\n原生 赋值 对象 复制内存地址\nObject.assign()\nconst let\n数组\nsplice 原地，可新加（相当于unshift)\nslice 新数组, 返回切出来的部分\nsplice(start) splice(start, deleteCount) splice(start, deleteCount, item1) splice(start, deleteCount, item1, item2) splice(start, deleteCount, item1, item2, /* …, */ itemN) slice() slice(start) slice(start, end) flex gap\ncss 画三角形\n​\tborder实现\n.triangle { width: 0; height: 0; border: 100px solid; border-color: orangered skyblue gold yellowgreen; } 三个方向设置透明一个方向设置颜色\n盒模型\nanimation vs. transition\n他们虽然都可以做出动画效果，但是transition主要做简单的过渡效果，而animation可以做复杂的动画效果，在语法和用法上有非常大的区别。\ntransition是过渡属性，强调过渡，他的实现需要触发一个事件（比如鼠标移动上去，焦点，点击等）才执行动画，过渡只有一组（两个：开始-结束）关键帧。\nanimation是动画属性，他的实现不需要触发事件，设定好时间之后可以自己执行，且可以循环一个动画（设置多个关键帧）。\nv-if, v-show\nref, reactive\nref() 函数可以接受原始类型（最常见的是布尔值、字符串和数字）以及对象作为参数，而 reactive() 函数只能接受对象作为参数。 ref() 有一个 .value 属性，你必须使用 .value 属性获取内容，但是使用 reactive() 的话可以直接访问 使用 ref() 函数可以替换整个对象实例，但是在使用 reactive() 函数时就不行 响应码处理\n伪类 伪元素\ninstanceof\nhash 模式\nhash 模式是一种把前端路由的路径用井号 # 拼接在真实 URL 后面的模式。当井号 # 后面的路径发生变化时，浏览器并不会重新发起请求，而是会触发 hashchange 事件。\nhistory 模式\nhistory API 是 H5 提供的新特性，允许开发者直接更改前端路由，即更新浏览器 URL 地址而不重新发起请求。\n二面因拿到百度offer放弃\n9-迈步科技 输入网址到页面呈现的过程\nURL解析：浏览器解析输入的URL，提取出协议、主机、端口、路径等信息。\nDNS解析：浏览器使用主机名查询DNS服务器，获取对应的IP地址。\n建立TCP连接：浏览器使用获取到的IP地址和端口号，与服务器建立TCP连接。这涉及到三次握手的过程。\n发送HTTP请求：一旦TCP连接建立，浏览器向服务器发送HTTP请求。请求的内容包括请求方法、请求头和请求体。\n服务器处理请求：服务器接收到浏览器发送的HTTP请求后，根据请求的路径和参数，处理请求并准备响应。\n服务器返回响应：服务器根据请求的处理结果，生成相应的HTTP响应。响应的内容包括响应状态码、响应头和响应体。\n接收响应：浏览器接收到服务器返回的HTTP响应。\n渲染页面：浏览器开始解析接收到的响应内容，构建DOM树、CSSOM树和渲染树。然后进行布局和绘制，最终将页面呈现在屏幕上。\n页面展示：页面渲染完成后，浏览器将页面展示给用户，用户可以看到页面内容并与页面进行交互。\ncookie和session的区别\n1. Session : A session is used to save information on the server momentarily so that it may be utilized across various pages of the website. It is the overall amount of time spent on an activity. The user session begins when the user logs in to a specific network application and ends when the user logs out of the program or shuts down the machine.\nSession values are far more secure since they are saved in binary or encrypted form and can only be decoded at the server. When the user shuts down the machine or logs out of the program, the session values are automatically deleted. We must save the values in the database to keep them forever.\n2. Cookie : A cookie is a small text file that is saved on the user’s computer. The maximum file size for a cookie is 4KB. It is also known as an HTTP cookie, a web cookie, or an internet cookie. When a user first visits a website, the site sends data packets to the user’s computer in the form of a cookie.\nThe information stored in cookies is not safe since it is kept on the client-side in a text format that anybody can see. We can activate or disable cookies based on our needs.\nCookie Session Cookies are client-side files on a local computer that hold user information. Sessions are server-side files that contain user data. Cookies end on the lifetime set by the user. When the user quits the browser or logs out of the programmed, the session is over. It can only store a certain amount of info. It can hold an indefinite quantity of data. The browser’s cookies have a maximum capacity of 4 KB. We can keep as much data as we like within a session, however there is a maximum memory restriction of 128 MB that a script may consume at one time. Because cookies are kept on the local computer, we don’t need to run a function to start them. To begin the session, we must use the session start() method. Cookies are not secured. Session are more secured compare than cookies. Cookies stored data in text file. Session save data in encrypted form. Cookies stored on a limited data. Session stored a unlimited data. We can set an expiration date to delete the cookie’s data. It will automatically delete the data at that specific time. In PHP, to destroy or remove the data stored within a session, we can use the session_destroy() function, and to unset a specific variable, we can use the unset() function 具体来说cookie机制采用的是在客户端保持状态的方案，而session机制采用的是在服务器端保持状态的方案。\n同时我们也看到，由于采用服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制\n来达到保存标识的目的，但实际上它还有其他选择。\ncookie机制。正统的cookie分发是通过扩展HTTP协议来实现的，服务器通过在HTTP的响应头中加上一行特殊的指示以提示\n浏览器按照指示生成相应的cookie。然而纯粹的客户端脚本如JavaScript或者VBScript也可以生成cookie。而cookie的使用\n是由浏览器按照一定的原则在后台自动发送给服务器的。浏览器检查所有存储的cookie，如果某个cookie所声明的作用范围\n大于等于将要请求的资源所在的位置，则把该cookie附在请求资源的HTTP请求头上发送给服务器。\nSet-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Domain=\u0026lt;domain-value\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Expires=\u0026lt;date\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; HttpOnly Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Max-Age=\u0026lt;number\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Partitioned Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Path=\u0026lt;path-value\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Secure Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; SameSite=Strict Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; SameSite=Lax Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; SameSite=None; Secure // Multiple attributes are also possible, for example: Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Domain=\u0026lt;domain-value\u0026gt;; Secure; HttpOnly cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围。若不设置过期时间，则表示这\n个cookie的生命期为浏览器会话期间，关闭浏览器窗口，cookie就消失。这种生命期为浏览器会话期的cookie被称为会话cookie。\n会话cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把cookie\n保存到硬盘上，关闭后再次打开浏览器，这些cookie仍然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏\n览器进程间共享，比如两个IE窗口。而对于保存在内存里的cookie，不同的浏览器有不同的处理方式session机制。session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。\n​\t当程序需要为某个客户端的请求创建一个session时，服务器首先检查这个客户端的请求里是否已包含了一个session标识\n（称为session id），如果已包含则说明以前已经为此客户端创建过session，服务器就按照session id把这个session检索出来\n使用（检索不到，会新建一个），如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相\n关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个session id将被在本次响应\n中返回给客户端保存。保存这个session id的方式可以采用cookie，这样在交互过程中浏览器可以自动的按照规则把这个标识发送给\n服务器。一般这个cookie的名字都是类似于SEEESIONID。但cookie可以被人为的禁止，则必须有其他机制以便在cookie被禁止时\n仍然能够把session id传递回服务器。经常被使用的一种技术叫做URL重写，就是把session id直接附加在URL路径的后面。还有一种技术叫做表单隐藏字段。就是服务器\n会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把session id传递回服务器。\nCookie总是保存在客户端中，按在客户端中的存储位置，可分为会话Cookie和持久Cookie\nreact生命周期\nget和post区别\n（1）post更安全（不会作为url的一部分，不会被缓存、保存在服务器日志、以及浏览器浏览记录中） （2）post发送的数据更大（get有url长度限制） （3）post能发送更多的数据类型（get只能发送ASCII字符）\nGET用于获取信息，是无副作用的，是幂等的，且可缓存POST用于修改服务器上的数据，有副作用，非幂等，不可缓存\n在约定中，GET方法的参数应该放在 url 中，POST方法参数应该放在 body 中\nuseState useRef\nPromise的理解\n热加载 原理\nwebpack\ngit命令\nlinux命令\n25匹马求最快的3匹\n最近读的一本书\n10-货拉拉 盒模型\nposition\n是否在流里\nabsolute和fixed被移出flow\nstatic\nThe element is positioned according to the normal flow of the document. The top, right, bottom, left, and z-index properties have no effect. This is the default value.\nrelative\nThe element is positioned according to the normal flow of the document, and then offset relative to itself based on the values of top, right, bottom, and left. The offset does not affect the position of any other elements; thus, the space given for the element in the page layout is the same as if position were static.\nThis value creates a new stacking context when the value of z-index is not auto. Its effect on table-*-group, table-row, table-column, table-cell, and table-caption elements is undefined.\nabsolute\nThe element is removed from the normal document flow, and no space is created for the element in the page layout. The element is positioned relative to its closest positioned ancestor (if any) or to the initial containing block. Its final position is determined by the values of top, right, bottom, and left.\nThis value creates a new stacking context when the value of z-index is not auto. The margins of absolutely positioned boxes do not collapse with other margins.\nfixed\nThe element is removed from the normal document flow, and no space is created for the element in the page layout. The element is positioned relative to its initial containing block, which is the viewport in the case of visual media. Its final position is determined by the values of top, right, bottom, and left.\nThis value always creates a new stacking context. In printed documents, the element is placed in the same position on every page.\nsticky\nThe element is positioned according to the normal flow of the document, and then offset relative to its nearest scrolling ancestor and containing block (nearest block-level ancestor), including table-related elements, based on the values of top, right, bottom, and left. The offset does not affect the position of any other elements.\nThis value always creates a new stacking context. Note that a sticky element \u0026ldquo;sticks\u0026rdquo; to its nearest ancestor that has a \u0026ldquo;scrolling mechanism\u0026rdquo; (created when overflow is hidden, scroll, auto, or overlay), even if that ancestor isn\u0026rsquo;t the nearest actually scrolling ancestor.\nBFC\nFormatting context 是 W3C CSS2.1 规范中的一个概念。它是页面中的一块渲染区域，并且有一套渲染规则，它决定了其子元素将如何定位，以及和其他元素的关系和相互作用。最常见的 Formatting context 有 Block fomatting context (简称BFC)和 Inline formatting context (简称IFC)。\nBlock Formatting Context(区块格式化上下文)\n内部的Box会在垂直方向，一个接一个地放置。 Box垂直方向的距离由margin决定。属于同一个BFC的两个相邻Box的margin会发生重叠。 每个盒子（块盒与行盒）的margin box的左边，与包含块border box的左边相接触(对于从左往右的格式化，否则相反)。即使存在浮动也是如此。 BFC的区域不会与float box重叠。 BFC就是页面上的一个隔离的独立容器，容器里面的子元素不会影响到外面的元素。反之也如此。 计算BFC的高度时，浮动元素也参与计算。 产生\nThe root element of the document (\u0026lt;html\u0026gt;). Floats (elements where float isn\u0026rsquo;t none). Absolutely positioned elements (elements where position is absolute or fixed). Inline-blocks (elements with display: inline-block). Table cells (elements with display: table-cell, which is the default for HTML table cells). Table captions (elements with display: table-caption, which is the default for HTML table captions). Anonymous table cells implicitly created by the elements with display: table, table-row, table-row-group, table-header-group, table-footer-group (which is the default for HTML tables, table rows, table bodies, table headers, and table footers, respectively), or inline-table. Block elements where overflow has a value other than visible and clip. display: flow-root. Elements with contain: layout, content, or paint. Flex items (direct children of the element with display: flex or inline-flex) if they are neither flex nor grid nor table containers themselves. Grid items (direct children of the element with display: grid or inline-grid) if they are neither flex nor grid nor table containers themselves. Multicol containers (elements where column-count or column-width isn\u0026rsquo;t auto, including elements with column-count: 1). column-span: all should always create a new formatting context, even when the column-span: all element isn\u0026rsquo;t contained by a multicol container (Spec change, Chrome bug). 数组方法 原地与否\nfind （没有undefined\nfilter\nMutating method Non-mutating alternative copyWithin() No one-method alternative fill() No one-method alternative pop() slice(0, -1) push(v1, v2) concat([v1, v2\\]) reverse() toReversed() shift() slice(1) sort() toSorted() splice() toSpliced() unshift(v1, v2) toSpliced(0, 0, v1, v2) ES6新特性\n事件循环\nvar let const\n响应式\nmedia query 百分比 rem布局 （rem单位都是相对于根元素html的font-size来决定大小的） vw/vh Object方法 Object.keys(), Object.values(), Object.entries()\nObject.create, Object.is, Object.getPrototypeOf(), Object.setPrototypeOf(), Object.assign(), Object.defineProperty()\n防抖节流\n闭包\n闭包（closure）是一个函数以及其捆绑的周边环境状态（lexical environment，词法环境）的引用的组合。\n缺点\n延长生命周期，内存消耗大，内存泄漏\n由于闭包会使得函数中的变量都被保存在内存中，内存消耗很大，所以不能滥用闭包，否则会造成网页的性能问题，在IE中可能导致内存泄露。 解决方法是，在退出函数之前，将不使用的局部变量全部删除。\n原型链\nvue通信\nvue响应式原理\nObject.defineProperty get set方法追踪依赖，在 property 被访问和修改时通知变更\n页面展现过程\npost 和 get\n深浅拷贝\n算法题 Set,Map\nSet(iterable)\n项目中最难的点\n字符串去重 两种\n11-百度 基础架构 难点，如何解决，解决之后有没有形成一些方法论\nCD实现原理\n持续集成的工作原理是将小的代码块推送到Git仓库中托管的应用程序代码库中，并且每次推送时，都要运行一系列脚本来构建、测试和验证代码更改，然后再将其合并到主分支中。\n持续交付和部署相当于更进一步的CI，可以在每次推送到仓库默认分支的同时将应用程序部署到生产环境。\nref和reactive的区别\nvue3的响应性\nProxy\nVue2: Object.defineProperty(), get set方法\n虚拟dom\nvue3 diff算法\nvue3 hook\nhttp是什么，结构，解决什么问题，注意的点，版本及其区别，方法及其区别，报文结构\n跨域\nflex flex-basis和width谁优先，flex上下左右居中\nblock formatting context, 解决margin重叠\n父元素设置border和padding?\nstacking context，生效策略\n二面因拿到另一个部门的offer而取消\n12-百度 工程效能 一面\nes6新特性\nvue2到vue3\n三角形\n扁平化 flatten\npromise.any\nsettled: fulfilled or rejected\nresolved: settled or \u0026rsquo;locked-in\u0026rsquo; to match the eventual state of another promise.\ncss优先级\n二面\nhttp状态码\ntcp和udp\nudp变可靠\n最简单的方式是在应用层模仿传输层TCP的可靠性传输。下面不考虑拥塞处理，可靠UDP的简单设计。\n1、添加seq/ack机制，确保数据发送到对端 2、添加发送和接收缓冲区，主要是用户超时重传。 3、添加超时重传机制。 es6新特性\n常用hook\nuseMemo和useCallback区别\nuseMemo 和 useCallback 接收的参数都是一样,第一个参数为回调 第二个参数为要依赖的数据\n共同作用： 1.仅仅 依赖数据 发生变化, 才会重新计算结果，也就是起到缓存的作用。\n两者区别： 1.useMemo 计算结果是 return 回来的值, 主要用于 缓存计算结果的值 ，应用场景如： 需要 计算的状态 2.useCallback 计算结果是 函数, 主要用于 缓存函数，应用场景如: 需要缓存的函数，因为函数式组件每次任何一个 state 的变化 整个组件 都会被重新刷新，一些函数是没有必要被重新刷新的，此时就应该缓存起来，提高性能，和减少资源浪费。\nuseMemo的作用\n实现useCounter\n实现 objDiff\n总结 不要等到准备周全才开始面试，边面边学，熟能生巧\n自我介绍，表达能力，如何展示自己很重要\n可以考虑从事更有门槛的赛道\n要自信\n","date":"2023-10-29T00:00:00Z","permalink":"https://dyhes.github.io/p/interview%E5%89%8D%E7%AB%AF%E7%A0%94%E5%8F%91%E5%B2%97/","title":"【Interview】前端研发岗"},{"content":"Instruction definition instruction set\nThe vocabulary of commands understood by a given architecture.\nstored-program\nThe idea that instructions and data of many types can be stored in memory as numbers, leading to the stored-program computer.\nword\nThe natural unit of access in a computer, usually a group of 32 bits; corresponds to the size of a register in the MIPS architecture\naddress\nA value used to delineate the location of a specific data element within a memory array\ninstruction format\nA form of representation of an instruction composed of fields of binary numbers.\nmachine language\nBinary representation used for communication within a computer system.\nopcode\nThe field that denotes the operation and format of an instruction.\ntext segment\nThe segment of a UNIX object file that contains the machine language code for routines in the source file\nprogram counter (PC)\nThe register containing the address of the instruction in the program being executed\ntype MIPS\nARMv7 (32-bit)\nintel x86\nARMv8 (64-bit)\nregister MIPS instruction type arithmetic\ndata transfer\na command that moves data between memory and registers\nlogical\nconditional branch\nunconditional jump\nNumber Representation Numbers are kept in computer hardware as a series of high and low electronic signals, and so they are considered base 2 numbers.\nA single digit of a binary number is thus the “atom” of computing, since all information is composed of binary digits or bits.\nThe phrase least significant bit is used to refer to the rightmost bit (bit 0 above) and most significant bit to the leftmost bit (bit 31).\nsign and magnitude Unsign number are quite natural, but computer programs calculate both positive and negative numbers, so we need a representation that distinguishes the positive from the negative. The most obvious solution is to add a separate sign, which conveniently can be represented in a single bit; the name for this representation is sign and magnitude\nshortcomings it’s not obvious where to put the sign bit. adders for sign and magnitude may need an extra step to set the sign because we can’t know in advance what the proper sign will be. a separate sign bit means that sign and magnitude has both a positive and a negative zero, which can lead to problems for inattentive programmers two’s complement(补码) reason In the search for a more attractive alternative, the question arose as to what would be the result for unsigned numbers if we tried to subtract a large number from a small one. Th e answer is that it would try to borrow from a string of leading 0s, so the result would have a string of leading 1s. Given that there was no obvious better alternative, the final solution was to pick the representation that made the hardware simple: leading 0s mean positive, and leading 1s mean negative.\nEvery computer today uses two’s complement binary representations for signed numbers.\nfeature Two’s complement representation has the advantage that all negative numbers have a 1 in the most significant bit. This bit is oft en called the sign bit. $$ (x_{31}\\times-2^{31})+\\sum_{i=0}^{30}x_{i}\\times2^i $$ Signed versus unsigned applies to loads as well as to arithmetic.\nSigned load\ncopy the sign repeatedly to fill the rest of the register—called sign extension\nits purpose is to place a correct representation of the number within that register\nUnsigned load\nsimply fill with 0s to the left of the data\nnegation Simply invert every 0 to 1 and every 1 to 0, then add one to the result\nsign extension take the most significant bit from the smaller quantity—the sign bit—and replicate it to fill the new bits of the larger quantity\nname Two’s complement gets its name from the rule that the unsigned sum of an n-bit number and its n-bit negative is $2^n$; hence, the negation or complement of a number x is $2^n-x$, or its “two’s complement.”\none\u0026rsquo;s complement(反码) A notation that represents the most negative value by $10 . . . 000_{two}$ and the most positive value by $01 . . . 11_{two}$, leaving an equal number of negatives and positives but ending up with two zeros, one positive ($00 . . . 00$) and one negative ($11 . . . 11$).\nThe term is also used to mean the inversion of every bit in a pattern: 0 to 1 and 1 to 0.\nThis relation helps explain its name since the complement of x is $2^n-x-1$\nIt was also an attempt to be a better solution than sign and magnitude, and several early scientific computers did use the notation.\nOne’s complement adders did need an extra step to subtract a number, and hence two’s complement dominates today.\nbiased noatation A notation that represents the most negative value by $00 . . . 000$ and the most positive value by $11 . . . 11$, with 0 typically having the value $10 . . . 00$, thereby biasing the number such that the number plus the bias has a non-negative representation.\nNote The processor can keep only a small amount of data in registers, but computer memory contains billions of data elements. Hence, data structures (arrays and structures) are kept in memory. Memory is just a large, single-dimensional array, with the address acting as the index to that array, starting at 0. arithmetic operations occur only on registers in MIPS instructions In MIPS, words must start at addresses that are multiples of 4. This requirement is called an alignment restriction, and many architectures have it. The process of putting less commonly used variables (or those needed later) into memory is called spilling registers Instruction representation Instructions are kept in the computer as a series of high and low electronic signals and may be represented as numbers. In fact, each piece of an instruction can be considered as an individual number, and placing these numbers side by side forms the instruction.\nFields All MIPS instructions are 32 bits long, and can also be represented as fields of binary numbers.\n(R-format)\nHere is the meaning of each name of the fields in MIPS instructions:\nop\nBasic operation of the instruction, traditionally called the opcode\nrs\nThe first register source operand\nrt\nThe second register source operand\nrd\nThe register destination operand. It gets the result of the operation\nshamt: Shift amount.\nfunct: Function.\nThis field, often called the function code, selects the specific variant of the operation in the op field.\n(I-format)\nspilling registers The ideal data structure for spilling registers is a stack—a last-in-first-out queue.\nA stack needs a pointer to the most recently allocated address in the stack to show where the next procedure should place the registers to be spilled or where old register values are found.\nThe stack pointer is adjusted by one word for each register that is saved or restored. MIPS soft ware reserves register 29 for the stack pointer, giving it the obvious name $sp.\nBy historical precedent, stacks “grow” from higher addresses to lower addresses. This convention means that you push values onto the stack by subtracting from the stack pointer. Adding to the stack pointer shrinks the stack, thereby popping values off the stack.\nframe pointer Some MIPS soft ware uses a frame pointer ($fp) to point to the first word of the frame of a procedure. A stack pointer might change during the procedure, and so references to a local variable in memory might have different offsets depending on where they are in the procedure, making the procedure harder to understand. Alternatively, a frame pointer offers a stable base register within a procedure for local memory-references.\nheap ","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/computer-architecturenotes/","title":"【Computer Architecture】Notes"},{"content":"Lecture 1 (introduction) Internet network of networks infrastructure that provides services to applications provides programming interface to distributed applications protocol Protocols define the format, order of messages sent and received among network entities, and actions taken on msg transmission, receipt\nAccess network cable-based access\nfrequency division multiplexing (FDM): different channels transmitted in different frequency bands\ndigital subscriber line (DSL)\nuse existing telephone line to central office DSLAM\ndata over DSL phone line goes to Internet voice over DSL phone line goes to telephone net wireless access\nWireless local area networks wide-area cellular access networks enterprise networks\ncompanies, universities Packets host sending function:\ntakes application message\nbreaks into smaller chunks known as packets, of length L bits\ntransmits packet into access network at transmission rate R\nlink transmission rate, aka link capacity, aka link bandwidth\npacket transmission delay:\ntime needed to transmit L-bit packet into link $=\\frac{L(bits)}{R(bits/sec)}$\nPhysical media guided media\nsignals propagate in solid media: copper, fiber, coax\nTwisted pair (TP)\ntwo insulated copper wires\nCoaxial cable\ntwo concentric copper conductors\nfiber optic cable\nglass fiber carrying light pulses, each pulse a bit\nunguided media\nsignals propagate freely: radio\nwireless radio\nsignal carried in various \u0026ldquo;bands\u0026rdquo; in electromagnetic spectrum\nnetwork-core two key functions forwarding aka \u0026ldquo;switching\u0026rdquo; local action: move arriving packet from router\u0026rsquo;s input link to appropriate router output link routing: global action: determine source-destination paths taken by packets packet switching hosts break application-layer messages into packets\nnetwork forwards packets from one router to the next, across links on the path from source to destination\nstore-and-forwarding\nentire packet must arrive at router before it can be transmitted on next link\nqueueing\nif arrival rate to link exceeds transmission rate of link for some period of time\npackets will queue packets can be dropped(lost) if memory (buffer) in router fills up curcuit switching end-end resources allocated to, reserved for \u0026ldquo;call\u0026rdquo; bewteen source and destination\neach link has four circuits\ndedicated resources: no sharing\ncircuit segment idle if not used by call\ncommonly used in traditional telephone networks\ntypes\nfrequency division multiplexing (FDM)\noptical, electromagnetic frequencies divides into (narrow) frequency bands\ntime division multiplexing (TDM)\ntime divided into slots\nPacket delay $$ d_{nodal}=d_{proc}+d_{queue}+d_{trans}+d_{prop} $$\n$d_{proc}$ proccessing delay\ncheck bit errors determine output link typically\u0026lt;microsecs $d_{queue}$ queueing delay\ndepends on congestion level of router $\\alpha$ : average packet arrival rate tranfic intensity: $\\frac{L\\cdot a}{R}$ $\\frac{L\\cdot a}{R}\\sim 0$ avg. queueing delay small $\\frac{L\\cdot a}{R}\\to 1$ avg. queueing delay large $\\frac{L\\cdot a}{R}\\gt 1$ average delay infinite $d_{trans}$ transmission delay\n$d_{trans=L/R}$ $d_{prop}$ propagation delay\n$d$ : length of physical link $s$ : propagation speed $d_{prop}=d/s$ Layers each layer implements a service\napplication\nsupporting network applications\nHTTP,IMAP,SMTP,DNS\ntransport\nprocess-process data transfer\nTCP,UDP\nnetwork\nrouting of datagrams from source to destination\nIP, routing protocols\nlink\ndata transfer bewteen neighboring network elements\nEthernet, WiFi, PPP\nphysical\nbits \u0026ldquo;on the wire\u0026rdquo;\nServices application\nexchanges message M to implement some application service using services of transport layer\ntransport\ntransfers application-layer M from one process to another, using network layer services\nencapsulates application-layer M with transport layer header $H_t$ to create a transport-layer segment\nnetwork\ntransfer transport-layer segment $[H_t,M]$ from one host to another, using link layer services\nencapsulates transport-later segment $[H_t,M]$ with network layer header $H_n$ to create a network-layer datagram\nlink\ntransfer network-layer datagram $[H_n|H_t,M]$ from host to neighboring host, using physical layer services\nencapsulates network-later datagram $[H_n|H_t,M]$, with link-layer header $H_l$ to crate a link-layer frame\nHistory 1961-1972: early packet-switching principles 1972-1980： internetworking; new, proprietary networks 1980-1990: new protocols, many new networks 1990-2000s: commercialization, the Web, new applications 2005-now: more application, mobility, cloud Lecture 2 (application layer) client-server paradigm server\nalways-on host permanent IP address often in data centers, for scaling clients\ncontact, communicate with server may be intermittently connected may have dynamic IP addresses do not communicate directly with each other example\nHTTP, IMAP, FTP\npeer-to-peer architecture no always-on host arbitrary end systems directly communicate peers request service from other peers, provide service in return to other peers peers are intermittently connected and change IP addresses example: P2P file sharing process communicating process: program running within a host\nwithin same host, two processes communicate using inter-process communication (defined by OS) processes in different hosts communicate by exchanging messages client process: process that initiates communication server process: process that waits to be contacted sockets process sends/receives messages to/from its socket socket analogous to door sending process shoves message out door sending process relies on transport infrastructure on other side of door to deliver message to socket at receiving process addressing processes to receive messages, process must have identifier which includes both IP address and port numbers associated with process on host\napplication-layer protocol An application-layer protocol defines:\ntypes of messages exchanged\ne.g. request, response\nmessage syntax\nwhat fields in messages \u0026amp; how fields are delineated\nmessage semantics\nmeaning of information in fields\nrules\nfor when and how send \u0026amp; respond to messages\ntype of protocol\nopen protocols defined in RFCs, everyone has access to protocol definition allows for interoperability e.g. HHTP,SMTP proprietary protocols e.g. Skype, Zoom Internet transport protocols services TCP service\nreliable transport\nflow conrtol\nsender won\u0026rsquo;t overwhelm receiver\ncongestion control\nthrottle sender when network overloaded\nconnection-oriented\nsetup required between client and server processes\ndoes not provide: timing, minimum throughput guarantee, security\nUDP service\nunreliable transfer HTTP hypertext transfer protocol (application-layer)\n\u0026ldquo;stateless\u0026rdquo;: server maintains no information about past client requests\nnon persistent\ntcp connection opened at most one object sent over tcp connection tcp connection closed downloading multiple objects required multiple connections\npersistent (HTTP1.1)\ntcp connection opened multiple objects can be sent over single tcp connection tcp connection closed cookies used to maintain some state between transactions\ncan be used for\nauthorization shopping carts recommendations user session state Email user agents mail servers simple mail transfer protocol: SMTP SMTP SMTP handshaking SMTP transfer of messages SMTP closure IMAP Internet Mail Access Protocol: messages stored on server, IMAP provides retrieval, deletion, folders of stored messages on server\nDomain Name System (DNS) distributed database implemented in hierarchy of many name servers\napplication-layer protocol: hosts, DNS servers communicate to resolve name( address/name translation)\nservices:\nhostname-to-IP-address translation\nhost aliasing\ncanonical, alias names\nmail server aliasing\nload distribution\nreplicated Web servers: many IP addresses correspond to one name\nHierarchy root\nofficial, contact-of-last-resort by name servers that can not resolve name\nTop level Domain\nAuthoritative\nLocal DNS name servers when host makes DNS query, it is sent to its local DNS server\nLocal DNS server returns reply, answering: from its local cache of recent name-to-address translation pairs( possibly out of dates) forwarding request into DNS hierarchy for resolution each ISP has local DNS name server DNS name resolution iterated query\nrecursive query\nputs burden of name resolution on contacted name server\nCaching DNS information one name server learns mapping, it caches mapping, and immediately returns a cached mapping in response to a query\nimproves response time cache entries timeout (disappear) after some time (TTL) TLD server typically cached in local name servers cached entries may be out-of-date DNS records resource records(RR) format: (name, value, type, ttl)\nType A\nname is host\nvalue is IP address\nNS\nname is domain (e.g. foo.com)\nvalue is hostname of authoritative name server for this domain\nCNAME\nname is alias name for some \u0026ldquo;canonnical\u0026rdquo; (the real) name\nvalue is canonical name\ne.g. www.ibm.com is really servereasy.backup2.ibm.com\nDNS protocal messages DNS query and reply messages, both have same format\nmessage header\nidentification\n16 bit ## for query\nreply to query uses same #\nflags\nquery or reply recursion desired recursion avaliable reply is authoritative P2P two main challenges the peers may join or leave the network, so the service provided by a particular peer will come and go the peer address is likely to change BitTorrent P2P file distribution\nfile devided into 256kb chunks peers in torrent send/receive file chunks participant\ntracker\ntracks peers participating in torrent\ntorrent\ngroup of peers exchanging chunks of a file\nVideo streaming and CDNs stream video traffic: major consumer of Internet bandwidth\nvideo\nsequence of images displayed at constant rate\ndigital image\narray of pixels\neach pixel represented by bits\ncoding\nuse redundancy within and between images to decrease ## bits used to encode image\nspatial temporal two type of video encoding method\nCBR: (constant bit rate)\nvideo encoding rate fixed\nVBR: (variable bit rate)\nvideo encoding rate changes as amout of spatial, temporal encoding changes\nstreaming\nclient playout early part of video, while server still sending later part of video\nmain challenges\nserver-to-client bandwidth will vary over time packet loss, delay due to congestion DASH Dynamic, Adaptive Streaming over HTTP\nserver divides video file into multiple chunks each chunk encoded at multiple different rates different rate encodings stroed in different files files replicated in various CDN nodes manifest file: provides URLs for different chunks client periodically estimates server-to-client bandwidth consulting manifest, requests one chunk at a time chooses maximum coding rate sustainable given current bandwidth can choose different coding rates at different points in time (depending on available bandwidth at time), and from different servers CDNs store/serve multiple copies of videos at multiple geographically distributed sites\nenter deep\npush CDN servers deep into many access networks\nbring home\nsmaller number of larger clusters in POPs near access nets\nSocket Programming socket: the only api that sits between application layer and transport layer\nLecture 3(Transport Layer) provide logical communication between application processes running on different hosts\nactions in end systems:\nsender: break application messages into segments, passes to network layer receiver: reassembles segments into messages, passes to application layer Transport layer vs. Network layer network layer: logical communication between hosts\ntransport layer: logical communication between processes\nrelies on, enhances, network layer services\nanalogy\n12 kids in Ann\u0026rsquo;s house sending letters to 12 kids in Bill\u0026rsquo;s\nhosts=houses processes=kids app messages =letters in envelops transport protocol= Ann and Bill who demux to in-hohuse siblings network-layer protocol= postal service Actions Sender: is passed an application-layer message determines segment header fields values creates segment passes segment to IP Receiver receives segment from IP checks header values extracts application-layer message demultiplexes messages up to application via socket Types TCP: transmission control protocol reliable, in-order delivery congestion control flow control connection setup UDP: user datagram protocol unreliable, unordered delivery no-frills extension of \u0026ldquo;best-effort\u0026rdquo; IP Services not available: delay guarantees bandwidth guarantees Demultiplexing and multiplexing demultiplexing at receiver:\nuse header info to deliver received segments to correct socket\nhost receives IP datagrams\neach datagram has source IP address, destination IP address each datagram carries one transport-layer segement each segment has source, destination number host uses IP address \u0026amp; port number to direct segment to appropriate socket\nconnection-oriented demultiplex tcp socket identified by 4-tuple:\nsource IP address source port number dest IP address dest port number UDP \u0026ldquo;no frills\u0026rdquo;,\u0026ldquo;bare bones\u0026rdquo; Internet transport protocol \u0026ldquo;best effort\u0026rdquo; service, UDP segments may be： lost delivered out-of-order to app connectionless no handshaking each UDP segment handled independently segment format source port dest port length checksum application data checksum goal: detect errors in transmitted segment\nsender treat contents of UDP segment( including UDP header fields and IP addresses) as sequence of 16-bit integers checksum: addition (one\u0026rsquo;s complement sum) of segment content put checksum value into UDP checksum field receiver compute checksum check if equals Reliable Data Transfer \u0026hellip;\nTCP sequence number\nindicate the byte stream number of the first byte in segment payload\ncount of bytes( not segments)\nAcknowledgement number\nused by receiver to tell sender the sequence number of the next byte that\u0026rsquo;s expected to be received from the sender\nserves as a cumulative acknowledgement for all bytes of data that have occurred before that sequence number and\ncongestion control multiple senders/receivers\nend-end congestion control no explicit feedback from network congestion inferred from observed loss, delay approch taken by TCP network-assisted congestion control routers provides direct feedback to sending/receiving hosts with flows passing through congested router may indicate congestion level or explicitly set sending rate TCP congestion control Loss-based AIMD senders can increase sending rate until packet loss (congestion) occurs, then decrease sending rate on loss event\nAdditive Increase\nincrease sending rate by 1 maximum segment size every RTT until loss detected\nMultiplicative Decrease\ncut sending rate in half at each loss event\nslow start Delay-based Keeping the just pipe full but not fuller\nflow control one sender one receiver\nLecture 4 (Network-layer Data Plane) transport segment from sending to receiving host\nsender\nencapsulates segments into datagrams, passes to link layer\nreceiver\ndelivers segments to transport layer protocol\nin every internet devices: hosts, routers\nrouters\nexamines header fields in all IP datagrams passing through it moves datagrams from input ports to output ports to transfer datagrams along end-end path Two key function forwarding\nmove packets from a router\u0026rsquo;s input link to appropriate router output link\nrouting\ndetermine route taken by packets from source to destination\nRouter Input port functions decentralized switching using header field values, lookup output port using forwarding table in input port memory(\u0026quot; match plus action\u0026quot;) destination-based forwarding: forward based only on destination IP address (traditional) generalized forwarding: forward based on any set of header field values Longest prefix matching when looking for forwarding table entry for given destination address, use longest address prefix that matches destination address\nSwitching fabrics transfer packet from input link to appropriate output link switching rate: rate at which packets can be transfer from inputs to outputs often measured as multiple of input/output line rate N inputs: switching rate N times line rate desirable type via memory\nfirst generation routers:\ntraditional computers with switching under direct control of CPU via bus\ndatagram from input port memory to output port memory via a shared bus\nvia interconnection network\ncan exploiting parallelism:\nfragment datagram into fixed length cells on entry switch cells through the fabric, reassemble datagram at exit Input port queueing if switch fabric slower than input ports combined queueing delay and loss due to input buffer overflow Head-of-the-Line(HOL) blocking: queued datagram at front of queue prevents others in queue from moving forward output port queueing buffering required when datagrams arrive from fabric faster than link transmission rate.\ndrop policy:\ntail drop: drop arriving packet priority: drop on priority basis scheduling discipline chooses among queued datagrams for transmission\nfirst come, first served priority round robin weighted fair queueing Internet Protocol IPV4 IP Datagram format address ip address\n32-bit identifier associated with each host or router interface\ninterface\nconnection between host/router and physical link\nrouter\u0026rsquo;s typically have multiple interfaces host typically has one or two interfaces (e.g., wired Ethernet, wireless 802.11) blue are are link layer detail\nsubnet part devices in same subnet have common high order bits\nhost part\nremaining low order bits\nways to get IP hard-coded by sysadmin in config file\nDynamic Host Configuration Protocol: dynamically get address from a server\nplug-and-play CIDR Classless InterDomain Routing( pronounced \u0026ldquo;cider\u0026rdquo;)\nsubnet portion of address of arbitrary length address format: a.b.c.d/x, where x is bit count in subnet portion of address Subnets definition\ndevice interfaces that can physically reach each other without passing through an intervening router a piece of the network that contains all devices that can reach each other without passing through a network layer router DHCP host dynamically obtains IP address from network server when it \u0026ldquo;joins\u0026rdquo; network\ncan renew its lease on address in use allows reuse of addresses (only hold address while connected/on) support for mobile users who join/leave network steps\nDHCP discover (optional)\nhsot broadcasts DHCP discover msg\nDHCP offer (optional)\nDHCP server responds with DHCP offer msg\nabove two steps can be skipped if a client remembers and wishes to reuse a previously allocated network address\nDHCP request\nhost requests IP address with DHCP request msg\nDHCP ack\nDHCP server sends address: DHCP ack msg\ntypically, DHCP server will be co-located in router, serving all subnets to which router is attached\nDHCP can return more than just allocated IP address on subnet:\naddress of first-hop router for client name and IP address of DNS server network mask (indicating network versus host portion of address) ICANN internet corporation for assigned names and numbers\nallocates IP addresses, through 5 regional registries( RRs) (who may allocate to local registries) manages DNS root zone, including delegation of individual TLD management NAT network address translation:\nall devices in local network share just one IPv4 address as far as outside world is concerned\nall devices in local network have 32-bit address in a \u0026ldquo;private\u0026rdquo; IP address space (10/8,172.16/12,192.168/16 prefixes) that can only be used in local network advantages: just one IP address needed form provider ISP for all devices can change addresses of host in local network without notifying outside world can change ISP without changing addresses of devices in local network security: devices inside local net not directly addressable, visible by outside world implimentation outgoing datagrams replacement translation pair remembrance incoming datagrams replacement IPV6 tunneling:\nipv6 datagram carried as payload in ipv4 datagram among ipv4 routers\nGeneralized Forwarding match plus action\nmany header fields can determine action many action possible: drop/copy/modify/log packet Flow table flow: defined by header field values (in link-,network-,transport layer fields) Lecture 5 (Network-layer Control Plane) Per-router control plane Routing algorithm determine \u0026ldquo;good\u0026rdquo; paths (equivalently routes), from sending hosts to receiving host, through network of routers\npath: sequence of routers packets traverse from given initial source host to final destination host \u0026ldquo;good\u0026rdquo;: least \u0026ldquo;cost\u0026rdquo;, \u0026ldquo;fastest\u0026rdquo;, \u0026ldquo;least congested\u0026rdquo; classification pers1\nglobal: all routers have complete topology, link cost info\n\u0026ldquo;link state\u0026rdquo; algorithm\ne.g. Dijkstra\ndecentralized: iterative process of computation, exchange of info with neighbors\nrouter initially only know link costs to attached neighbors\n\u0026ldquo;distance vector\u0026rdquo; algorithms\npers2\nstatic: routes change slowly over time dynamic: routes change more quickly periodic updates or in response to link cost changes scalable routing aggregate routers into regions known as \u0026ldquo;autonomous systems\u0026rdquo;(AS) (a.k.a \u0026ldquo;domains\u0026rdquo;)\nintra-AS (\u0026ldquo;intra-domain\u0026rdquo;) routing among routers within same AS (\u0026ldquo;netwrok\u0026rdquo;)\nall routers in AS must run same intra-domain protocol routers in different AS can run different intra-domain routing protocols gateway router: at \u0026ldquo;edge\u0026rdquo; of its own AS, has link(s) to router(s) in other AS\u0026rsquo;es most common intra-AS routing protocols\nRIP routing information protocol OSPF open shortest path first EIGRP enhanced interior gateway routing protocol inter-AS (\u0026ldquo;inter-domain\u0026rdquo;) routing among AS\u0026rsquo;es\ngateways perform inter-domian routing BGP( Border Gateway Protocol): the de facto inter-domain routing protocol\nallows subnet to advertise its exsistence, and the destinations it can reach, to rest of Internet BGP provides each AS a means to: obtain destination network reachability info from neighboring ASes (eBGP) determine routes to other networks based on reachability infomation and policy propagate reachability information to all AS-internal routers (iBGP) advertise (to neighboring) destination reachability info ICMP internet control message protocol\nused by hosts and routers to communicate network-level information error reporting: unreachable host, network, port, protocol echo request/reply (used by ping) network layer \u0026ldquo;above\u0026rdquo; IP: ICMP messages carried in IP datagrams, protocol number: 1 ICMP message: type, code plus header and first 8 bytes of IP datagram causing error Lecture 6 (Link-Layer) Terminology nodes\nhosts, routers\nlinks\ncommunication channels that directly connect physically adjacent nodes\nwired, wireless LANs frame\nlayer-2 packet encapsulates datagram\nContext datagram transferred by different link protocols over different links e.g. WiFi on first link, Ethernet on next link each link protocol provides different services e.g. may or may not provide reliable data tranfer over link Services framing, link access encapsulate datagram into frame, adding header, trailer channel access if shared medium \u0026ldquo;MAC\u0026rdquo; accesses in frame headers identify source, destination reliable delivery between adjacent nodes flow control pacing between adjacent sending and receiving nodes error detection error caused by signal attenuation, noise receiver detects errors, signals retranssmision, or drop frame error correction receiver identifies and corrects bit error(s) without retransmission half-duplex and full-duplex with half duplex, nodes at both ends of link can transmit, but not at same time implementation in each-and-every host link layer implemented on-chip or in network interface card(NIC) attaches into host\u0026rsquo;s system buses combination of hardware, software, firmware MAC addresses used \u0026ldquo;locally\u0026rdquo; to get frame from one interface to another physically connected interface 48-bits MAC address burned in NIC ROM ,also sometimes software settable MAC address allocation administered by IEEEE manufacturer buys portion of MAC address space (to assure uniqueness) analogy MAC address: like social security number IP address: like postal address MAC flat address: portability can move interface from one LAN to another recall IP address not portable: depends on IP subnet to which node is attac each interface on LAN\nhas unique 48-bit MAC address has a locally unique 32-bit IP address Multiple access links point to point point-to-point link between Ethernet switch, host PPP for dial-up access broadcast( shared wire or medium) old-school Ethernet upstream HFC in cable-based access network 802.11 wireless LAN, 4G/5G. satellite Multiple access protocols why\nsingle shared broadcast channel\ntwo or more simultaneous transmissions by nodes: interference\ncollision if node receives two or more signals at the same time distributed algorithm that determines how nodes share channel, i.e. ,determine which node can transmit\ncommunication about channel sharing must use channel itself\nno out-of-band channel for coordination taxonomy taking turns nodes take turns, but nodes with more to send can take longer turns random access channel not divided, allow collisions \u0026ldquo;recover\u0026rdquo; from collisions channel partitioning divide channel into smaller \u0026ldquo;pieces\u0026rdquo; (time slots, frequency, code) allocate piece to node for exclusive use channel partitioning TDMA time division multiple access\naccess to channel in \u0026ldquo;rounds\u0026rdquo; each station gets fixed length slot (length= packet transmission time) in each round unused slots go idle FDMA frequency division multiple access\nchannel spectrum divided into frequency bands each station assigned fixed frequency band unused transmission time in frequency bands go idle Random access when node has packet to send transmit at full channel data rate R no a priori coordination among nodes two+ sending nodes: \u0026ldquo;collision\u0026rdquo; random access protocol specifies: when to send how to detect collisions how to recover from collisions (e.g., via delayed retransmissions) examples ALOHA CSMA, CSMA/CD, CSMA/CA Slotted ALOHA allow collision to happen (and then recover via retransmission)\nuse randomization in choosing when to retransmit\nsetting\nall frames same size time divided into equal size slots (time to transmit 1 frame) nodes are synchronized nodes begin transmissions (if any) at slot start times if 2 or more nodes transmit in slot, collision detected by sender operation\nwhen node has new frame to send, transmit in next slot if no collision: success if collision: node retransmits frame in each subsequent slot with probability p until success Pros\nsingle active node can continuously transmit at full rate of channel highly decentralized: only slots in nodes need to be in sync simple Cons\nsynchronization collision, \u0026ldquo;wasting\u0026rdquo; slots idle slots, \u0026ldquo;wasting\u0026rdquo; slots efficiency: long-run fraction of successful slots (many nodes, all with many frames to send)\n\u0026hellip;\nat best 37%\nCSMA carrier sense multiple access\nsinle CSMA\nlisten before transmit\nif channel sensed idel: transmit entire frame if channel sensed busy: defer transmission CSMA/CD\nCSMA with collision detection\ncollisions detected within short time\ncolliding transmissions aborted, reducing channel wastage\ncollision detection easy in wired, difficult with wireless\nreduces the amount of time wasted in collisions\nalgorithm\nEthernet receives datagram form network layer, creates frame\nif Ethernet senses channel:\nif idle: start frame transmission\nif busy: wait until channel idle, then transmit\nif entire frame transmitted without collision: done\nif another transmission detected while sending: abort, send jam signal\nafter aborting, entire binary (enponential) backoff:\nafter mth collision, chooses K at random from {0,1,2,\u0026hellip;,2^m-1}. NIC waits K*512 bit times, returns to step 2\nmore collisions: longer backoff interval\ncollisions collision can still occur with carrier sensing: propagation delay means two nodes may not hear each other\u0026rsquo;s just started transmission collision: entire packet transmission time wasted distance \u0026amp; propagation delay play role in determining collision probability taking turns channel allocated explicitly nodes won\u0026rsquo;t hold channel for long if nothing to send two approaches: polling, token passing polling centralized controller uses polling messages to \u0026ldquo;invite\u0026rdquo; client nodes to transmit in turn token passing control token message explicitly passed from one node to next, sequentially transmit while holding token ARP address resolution protocol\nARP table: each IP node (host, router) on LAN has table\nIP/MAC address mappings for some LAN nodes:\n\u0026lt;IP,MAC,TTL\u0026gt;\nTTL(time to live): time after which address mapping will be forgotten (typically 20 min) Ethernet \u0026ldquo;dominant\u0026rdquo; wired LAN technology\nfirst widely used LAN technology simpler, cheap kept up with speed race: 10Mbps- 400 Gbps single chip, multiple speeds (e.g., Broadcom BCM5761) topology bus popular through mid 90s all nodes ihn same collision domain (can collide with each other) switched prevails today active link-layer 2 switch in center each \u0026ldquo;spoke\u0026rdquo; run a (seperate) Ethernet protocol (nodes do not collide with each other) frame structure sending interface encapsulates IP datagram (or other network layer protocol packet) in Ethernet frame\npreamble\nused to synchronize receiver, sender clock rates 7 bytes of 10101010 followed by one byte of 10101011 dest. source address\n6 byte mac address\nif adapter receives frame with matching destination address, or with broadcast address (e.g., ARP packet), it passes data in frame to network layer protocol otherwise, adapter discards frame type\nindicates higher layer protocol mostly IP but others possible e.g., Novell IPX, AppleTalk used to demultiplex up at receiver CRC\ncyclic redundancy check at receiver\nerror detected: frame is dropped feature connectionless: no handshaking between sending and receiving NICs\nunreliable: receiving NIC doesn\u0026rsquo;t send ACKs or NAKs to sending NIC\ndata in dropped frames recovered only if initial sender uses higher layer rdt (e.g. TCP), otherwise dropped data lost\nMAC protocol: unslotted CSMA/CD with binary backoff\nstandards standards:link \u0026amp; physical layers\nmany different Ethernet standards\ncommon MAC protocol and frame format different speeds: 2Mbps, 10Mbps, 100Mbps\u0026hellip; different physical layer media: fiber, cable Switch link-layer device: take an active role\nstore, forward Ethernet frames\nexamine incoming frame\u0026rsquo;s MAC address, selectively forward frame to one-or-more outgoing links when frame is to be forwarded on segment, uses CSMA/CD to access segment\ntransparent\nhosts unaware of presence of switches\nplug-and-play, self-learning don\u0026rsquo;t need to be configured\nmultiple simultaneous transmissions hosts have dedicated, direct connection to switch switches buffer packets Ethernet protocol used on each incoming link, so: no collisions; full duplex each link is tis own collision domain self-learning map host and mac address by link and source address in frame\nframe destination location known\nselectively send\nframe destination location unknown\nflood\nVLAN motivation\nsingle broadcast domain scaling efficiency, security, privacy, efficiency issues administrative issues definition\nswitch supporting VLAN capabilities can be configured to define multiple virtual LANs over single physical LAN infrastructure\nPort-based VLANs switch ports grouped (by switch management software) so that single physical switch operates as multiple virtual switches\n","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/computer-networknotes/","title":"【Computer Network】Notes"},{"content":"History 1959 Hubel\u0026amp; Wiesel\n1963 Roberts\n1970s David Marr\n1979 Gen.Cylinders\n1986 Canny\n1997 Norm.Cuts\n199 SIFT\n2001 V\u0026amp;J\none of the first successful applications of machine learning to vision\n2001 PASCAL\n2009 ImageNet\nOlympics of computer vision\n2012 AlexNet\ndeep learning\nAnother viewPoint 1958 perceptron\n1969 Minsky \u0026amp; Papert\nshowed that perceptrons could not learn the XOR function caused a lot of sidillusionment in the field\n1980 Neocognition: Fukushima\nComputational model the visual system, directly inspired by Hubel and Wiesel\u0026rsquo;s hierarchy of complex and simple cells\ninterleaved simple cells(convolution) and complex cells(pooling)\nno practical training algorithm\nBackprop: Rumelhart, Hinton, and williams, 1986\nintroduced backpropagation for computing gradients in neural networks\nsuccessfully trained perceptrons with multiple layers\nConvolutional Networks:LeCun et al, 1998\ndeep learning 2006\npeople tried to train neural networks that were deeper and deeper\nnot a mainstream research topic at this time\nReason Algorithms Data Computation DataSet CIFAR10 CIFAR100 ImageNet Places365 type train: train the model validation :choose hypterparameter test: check out performance on new data cross-validation split data into folds,try each fold as validation and average the results\nK-Nearest Neighbor very slow at test time distance metrics on pixels are not infomative Linear Classifier Three Viewpoints Algebraic Viewpoint\n$f(x,W)=Wx$\nVisual Viewpoint\nOne template per class\nGeometric Viewpoint\nHyperplanes cutting up space\nLoss Function a loss function tells how good our current classifier is\nAlso called: object function, cost function\nNegative loss function sometimes called reward function, profit function, utility function, fitness function, etc\nGiven a dataset $$ {(x_i,y_i)}^N_{i=1} $$ where $x_i$ is image and $y_i$ is (integer) label\nLoss function for it may be $$ L_i(f(x_i,W),y_i) $$ Loss for the dataset is average of per-example losses: $$ L=\\frac{1}{N}\\sum_iL_i(f(x_i,W),y_i) $$\nSVM Let $s=f(x_i,W)$ be scores\nThen the SVM loss has the form: $$ L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+margin) $$\nCross-Entropy(Multinomial Logistic Regression) interpret raw classifier scores as probabilities $$ s=f(x_i;W) $$ softmax $$ P(Y=k \\lvert X=x_i)=\\frac{e^{s_k}}{\\sum_je^{ s_j}} $$ then $$ L_i=-logP(Y=y_i|X=x_i) $$\nRegularization prevent the model from doing too well on training data $$ L(W)=L=\\frac{1}{N}\\sum_iL_i(f(x_i,W),y_i)+\\lambda R(W) $$ where $\\lambda$ is the regularization strength\nL2 Regularization\n$R(W)=\\sum_k\\sum_lW^2_{k,l}$\nL1 Regularization\n$R(W)=\\sum_k\\sum_l\\lvert W_{k,l}\\lvert$\nElastic net(L1+L2)\n$R(W)=\\sum_k\\sum_l\\beta W^2_{k,l}+\\lvert W_{k,l}\\lvert$\nPurpose express preferences in among models beyond \u0026ldquo;minimize training error\u0026rdquo; avoid overfitting: prefer simple models that generalize better improve optimization by adding curvature Optimization $$ w^*=arg,min_wL（w） $$\nSGD stochastic gradient descent $$ x_{t+1}=x_{t}-\\alpha \\nabla f(x_t) $$\nSGD+Momentum $$ v_{t+1}=\\rho v_{t}-\\alpha \\nabla f(x_t) \\newline x_{t+1}=x_t+ v_{t+1} $$\nadd velocity\nNesterov Momentum look ahead $$ v_{t+1}=\\rho v_t-\\alpha\\nabla f(x_t+\\rho v_{t}) \\newline x_{t+1}=x_t+v_{t+1} $$ let $\\tilde{x_t}=x_t+\\rho v_t$ $$ v_{t+1}=\\rho v_t-\\alpha \\nabla f(\\tilde{x_t}) \\newline \\tilde{x_{t+1}}=\\tilde{x_t}+v_{t+1}+\\rho(v_{t+1}-v_t) $$\nAdaGrad $$ S_{t+1}=S_{t}+\\nabla f(x_t)^2 \\newline x_{t+1}=x_t-\\alpha \\frac{\\nabla f(x_t)}{\\sqrt{S_{t}}+1e-7} $$\nmight stop before converge\nRMSProp \u0026ldquo;Leak Adagrad\u0026rdquo; $$ S_{t+1}=\\rho S_{t}+(1-\\rho)\\nabla f(x_t)^2 \\newline x_{t+1}=x_t-\\alpha \\frac{\\nabla f(x_t)}{\\sqrt{S_{t}}+1e-7} $$\nAdam RMSProp+Momentum $$ v_{t+1}=\\rho v_{t}+\\nabla f(x_t) \\newline s_{t+1}=\\beta s_{t}+(1-\\beta)\\nabla f(x_t)^2 \\newline v^{\u0026rsquo;}{t+1}=\\frac{v{t+1}}{1-\\rho ^t} \\newline s^{\u0026rsquo;}{t+1}=\\frac{s{t+1}}{1-\\beta ^t} \\newline x_{t+1}=x_t-\\alpha \\frac{v^{\u0026rsquo;}{t+1}}{\\sqrt{s^{\u0026rsquo;}{t+1}}+1e-7} $$\nNeural Network Activation Functions Sigmoid\n$\\sigma(x)=\\frac{1}{1+e^{-x}}$\ntanh\n$tanh(x)$\nReLU\n$max(0,x)$\nLeaky ReLU\n$max(0.1x,x)$\nMaxout\n$max(w_1^Tx+b1,w_2^Tx+b2)$\nELU\n$\\left{ \\begin{array}{lc} x \u0026amp; x \\geqslant 0 \\ \\alpha(e^x-1)\u0026amp;x\u0026lt;0\\ \\end{array} \\right.$\nBack propagation $$ \\frac{\\partial L}{\\partial y}=\\frac{\\partial g}{\\partial y}\\frac{\\partial L}{\\partial g} $$\nwhere\n$\\frac{\\partial y}{\\partial f}$ is the downstream gradient\n$\\frac{\\partial y}{\\partial g}$ is the local gradient\n$\\frac{\\partial g}{\\partial f}$ is the upstream gradient\nConvolutional Networks Convolutional Layer evaporative Input W Filter: K Output: W-K+1 add Padding\nPadding: P Output: W-K+1+2P same padding: Input equals Output\nReceptive Fields each successive convolution adds K-1 to the receptive field size\nWith L layers the receptive field size is 1+L*(K-1)\nto expand receptive fields\nstride: S output: (W-K+2P)/S+1 summary Input: $C_{in}\\times H \\times W$\nHyperparameters:\nKernel size: $K_H \\times K_W$ Number of filters: $C_{out}$ Padding: $P$ stride: $S$ Weight matrix: $C_{out} \\times C_{in} \\times K_H \\times K_W$\nBias Vector: $C_{out}$\nOuput: $C_{out} \\times H^{\u0026rsquo;}\\times W^{\u0026rsquo;}$\nwhere\n$H^{\u0026rsquo;}=(H-L+2P)/S+1$\n$W^{\u0026rsquo;}=(W-L+2P)/S+1$\ncommon settings:\n$K_H = K_W$ same padding $C_{out} ,C_{in} =32,64,128,256$ Pooling Layer to introduces invariance to small spatial shifts\nHyperparameters:\nkernel size stride pooling function Batch Normalization normalize the outputs of a layer so they have zero mean and unit variance\nIt helps reduce \u0026ldquo;internal covariate shift\u0026rdquo;, improves optimization $$ \\hat{x}^{(k)}=\\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}} $$ Input: $N\\times D$ when in fully connected layers. convolutional layers are the same\n$\\mu_j=\\frac{1}{N}\\sum_{i=1}^Nx_{i,j}$\nPer-channel mean, shape is $1\\times D$\naverage of values seen during training while testing\n$\\sigma_j^2=\\frac{1}{N}\\sum_{i=1}^N(x_{i,j}-\\mu_j)^2$\nper-channel std, shape is $1\\times D$\naverage of values seen during training while testing\n$\\hat{x}{i,j}=\\frac{x{i,j}-\\mu_j}{\\sqrt{\\sigma_j^2+\\epsilon}}$\nnormalized $x$, shape is $N\\times D$\n$y_{i,j}=\\gamma_j\\hat{x}_{i,j}+\\beta_j$\noutput $y$, shape is $N\\times D$\n$\\gamma$ and $\\beta$ is learnable scale and shift parameter of shape $1\\times D$\nLayer Normalization for fully connected layers\navoid different behavior between training and testing\nthe $\\mu$ and $\\sigma$ become shape of $N\\times 1$\nInstance Normalization for convolutional layers\nthe $\\mu$ and $\\sigma$ become shape of $N\\times C\\times 1\\times 1$ rather than shape of $1\\times C\\times 1\\times 1$ in batch normalization\nthe $\\gamma$ and $\\beta$ remain as shape of $1\\times C\\times 1\\times 1$\nResidual Networks deep networks hard to train\na residual network is a stack of many residual blocks\nProgramming GPUs CUDA(NVIDIA only)\nOpenCL\nsimilar to CUDA, but runs on anything\nusually slower on NVIDIA hardware\nPytorch Three levels of abstraction Tensor\nlike a numpy array, but can run on GPU\nAutograd\nPackage for building computational graphs out of Tensors, and automatically computing gradients\nModule\nA neural network layer; may store state or learnable weights\nTraining Neural Networks Activation Function Sigmoid \u0026lsquo;kill\u0026rsquo; the gradient when saturated outputs are not zero-centered exp() is a bit compute expensive tanh zero centered \u0026lsquo;kill\u0026rsquo; the gradient when saturated ReLU Does not saturate(in position region) very computationally efficient converges much faster than sigmoid/tanh in practice not zero-centered output will never activate \u0026lsquo;dead ReLU\u0026rsquo; Leaky ReLU Does not saturate(in position region) very computationally efficient converges much faster than sigmoid/tanh in practice not zero-centered output will not \u0026lsquo;die\u0026rsquo; Parametric Rectifier(PReLU) $$ f(x)=max*\\alpha x,x $$\nExponential Linear Unit(ELU) $$ \\left{ \\begin{array}{lc} x \u0026amp; x \\geqslant 0 \\ \\alpha(e^x-1)\u0026amp;x\u0026lt;0\\ \\end{array} \\right. $$\nall benefits of ReLU closer ti zero mean outputs negative saturation regime compared with Leaky ReLU adds some robustness to noise Scaled Exponential Linear Unit(SELU) $$ \\left{ \\begin{array}{lc} \\lambda x \u0026amp; x \\geqslant 0 \\ \\lambda\\alpha(e^x-1)\u0026amp;x\u0026lt;0\\ \\end{array} \\right. $$\nsummary don\u0026rsquo;t think too much. Just use ReLU Try out Leaky ReLU/ELU/SELU/GELU if you need to squeeze that last 0.1% Don\u0026rsquo;t use sigmoid or tanh Data Preprocessing normalization PCA whitening Weight Initialization small random numbers\nW=0.01*np.random.randn(Din,Dout) Xavier initialization\nW=np.random.randn(Din,Dout)/np.sqrt(Din) Kaiming/MSRA initialization\nW=np.random.randn(Din,Dout)/np.sqrt(2/Din) Regularization L2 L1 Elastic net dropout data augmentation random crops and scales translation rotation stretching shearing lens distortion fractional pooling stochastic depth cutout mixup Learning rate schedule Decay cosine $$ \\alpha_t=\\frac{1}{2}\\alpha_0(1+cos(\\frac{t\\pi}{T})) $$\nlinear $$ \\alpha_t=\\alpha_0(1-\\frac{t}{T}) $$\ninverse sqrt $$ \\alpha_t=\\frac{\\alpha_0}{\\sqrt{t}} $$ early stopping stop training the model when accuracy on the validation set decreases or train for a long time, but always keep track of the model snapshot that worked best on val.\nchoosing hyperparameters random search is better than grid search\nsteps\ncheck initial loss overfit a small sample find LR that makes loss go down coarse gird, train for 1-5 epochs refine grid,train longer look at loss curves Model Ensembles use multiple independent models use multiple snapshots of a single model during training Transfer Learning train on imagenet use cnn as a feature extractor bigger dataset: Fine-Tuning Recurrent Networks $$ h_t=f_W(h_{t-1},x_t) $$\nVanilla $$ h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t) \\newline y_t=W_{hy}h_t $$\nExploding gradients\ngradient clipping:scale gradient if its norm is too big\nvanishing gradients\nchange RNN architecture\nTruncated Backpropagation Through Time carry hidden states forward in time forever, but only backpropagate for some smaller number of steps\nLSTM compute four gates at each timestep $$ \\begin{pmatrix} i\\ f\\ o\\ g\\ \\end{pmatrix}= \\begin{pmatrix} \\sigma\\ \\sigma\\ \\sigma\\ tanh\\ \\end{pmatrix}W \\begin{pmatrix} h_{t-1}\\ x_t\\ \\end{pmatrix} $$\ni\ninput gate, whether to write cell\nf\nforget gate, whether to erase cell\no\noutput gate, how much to reveal cell\ng\ngate gate, how much to write to cell\n$$ c_t=f\\bigodot c_{t-1}+i\\bigodot g \\newline h_t=o\\bigodot tanh(c_t) $$\nwhere $c_t$ is the cell state and $h_t$ is the hidden state\nMultilayer RNN $$ h_t^l=tanhW^l\\begin{pmatrix}h_t^{l-1}\\h_{t-1}^l\\end{pmatrix} $$ where $h\\in R^n$ and $W^l [n\\times 2n]$\nLSTM $$ \\begin{pmatrix} i\\ f\\ o\\ g\\ \\end{pmatrix}= \\begin{pmatrix} sigm\\ sigm\\ sigm\\ tanh\\ \\end{pmatrix}W^l \\begin{pmatrix} h_{t}^{l-1}\\ h_{t-1}^l\\ \\end{pmatrix} \\newline $$\n$$ c_t^l=f\\bigodot c^l_{t-1}+i\\bigodot g \\newline h_t^l=o\\bigodot tanh(c_t^l) $$\nAttention above $\\sum_ia_{t,i}$ should be 1\nuse a different context vector in each timestep of decoder\nAttention Layer Input\nquery vector: $q$ (shape: $D_Q$)\ninput vectors: $X$ (shape:$N_X \\times D_X$)\nsimilarity function: $f_{att}$\nusually scaled dot product\nthe reason is\nlarge similarities will cause softmax to saturate and give vanishing gradients\nComputation\nsimilarities: $e$ (shape $N_X$)\n$e_i=f_{att}(q,X_i)$\nusually $e_i=q\\cdot X_i/sqrt(D_Q)$\nattention weights: $a=softmax(e)$ (shape $N_X$)\noutput vector: $y=\\sum_ia_iX_i$ (shape $D_X$)\nmultiple query multiply query vectors\nInput\nquery vector: $Q$ (shape: $N_Q\\times D_X$) input vectors: $X$ (shape:$N_X \\times D_Q$) Computation\nsimilarities: $E=QX^T$ (shape $N_Q\\times N_X$)\n$E_{i,j}=Q_i\\cdot X_j/sqrt(D_Q)$\nattention weights: $a=softmax(E,dim=1)$ (shape $N_Q\\times N_X$)\noutput vector: $Y=AX$ (shape $N_Q\\times D_X$)\n$Y_i=\\sum_jA_{i,j}X_j$\nseperate key and value Input\nquery vector: $Q$ (shape: $N_Q\\times D_Q$) input vectors: $X$ (shape:$N_X \\times D_X$) key matrix: $W_K$ (shape:$D_X \\times D_Q$) value matrix: $W_V$ (shape:$D_X \\times D_V$) Computation\nkey vectors: $K=XW_K$ (shape $N_X\\times D_Q$)\nvalue vectors: $V=XW_V$ (shape $N_X\\times D_V$)\nsimilarities: $E=QK^T$ (shape $N_Q\\times N_X$)\n$E_{i,j}=Q_i\\cdot K_j/sqrt(D_Q)$\nattention weights: $a=softmax(E,dim=1)$ (shape $N_Q\\times N_X$)\noutput vector: $Y=AV$ (shape $N_Q\\times D_V$)\n$Y_i=\\sum_jA_{i,j}V_j$\nSelf-Attention Layer one query per input vector\nInput\ninput vectors: $X$ (shape:$N_X \\times D_X$) key matrix: $W_K$ (shape:$D_X \\times D_Q$) value matrix: $W_V$ (shape:$D_X \\times D_V$) query matrix: $W_Q$ (shape:$D_X\\times D_Q$) Computation\nquery vectors:$Q=XW_Q$\nkey vectors: $K=XW_K$ (shape $N_X\\times D_Q$)\nvalue vectors: $V=XW_V$ (shape $N_X\\times D_V$)\nsimilarities: $E=QK^T$ (shape $N_Q\\times N_X$)\n$E_{i,j}=Q_i\\cdot K_j/sqrt(D_Q)$\nattention weights: $a=softmax(E,dim=1)$ (shape $N_Q\\times N_X$)\noutput vector: $Y=AV$ (shape $N_Q\\times D_V$)\n$Y_i=\\sum_jA_{i,j}V_j$\nmasked self-attention layer don\u0026rsquo;t let vectors \u0026ldquo;look ahead\u0026rdquo; in the sqeuence\nused for language modeling (predict next word)\nMultihead self-attention layer use $H$ independent \u0026ldquo;attention heads\u0026rdquo; in parallel\nCNN with self-attention processing sequences transformer block Transformer a Transformer is a sequence of transformer blocks\ncan be used for transfer learning\npretraining\ndownload a lot of text from the internet\ntrain a giant transformer model for language modeling\nfinetuning\nfine-tune the transformer on your own NLP task\nObject Detection Input:\nSingle RGB Image\nOutput:\nA set of detected objects, for each object predict:\nCategory label (from fixed,known set of categories) Bounding box (four numbers: x,y,width,height) challenges\nmultiple outputs multiple types of output large images method\nsliding window : too many boxes region proposals: find a small set of boxes that are likely to cover all objects often based on heuristics relatively fast to run Intersection over Union (IoU) $$ \\frac{area;of;Intersection}{Area;of;Union} $$\nOverlapping Boxes: Non-Max Suppression (NMS) problem: Object detectors often output many overlapping detections solution: post-process raw detections using NMS select next highest-scoring box eliminate lower-scoring boxes with $IoU\\gt threshold$ if any boxes remain. goto 1. Evaluating: Mean Average Precision (mAP) run object detector on all test images for each category, compute average precision(AP)=area under Precision vs Recall Curve for each detection( highest score to lowest score) if it matches some GT box with $IoU\\gt 0.5$, mark it as positive and eliminate the GT otherwise mark it as negetive Plot a point on PR curve average precision(AP)= area under PR curve mean average precision(mAP)=average of AP for each category for \u0026lsquo;COCO mAP\u0026rsquo;: compute mAP@thresh for each IoU threshold and take average \u0026ldquo;slow\u0026rdquo; R-CNN Region-Based CNN\nrun region proposal method to compute ~2000 region proposals resize each region to fixed size and run independently through CNN to predict class scores and bbox transform use scores to select a subset of region proposals to output compare with ground-truth boxes Fast R-CNN Faster R-CNN: learnable Region Proposals insert RPN to predict proposals from features\notherwise same as Fast R-CNN\njointly train with 4 losses:\nRPN classification RPN regression Object classification Object regression Semantic Segmentation Label each pixel in the image with a category label\nDon\u0026rsquo;t differentiate instances, only care about pixels\nUpsampling unpooling\nbed of nails nearest neighbor bilinear interpolation bicubic interpolation max unpooling learnable upsampling: Transposed Convolution Instance Segmentation things: object categories that can be separated into object instance stuff: object categories that cannot be separated into instances Mask R-CNN\nPanoptic Segmentation Label all pixels in the image (both things and stuff)\nfor \u0026ldquo;thing\u0026rdquo; categories also separate into ins\n3D Vision two problems\npredicting 3D shapes from single image Processing 3D input data more topics\nComputing correspondences multi-view stereo structure from motion simultaneous localization and mapping (SLAM) Self-supervised learning View Synthesis Differentiable graphics 3D Sensors Many non-Deep Learning methods alive and well in 3D\n3D Shape Representations Depth Map\nProblem: Scale/Depth Ambiguity\nScale invariant loss\nVoxel Grid\nfor each pixel, surface normals give a vector giving the normal vector to the object in the world for that pixel\n3D convolution Voxel Tubes Problem: Memory Usage\nImplicit Surface\nlearn a function to classify arbitrary 3D points as inside/outside the shape\n$o:R^3-\u0026gt;[0,1]$\nThe surface of the 3D object is the level set ${x:o(x)=\\frac{1}{2}}$\nPointCloud\nrepresent shape as a set of P points in 3D space\ncan represent fine structures without huge numbers of points requires new architectures, losses, etc doesn\u0026rsquo;t explicitly represent the surface of the shape: extracting a mesh for rendering or other applications requires post-processing Mesh\nrepresent a 3D shape as a set of triangles\nVertices: Set of V points in 3D space\nFaces: Set triangles over the vertices\nGraph Convolution $$ f^{\u0026rsquo;}i=W_0f_i+\\sum{j\\in N(i)}W_1f_j $$\nMetrics Voxel IoU\nChamfer Distance\nF1 Score $$ F1@t=2*\\frac{Precision@t*Recall@t}{Precision@t+Reca} $$\nGenerative Model Discriminative model learn a probability distribution p(y|x) used to: assign labels to data feature learning (with labels) Generative Model distribution p(x) used to: detect outliers feature learning (without label) sample to generate new data Conditional Generative Model learn p(x|y) used to: assign labels, while rejecting outliers. generate new data conditioned on input labels Taxonomy explicit density\nmodel can compute $p(x)$\nTractable density\nAutoregressive\nNADE/MADE\nNICE/RealNVP\nGlow\nFfjord\nApproximate density\nVariational\nVariational Autoencoder\nMarkov Chain\nBoltzmann Machine\nimplicit density\ndoes not explicitly compute $p(x)$, but can sample from $p(x)$\nMarkov Chain\nGSN\nDirect\nGenerative Adversarial Networks (GANs)\nAutoregressive Goal: write down an explicit function for $p(x)=f(x,W)$\nGiven dataset $x^{(1)},x^{(2)}\u0026hellip;x^{(N)}$, train the model by solving $$ W^{*}=arg,max_W\\Pi_ip(x^{i}) $$ log trick to exchange product for sum $$ arg,max_W\\sum_ilogp(x^{i}) $$ namely $$ arg,max_W\\sum_ilogf(x^{i},W) $$ this is the loss function.\nAssume x consists of multiple subparts $$ x=(x_1,x_2,x_3,\u0026hellip;,x_N) $$ break down probability using the chain rule $$ \\begin{equation} \\label{eq1} \\begin{split} p(x) \u0026amp; =p(x_1,x_2,x_3,\u0026hellip;,x_N) \\ \u0026amp; = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\u0026hellip;\\ \u0026amp;=\\Pi_{t=1}^Tp(x_t|x_1,\u0026hellip;,x_{t-1}) \\end{split} \\end{equation} $$\nPixelRNN Generate image pixels one at a time, starting at the upper left corner\nCompute a hidden state for each pixel that depends on hidden states and RGB values from the left and from above $$ h_{x,y}=f(h_{x-1,y},h_{x,y-1},W) $$ PixelCNN still generate image pixels starting from corner\ndependency on previous pixels now modeled using a CNN over context region\nPros and Cons Pros can explicitly compute likelihood $p(x)$ explicit likelihood of training data gives good evaluation metric good samples Cons sequential generation =\u0026gt; slow Variational Autoencoder variational autoencoder define an intractable density that we cannot explicitly compute or optimize\nBut we will be able to directly optimize a lower bound on the density\nAutoencoders unsupervised method for learning feature vectors from raw data x, without any labels\nfeatures should extract useful information that we can use for downstream tasks\nidea: use the features to reconstruct the input data with a decoder\nfeatures need to be lower dimensional that the data\nafter training, throw away decoder and use encoder for a downstream task\nVariational Autoencoder assume training data ${x^{(i)}}_{i=1}^N$ is generated from unobserved (latent) representation $z$\nassume simple prior $p(z)$, e.g. Gaussian\nrepresent $p(x|z)$ with a neural network (similar to decoder from autoencoder)\ndecoder must be probabilistic:\nDecoder input $z$, output mean $\\mu_{x|z}$ and (diagonal) covariance $\\sum_{x|z}$\nsample $x$ from Gaussian with mean $\\mu_{x|z}$ and (diagonal) covariance $\\sum_{x|z}$\nso encoder and decoder be like\n$$ \\begin{equation} \\label{eq2} \\begin{split} logp_{\\theta}(x) \u0026amp; =log\\frac{p_{\\theta}(x|z)p(z)}{p_{\\theta}(z|x)}\\ \u0026amp; = log\\frac{p_{\\theta}(x|z)p(z)q_{\\phi}(z|x)}{p_{\\theta}(z|x)q_{\\phi}(z|x)}\\ \u0026amp;=logp_{\\theta}(x|z)-log\\frac{q_{\\phi}(z|x)}{p(z)}+log\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)}\\ \u0026amp;=E_z[logp_{\\theta}(x|z)]-E_z[log\\frac{q_{\\phi}(z|x)}{p(z)}]+E_z[log\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)}]\\ \u0026amp;=E_{z\\sim q_{\\phi}(z|x)}[logp_{\\theta}(x|z)]-D_{KL}(q_{\\phi}(z|x),p(z))+D_{KL}(q_{\\phi}(z|x),p(z))\\ \u0026amp;\\ge E_{z\\sim q_{\\phi}(z|x)}[logp_{\\theta}(x|z)]-D_{KL}(q_{\\phi}(z|x),p(z)) \\end{split} \\end{equation} $$ jointly train encoder q and decoder p to maximize the variational lower bound on the data likelihood\nrun input data through encoder to get a distribution over latent codes (encoder output should match the prior $p(z)$) sample $z$ from encoder output run sampled $z$ through decoder to get a distribution over data samples original input data should be likely under the distribution output generate new data sample $z$ from prior $p(z)$ run sampled $z$ through decoder to get distribution over data $x$ sample from distribution obtained above to generate data Pros and Cons Pros principled approach to generative models allows inference of $q(z|x)$, can be useful feature representation for other tasks Cons maximizes lower bound of likelihood: okay, but not as good evaluation as PixelRNN/PixelCNN samples blurrier and lower quality compared to state-of-the-art (GANs) GAN Generative adversarial networks give up on modeling $p(x)$, but allow us to draw samples from $p(x)$\nAssume we have data $x_i$ drawn from distribution $p_{data}(x)$. Want to sample from $p_{data}$\nIntroduce a latent variable $z$ with simple prior $p(z)$.\nSample $z\\sim p(z)$ and pass to a Generator network $x=G(z)$\nThen $x$ is a sample from the Generator distribution $p_G$. want $P_G=P_{data}$\nTrain Generator Network G to convert $z$ into fake data $x$ sampled from $p_G$\nby \u0026ldquo;fooling\u0026rdquo; the discriminator D\nTrain Discriminator Network D to classify data as real or fake(1/0)\njointly train generator G and discriminator D with a minmax game\n$$ \\begin{equation} \\begin{split} \u0026amp; min_G,max_D(E_{x\\sim p_{data}}[logD(x)]+E_{z\\sim p(z)}[log(1-D(G(z)))])\\ \u0026amp;=min_G,max_DV(G,D) \\end{split} \\end{equation} $$ update $$ D=D+\\alpha_D\\frac{\\partial V}{\\partial D} \\newline G=G-\\alpha_G\\frac{\\partial V}{\\partial G} $$ this minimax game achieves its global minimum when $p_G=p_{data}$\n=\u0026gt;some mathematical derivation\nAssignments pytorch PyTorch is an open source machine learning framework. At its core, PyTorch provides a few key features:\nA multidimensional Tensor object, similar to numpy but with GPU accelleration. An optimized autograd engine for automatically computing derivatives A clean, modular API for building and deploying deep learning models A torch tensor is a multidimensional grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the tensor; the shape of a tensor is a tuple of integers giving the size of the array along each dimension.\nAccessing an element from a PyTorch tensor returns a PyTorch scalar; we can convert this to a Python scalar using the .item() method\ntensor indexing slice indexing\nSlicing a tensor returns a view into the same data, so modifying it will also modify the original tensor. To avoid this, you can use the clone() method to make a copy of a tensor.\ninteger indexing\nWe can also use index arrays to index tensors; this lets us construct new tensors with a lot more flexibility than using slices.\na[idx0, idx1] is equi torch.tensor([ a[idx0[0], idx1[0]], a[idx0[1], idx1[1]], ..., a[idx0[N - 1], idx1[N - 1]] ]) A one-hot vector for an integer n is a vector that has a one in its nth slot, and zeros in all other slots. One-hot vectors are commonly used to represent categorical variables in machine learning models.\nboolean indexing\nBoolean tensor indexing lets you pick out arbitrary elements of a tensor according to a boolean mask. Frequently this type of indexing is used to select or modify the elements of a tensor that satisfy some condition.\nIn PyTorch, we use tensors of dtype torch.bool to hold boolean masks.\nReshape PyTorch provides many ways to manipulate the shapes of tensors. The simplest example is .view(): This returns a new tensor with the same number of elements as its input, but with a different shape.\nAs its name implies, a tensor returned by .view() shares the same data as the input, so changes to one will affect the other and vice-versa\nWe can use .view() to flatten matrices into vectors, and to convert rank-1 vectors into rank-2 row or column matrices:\nAs a convenience, calls to .view() may include a single -1 argument; this puts enough elements on that dimension so that the output has the same number of elements as the input. This makes it easy to write some reshape operations in a way that is agnostic to the shape of the tensor\nIn general, you should only use .view() to add new dimensions to a tensor, or to collapse adjacent dimensions of a tensor.\nFor tensors with more than two dimensions, we can use the function torch.transpose) to swap arbitrary dimensions.\nComputation Elementwise Basic mathematical functions operate elementwise on tensors, and are available as operator overloads, as functions in the torch module, and as instance methods on torch objects; all produce the same results:\n* + - / * matrix Note that unlike MATLAB, * is elementwise multiplication, not matrix multiplication. PyTorch provides a number of linear algebra functions that compute different types of vector and matrix products. The most commonly used are:\ntorch.dot: Computes inner product of vectors torch.mm: Computes matrix-matrix products torch.mv: Computes matrix-vector products torch.addmm / torch.addmv: Computes matrix-matrix and matrix-vector multiplications plus a bias torch.bmm / torch.baddmm: Batched versions of torch.mm and torch.addmm, respectively torch.matmul: General matrix product that performs different operations depending on the rank of the inputs. Confusingly, this is similar to np.dot in numpy. You can find a full list of the available linear algebra operators in the documentation. All of these functions are also available as Tensor instance methods, e.g. Tensor.dot instead of torch.dot.\nHere is an example of using torch.dot to compute inner products. Like the other mathematical operators we\u0026rsquo;ve seen, most linear algebra operators are available both as functions in the torch module and as instance methods of tensors:\nvectorization In many cases, avoiding explicit Python loops in your code and instead using PyTorch operators to handle looping internally will cause your code to run a lot faster. This style of writing code, called vectorization, avoids overhead from the Python interpreter, and can also better parallelize the computation (e.g. across CPU cores, on on GPUs). Whenever possible you should strive to write vectorized code.\nGPU All PyTorch tensors also have a device attribute that specifies the device where the tensor is stored \u0026ndash; either CPU, or CUDA (for NVIDA GPUs). A tensor on a CUDA device will automatically use that device to accelerate all of its operations.\nJust as with datatypes, we can use the .to() method to change the device of a tensor. We can also use the convenience methods .cuda() and .cpu() methods to move tensors between CPU and GPU.\nsquared_euclidean_distance(train_data,train_data.view(-1))\n","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/computer-visionnotes/","title":"【Computer Vision】Notes"},{"content":"Purpose of the compiler is\nto detect non-valid programs to translate to valid ones Lexical Analysis partition the strings into tokens\nToken Class: Identifier, Keyword \u0026hellip;\nRegular Languages Def. The regular expressions over $\\sum$ are the smallest set of expressions including\nUnion Concatenation Iteration Formal Languages Def. Let $\\sum$ be a set of characters (an alphabet). A language over $\\sum$ is a set of strings of characters drawn from $\\sum$\nA regular language is a type of formal language that can be generated by a regular expression or recognized by a finite automaton (FA). Regular languages are a subset of the class of formal languages.\nmeaning(mapping from syntax to semantics) is many to one rather than one to many.\nLexical Specification maximal match\nprioritized match\nError match\nError = {all strings not in the lexical specification}\nFinite Automata regular expressions = specification finite automata = implementation a finite automaton consists of\nAn input alphabet $\\sum$ A set of states $S$ A start state $n$ A set of accepting states $F\\subseteq S$ A set of transitions $state \\rightarrow^{input} state$ Deterministic Finite Automata(DFA) one transition per input per state No $\\epsilon-$moves Nondeterministic Finite Automata(NFA) can have multiple transitions for one input in a given state can have $\\epsilon-$moves DFAs are faster to execute\nSubset Method for NFA Determinization $\\epsilon-closure$ transition Partition Method for DFA Minimization Syntactic Analysis (Parsing) CFG CFG (context-free grammars) are a natural notation for the recursive structure of programming languages.\nA CFG consists of\nA set of terminals $T$ A set of non-terminals $N$ A start symbol $S$ A set of productions (Productions can be read as rules) steps:\nBegin with a string with only the start symbol $S$ Replace any non-terminal $X$ in the string by the right-hand side of some production $X \\rightarrow Y_1\u0026hellip;Y_n$ Repeat 2. until there are no non-terminals Def. Let $G$ be a context-free grammar with start symbol $S$. Then the language $L(G)$ of $G$ is: $$ {a_1\u0026hellip;a_n| a} $$\nTerminals are so-called because there are no rules for replacing them Once generated, terminals are permanent Terminals ought to be tokens of the language Derivations A derivation is a sequence of productions $$ S\\rightarrow \u0026hellip; \\rightarrow \u0026hellip; \\rightarrow \u0026hellip; \\rightarrow \u0026hellip; $$ A derivation can be drawn as a tree\nstart symbol is the tree\u0026rsquo;s root For a production $X\\rightarrow Y_1\u0026hellip;Y_n$ add children $Y_1\u0026hellip;Y_n$ to node $X$ left-most derivation: at each step, replace the left-most non-terminal\nright-most derivation: at each step, replace the right-most non-terminal\nright-most and left-most derivations have the same parse tree\nParse Tree A parse tree has\nterminals at the leaves non-terminals at the interior nodes A in-order traversal of the leaves is the original input\nThe parse tree shows the association of operations, the input string does not\nAmbiguity A grammar is ambiguous if it has more than one parse tree for some string. Equivalently, there is more than one right-most or left-most derivation for some string\nImpossible to convert automatically an ambiguous grammar to an unanbiguous one Used with care, ambiguity can simplify the grammar sometimes allows more natural definitions we need disambiguation mechanisms most tools allow precedence and associativity declarations to disambiguate grammars Error Handling Panic mode when an error is detected: discard tokens until one with a clear role is found continue from there Looking for synchronizing tokens typically the statement or expression terminators Error productions specify known common mistakes in the grammar\nAbstract Syntax Tree Like parse trees but ignore some details\nA parse tree\nTraces the operation of the parser Captures nesting structure But too much information Parentheses Single-successor nodes Abstract Syntax Tree\nAlso captures the nesting structure But abstracts from the concrete syntax more compact and easier to use An important data structure in a compiler Recursive Descent Algorithm The parse tree is constructed from the top to down from left to right Terminals are seen in order of appearance in the token stream Limitation\nif a production for non-terminal X succeeds cannot backtrack to try a different production for X later Left-recursion must be eliminated first sufficient for grammars where for any non-terminal at most one production can succeed\nleft recursion\neg: $S-\u0026gt;Sa$, can be eliminated\nPredictive Parser like recursive-descent but parser can \u0026ldquo;predict\u0026rdquo; which production to use\nby looking at the next few tokens no backtracking Predictive parsers accept LL(k) (left-to-right left-most k-tokens-lookahead) grammars\nLL(1): at each step, only one choice of production\nleft-factoring: to eliminate the common prefixes of multiple productions for one non-terminal\nsteps:\nfor the leftmost non-terminal $S$ look at the next input token $a$ choose the production shown at $[S, a]$ (parsing table) reject on reaching error state accept on end of input \u0026amp; empty stack use a stack to record frontier of parse tree\nfrontier:\nnon-terminals that have yet to be expanded terminals that have yet to matched against the input top of stack: leftmost pending terminal or non-terminal\nconsider non-terminal $A$, production $A\\rightarrow \\alpha$ and token $t$ $T[A,t] = \\alpha$ in two cases: if $\\alpha \\rightarrow^{*} t\\beta$ $\\alpha$ can derive a $t$ in the first position we say that $t\\in First(\\alpha)$ if $A\\rightarrow \\alpha$ and $\\alpha \\rightarrow^{} \\epsilon$ and $S\\rightarrow^{} \\beta A t \\delta$ useful if stack has $A$, input is $t$, and $A$ cannot derive $t$ in this case only option is to get rid of $A$ (by deriving $\\epsilon$) : can work only if $t$ can follow $A$ in at least one derivation We say that $t\\in Follow(A)$ First Set Def. $$ First(X)={t|X\\rightarrow^{}t\\alpha}\\cup{\\epsilon | X\\rightarrow^{}\\epsilon} $$ Algorithm\n$First(t)={t}$ $\\epsilon\\in First(X)$ if $X\\rightarrow \\epsilon$ if $X\\rightarrow A_1\u0026hellip;A_n$ and $\\epsilon\\in First(A_i)$ for $1\\le i \\le n$ $First(\\alpha)\\subseteq First(X)$ if $X\\rightarrow A_1\u0026hellip;A_n\\alpha$ and $\\epsilon\\in First(A_i)$ for $1\\le i \\le n$ Follow Set Def. $$ Follow(X) = {t|S\\rightarrow^{*} \\beta A t \\delta} $$ intuition\nif $X\\rightarrow AB$ then $First(B)\\subseteq Follow(A)$ and $Follow(X)\\subseteq Follow(B)$ if $B\\rightarrow^{*}\\epsilon$ then $Follow(X)\\subseteq Follow(A)$ if $S$ is the start symbol then $$\\in Follow(S)$ Algorithm\n$$\\in Follow(S)$ $First(\\beta) -{\\epsilon}\\subseteq Follow(X)$ for each production $A\\rightarrow\\alpha X\\beta$ $Follow(A)\\subseteq Follow(X)$ for each production $A\\rightarrow \\alpha X\\beta$ where $\\epsilon\\in First(\\beta)$ Parsing Table for each production $A\\rightarrow \\alpha$ in G do:\nfor each terminal $t\\in First(\\alpha)$ do $T[A,t] = \\alpha$ if $\\epsilon\\in First(\\alpha)$, for each $t\\in Follow(A)$ do $T[A,t] = \\alpha$ if $\\epsilon \\in First(\\alpha)$ and $$\\in Follow(A)$ do $T[A,$] = \\alpha$ LL(1) Table\nif any entry is multiply defined then G is not LL(1) Most programming language CFGs are not LL(1) Bottom-Up Parsing Bottom-Up parsing is more general than (deterministic) top-down parsing Bottom-Up is the preferred method Bottom-Up parsing reduces a string to the start symbol by inverting productions A bottom-up parser traces a rightmost derivation in reverse Bottom-up parsing uses only two kinds of actions Shift Reduce shift-reduce conflict: it is legal to shift or reduce\nreduce-reduce conflict: it is legal to reduce by two different productions (bad)\nHandle: a reduction that also allows further reductions back to the start symbol\nhandles always appear at the top of the stack never to the left of the rightmost non-terminal Bottom-up parsing algorithms are based on recognizing handles\nviable prefix Def. $\\alpha$ is a viable prefix if there is an $\\omega$ such that $\\alpha|\\omega$ is a state of a shift-reduce parser\na viable prefix does not extend past the right end of the handle viable prefix is a prefix of the handle as long as a parser has viable prefixes on the stack no parsing error has been detected For any grammar, the set of viable prefixes is a regular language\nRecognizing VP steps:\nadd a dummy production $S^{\u0026rsquo;}\\rightarrow S$ to $G$\nThe NFA states are the items of $G$\nfor item $E\\rightarrow \\alpha .X\\beta$ add transition: $$ (E\\rightarrow \\alpha.X\\beta)\\rightarrow^{X}(E\\rightarrow \\alpha X.\\beta) $$\nFor item $E\\rightarrow \\alpha.X\\beta$ and production $X\\rightarrow \\gamma$ add $$ (E\\rightarrow \\alpha.X\\beta)\\rightarrow^{\\epsilon}(X\\rightarrow.\\gamma) $$\nevery state is an accepting state\nstart state is $S^{\u0026rsquo;}\\rightarrow .S$\nLR(0) Parsing LR(0) Parsing steps:\nLR(0) Parsing: Assume stack contains $\\alpha$ next input is $t$ DFA on input $\\alpha$ terminates in state $s$ Reduce by $X\\rightarrow\\beta$ if $s$ contains item $X\\rightarrow\\beta.$ Shift if $s$ contains item $X\\rightarrow\\beta.t\\omega$ equivalent to saying $s$ has a transition labeled $t$ LR(0) gas a reduce/reduce conflict if: any state has two reduce items $X\\rightarrow \\beta.$ and $Y\\rightarrow \\omega$ LR(0) has a shift/reduce conflict if: any state has a reduce item and a shift item $X\\rightarrow \\beta.$ and $Y\\rightarrow\\omega .t \\delta$ SLR Parsing simple left-to-right right-most parsing\nAssume stack contains $\\alpha$ next input is $t$ DFA on input $\\alpha$ terminates in state $s$ Reduce by $X\\rightarrow\\beta$ if $s$ contains item $X\\rightarrow\\beta.$ and $t\\in Follow(X)$ Shift if $s$ contains item $X\\rightarrow\\beta.t\\omega$ equivalent to saying $s$ has a transition labeled $t$ LR(0) gas a reduce/reduce conflict if: any state has two reduce items $X\\rightarrow \\beta.$ and $Y\\rightarrow \\omega$ LR(0) has a shift/reduce conflict if: any state has a reduce item and a shift item $X\\rightarrow \\beta.$ and $Y\\rightarrow\\omega .t \\delta$ steps:\nlet $M$ be DFA for viable prefixes of $G$ let $|x_1\u0026hellip;x_n$$ be initial configuration repeat until configuration is $S|$$ let $\\alpha|\\omega$ be current configuration run $M$ on current stack $\\alpha$ if $M$ rejects $\\alpha$, report parsing error if $M$ accepts $\\alpha$ with items $I$, let $a$ be next input shift if $X\\rightarrow \\beta.a\\gamma\\in I$ reduce if $X\\rightarrow \\beta.\\in I$ and $a\\in Follow(X)$ report parsing error if neither applies Improvement change stack to contain pairs $$ \u0026lt;Symbol,DFA\\space State\u0026gt; $$ the bottom of the stack is $\u0026lt;any, start\u0026gt;$ where\n$any$ is any dummy symbol\n$start$ is the start of state of the DFA\nDefine $goto[i,A]=j$ if $state_i\\rightarrow^{A}state_j$\n$goto$ is just the transition function of the DFA\nSemantic Analysis last \u0026ldquo;front end\u0026rdquo; phase\ncatches all remaining errors\nScope matching identifier declarations with uses\nthe scope of an identifier is the portion of a program in which that identifier is accessible\nmost languages have static scope: scope depends only on the program text, not run-time behavior\na few languages are dynamically scoped: scope depends on execution of the program\nSymbol Tables much of semantic analysis can be expressed as a recursive descent of an AST\nA symbol table is a data structure that tracks the current bindings of identifiers\nTypes the notion varies from language to language consensus a set of values a set of operations on those values classes are one instantiation of the modern notion of type the goal of type checking is to ensure that operations are used with the correct types\nthree kinds of languages:\nStatically typed: all or almost all checking of types is done as part of compilation (C, Java) Dynamically typed: Almost all checking of types is done as part of program execution Untyped: No type checking (machine code) A lot of code is written in statically typed languages with an \u0026ldquo;escape\u0026rdquo; mechanism\nType Checking is the process of verifying fully typed programs Type Inference is the process of filling in missing type information The two are different, but the terms are often used interchangeably Type Checking A type system is sound if\nwhenever $\\vdash e: T$ then $e$ evaluates to a value of type $T$ we only want sound rules, but some sound rules are better than others\nType checking proves fact $e: T$\nproof is on the structure of the AST proof has the shape of the AST one type rule is used for each AST node in the type rule used for a node $e$:\nHypotheses are the proofs of types of $e$\u0026rsquo;s subexpressions conclusion is the type of $e$ types are computed in a bottom-up pass over the AST\nType environment a type environment gives types for free variables\na type environment is a function from Object Identifiers to Types A variable is free in an expression if it is not defined within the expression let $O$ be a function from $ObjectIdentifiers$ to $Types$\nthe sentence $O \\vdash e: T$ is read: Under the assumption that variables have the types given by $O$, it is provable that the expression $e$ has the type $T$\nand we can write new rules: $$ \\frac{O(x)=T}{O\\vdash x: T} $$ The type environment is passed down the AST from the root towards the leaves\nTypes are computed up the AST from the leaves towards the root\nSubtyping Define a relation $\\le$ on classes\n$X\\le X$ $X\\le Y$ if $X$ inherits from $Y$ $X\\le Z$ if $X\\le Y$ and $Y\\le Z$ Typing Methods method environment $M$\nGeneral themes Type rules are defined on the structure of expressions Types of variables are modeled by an environment cam be implemented in a single traversal over the AST Type environment is passes down the tree Types are passed up the tree SELF_TYPE if $SELF_TYPE $ appears textually in the class $C$ as the declared type of $E$ then $$ dynamic_type(E)\\le C $$\nin type checking it is always safe to replace $SELF_TYPE_c$ by $C$ Code Generation Management of run-time resources Correspondence between static (compile-time) dynamic (run-time) Storage organization compiler is responsible for:\nGenerating code Orchestrating use of the data area two goals:\ncorrectness speed Activations an invocation of procedure $P$ is an activation of $P$\nthe lifetime of an activation of $P$ is\nall the steps to execute $P$ including all the steps in procedures $P$ calls the lifetime of a variable $x$ is the portion of execution in which $x$ is defined\nnote\nlifetime is a dynamic (run-time) concept scope is a static concept lifetimes of procedure activations are properly nested\nactivation lifetimes can be depicted as a tree\nthe activation tree depends on run-time behavior\nsince activations are properly nested, a stack can track currently active procedures\nActivation Record (AR) the information needed to manage one procedure activation is called an activation record (AR) or frame\nif procedure $F$ calls $G$, then $G$\u0026rsquo;s activation record contains a mix of info about $F$ and $G$, $G$\u0026rsquo;s AR contains information needed to\nComplete execution of $G$ Resume execution of $F$ The compiler must determine, at compile-time, the layout of activation records and generate code that correctly accesses locations in the activation record\nGlobals \u0026amp; Heap Globals are assigned a fixed address once\nVariables with fixed address are \u0026ldquo;statically allocated\u0026rdquo; A value that outlives the procedure that creates it cannot be kept in the AR\nLanguages with dynamically allocated data use a Heap to store dynamic data\nSpaces\nThe code area contains object code\nfor many languages, fixed size and read only\nThe static area contains data (not code) with fixed addresses\nfixed size, may be readable and writable\nThe stack contains an AR for each currently active procedure\neach AR usually fixed size, contains locals\nHeap contains all other data\nheap is managed by malloc and free in C\nAlignment Data is word aligned if it begins at a word boundary\nMost machines have some alignment restrictions or performance penalties for poor alignment\nStack Machines only storage is a stack\nan instruction $r = F(a_1,\u0026hellip;a_n)$ :\nPops $n$ operands from the stack Computes the operation $F$ using the operands Pushes the result $r$ on the stack location of the operands/ result is not explicitly stated\nalways the top of the stack In contrast to a register machine\nadd instead of add r1, r2, r3 more compact programs There is an intermediate point between a pure stack machine and a pure register machine\nAn n-register stack machine\nconceptually, keep the top n locations of the pure stack machine\u0026rsquo;s stack in registers consider a 1-register stack machine\nthe register is called the accumulator consider an expression $op(e_1,\u0026hellip;,e_n)$, $e_1,\u0026hellip;,e_n$ are subexpressions for each $e_i$ compute $e_i$ push result on the stack pop $n - 1$ values from the stack, compute $op$ store result in the accumulator code generation for each expression $e$ we generate MIPS code that\ncomputes the value of $e$ in $$a_0$ Preserves $$sp$ and the contents of the stack define a code generation function $cgen(e)$ whose result is the code generated for $e$\ncode generation can be written as a recursive-descent of the AST\nvariables use a frame pointer\nalways points to the return address on the stack since it does not move it can be used to find the variables let $x_i$ be the $i-th$ formal parameter of the function for which code is being generated $$ cgen(x_i) = lw\\space $a_0\\space 4*i($sp) $$ The activation record must be designed together with the code generator\nUsing a stack machine for compiler is recommended.\nProduction compilers do different things\nemphasis is on keeping values in registers intermediate results are laid out in the AR, not pushed and popped from the stack temporaries the code generator must assign a fixed location in the AR for each temporary\nlet $NT(e)$ = count of temps needed to evaluate $e$\n$NT(e_1+e_2)$\nneeds at least as many temporaries as $NT(e_1)$ needs at least as many temporaries as $NT(e_2) + 1$ for a function definition $f(x_1,\u0026hellip;,x_n) = e$ then AR has $2+n+NT(e)$ elements\nreturn address frame pointer n arguments $NT(e)$ locations for intermediate results code generation must know how many temporaries are in use at each point\nadd a new argument to code generation: the position of the next available temporary\nthe temporary area is used like a small, fixed-size stack\nObject Layout OO implementation = Basic code generation + More stuff\nOO slogan: If B is a subclass of A, then an object of class B can be used wherever an object of class A is expected\nThis means that code in class A works unmodified for an object of class B\nObject are laid out in contiguous memory\nClass tag is an integer\nidentifies class of the object\nObject size is an integer\nsize of the object in words\nDispatch ptr is a pointer to a table of methods\nattributes in subsequent slots\nEach attribute stored at a fixed offset in the object\nObservation: Given a layout for class A, a layout for subclass B can be defined by extending the layout of A with additional slots for the additional attributes of B\nSemantics many ways to specify semantics:\nOperational Semantics\ndescribes program evaluation via execution rules (on an abstract machine) most useful for specifying implementations Denotational Semantics\nprogram\u0026rsquo;s meaning is a mathematical function Axiomatic Semantics\nProgram behavior described via logical formulae if execution begins in state satisfying X, then it ends in state satisfying Y X, Y formulas Foundation of many program verification systems Operational Semantics Recall the typing judgment $$ Context \\vdash e: C $$ in the given context, expression e has type C\nWe use something similar for evaluation $$ Context \\vdash e: v $$ In the given context, expression e evaluates to value v\nWe track variables and their values with:\nAn environment : where in memory a variable is A store: what is in the memory A variable environment maps variables to locations\nkeep track of which variables are in scope tells us where those variables are $$ E = [a: I_1,b : I_2] $$\nA store maps memory locations to values $$ S=[I_1\\rightarrow5,I_2\\rightarrow 7] $$ $S^{\u0026rsquo;}= S[12/I_1]$ defines a store $S^{\u0026rsquo;}$ such that $S^{\u0026rsquo;}(I_1)=12$ and $S^{\u0026rsquo;} = S(I)$ if $I\\ne I_1$\nThe evaluation judgement is $$ so, E, S\\vdash e: v, S^{\u0026rsquo;} $$\nGiven so the current value of self And E the current variable environment And S the current store If the evaluation of e terminates then The value of e is v And the new store is $S^{\u0026rsquo;}$ $$ E(id) = I_{id}\\newline S(I_{id}) = v \\newline \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\\newline so,E,S\\vdash id:v,S $$\nIntermediate Code A language between the source and the target\nProvides an intermediate level of abstraction\nmore details than the source fewer details than the target high-level assembly\nuses register names, but has an unlimited number uses control structures like assembly language uses opcodes but some are higher level common form of intermediate code\neach instruction is of the form $$ x:= y\\space op\\space z \\newline x:= op\\space y $$ $y$ and $z$ are registers or constants\nsimilar to assembly code generation\nbut use any number of IL registers to hold intermediate results\nOptimization optimization is out last compiler phase\nmost complexity in modern compilers is in the optimizer\nseeks to improve a program\u0026rsquo;s resource utilization\nexecution time (most often) code size network messages sent, etc. when：\nOn AST\nPro: machine independent Con: too high level On assembly language\nPro: Exposes optimization opportunities\nCon:\nMachine dependent\nMust reimplement optimizations when retargeting\nOn an intermediate language\nPro:\nMachine independent\nExposes optimization opportunities\nBasic Block A basic block is a maximal sequence of instructions with:\nno labels (except at the first instruction) no jumps (except at the last instruction) idea\ncannot jump into a basic block (except at beginning) cannot jump out of a basic block (except at end) a basic block is a single-entry, single-exit, straight-line code segment control-flow graph a control-flow-graph is a directed graph with\nbasic blocks as nodes an edge from block A to block B if the execution can pass form the last instruction in A to the first instruction in B the body of a method (or procedure) can be represented as a control-flow graph\nGranularity local optimizations\napply to a basic block in isolation\nGlobal optimizations\napply to a control-flow graph (method body) in isolation\nInter-procedural optimizations\napply across method boundaries\nmost compilers do 1. many do 2. few do 3.\nIn practice, often a conscious decision is made not to implement the fanciest optimization known\nsome optimizations are hard to implement some optimizations are costly in compilation time some optimizations have low payoff many fancy optimizations are all three! Local Optimization optimize one basic block\nno need to analyze the whole procedure body\nsome statement can be deleted\n$x:= x + 0$\n$x:= x*1$\nsome statements can be simplified\n$x:=x*0$ to $x:=0$\n$y:= y**2$ to $y:=y*y$\n$x:=x*8$ to $x:=x\u0026laquo;3$\n$x:=x*15$ to $t:=x \u0026laquo; 4; x:=t-x$\nOperations on constants can be computed at compile time\nif there is a statement $x:=y\\space op\\space z$ and $y$ and $z$ are constants then $y$ op $z$ can be computed at compile time constant folding can be dangerous\nin cross-compiler\nEliminate unreachable blocks\nsome optimizations are simplified if each register occurs only once on the left-hand side of an assignment\ncommon subexpression elimination\nif\nbasic block is in single assignment form a definition $x:=$ is the first use of $x$ in a block then\nWhen two assignments have the same rhs, they compute the same value copy propagation\nif $w:= x$ appears in a block, replace subsequent uses of $w$ with uses of $x$\ndead elimination\nif\n$w:=rhs$ appears in a basic block\n$w$ does not appear anywhere else in the program\nthen\nthe statement $w:=rhs$ is dead and can be eliminated\ntypically optimizations interact: performing one optimization enables another\noptimizing compilers repeat optimizations until no improvement is possible\nPeephole optimization optimizations can be directly applied to assembly code\nPeephole optimization is effective for improving assembly code\nthe \u0026ldquo;peephole\u0026rdquo; is a short sequence of (usually contiguous) instructions\nthe optimizer replaces the sequence with another equivalent one (but faster)\nGlobal Optimization Global optimization tasks share several traits:\nThe optimization depends on knowing a property X at a particular point in program execution Proving X at any point requires knowledge of the entire program Dataflow Analysis To replace a use of x by a constant k we must know: on every path to the use of x, the last assignment to x is $x:=k$\nChecking the condition requires global dataflow analysis: an analysis of the entire control-flow graph\n","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/stanford-compilersnotes/","title":"【Stanford Compilers】Notes"},{"content":"Lecture 1 (intro) Element environment agent action observation and reward history History $h_t=(a_1,o_1,r_1,\u0026hellip;,a_t,o_t,r_t)$\nagent choose action based on history\nstate state is information assumed to determine what happens next\nfunction of history $s_t=f(h_t)$\nThe true state of the world used to determine how world generates next observation and reward are often hidden or unknown to agent\nassume state used by the agent is sufficient statistic of history\nIn practice often assume most recent observation is sufficient statistic of history: $s_t=o_t$\nstate representation has big implications for:\ncomputational complexity data required resulting performance Markov state $s_t$ is Markov if and only if: $$ p(s_{t+1}|s_{t},a_{t})=p(s_{t+1}|h_t,a_{t}) $$ future is independent of past given present\nsetting state as history always Markov: $s_t=h_t$\nRL Algorithm component often include one or more of\nModel\nrepresentation of how the world changes in response to agent\u0026rsquo;s action\nPolicy\nfunction mapping agent\u0026rsquo;s states to action\nValue function\nfuture rewards from being in a state and/or action when following a particular policy\nModel mathematical models of dynamics and reward, including:\nTransition/dynamics model predicts next agent state $$ p(s_{t+1}=s^{\u0026rsquo;}|s_t=s,a_t=a) $$ Reward model predicts immediate reward $$ r(s_t=s,a_t=a)=E[r_t|s_t=s,a_t=a] $$\nPolicy policy $\\pi:S\\to A$, mapping from states to actions, determines how the agent chooses actions\ndeterministic: $\\pi(s)=a$ stochastic: $\\pi(a|s)=Pr(a_t=a|s_t=s)$ Value value function $V^\\pi$ expected discounted sum of future rewards under a particular policy $\\pi$ $$ V^\\pi(s_t=s)=E_\\pi[r_t+\\gamma r_{t+1}+\\gamma^2r_{t+2}+\u0026hellip;|s_t=s] $$ discount factor $\\gamma$ weighs immediate vs future rewards\ncan be used to quantify goodness/badness of states and actions and decide how to act by comparing policies\nTypes of RL agents Model-based Explicit: Model May or may not have policy and/ or value function Model-free Explicit: Value function and /or policy function No model Lecture 2 (MDP) Markov Process $S$ is a (finite) set of states ($s\\in S$)\n$P$ is dynamics/transition model that specifies $p(s_{t+1}=s{\u0026rsquo;}|s_t=s)$\nno reward, no actions\nif finite number ($N$) of states, can express $P$ as a matrix $$ P=\\begin{pmatrix} P(s_1|s_1) \u0026amp; P(s_2|s_1) \u0026amp; \u0026hellip; \u0026amp;P(s_N|s_1)\\ P(s_1|s_2) \u0026amp; P(s_2|s_2) \u0026amp; \u0026hellip; \u0026amp;P(s_N|s_2)\\ \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip;\\ P(s_1|s_N) \u0026amp; P(s_2|s_N) \u0026amp; \u0026hellip; \u0026amp;P(s_N|s_N)\\ \\end{pmatrix} $$\nA Markov chain is the discrete-time version of a Markov process.\nMarkov Reward Process(MRP) $S$ is a (finite) set of states ($s\\in S$) $P$ is dynamics/transition model that specifies $P(s_{t+1}=s{\u0026rsquo;}|s_t=s)$ $R$ is a reward function $R(s_t=s)=E[r_t|s_t=s]$ Discount factor $\\gamma\\in [0,1]$ no actions if finite number ($N$) of states, can express $R$ as a vector horizon number of time steps in each episode can be infinite otherwise called finite Markov reward process Return $G_t$ (for a MRP) discounted sum of rewards from time step $t$ to horizon $$ G_t=r_t+\\gamma r_{t+1}+\\gamma^2r_{t+2}+\u0026hellip; $$\nState Value Function, $V(s)$ (for a MRP) expected return from starting in state $s$ $$ \\begin{split} V(s) \u0026amp; = E[G_t|s_t=s] \\ \u0026amp; = E[r_t+\\gamma r_{t+1}+\\gamma^2r_{t+2}+\u0026hellip;|s_t=s] \\end{split} $$\nMRP value function satisfies $$ V(s)=R(s)+\\gamma\\sum_{s^{\u0026rsquo;}\\in S}P(s^{\u0026rsquo;}|s)V(s^{\u0026rsquo;}) $$ where $R(s)$ is immediate reward and the later part is the discounted sum of future rewards\nthen: $$ V=R+\\gamma PV $$ namely: $$ V=(I-\\gamma P)^{-1}R $$\nMarkov Decision Process (MDP) $S$ is a (finite) set of states ($s\\in S$) $A$ is a (finite) set of actions $a\\in A$ $P$ is dynamics/transition model that specifies $P(s_{t+1}=s{\u0026rsquo;}|s_t=s，a_t=a)$ $R$ is a reward function $R(s_t=s,a_t=a)=E[r_t|s_t=s,a_t=a]$ Discount factor $\\gamma\\in [0,1]$ MDP is a tuple: $(S,A,P,R,\\gamma)$\nPolicy policy specifies what action to take in each state\ncan be deterministic or stochastic\nfor generality, consider as a conditional ditribution\ngiven a state, specifies a distribution over actions\nPolicy: $\\pi(a|s)=P(a_t=a|s_t=s)$\nMDP+$\\pi(a|s)$=Markov Reward Process\nprecisely, it is the MRP $(S,R^\\pi,P^\\pi,\\gamma)$, where $$ R^\\pi(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a) \\newline P^\\pi(s^{\u0026rsquo;}|s)=\\sum_{a\\in A}\\pi(a|s)P(s^{\u0026rsquo;}|s,a) $$ implies we can use same techniques to evaluate the value of a policy for a MDP as we could to compute the value of a MRP, by defining a MRP with $R^{\\pi}$ and $P^{\\pi}$\ncontrol compute the optimal policy $$ \\pi^*(s)=arg,max_{\\pi}V^{\\pi}(s) $$ there exists an unique optimal value function\nbut the optimal policy is not unique\noptimal policy for a MDP in an infinite horizon problem (agent a acts forever) is:\ndeterministic stationary (does not depend on time step) not necessarily unique Policy Iteration computes optimal value and policy\nset $i= 0$ initialize $\\pi_0(s)$ randomly for all states $s$ while $i==0$ or $||\\pi_i-\\pi_{i-1}||_1\u0026gt;0$ (L1-norm, measures if the policy changed for any state): $V^{\\pi_i}\\leftarrow$ MDP V function policy evaluation of $\\pi_i$ $\\pi_{i+1}\\leftarrow $ Policy improvement $i=i+1$ State-Action Value Q State-action value of a policy $$ Q^{\\pi}(s,a)=R(s,a)+\\gamma\\sum_{s^{\u0026rsquo;}\\in S}P(s^{\u0026rsquo;}|s,a)V^{\\pi}(s^{\u0026rsquo;}) $$ take action a, then follow the policy $\\pi$ later on\npolicy improvement compute state-action value of a policy $\\pi_i$\nfor $s$ in $S$ and $a$ in $A$: $$ Q^{\\pi_i}(s,a)=R(s,a)+\\gamma\\sum_{s^{\u0026rsquo;}\\in S}P(s^{\u0026rsquo;}|s,a)V^{\\pi_i}(s^{\u0026rsquo;}) $$ conpute new policy $\\pi_{i+1}$, for all $s\\in S$ $$ \\pi_{i+1}(s)=arg,max_a,Q^{\\pi_i}(s,a),\\forall s\\in S $$\ndefinition $$ V^{\\pi_1}\\ge V^{\\pi_2}:V^{\\pi_1}(s)\\ge V^{\\pi_2}(s),\\forall s\\in S $$\nValue Iteration miantain optimal value of starting in a state $s$ if have a finite number of steps $k$ left in the episode\niterate to consider longer and longer episodes\nset $k=1$\ninitialize $V_0(s)=0$ all states $s$\nloop until finite horizon or convergence:\nfor each state $s$ $$ V_{k+1}(s)=max_aR(s,a)+\\gamma\\sum_{s^{\u0026rsquo;}\\in S}P^{\\pi}(s^{\u0026rsquo;}|s)V^{\\pi}(s^{\u0026rsquo;}) $$\nview as bellman backup on value function $$ V_{k+1}=BV_k \\newline \\pi_{k+1}(s)=arg,max_aR(s,a)+\\gamma\\sum_{s^{\u0026rsquo;}\\in S}P^{\\pi}(s^{\u0026rsquo;}|s)V^{\\pi}(s^{\u0026rsquo;}) $$\nBellman value function of a policy must satisfy the Bellman equation $$ V^{\\pi}(s)=R^{\\pi}(s)+\\gamma\\sum_{s^{\u0026rsquo;}\\in S}P^{\\pi}(s^{\u0026rsquo;}|s)V^{\\pi}(s^{\u0026rsquo;}) $$ bellman backup operator\napplied to a value function returns a new value function improves the value if possible $$ BV(s)={max}aR(s,a)+\\gamma\\sum{s^{\u0026rsquo;}\\in S}P^{\\pi}(s^{\u0026rsquo;}|s)V^{\\pi}(s^{\u0026rsquo;}) $$\n$BV$ yields a value function over all states $s$ Policy iteration as Bellman Operations Bellman backup operator $B^{\\pi}$ for a particular policy is defined as $$ B^{\\pi}V(s)=R^{\\pi}(s)+\\gamma\\sum_{s^{\u0026rsquo;}\\in S}P^{\\pi}(s^{\u0026rsquo;}|s)V^{\\pi}(s^{\u0026rsquo;}) $$ policy evaluation amounts to computing the fixed point of $B^{\\pi}$\nto do policy evaluation. repeatedly apply operator until $V$ stops chaning $$ V^{\\pi}=B^{\\pi}\u0026hellip;B^{\\pi}V $$\nLecture 3 (model-free evaluation) Dynamic programming for policy evaluation given dynamics model $p$ and reward model $r$, namely the model is known\ninitialize $V_0(s)=0$ all states $s$\nfor $k=1$ until convergence\nfor all $s$ in $S$\n$V_{k}^{\\pi}(s)=r(s,\\pi(s))+\\gamma\\sum_{s^{\u0026rsquo;}\\in S}P(s^{\u0026rsquo;}|s,\\pi(s))V^{\\pi}_{k-1}(s^{\u0026rsquo;})$\n$V_{k}^{\\pi}(s)$ is exact value of $k$-horizon value of state $s$ under policy $\\pi$, and it\u0026rsquo;s an estimate of infinite horizon value of state $s$ under policy $\\pi$ $$ V_{k}^{\\pi}(s)=E_{\\pi}[G_t|s_t=s]\\approx E_{\\pi}[r_t+\\gamma V_{k-1})|s_t=s] $$\nMonte Carlo policy evaluation does not require MDP dynamics/rewards so are used when model is unkown $$ V^{\\pi}(s)=E_{T\\sim \\pi}[G_t|s_t=s] $$\nexpectation of trajectories $T$ generated by following $\\pi$\nthe idea is Value = mean return\ndoes not assume state is Markov\ncan only be applied to episodic MDPs\naveraging over returns from a complete episode requires each episode to terminate aim: estimate $V^\\pi(s)$ given episodes generated under policy $\\pi$\n$s_1,a_1,r_1,s_2,a_2,r_2,\u0026hellip;$ where the actions are sampled from $\\pi$\nsteps:\ninitialize $N(s)=0,G(s)=0\\forall s\\in S$ loop sample episode $i=s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},\u0026hellip;,s_{i,T_i},a_{i,T_i},r_{i,T_i}$ define $G_{i,t}=r_{i,t}+\\gamma r_{i,t+1}+\\gamma^2r_{i,t+2}+\u0026hellip;\\gamma^{T_i-1}r_{i,T_i}$ as return from time step $t$ onwards in $ith$ episode for each state $s$ visited in episode $i$ for first time $t$ that state $s$ is visited in episode $i$ increment counter of total first visits: $N(s)=N(s)+1$ increment total return $G(s)=G(s)+G_{i,t}$ update estimate $V^{\\pi}(s)=G(s)/N(s)$ Every-Visit MC instead of for first time $t$ that state $s$ is visited in episode $i$, update $N(s),G(s)$ and $V^{\\pi}(s)$ for every time $t$ that state $s$ is visited in episode $i$\nthis estimator is biased but is consistent and often has better MSE\nIncremental MC sample episode $i=s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},\u0026hellip;,s_{i,T_i},a_{i,T_i},r_{i,T_i}$\ndefine $G_{i,t}=r_{i,t}+\\gamma r_{i,t+1}+\\gamma^2r_{i,t+2}+\u0026hellip;\\gamma^{T_i-1}r_{i,T_i}$ as return from time step $t$ onwards in $ith$ episode\nfor state $s$ visited at time step $t$ in episode $i$\nincrement counter of total visits: $N(s)=N(s)+1$\nupdate estimate $$ V^{\\pi}(s)=V^{\\pi}(s)\\frac{N(s)-1}{N(s)}+\\frac{G_{i,t}}{N(s)}=V^{\\pi}(s)+\\frac{1}{N(s)}(G_{i,t}-V^{\\pi}(s)) $$\nRunning Mean estimation is updated as $$ V^{\\pi}(s)=V^{\\pi}(s)+\\alpha(G_{i,t}-V^{\\pi}(s)) $$\n$\\alpha=\\frac{1}{N(s)}$: identical to every visit MC $\\alpha\\gt\\frac{1}{N(s)}$: forget older data, helpful for non-stationary domains Temporal Difference(TD) combination of Monte Carlo \u0026amp; dynamic programming methods\nModel-free\nBootstraps and samples\ncan be used in episodic or infinite-horizon non-episodic settings\nimmediately updates estimate of $V$ after each $(s,a,r,s^{\u0026rsquo;})$ tuple\naim: estimate $v^{\\pi}(s)$ given episodes generated under policy $\\pi$\nhave an estimate of $V^{\\pi}$, use to estimate expected return $$ V^{\\pi}(s)=V^{\\pi}(s)+\\alpha([r_t+\\gamma V^{\\pi}(s_{t+1})]-V^{\\pi}(s)) $$\nTD learning\nInitialize $V^{\\pi}(s)=0\\forall s\\in S$ loop sample tuple $(s_t,a_t,r_t,s_{t+1})$ $V^{\\pi}(s_t)=V^{\\pi}(s_t)+\\alpha([r_t+\\gamma V^{\\pi}(s_{t+1})]-V^{\\pi}(s_t))$ Lecture 4 (model-free control) on-policy learning\ndirect experience learn to estimate and evaluate a policy from experience obtained from following that policy off-policy learning\nlearn to estimate and evaluate a policy using experience gathered from following a different policy MC for On Policy Q Evaluation initialize $N(s,a)=0,G(s,a)=0,Q^{\\pi}(s,a)=0,\\forall s\\in S,\\forall a\\in A$\nloop\nusing policy $\\pi$ sample episode $i=s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},\u0026hellip;,s_{i,T_i},a_{i,T_i},r_{i,T_i}$ $G_{i,t}=r_{i,t}+\\gamma r_{i,t+1}+\\gamma^2r_{i,t+2}+\u0026hellip;\\gamma^{T_i-1}r_{i,T_i}$ for each state,action $(s,a)$ visited in episode $i$ for first or every time $t$ that state $(s,a)$ is visited in episode $i$ $N(s,a)=N(s,a)+1,G(s,a)=G(s,a)+G_{i,t}$ update estimate $Q^{\\pi}(s,a)=G(s,a)/N(s,a)$ given an estimate $Q^{\\pi_i}(s,a)\\forall s,a$\nupdate new policy $$ \\pi_{i+1}(s)=arg,max_a,Q^{\\pi_i}(s,a) $$\nMonotonic $\\epsilon$-greedy policy improvement for any $\\epsilon$-greedy policy $\\pi_i$, the ϵ-greedy policy w.r.t.(with respect to) $Q^{\\pi_i}$, $\\pi_{i+1}$ is a monotonic improvement $V^{\\pi_{i+1}}\\ge V^{\\pi}$ $$ \\begin{split} Q^{\\pi_i}(s,\\pi_{i+1}(s)) \u0026amp; = \\sum_{a\\in A}\\pi_{i+1}(a|s)Q^{\\pi}(s,a) \\ \u0026amp; = \\frac{\\epsilon}{|A|}\\sum_{a\\in A}Q^{\\pi_i}(s,a)+(1-\\epsilon)max_aQ^{\\pi_i}(s,a) \\end{split} $$\nGreedy in the Limit of Infinite Exploration(GLIE) all state-action pair are visited an infinite number of times $$ {lim}_{i-\u0026gt;\\infty}N_i(s,a)\\to\\infty $$\nbehavior policy converges to greedy policy\na simple GLIE strategy is $\\epsilon$-greedy where $\\epsilon$ is reduced to 0 with the following rate $$ \\epsilon_i=\\frac{1}{i} $$\nMonte Carlo Online Control /On Policy Improvement initialize $N(s,a)=0,G(s,a)=0,Q(s,a)=0,\\forall s\\in S,\\forall a\\in A,set,\\epsilon=1,k=1$ $\\pi_1= \\epsilon-greedy(Q)$ loop sample $k-th$ episode $(s_{k,1},a_{k,1},r_{k,1},s_{k,2},a_{k,2},r_{k,2},\u0026hellip;,s_{k,T},a_{k,T},r_{k,T})$ $G_{k,t}=r_{k,t}+\\gamma r_{k,t+1}+\\gamma^2r_{k,t+2}+\u0026hellip;\\gamma^{T-1}r_{k,T}$ for $t=1,\u0026hellip;,T$ do if first(or every ) vistit to $(s,a)$ in episode $k$ then $N(s,a)=N(s,a)+1$ $Q(s_t,a_t)=Q(s_t,a_t)+\\frac{1}{N(s,a)}(G_{k,t}-Q(s_t,a_t))$ $k=k+1,\\epsilon=\\frac{1}{k}$ $\\pi_k= \\epsilon-greedy(Q)$ SARSA set initial $\\epsilon$-greedy policy $\\pi$ randomly, $t$=0, initial state $s_t=s_0$ take $a_t\\sim\\pi(s_t)$ //sample action from policy observe $(r_t,s_{t+1})$ loop take action $a_{t+1}\\sim\\pi(s_{t+1})$ observe $(r_{t+1},s_{t+2})$ $Q(s_t,a_t)=Q(s_t,a_t)+\\alpha(r_t+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))$ $\\pi(s_t)=arg,\\max_aQ(s_t,a)$ w.prob $1-\\epsilon$,else random $t=t+1$ Q-Learning initialize $Q(s,a),\\forall s\\in S,a\\in A,t=0 $initial state $s_t=s_0$\nset $\\pi_b$ to be $\\epsilon$-greedy w.r.t $Q$\nloop\ntake action $a_{t}\\sim\\pi_b(s_{t})$ observe $(r_{t},s_{t+1})$ $Q(s_t,a_t)=Q(s_t,a_t)+\\alpha(r_t+\\gamma max_aQ(s_{t+1},a)-Q(s_t,a_t))$ $\\pi(s_t)=arg,\\max_aQ(s_t,a)$ w.prob $1-\\epsilon$,else random $t=t+1$ Double Q-learning initialize $Q_1(s,a)$ and $Q_2(s,a) ,\\forall s\\in S,a\\in A,t=0 $ initial state $s_t=s_0$\nloop\nselect $a_t$ using $\\epsilon$-greedy $\\pi(s)=arg,max_aQ_1(s_t,a)+Q_2(s_t,a)$\nobserve $(r_{t},s_{t+1})$\nif (with 0.5 probability then\n$Q_1(s_t,a_t)=Q_1(s_t,a_t)+\\alpha(r_t+\\gamma max_aQ_1(s_{t+1},a)-Q_1(s_t,a_t))$\nelse\n$Q_2(s_t,a_t)=Q_2(s_t,a_t)+\\alpha(r_t+\\gamma max_aQ_2(s_{t+1},a)-Q_2(s_t,a_t))$\n$t=t+1$\nLecture 5 (Value Function Approximation) represent a (state-action/state) value function with a parameterized function instead of a table $$ \\hat{V}(s;w) \\newline \\hat{Q}(s,a;w) $$\nmotivation don\u0026rsquo;t want to have to explicitly store or learn for every single state a dynamics or reward model value state-action value policy want more compact representation that generalized acress state or states and actions function approximators linear combinations of features neural networks decision trees nearest neighbors fourier / wabelet bases Linear Value Function Approximation represent a value function (or state-action value function) for a particular policy with a weighted linear combination of features $$ \\hat{V}(s;w)=\\sum_{j=1}^nx_j(s)w_j=x(s)^Tw $$\nobjective function is $$ J(w)=E_{\\pi}[(V^{\\pi}(s)-\\hat{V}(s;w))^2] $$\nrecall weight update is $$ \\Delta w=-\\frac{1}{2}\\alpha \\nabla_wJ(w) $$\nconvergence guarantees the markov chain defined by a MDP with a paticular policy will eventually converge to a probability distribution over state $d(s)$\n$d(s)$ is called the stationary distribution over state of $\\pi$\n$\\sum_sd(s)=1$\n$d(s)$ satisfies the following balance equation: $$ d(s)=\\sum_{s\u0026rsquo;}\\sum_a\\pi(s\u0026rsquo;|a)p(s\u0026rsquo;|s,a)d(s\u0026rsquo;) $$\ndefine the mean squared error of a linear value function approximation for a particular policy $\\pi$ relative to the true value as $$ MSVE(w)=\\sum_{s\\in S}d(s)(V^{\\pi}(s)-\\hat{V}^{\\pi}(s;w))^2 $$\nMonte Carlo Value Function Approximation $$ J(w)=E_{\\pi}[(G_t-\\hat{V}(s;w))^2] \\newline \\Delta w=\\alpha(G_t-x(s_t)^Tw)x(s_t) $$\nmonte carlo policy evaluation with VFA converges to the weights $w_{MC}$ which has the minimum mean squared error possible: $$ MSVE(w_{MC})=min_w\\sum_{s\\in S}d(s)(V^{\\pi}(s)-\\hat{V}^{\\pi}(s;w))^2 $$\nBatch Monte Carlo Value Function Approximation let $G(s_i)$ be an unbiased sample of the true expected return $V^{\\pi(s_i)}$ $$ arg,min_w\\sum_{i=1}^N(G(s_i)-x(s_i)^Tw)^2 $$ take the derivative and set to 0 $$ w=(X^TX)^{-1}X^TG $$ where $G$ is a vector of all N returns, and $X$ is a matrix of the features of each of the N states $x(s_)$\nTD value Function Approximation $$ J(w)=E_{\\pi}[r+\\gamma \\hat{V}(s\u0026rsquo;;w))^2] \\newline \\Delta w=\\alpha(r+\\gamma x(s\u0026rsquo;)^Tw-x(s)^Tw)x(s) $$\nTD(0) policy evaluation with VFA converges to the weights $w_{TD}$ which is within a constant factor of the minimum mean squared error possible: $$ MSVE(w_{TD})\\le \\frac{1}{1-\\gamma}min_w\\sum_{s\\in S}d(s)(V^{\\pi}(s)-\\hat{V}^{\\pi}(s;w))^2 $$\ncontrol using Value Function Approximation use value function approximation to represent state-ation values $\\hat{Q}^{\\pi}(s,a;w)\\approx Q^{\\pi}$ $$ J(w)=E_{\\pi}[(Q^{\\pi}(s,a)-\\hat{Q}(s,a;w))^2] \\newline \\Delta w=\\alpha E[(Q^{\\pi}(s,a)-\\hat{Q}(s,a;w))\\nabla_w\\hat{Q}^{\\pi}(s,a;w)] $$\nuse features to represent both the state and action $$ x(s,a)=\\begin{pmatrix} x_1(s,a)\\ x_2(s,a)\\ \u0026hellip;\\ x_n(s,a) \\end{pmatrix} $$\nmonte carlo $$ \\Delta w=\\alpha (G_t-\\hat{Q}(s,a;w))\\nabla_w\\hat{Q}^{\\pi}(s,a;w) $$\nSARSA $$ \\Delta w=\\alpha (r+\\gamma\\hat{Q}(s\u0026rsquo;,a\u0026rsquo;;w)-\\hat{Q}(s,a;w))\\nabla_w\\hat{Q}^{\\pi}(s,a;w) $$\nQ_learning $$ \\Delta w=\\alpha (r+\\gamma,max_{a\u0026rsquo;}\\hat{Q}(s\u0026rsquo;,a\u0026rsquo;;w)-\\hat{Q}(s,a;w))\\nabla_w\\hat{Q}^{\\pi}(s,a;w) $$\nLecture 6 (Deep Q Learning) Linear VFA often work well given the right set of features But can require carefully hand designing that feature set An alternative is to use a much richer function approximation class that is able to directly go from states without requiring an explicit specification of features Deep Q-Networks(DQNs) represent state-action value function by Q-network with weights $w$ $$ \\hat{Q}(s,a;w)\\approx Q(s,a) $$ Experience Replay to help remove correlations, store dataset (called a replay buffer) $D$ from prior experience to perform experience replay., repeat: $(s,a,r,s\u0026rsquo;)\\sim D$ :sample an experience tuple from the dataset compute the target value for the sampled $s$：$r+\\gamma,max_{a\u0026rsquo;}\\hat{Q}(s\u0026rsquo;,a\u0026rsquo;;w)$ use stochastic gradient descent to update the network weight Prioritized Experience Replay let $i$ be the index of the $i$-th tuple of experience $s_i,a_i,r_i,s_{i+1} $\nsample tuples for update using priority function\npriority of a tuple is proportional to DQN error $$ p_i=|r+\\gamma,max_{a\u0026rsquo;}\\hat{Q}(s\u0026rsquo;,a\u0026rsquo;;w)-\\hat{Q}(s,a;w)| $$\nupdate $p_i$ every update\n$p_i$ for new tuples is set to 0\none method: $$ P(i)=\\frac{p_i^{\\alpha}}{\\sum_kp_k^{\\alpha n}} $$\nFixed Q-Targets to help improve stability, fix the target weights used in the target calculation for multiple udpdates use a different set of weights to compute target that is being updated let parameters $w^-$ be the set of weights used in the target, and $``w$ be the weights that are being updated slight change to computation of target value: sample $(s,a,r,s\u0026rsquo;)\\sim D$ compute $r+\\gamma,max_{a\u0026rsquo;}\\hat{Q}(s\u0026rsquo;,a\u0026rsquo;;w^-)$ update $\\Delta w=\\alpha (r+\\gamma,max_{a\u0026rsquo;}\\hat{Q}(s\u0026rsquo;,a\u0026rsquo;;w^-)-\\hat{Q}(s,a;w))\\nabla_w\\hat{Q}(s,a;w)$ Double DQN current Q-network $w$ is used to select actions older Q-network $w^-$ is used to evaluate actions Advantage Function $$ A^{\\pi}(s,a)=Q^{\\pi}(s,a)-V^{\\pi}(s) $$\nLecture 7 (Imitation Learning) Expert provides a set of demonstration trajectories: sequences of states and actions\nImitation learning is useful when it is easier for the expert to demonstrate the desired behavior rather than:\nspecifying a reward that would generate such behavior specifying the desired policy directly Input\nstate space, action space transition model $P(s\u0026rsquo;|s,a)$ No reward function $R$ set of one or more teacher\u0026rsquo;s demonstrations $(s_0,a_0,s_1,\u0026hellip;)$ (actions drawn from teacher\u0026rsquo;s policy $\\pi^*$ ) type\nBehavioral cloning\ndirectly learn the teacher\u0026rsquo;s policy\nInverse RL\nrecover $R$\nApprenticeship learning via Inverse RL\nuse $R$ to generate a good policy\nBehavioral Cloning formulate problem as a standard machine learning problem: fix a policy class (e.g. neural network, decision tree, etc.) estimate a policy from training examples $(s_0,a_0),s(_1,a_1)\u0026hellip;$ not a good choice\nInverse reinforcement learning Goal: given input, infer the reward function R Linear Feature Reward Inverse RL consider reward is linear over features\n$$R(s)=w^Tx(s)$$\nwhere $w\\in R^n,x:s\\to R^n$\nGoal: identify weight vector $w$ given a set of demonstrations\nThe resulting value function for a policy $\\pi$ can be expressed as $$ \\begin{equation} \\begin{split} V^{\\pi}\u0026amp;=E[\\sum_{t=0}^{\\infty}\\gamma^tR(s_t)|\\pi]\\ \u0026amp;=E[\\sum_{t=0}^{\\infty}\\gamma^tw^Tx(s_t)|\\pi]\\ \u0026amp;=w^TE[\\sum_{t=0}^{\\infty}\\gamma^tx(s_t)|\\pi]\\ \u0026amp;=w^T\\mu(\\pi) \\end{split} \\end{equation} $$ where $\\mu(\\pi)(s)$ is defined as the discounted weighted frequency of state features under policy $\\pi$\nApprenticeship Learning step\nassumption:$R(s)=w^Tx(s)$\nInitialize policy $\\pi_0$\nloop\nfind a reward function such that the teacher maximally outperforms all previous controllers: $$ arg,max_wmax_{\\gamma}s.t.w^T\\mu(\\pi^*)\\ge w^T\\mu(\\pi)+\\gamma,,\\forall\\pi\\in{\\pi_0,\\pi_1,\u0026hellip;,\\pi_{i-1}} $$\ns.t. $||w||_2\\le 1$\nfind optimal control policy $\\pi_i$ for the current $w$\nexit if $\\gamma\\le\\epsilon/2$\nsummary\nif expert policy is suboptimal then the resulting policy is a mixture of somewhat arbitrary policies which have expert in the convex hull in practice: pick the best one of this set and pick the corresponding reward function Lecture 8-10 (Policy Gradient) Directly parametrize the policy $$ \\pi_{\\theta}(s,a)=P[a|s;\\theta] $$ Goal is to find a policy $\\pi$ with the highest value function $V^{\\pi}$\nAdvantages better convergence properties effective in high-dimensional or continuous action spaces can learn stochastic policies Disadvantages Typically converge to a local rather than global optimum Evaluating a policy is typically inefficient and ","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/stanford-reinforcement-learningnotes/","title":"【Stanford Reinforcement Learning】Notes"},{"content":"组成要素 基本构造块\n建模元素，模型的主体\n规则\n支配基本构造块如何组织的规则\n公共机制\n运用于整个UML模型中的公共机制、拓展机制\n基本构造块 事物构造块 结构事物\n静态部分，描述概念或物理元素\n行为事物\n动态部分\n分组事物\n组织模型\n注释事物\n描述模型\n结构事物 类（class）和对象（object）\n类为一组同质对象的抽象，对象为类的实例\n接口（interface）\n描述某个类或构件的一个服务操作集\n主动类（active class）\n其对象至少拥有一个进程或线程，能够启动控制活动的类\n用例（use case）\n用例实例是在系统中执行的一系列动作，这些动作将生成特定执行者可见的价值结果\n一个用例定义一组用例实例\n协作（Collaboration）\n协作定义了一个交互，它是由一组共同工作以提供某协作行为的角色和其他元素构成的一个群体。\n对于某个用例的实现就可以表示为一个协作\n构件（Component）\n比“类”更大的实体\n节点（Node）\n运行时存在的物理元素，它表示了一种可计算的资源，通常至少有存储空间和处理能力\n行为事物 交互（interaction）\n是在特定语境中，共同完成某个任务的一组对象之间交换的信息集合 交互的表示法很简单，就是一条有向直线，并在上面标有操作名\n状态机（state machine）\n一个对象或交互在生命周期内响应事件所经历的状态序列\n分组事物 对结构事物、行为事物进行分组以更加有效地对其进行整合，生成或简或繁、或宏观或微观的模型，在UML中，提供了 “包（Package）”来完成这一目标\n关系构造块 规则 命名 范围（作用域） 可见性 公共机制 规格描述 构造型 标记值 约束 需求定义 用户为了解决问题或达到某些目标所需要的条件或能力 系统或系统部件为了满足合同、标准、 规范或其它正式文档所规定的要求而需要具备的条件或能力； 对上述两种情况中的一个条件或一种能力的一种文档化表述 需求内涵 问题域 当现实的状况与人们期望的状况产生差距时，就产生了问题\n要解决问题，就需要改变现实当中某些实体的状态或改变实体状态变化的演进顺序，使其达到期望的状态或演进顺序\n这些实体和状态构成了问题解决的基本范围，称为该问题的问题域（Problem Domain）\n####### 特性\n问题域自治的规律性\n结构特性 行为特性 约束 假设 解系统 软件系统通过影响问题域，能够帮助人们解决问题，称为解系统 。\n共享现象 软件系统能够与问题域进行交互和相互影响的原因在于，软件系统中的某些部分对问题域中的某些部分的具有模拟特性\n问题域中的某些信息能够和模型中的信息建立映射关系\n这些通过映射建立的共同知识，就是问题域和解系统之间的 共享现象\n需求 需求是用户对问题域当中的实体状态或事件的期望描述\n规格说明 规格说明是解系统为满足用户需求而提供的解决方案，规定了解系统的行为特征\n主要包括两个部分：\n– （1）对共享现象（模型）的描述；\n– （2）系统对共享现象所施加的操作的描述\n也可以看作是一种需求\n– 完全针对系统行为发出的期望\n– 一种理想的、完全不需要进行任何额外努力即可以转换为系统行为的需求。\n需求工程 描述明确的问题域特性E;\n定义良好的系统行为S ;\n预期的需求R；\n需求工程的目的就是根据E，构建S，使得 E, S |-\u0026gt; R\n####### 困难之处\n– （1）不存在描述明确的E；\n– （2）不存在确定的针对S的评估标准R；\n– （3） E, S =\u0026gt; R 是一个创造性的过程\n####### 主要工作\n需求开发,确定R 研究问题背景，描述问题域特性E 构建解系统，描述解系统行为S ，使得 E, S |-\u0026gt; R 需求分类 功能需求（Functional Requirement）\n和系统主要工作相关的需求，即在不考虑物理约束的情况下， 用户希望系统所能够执行的活动，这些活动可以帮助用户完 成任务。功能需求主要表现为系统和环境之间的行为交互\n性能需求（Performance Requirement）\n系统整体或系统组成部分应该拥有的性能特征，例如CPU使 用率、内存使用率等。\n质量属性（Quality Attribute）\n系统完成工作的质量，即系统需要在一个“好的程度”上实 现功能需求，例如可靠性程度、可维护性程度等。\n对外接口（External Interface）\n系统和环境中其他系统之间需要建立的接口，包括硬件接口、软件接口、数据库接口等等\n接口的用途 接口的输入输出 数据格式 命令格式 异常处理要求 约束（Constraint）\n进行系统构造时需要遵守的约束，例如编程语言、硬 件设施等\n系统需求（System Requirement）\n硬件需求（Hardware Requirement） 软件需求（Software Requirement） 其他需求 功能需求 业务需求\n系统建立的战略出发点，表现为高层次的目标（Objective），它描述了组织为什么要开发系统\n为了满足用户的业务需求，需求工程师需要描述系统高层次的解决方案，定义系统应该具备的特性（Feature）\n参与各方必须要对高层次的解决方案达成一致， 以建立一个共同的前景（Vision）\n特性说明了系统为用户提供的各项功能，它限定了系统的范围（Scope）\n用户需求\n执行实际工作的用户对系统所能完成的具体任务的期望，描述了系统能够帮助用户做些什么\n系统需求\n用户对系统行为的期望，一系列的系统行为联系在一起可以帮助用户完成任务，满足业务需求\n用户需求转化为系统需求：\n首先需要分析问题领域及其特性，从中发现问题域和计算机 系统的共享知识，建立系统的知识模型； 然后将用户需求部署到系统模型当中，即定义系列的系统行 为，让它们联合起来实现用户需求，每一个系统行为即为一 个系统需求。 该过程就是需求工程当中最为重要的需求分析活动 ，又称建模与分析活动 需求工程路线 问题分析和背景分析\n明确问题 定义业务需求 指定解决方案 确定系统特性 需求获取\n需求分析\n建立综合考虑问题域特性和需求的系统模型 根据系统模型将用户需求转化为系统需求 文档化和验证\n产生规格说明 进行验证 优秀需求 完整性 正确性 精确性 可行性 必要性 无歧义 可验证：通过分析、检查、模拟或者测试等方法能够判断需求是否被 满足 过程 需求开发 需求管理 需求开发 需求获取 是从人、文档或者环境当中获取需求的过程\n需求获取和需求分析是交织在一起的\n收集背景资料 定义项目前景和范围 选择信息的来源 选择获取方法，执行获取 记录获取结果 需求分析 建模来整合各种信息，以使得人们更好的理解问题\n为问题定义出一个需求集合，这个集合能够为问题界定一个有效的解决方案\n检查需求当中存在的错误、遗漏、不一致等各种缺陷，并加以修正\n背景分析 确定系统边界 需求建模 需求细化 确定优先级 需求协商 需求规格说明 获取的需求需要被编写成文档，主要目的是为了在系统涉众之间交流需求信息\n业务需求被写入项目前景和范围文档\n用户需求被写入用户需求文档（或者用例文档）\n系统需求被写入需求规格说明\n需求验证 确保需求规格说明文档能正确、准确的反映用户的意图\n需求管理 保证需求作用在整个软件的产品生命周期中的 续、稳定和有效发挥\n建立和维护需求基线集 建立需求跟踪信息 进行变更控制 并发和迭代 过程模型 软件开发过程\n瀑布式模型 各阶段之间是紧密相关的，后一阶段的工作是依据前一阶段的工作结果而开展的。\n快速原型模型 螺旋式模型 将瀑布式模型与快速原型模型结合到一起，并加上风险分析。\n将瀑布式模型与快速原型模型结合到一起，并加上风险分析。\n来源 涉众 用户 客户 领域专家 市场人员、销售人员等其他用户代替源 相关产品 原有系统 竞争产品 协作产品（与解系统存在接口的其他软件系统） 硬数据 登记表格、单据、报表等定量文档 备忘录、日志等定性文档 重要文档 原有系统的规格说明 竞争产品的规格说明 协作产品的规格说明 客户的需求文档（委托开发的规格说明、招标书） 相关技术标准和法规 相关法律、法规及规章制度 行业规范、行业标准 方法 传统方法\n面谈、问卷调查、硬数据分析、文档检查、需求剥离等\n集体获取方法\n头脑风暴（Brainstorming）、专题讨论会（Workshop）、JAD\n原型\n认知方法\n任务分析（Task Analysis）、协议分析（Protocol Analysis）\n基于上下文的方法\n观察、民族志（Ethnography）和话语分析（Conversation Analysis）\n前景和范围 前景\n所有的涉众都从共同认同的项目前景出发，理解和描述问题域及需求\n范围\n范围内的事物和事件是描述的目标\n问题分析 明确问题\n发现业务需求\n确定高层次的解决方案\n确定系统特性和解决方案的边界\n确定解决方案的约束\n建立系统边界 将所有问题的解决方案进行综合，就可以得到整个解系统的功能和边界\n系统边界的常用技术描述手段是系统用例图和上下文图\n文档 业务需求、高层次解决方案和系统特性都应该被定义到项目前景与范围文档之中\n涉众分析与硬数据采样 涉众分析 涉众：所有能够影响软件系统的实现，或者会被 实现后的软件系统所影响的个人和团体\n主要内容 根据功能前景寻找涉众 从涉众对象获取需求 分析涉众的赢输条件，实施共赢策略 硬数据采样 定量硬数据 统计报表 定性硬数据 组织描述文档 业务指导文档 业务备忘 面谈 类型 结构化面谈\n完全按照事先的问题和结构来控制面谈\n半结构化面谈\n事先需要根据面谈内容准备面谈的问题和面谈结构，但在面 谈过程当中，会见者可以根据实际情况采取一些灵活的策略\n非结构化面谈\n没有事先预定的议程安排\n甚至会在没有太多事前准备的情况下就直接到访被会见者的 工作地，就某个主题开展会谈\n会见者和被会见者谈话的主题可能非常广泛，而且每个主题 都不会非常深入\n也可能在非结构面谈中仅就某个特殊的主题进行深入的讨\n原型 “原型是一个系统，它内化了（capture）一 个更迟系统（later system）的本质特征。 原型系统通常被构造为不完整的系统，以 在将来进行改进、补充或者替代。 ”\n类别 根据开发方法\n探索式（exploratory） 实验式（experimental） 演化式（evolutionary） 前两种又被称为抛弃式原型\n根据构建技术\n水平原型方法（horizontal prototyping）\n仅仅实现选定功能的某些特定层次\n垂直原型方法（vertical prototyping）\n触及到选定功能实现的所有层次\n过程 ####### 需求内容\n角色\n指原型物件在用户工作中的价值，也就是说它为什么是对用户有用的。\n原型物件到底能够帮助用户完成什么样的工作\n外观\n指用户对原型物件的具体感觉体验，即用户在使用原型物件时会看到什么、听到什么和感觉到什么\n实现\n指原型物件完成功能的细节技术和方法\n####### 评估\n####### 修正\n根本任务 建立分析模型 将复杂的系统分解成为简单的部分以及它们之间的联系，确定本质特征\n和用户达成对信息内容的共同理解\n分析的活动主要包括识别、定义和结构化，它的目的是获取某个可以转换为知识的事物的信息\n模型 “模型是对事物的抽象，帮助人们在创建一个事物之前可以有更好的理解”\n集中关注问题的计算特性（数据、功能、规则等等） “它是对系统进行思考和推理的一种方式。建模的目标是建立系统的一个表示，这个表示以精确一致的方式描述系统，使得系统的使用更加容易” 建模方法\n抽象 只关注重要的信息，忽略次要的内容\n分解\n将单个复杂和难以理解的问题分解成多个相对更容易的子问题，并掌握各子问题之间的联系\n投影 多视点方法\n类别 ####### 业务模型\n使用问题域中的重要概念作为模型的组元 使用概念之间的业务联系作为组元之间的关系 使用了业务描述的方式，具有非形式化特征\n####### 计算模型\n使用软件的构成单位作为模型的组元 软件构建单位之间的关系作为模型组元之间的关系 基于计算科学建立的，具有形式化的特征\n####### 软件分析模型\n介于业务模型和计算模型二者之间的模型形式\n使用了计算模型的组元形式\n在组元的表现上采用了业务模型的表现方式\n具有半形式化的特征\n创建解决方案 将一个问题分解成独立的、更简单和易于管理的子问题来帮助寻找解决方案\n帮助开发者建立问题的定义，并确定被定义的事物之间的逻辑关系\n常用技术 结构化技术\n过程建模 数据流图 DAata Flow Diagram 上下文图 Context Diagram 微规格说明 Mini-Specification 数据字典 Data Dictionary 数据建模 实体关系图 Entity Relationship Diagram 行为建模 状态（转换）图/矩阵 State (Transition) Diagram/Matrix 过程/数据关系建模 功能实体矩阵Function/Entity Matrix 信息工程方法 功能分解图 Function Decomposition Diagram 过程依赖图 Process Dependency Diagram 面向对象技术\nUML 用例图 Use-Case Diagram 类图 Class Diagram 交互图（顺序图/通信图） Interaction ( Sequence/communication ) Diagram 活动图 Activity Diagram 对象约束语言 Object Constraint Language 状态图 State Chart Diagram 结构化需求建模 过程建模 结构化分析的核心是数据。数据包括在分析、设计和实现中涉及的概念、术语、属性等所有内容，并把这些内容定义在数据字典中。围绕数据字典，完成功能/过程模型、数据模型和行为模型的结构化建模过程。\n数据流图 ####### 基本元素\n数据加工过程\n过程是指施加于数据的动作或者行为，它们使得数据发生变化，包括被转换（transformed）、被存储（stored）或者被分布（distributed） 可能是由软件系统控制的，也可能是由人工执行的，它重在数据发生变化的效果而不是其执行者 可能会表现为不同的抽象层次，其中内容足够细节和具体，能够对其直接进行“编码”处理的过程被称为原始过程（Primitive Process，又称为基本过程Elementary Process）\n外部实体\n外部实体是指处于待构建系统之外的人、组织、设备或者其他软件系统，它们不受系统的控制，开发者不能以任何方式操纵它们 需要进行建模的外部实体是那些和待构建的软件系统之间存在着数据交互的外部实体，它们是待构建系统的数据源或者数据目的地 所有的外部实体联合起来构成了软件系统的外部上下文环境\n数据流\n数据流是指数据的运动，它是系统与其环境之间或者系统内两个过程之间的通信形式 数据流可以分割和组合\n数据字典和实体关系图ERD通常被用来描述DFD的详细内容\n数据存储\n数据存储是软件系统需要在内部收集、保存，以供日后使用的数据集合 数据存储的详细内容通常也是用数据字典和ERD来进行描述的\n####### 构建规则\n过程是对数据的处理，必须有输入，也必须有输出，而且输入数据集和输出数据集应该存在差异 数据流是必须和过程产生关联的，它要么是过程的数据输入，要么是过程的数据输出 DFD当中所有的对象都应该有一个可以唯一标识自己的名称 过程使用动词 外部实体、数据流和数据存储使用名词 ####### 层次结构\n依据所含过程的不同抽象程度，DFD可以在不同的抽象层次上进行系统的描述 一个比较抽象的过程可以被展开为一个子过程更加具体的DFD图\n上下文图 Context diagram\n将整个系统看做是一个过程，这个过程实现系统的所有功能 ，是系统功能的最高抽象。上下文图中存在且仅存在一个过程，表示整个系统。这个单一的过程通常编号为0。上下文图中需要表示出所有和系统交互的外部实体，并描述交互的数据流，包括系统输入和系统输出。上下文图中不会出现数据存储实例，它非常适合于描述系统的应用环境、定义系统的边界\n0层图（顶层图）Level-0 diagram\n位于上下文图下面一层，是上下文图中单一过程的细节描述，是对该单一过程的第一次功能分解， 是整个系统的功能概图\n0层图应该被描述的简洁、清晰，需求工程师要根据系统的复杂度掌握0层图中过程的抽象程度\nN层图 Level-N diagram\n对0层图的过程分解产生的子图称为1层图，对N层图的过程分解后产生的子图称为N+1层图（N\u0026gt;0） ，过程分解是可以持续进行的， 直至最终产生的子图都是原始DFD图：\n所有过程都已经被简化为一个选择、计算或者数据库操作； 所有数据存储都仅仅表示了一个单独的数据实体； 用户已经不关心比子图更为细节的内容，或者子图的描述已经详细的足以支持后续的开发活动； 每一个数据流都已经不需要进行更详细的切分，以展示对不同数据的不同处理方式； 每一个业务表单、事务、计算机的屏幕显示（computer on\u0002line display）和业务报表都已经被表示为一个单独的数据流； 系统的每一个最低层菜单选项都能在子图中找到独立的过程 原始DFD图可以进一步展开为\n微规格说明 数据字典 在低于0层图的子图上通常不显示外部实体\n####### 验证\n验证DFD的语法 确保DFD中不会发生语法错误 验证DFD的结构 验证DFD层次结构之间的一致性 验证DFD层次结构说明的完备性 验证DFD的语义 确保DFD所说明内容的正确性和准确性 微规格说明 目的：细化原始DFD图 方式 结构化英语/伪码 行为图 决策表 决策树 数据字典 数据字典以结构化方式定义了在建模过程中涉及到的所有数据信息、控制信息。 它是当前系统的软件词典，提供用户和软件人员的概念解释，也提供在系统开发过程中各种有关数据和控制的描述信息，使得系统所有的相关人员对信息有共同的、一致的理解。 常用方式\n词条描述\n词条描述为每个数据元素组织描述信息\n定义式\n定义式要求对数据元素（尤其是其结构）的描述要精确、严格和明确\n数据建模 数据模型：描述数据的定义、结构和关系等特性的模型说明了问题域和解系统共享的事物、对共享事物的描述和共享事物之间的关系能够反映企业业务的核心知识 建立数据模型的过程被称为数据建模\n面向对象分析与UML建模 UML（Unified Modeling Language）是一种统一的、标准化的建模语言\nOMT (James Rumbaugh) Booch方法(Grady Booch) OOSE (Ivar Jacobson） 组成元素 基本构造块\n也就是建模元素，是模型的主体\nUML规则\n也就是支配基本构造块如何放在一起的规则\n公共机制\n运用于整个UML模型中的公共机制、扩展机制\n基本构造块 事物构造块\n对模型中最具有代表性的成分的抽象\n结构事物：UML中的名词，它是模型的静态部分，描述概念或物理元素。\n类（class）和对象（object）\nUML中类是用一个矩形表示的，它包含三个区域，最上面是类名、中间是类的属性、最下面是类的方法\n对象则是类的一个实例\n接口（interface）\n接口是描述某个类或构件的一个服务操作集\n主动类（active class）\n主动类实际上是一种特殊的类。引用它的原因，实际上是在开发中需要有一些类能够起到启动控制活动的作用\n主动类是指其对象至少拥有一个进程或线程，能够启动控制活动的类\n用例（use case）\n用例实例是在系统中执行的一系列动作，这些动作将生成特定执行者可见的价值结果\n一个用例定义一组用例实例\n协作（collaboration）\n定义了一个交互，它是由一组共同工作以提供某协作行为的角色和其他元素构成的一个群体。\n构件（component）\n构件是系统设计的一个模块化部分，它隐藏了内部的实现，对外提供了一组外部接口。在系统中满足相同接口的组件可以自由地替换 可以用来描述实际的PC机、打印机、服务器等软件运行的基础硬件\n节点（node）\n运行时存在的物理元素，它表示了一种可计算的资源，通常至少有存储空间和处理能力\n行为事物：UML中的动词，它是模型中的动态部分，是一种跨越时间、空间的行为。\n交互（Interaction）\n共同完成某个任务的一组对象之间交换的信息集合\n状态机（state machine）\n是一个对象或交互在生命周期内响应事件所经历的状态序列\n分组事物：UML中的容器，用来组织模型，使模型更加的结构化。\n对于一个中大型的软件系统而言，通常会包含大量的类， 因此也就会存在大量的结构事物、行为事物，为了能够 更加有效地对其进行整合，生成或简或繁、或宏观或微 观的模型，就需要对其进行分组。在UML中，提供了 **“包（Package）”**来完成这一目标\n注释事物：UML中的解释部分，和代码中的注释语句一样，是用来描述模型的\n关系构造块\n关联关系\n表示两个类之间存在某种语义上的联系。关联关系提供了通信的路径，它是所有关系中最通用、语义最弱的。\n在UML中，使用一条实线来表示关联关系\n在关联关系中，有两种比较特殊的关系：聚合和组合\n聚合关系：聚合（Aggregation）是一种特殊形式的关联。聚合表 示类之间的关系是整体与部分的关系 组合关系：如果发现“部分”类的存在，是完全依赖于“整体”类的，那么 就应该使用“组合”关系来描述 泛化关系\n描述了一般事物与该事物中的特殊种类之间的关系，也就是父类与子类之间的关系\n实现关系\n用来规定接口和实现接口的类或组件之间的关系。接口是操作的集合，这些操作用于规定类或组件的服务。\n拓展关系\n表示将一个构造型附加到一个元类（metaclass）上，使得元类的定义中包括这个构造型\n依赖关系\n有两个元素X、Y，如果修改元素X的定义可能会引起对另一个元素Y的定义的修改，则称元素Y依赖（Dependency）于元素X\n图\n规则 公共机制 规格描述\n在图形表示法的每个部分后面都有一个规格描述（也称为详述），它用来对构造块的语法和语义进行文字叙述。 这种构思，也就使可视化视图和文字视图的分离\n扩展机制\n构造型\n在实际的建模过程中，可能会需要定义一些特定于某个领域或某个系统的构造块\n标记值\n用来为事物添加新特性。标记值的表示方法是用形如“{标记信息}”的字符串\n约束\n是用来增加新的语义或改变已存在规则的一种机制。约束的表示法和标记值法类似，都是使用花括号括起来的串来表示，不过它是不能够放在元素中的，而是放在相关的元素附近\n模型划分 用例模型 Use Case Model\n描述使用系统功能的角色和系统相关的功能，是需求建模的重要工具\n静态模型（领域模型）\n类图（class diagram）\n类图是系统模型的基础，描述系统的静态结构\n对象层：描述系统实体以及承载的系统责任 特征层：描述实体抽象的特征 关系层：实体类的固有关系 包图（package diagram）\n描述系统的组织模型，为控制表示的复杂性\n动态模型（行为模型）\n顺序图（sequence diagram）\n描述按时间顺序排列的对象交互\n协作图（collaboration diagram）\n表示交互对象的行为组织结构\n状态转换图（state chart diagram）\n描述对象在生命周期内，响应事件的状态转换过程，以及响应事件后所做的反映\n活动图（activity diagram）\n用来描述任务流程或算法过程，可用来分析系统并发事务流程\n物理实现\n部署图（deplotment diagram）\n用来描述系统中计算结点的拓扑结构，一个系统只有一个部署图，可用来分析分布式系统\n构件图（component diagram）\n描述一组构件以及相互间的关系，是系统实现的物理建模\n建模过程 用例模型 用例图（Use Case）用于对系统的功能及与系统进行交互的外部事物建模\n用例 一个用例描述系统的一项功能，该项功能可被描述为参与者可视的一组操作，其中的每个操作表示参与者与系统的一个交互过程\n用例描述系统外部可见的功能需求，只描述做什么，不描述怎么做 多数是由参与者发起的动作，也允许系统发起的动作，例如：异常情况处理\n用例关系 包含关系《include》：描述用例间具有的公用行为 扩展关系《extend》 ：描述用例间可选的独立行为 泛化关系 generalization ： 用例之间的继承关系 静态模型 面向对象建模 类图，是系统建模过程中最重要的部分，也是花费精力最大的活动。类图描述系统中各个对象之间存在的关系，表达系统的静态结构，也叫做“对象建模”\n接口类 把类的公共可见性操作组织在一起， 提供的服务集合\n接口类作为类之间交互操作的契约\n包图 包是对模型成分分组的机制\n多个包可以形成严格的树形层次结构，用于描述系统的组织结构\n一个包可以嵌套在另一个包内，内层的包成分，同时属于内层和外层两个包。\n动态模型 \u0026hellip;\n物理实现 构件图Component Diagram 与部署图Deployment Diagram 是在系统设计时，用来表示系统软件成分以及之间关系结构的工具\n分析构件及其间的关系，并对它们在运行节点上的成分给与描述，也叫“物理事物建模”\n构件图 类表示逻辑抽象，是逻辑模块，构件表示机器空间中的物理模块，是逻辑元素及协作关系的物理实现\n类有属性和操作，构件仅通过接口向外提供可请求的操作\n构件种类 部署构件\n可用于构造的执行系统，如：动态连接库（DLL）和可执行程序（EXE）\n产品构件\n开发过程的产物，包括创建部署构件的源代码文件及数据文件等\n可执行构件\n由执行系统创建的构件\n部署图 分析构件及其间的关系，并对它们在运行节点上的成分给与描述，也叫“物理事物建模”\n构件是系统执行的事物，节点是执行构件的事物。 构件代表逻辑元素的物理打包，节点可表示构件的物理部署\n节点上可以有一个或多个构件，一个构件也可以部署在一个或多个节点上。\n需求验证 方法 评审\n由作者之外的其他人来检查产品问题的方法 是主要的静态分析手段 原则上，每一条需求都应该进行评审\n原型与测试\n开发测试用例\n如果无法为某条需求定义完备的测试用例，那么它可能就存在着模糊、信息遗漏、不正确等缺陷\n用户手册编制\n利用跟踪关系\n业务需求 -\u0026gt; 用户需求 -\u0026gt; 系统需求\n如果业务需求和用户需求没有得到后项需求（用户需求和系统需求）的充分支持，那么软件需求规格说明文档就存在不完备的缺陷。\n系统需求 -\u0026gt; 用户需求 -\u0026gt; 业务需求\n如果不能依据跟踪关系找到一条系统需求的前项用户需求和前项业务需求，那么该需求就属于非必要的需求。\n自动化分析\n需求管理 维护需求基线 实现需求跟踪 控制变更 需求基线 已经通过正式评审和批准的规格说明或产品，它可以作为进一步开发的基础，并且只有通过正式的变更控制过程才能修改它 定义：是被明确和固定下来的需求集合，是项目团队需要在某一特定产品版本中实现的特征和需求集合\n描述内容 标识符（ID），为后续的项目工作提供一个共同的交流参照。 当前版本号（Version），保证项目的各项工作都建立在最新的一致需求基础之上。 源头（Source），在需要进一步深入理解或者改变需求时，可以回溯到需求的源头。 理由（Rational），提供需求产生的背景知识。 优先级（Priority），后续的项目工作可以参照优先级进行安排和调度。 状态（Status），交流和具体需求相关的项目工作状况。 成本、工作量、风险、可变性（Cost、Effort、Risk、Volatility），为需求的设计和实现提供参考信息，驱动设计和实现工作。 需求创建的日期； 和需求相关的项目工作人员，包括需求的作者、设计者、实现者、测试者等； 需求涉及的子系统； 需求涉及的产品版本号； 需求跟踪 避免在开发过程或者演化过程中与需求基线不一致或者偏离的风险\n前向跟踪是指被定义到软件需求规格说明文档之前的需求演化过程 向前跟踪到需求：说明涉众的需要和目标产生了哪些软件需求 从需求向后回溯：说明软件需求来源于哪些涉众的需要和目标 后向跟踪是指被定义到软件需求规格说明文档之后的需求演化过程 从需求向前跟踪：说明软件需求是如何被后续的开发物件支持和实现的 回溯到需求的跟踪：说明各种系统开发的物件是因为什么原因（软件需求）而被开发出来的 需求变更控制 以可控、一致的方式进行需求基线中需求的变更处理，包括对变化的评估、协调、批准或拒绝、实现和验证\n","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/uml%E9%9C%80%E6%B1%82%E5%BB%BA%E6%A8%A1%E7%AC%94%E8%AE%B0/","title":"【UML需求建模】笔记"},{"content":"数字信号与模拟信号 模拟信号 Analog 连续 数字信号 Digital 离散 逻辑电平 正逻辑：高1低0 负逻辑：高0低1 标准：TTL,CMOS\n时序图 将输入信号和输出信号的关系按照时间顺序排列得到的波形图\n进制转换 整数连除，小数连乘（10\u0026ndash;\u0026gt;N）\n二进制符号表示 原码 最高位为符号位（0正1负）\n大小以绝对值表示\n反码 绝对值的原码表示每一位取反再加上符号位\n补码 定义: $$ (N)_{补n}=2^n-N $$\n与其小数位无关\n求法：\n原码补足n位后整体求反加一\n$$ N_{补}=N_{反}+1 $$\n原码补足n位后从右向左第一个1往右不变，其余取反\n符号数表示法 反码表示法：正数用原码表示，负数用反码表示 补码表示法：正数用原码表示，负数用绝对值的补码表示（便于运算） BCD码 Binary Coded Decimal\n有权码（前五位自然） 8421码：自然码 5421码：第一位镜像自反，其余三位对应 2421码：镜像自反 无权码 余3码：8421码+3 余3循环码：第一位镜像自反，其余镜像相等,逻辑相邻 格雷(Gray)码 格雷转二进制 $$ G_{n-1}=B_{n-1},B_{i}=G_{i} \\oplus B_{i+1} $$\n二进制转格雷 $$ G_{n-1}=B_{n-1},G_{i}=B_{i+1} \\oplus B_{i} $$\n逻辑函数 相等 输入变量相同，任意一组变量取值结果相同 亦即真值表相同 反函数 输入相同，输出相反，成为互反（互补）\n对偶式 \u0026lsquo;*\u0026lsquo;与\u0026rsquo;+\u0026lsquo;互换 0与1互换 标准形式 与或（最小项之和） 或与（最大项之积） 与非-与非 或非-或非 与或非 e.g.\n基本定律 运算规则 代入规则 等式两边同一逻辑变量代以同样逻辑函数等式仍成立\n反演规则 \u0026lsquo;*\u0026lsquo;与\u0026rsquo;+\u0026lsquo;互换 0与1互换 变量取反 得反函数\n对偶规则 两个函数相等则其对偶式相等\n基本定理 复合运算 与非，或非，与或非 异或 同或 最大项、最小项 最大（小）项：n个变量或其反变量仅出现一次之和（积），均为2^n个\n代数化简法 变换方法 卡诺图化简法 卡诺图 特点 n变量卡诺图有2^n个方格 大于4时无法用二维表示 形式不一 填法 真值表：按0，1填 与或：与项覆盖区域填1其余填0 或与：或项覆盖区域填0其余填1 性质 均1则1 均0则0 取反得反 （两图）相乘为（各小格）与 相加为或 异或为异或 最小覆盖原则 每个\u0026quot;1\u0026quot;至少被覆盖一次 圈尽量少亦即每个圈包含的\u0026quot;1\u0026quot;尽量多 每个圈至少包含一个独有的\u0026quot;1\u0026quot;小格 非完全描述逻辑函数 完全描述逻辑函数：输入变量的每一组取值都有确定的函数值（\u0026ldquo;0\u0026quot;或\u0026quot;1\u0026rdquo;）与之对应 非完全描述逻辑函数：有些取值组没有确定的函数值，可能有多种不同的最简形式 任意项（无关项） 代数法化简 根据需要舍弃（定义为\u0026quot;0\u0026quot;）或加进（定义为\u0026quot;1\u0026quot;）以使逻辑相邻的最小项个数最大化\n逻辑函数描述方法 真值表 卡诺图 逻辑表达式 逻辑图 时序图 门电路 二极管 单向导电性\n钳位特性：导通时端电压可视为不变\nDDL“与门” DDL“或门” 缺点：\nDDL与门，输入为0V时，输出低电位被钳位在0.3V\nDDL或门，输入为3V时，输出高电位被钳位在3- 0.3=2.7V。\n存在电压阈值损失，不能多级互联\n三极管 组合逻辑电路 由逻辑门组成 内部无反馈环节 不具有\u0026quot;记忆\u0026quot;功能（输出仅由当前时刻的输入决定） compared with 时序逻辑电路：有反馈环节，具有“记忆”功能，输出和之前的状态有关\n组合电路是由逻辑门组成的多输入，多输出（或单输出）的逻辑电路\n分析一般步骤 写表达式 列真值表 分析功能 常用组合电路 半加器 全加器 应用 四位全加器 减法器 数据选择器 又称为多路开关，多路转换器\n自扩展 用多片某类选择器构成更大范围的选择器，如：\n多路分配器 与数据选择器功能相反，将一个输入分别送到多个输出端\n编码器 编码：用数码信号表示特定对象\n二进制编码：用多位二进制数形成一组二进制代码，并将代码赋予特定的含义\n8线-3线普通编码器 输入8个高低电平信号，输出三位二进制数\n组合逻辑电路设计 用小规模电路(SSI)实现——各种逻辑门 实际逻辑问题=》真值表=》逻辑表达式=》逻辑化简=》逻辑电路图\n优先编码器 允许多个输入同时有效，按规定的优先级别进行编码\n典型优先编码器 优先编码器扩展 译码器 编码的逆过程，将代码“翻译为特定的对象，将一组二进制代码“翻译”为一组高低电平信号\n通用译码器\n二进制译码器\n将n位二进制代码，译为特定含义的2^n个输出信号\n2线-4线，3线-8线，4线-16线\n二-十进制译码器\n代码转换器\n显示译码器\n通用译码器 74LS138 4线-16线译码器（74LS138扩展） 二-十进制译码器 显示译码器 将4位二进制代码译为数码显示器所需的信号\n七段译码显示器 74LS47：输出低电平有效，用于共阳极数码管\n74LS48：输出高电平有效，用于共阴极数码管\n数值比较器 比较两个相同位数的二进制数的大小，由F_A\u0026lt;B,F_A=B,F_A\u0026gt;B三个输出表示比较的结果\n用中规模集成电路（MSI）实现——译码器，选择器 采用中规模器件（一般指译码器、数据选择器和全加器等）设计组合电路，应对逻辑函数进行变换，得到与指定器件相一致的表达式\n用数据选择器设计 如果逻辑函数输入变量数与数据选择器控制端数量相同（如用8选1实现3变量函数），则输入变量与控制 变量一一相接，数据输入端接高、低电平。\n如果逻辑函数输入变量数多于数据选择器控制端数 （如用4选1实现3变量函数），则需分离多余的变量。 未被分离的输入变量与控制变量相接，被分离变量 则与数据输入端相接\n一般情况下，一个n变量的逻辑函数可用（2 n）选1 或（2 n-1）选1数据选择器实现。 如果部分变量出现的频率更低的话，则通过一些门 电路可实现更多变量的逻辑函数。\n译码器 对于最小项译码器来说，其输出是输入变量的所有 最小项。 由于所有逻辑函数都可转化成其最小项的和的形式， 因此任何逻辑函数都可采用译码器实现。 首先需要将表达式转换成最小项的和的形式。\n3线-8线译码器可实现任何3变量的逻辑函数。\n4线-16线译码器可实现任何4变量的逻辑函数\n竞争和冒险 信号通过门传输需要时间，即实际的逻辑门存在传 输延迟时间。\n输入有竞争现象时，输出不一定都产生冒险。 冒险分为逻辑冒险和功能冒险两种\n逻辑冒险 当多个输入信号中某一个发生变化时，由于此信号在电路中经过的途径不同，使到达电路某个门的多个 输入信号之间产生时间差，即存在由所有的逻辑部件 的延迟时间引起的竞争，称为“逻辑竞争”，由此产生的冒险为“逻辑冒险”。\n功能冒险 在组合电路的输入端，当有几个变量变化时，由于 其变化的快慢不同，传递到某个门的输入端必然存在 时间差，这种现象叫作“功能竞争” 。 由此产生的冒险为“功能冒险”。\n消除\n加滤波电容 引入禁止脉冲 触发器 能够存储1位二进制数字信号的基本单元电路叫做 触发器。 触发器是构成各种复杂数字系统的基本逻辑单元。\n原态与次态 触发器在接收信号之前所处的状态称为原态/初态， 用Q^n表示；\n触发器在接收信号之后建立的新的稳定状态，叫做次态/新态，用Q^n+1表示。\n显然，触发器的次态 Q^n+1是由输入信号和原态Q^n的取值情况所决定。\n基本RS触发器 同步RS触发器 在较复杂的数字系统中，当采用多个触发器时，往往要求各个触发器的翻转在时间上同步，因此需引入一个公用的同步信号，使这些触发器只有在同步信号到达时才按输入信号改变输出状态。 通常称此同步信号为时钟脉冲信号，简称时钟，用CP表示。\n同步触发器又称作电平触发型触发器\n空翻 同一时钟脉冲作用期间，引起触发器发生两次以及多次翻转的现象\n同步RS触发器存在空翻现象，为了提高抗干扰能力， 克服空翻，希望一个CP脉冲作用期间Q只改变一次。可采用以下结构形式：主从型、边沿型。\n主从型触发器 主从结构的触发器也叫做脉冲触发型触发器\nRS 主从RS触发器和同步RS触发器的特性表相同，但 工作时序不同。 ❑ 主从RS触发器在CP由1→0（下降沿）后根据CP＝1 期间S、R的状态而改变状态。 ❑ 即输出状态的变化发生在CP信号的下降沿。\nJK CP=1，主触发器根据J、K状态而动作，从触发器保持\nCP=0，从触发器根据主触发器的状态进行输出\n注意 只有CP=1期间输入信号未发生过变化的条件下， 用CP下降沿到达时输入的状态决定触发器的次态才是 正确的。 主从JK触发器存在一次变化问题。 边缘型触发器 为了进一步增强触发器的抗干扰能力，提高工作的可靠性，希望触发器的次态仅仅取决于CP的上升沿或下降沿到来时刻输入信号的状态，而在此之前的或之后输入信号状态的任何变化对触发器的次态都没有影响。\n维持阻塞结构的边沿触发器 利用CMOS传输门的边沿触发器 逻辑功能 RS\nJK\nD\nT\nT\u0026rsquo;\n相互转换 将具有某种逻辑功能的触发器FF，在其输入端加一转换电路（组合逻辑电路），可完成另一待求触发器的逻辑功能。\n时序逻辑电路 由组合逻辑电路和存储电路构成，它在某一时刻的输出状态不仅与该时刻输入信号有关， 还与电路原来的输出状态有关。\n包含组合电路和存储电路两部分 存储电路的输出反馈到组合电路的输入端。\n功能描述方法 逻辑函数表达式（3个方程） 状态转换真值表 状态转换图 时序图 分类 按照存储单元状态变化特点（动作特点） 同步时序逻辑电路\n所有触发器状态变化受同一CP控制\n异步时序逻辑电路\n触发器状态不同时变化\n按照输出信号特点 Mealy型\n输出信号取决于存储电路与输入变量\nMoore型\n输出仅仅取决于存储电路的状态\n按照逻辑功能 计数器 寄存器 移位寄存器 顺序脉冲发生器 电路分析 写方程：根据逻辑电路图写出各触发器的时钟方程、驱动方程、输出方程\n求状态方程：将驱动方程代入相应触发器的特性方程，得到各触发器的状态方程（即次态方程）\n列出状态转换真值表：依次设初态，求次态，列出状态转换真值表\n或者画出状态转换图（有效循环无效循环，自启动）\n（或时序图）\n说明逻辑功能\n名词解释 有效状态：使用的状态\n无效状态：未使用的状态\n有效循环： 在CP脉冲作用下，电路在有效状态中的循环\n无效循环： 在CP脉冲作用下，电路在无效状态中的循环\n自启动：电路一旦进入无效状态，在CP脉冲作用下，能自 返回到有效循环中去的电路叫能自启动，否则叫不能自启动。\n中规模时序逻辑电路 寄存器 寄存器是存放二进制数码的逻辑部件，由触发器构成。 一个触发器可寄存一位二进制代码，N 个触发器构成的寄存器可寄存N 位二进制数码\n移位寄存器 功能：存储代码, 移位。\n移位－寄存器中的代码在CP脉冲作用下，逐位左移或右移\n用途\n存数 数据串行\u0026ndash;并行转换 数值运算 数据处理 分类\n单向移位寄存器\n双向移位寄存器\n计数器 用于计算输入脉冲CP的个数\n应用 计数、分频（因此又称为分频器）、定时、产生脉冲序列及节拍脉冲， 进行数字运算等\n分类 按计数增减： 加法计数器 减法计数器 可逆计数器 其他计数器 按动作特点： 同步计数器 异步计数器 按进制： 二进制计数器 二-十进制计数器 任意计数器 同步计数器 ####### 二进制加法\n特点：\n最低位每来一个CP改变一次状态 第i位在第0～(i－1)全为1时，改变状态。 ####### 十进制加法\n####### 二进制减法\n在多位二进制数末位减1 第i位以下皆为0时， 则第i位应翻转 ####### 十进制减法\n####### 小结\n异步计数器 对比 异步计数器由于触发器逐级翻转，工作速度低； 将某些状态译码时，译码器输出端会有竞争冒险产 生的尖峰脉冲；但其结构简单，可自启动。 同步计数器工作频率较高，传输延迟短，但结构 复杂\n74LS191 移位计数器 移位计数器是一种特殊形式的计数器。 它是在移位寄存器的基础上增加反馈电路构成的。 常用的移位计数器有环形计数器和扭环形计数器\n优点：不需要译码\n缺点：状态利用率低，不能自启动\n解决自启动：\n修改输出与输入之间的反馈逻辑，使电路具有自启动能力。\n当电路进入无效状态时，利用触发器的异步置位、 复位端，把电路置成有效状态。\n中规模集成计数器 **同步十进制加法计数器74160 ** **同步四位二进制加法计数器74161 ** 74161（16进制）除了进制与74160（10进制）不同之外，其他功能与74160相同\n**异步二-五-十进制加法计数器74290 ** 同步四位二进制加减法计数器CC4516 任意进制计数器设计 若已有N进制计数器芯片，需M进制计数器，分两种情况：\nM \u0026lt; N: 用一片N进制计数器即可。\n想办法跳过N-M个状态。\n清零法（复位法）（反馈归零法）： 适用于有清零端的计数器。\n异步清零 同步清零 （相与或与非取决于高电平有效还是低电平有效）\n异步控制可靠性差。清零信号随着计数器被置零立即消失，持续时间极短，易导致触发器的误动 作，该电路不可靠。\n置数法（置位法）: 适用于有预置数功能的计数器。\n通过给计数器重复置入某数值的方法跳越N-M个状态，从而获得M进制计数器。\n同步式预置数的计数器（74160，74161）： 预置数信号从Si状态译出，待下一个CP信号到来， 才将所需数据置入。 异步式预置数的计数器（74191）： 预置数信号从Si+1状态译出，只要预置数信号有效，立即将所需数据置入，不受CP信号控制 M \u0026gt; N:视情况需用多片N进制计数器\n适合于M=N1 X N2的情况\n串行进位方式：低位片的进位输出信号作为高位片的时钟输入 并行进位方式 ：低位片的进位输出信号作为高位片的工作状态控制信号（使能），两片的时钟输入端同时接输入信号 适合于所有情况\n将2片N进制计数器通过级联构成NxN进制计数 器，并且假定M\u0026lt;NxN\n整体清零方式 整体置数方式 顺序脉冲发生器 在计算机和控制系统中，常常要求系统的某些操作按时间顺序分时工作，因此需要产生一个节拍控制脉冲，以协调各部分的工作。 能产生节拍脉冲的电路叫做节拍脉冲发生器，又称顺序脉冲发生器(脉冲分配器)\n分类 计数器型\n该电路由计数器和译码器构成。 n个触发器构成的计数器有2 n个状态。在时钟脉冲 作用下，计数器不断改变状态，经译码后在2 n个输出端上每一时刻只有相应的一条输出线上出现高电 平(或低电平)，其他输出线上均出现低电平(或高电平)\n缺点：触发器翻转时刻不可能完全一致，可能存在干扰脉冲\n移位寄存器型\n采用环形计数器和扭环形计数器构成顺序脉冲发生器。可以避免在译码过程中出现干扰脉冲。\n环形计数器的每个触发 器的Q端输出就是节拍脉冲。 不需要另加译码器。\n序列脉冲发生器 在数字信号的传输和数字系统的测试中，有时需要用到一组特定的串行数字信号，通常把这种串行数字信号叫做序列信号。 产生序列信号的电路称为序列信号/脉冲发生器\n构造方法 用计数器和数据选择器组成\n用计数器加输出电路\n时序逻辑电路设计 时序电路设计是时序逻辑电路分析的逆过程。\n根据给出的逻辑问题（命题要求）， 设计出能实现逻辑要求的时序电路，画出逻辑图。\n经典设计方法，采用尽可能少的小规模器件 （触发器和门电路）； 采用标准中规模、大规模集成器件进行逻辑 设计； 采用可编程逻辑器件进行设计 同步时序逻辑电路 逻辑抽象\n确定输入变量、输出变量、及电路的状态数。 定义输入、输出状态及电路状态含义。 画原始状态转换图(表)\n状态化简\n合并等价状态,进行状态化简，求出最简状态转换图(表)。\n等价状态： 若两个状态在输入相同时输出相同,次态也相同, 称其为等价状态\n状态分配\n确定触发器数目n，2^n-1\u0026lt;M\u0026lt;=2^n，进行状态编码(状态分配)\n选定触发器类型\n确定触发器类型，求出： 状态方程 输出方程 驱动方程\n判断能否自启动\n","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/%E6%95%B0%E5%AD%97%E9%80%BB%E8%BE%91%E7%AC%94%E8%AE%B0/","title":"【数字逻辑】笔记"},{"content":"发展过程 人工管理 文件系统 数据只能是定长的 文件间是独立的 数据整体无结构 最小存取单位是记录 数据库系统 数据结构化 数据的共享性高，冗余度低，易扩充 数据独立性高 数据由DBMS统一管理和控制 最小存取单位是数据项 DBMS提供的数据控制功能具有:\n安全性（Security） 完整性（Integrity) 并发（Concurrency) 恢复（Recovery） 概念 数据库（DB） 存放数据的仓库，长期存储在计算机内，有组织的数据集合\n数据库管理系统（DBMS） 用户与操作系统之间的数据管理软件，负责对数据库的管理和维护\n数据库系统（DBS） 在计算机系统中引入数据库后的系统，一般由数据库、数据库管理系统（及其开发工具）、应用系统 、数据库管理员构成。\n数据库管理员（DBA） 进行数据库建立，使用和维护的专门人员\n数据模型 对现实世界的模拟，表示现实世界中的各种事物及其联系的方法\n组成 数据结构 数据操作 数据完整性约束：对数据静态和动态特性的限定，定义相容数据库状态的集合及可允许的状态改变 概念模型（信息模型） 独立于计算机之外，不涉及信息在计算机中的表示\ne. g. 实体-联系模型，语义数据模型\n数据模型 面向计算机\ne. g. 层次模型，网状模型，关系模型\n三级模式结构 外模式（External Schema） 亦称子模式（Subschema），对数据库用户的数据视图，体现用户的数据观点，是对用户数据结构的逻辑描述\n模式（Schema） 亦称概念模式或逻辑模式，数据库的总框架，对全体数据逻辑结构和特性的描述，独立于应用程序和物理存储\n内模式（Internal Schema） 亦称存储模式，对数据库存储结构和存取方法的描述，规定数据在存储介质上的物理组织方式，记录寻址方式，定义物理存储块大小，溢出处理方法。\n数据抽象 三级模式结构提供了视图级，概念级和物理级的数据抽象\n二级映像 数据库技术发展 第一代层次、网状数据库系统 第二代关系数据库系统 新一代数据库系统 E-R数据模型（Entity-Relationship） 实体（集）【方框】：现实世界中存在的（具有同类特性的）个体（的集合） 属性【椭圆】：实体具有的特性或特质 联系【菱形】： 一对一，一对多，多对多 实体的参与度：参与联系的最小和最大次数 弱实体【双框双线】：依赖于其他实体存在的实体 EER模型（extended E-R） 增加了子类和超类的概念,使E-R数据模型具有更多的语义\n下层实体用两竖边为双线的矩形框表示，上层实体和下层实体之间用加小圈 的直线连接起来，下层实体称为上层实体的子 类，上层实体称为超类。\n层次数据模型 也叫树形结构，树中的每个节点代表一种记录类型，其应满足：\n有且仅有一个结点无双亲，这个结点称为根结点 其他结点有且仅有一个双亲结点 层次模型中的基本数据结构是记录和由记 录组成的层次结构\n网状数据模型 允许\n一个结点可以有多个双亲结点 多个结点可以无双亲结点 关系模型 关系模式(relation schema) 对关系结构的描述，可用\n关系名（属性名1，属性名2,\u0026hellip;,属性名n）\n关系 (relation) 一张二维表，可用于描述一个实体集\n属性 (attribute) 每一列为关系的一个属性\n域 (domain) 属性的取值范围\n元组 (tuple) 一个元组对应实体集中的一个个体\n键 (key) 一个或多个属性组成，能唯一标识一个元组\n一个关系可能有多个键，选取一个作为主键，其余作为候选键\n关系 D1 X D2 X D3 \u0026hellip; X Dn 的子集称为在域D1, D2, \u0026hellip; , Dn上的关系，表示为 R（D1, D2, \u0026hellip; , Dn）\n关系是笛卡尔积的有限子集 关系满足交换律而笛卡尔积不满足，因此为关系的每个列附加一个属性名以取消有序性 三类关系 基本关系（基本表/基表）：实际存在的表，是实际存储数据的逻辑表示 查询表 ：查询结果对应的表 视图表 : 由基本表或其他视图表导出的虚表 基本关系性质 列是同质的 不同列可出自同一个域 分量应为不可分的数据项 关系模式(Relation Schema) 关系模式**（型）：对关系（值）**的描述，静态且稳定\n元组集合的结构\n​\t属性构成\n​\t属性来自的域\n​\t属性与域之间的映象关系\n元组语义以及完整性约束条件\n属性间的数据依赖关系集合\n关系 关系模式在某一时刻的状态或内容\n动态、随时间变化\n二者往往统称为关系，关系模式为关系的内涵，关系的值为关系的外延\n模式中属性到对应域映射的有限集，通常写为r(R)\n关系模式的形式化表示 R(U, D, dom, F)\nR: 关系名 U: 组成该关系的属性名集合 D: 属性组U中属性来自的域 dom: 属性向域的映象集合 F: 属性间的数据依赖关系集合 简记为 R(U) 或 R(A1, A2, ……, An )\n关系数据库模式 关系模式的集合，是对数据库中所有数据逻辑结构的描述，表示为R={R1, R2,……, Rp}\n关系数据库 关系数据库模式中每个关系模式上的关系的集合，表示为d={r1, r2, \u0026hellip;, rp} ，其中ri对应关系模式Ri上的一个关系\n候选键（Candidate Key） 能唯一标识一个元组的某一属性组，最简单情况下只包含一个属性，最极端情况下所有属性组为候选键，成为全键（All Key）\n主键（Primary Key） 多个候选键中选定一个，其余作为隐含键（Implicit Key）或候补键（Alternate Key）\n主属性（Prime Attribute） Prime attributes are the attributes of the candidate key which defines the uniqueness.\n包含在任何一个候选键中的属性\n完整性约束 对关系的某种约束条件\n实体完整性（Entity Integrity Constraint） 主属性不能取空值\n参照完整性（Referential Integrity Constraint） 对作为外键（Foreign Key）的值的约束\nR的外键为其他基本关系S的主键：\n称R为参照关系（Referencing Relation）\n称S为被参照关系（Referenced Relation）或目标关系 （Target Relation）\n外键为空或等于S中的某个主键值\n用户定义的完整性（User-defined integrity）：保证一个数据的取值合理 关系代数 一种抽象的查询语言 用对关系的运算来表达查询 关系数据操纵语言的一种传统表达方式 表示记号 运算符 集合运算符\n并 差 交 笛卡尔积 关系运算符\n选择（Selection） 又称为限制（Restriction），从行的角度进行\n投影（Projection） 从R中选择出若干属性列组成新的关系,从列的角度进行运算（但可能为了避免重复取消某些元组）\n连接（Join） 从两个关系的笛卡尔积中选取属性间满足一定条件的元组\n等值连接(equijoin):比较关系为等号 自然连接(Natural join) 除法（Division） 同时从行和列的角度进行运算\n扩充的关系运算\n属性重命名 外连接(Outer Join) 对自然连接的扩展。除了满足连接的元组外还包含没有被连接的元组\n左外连接 左外连接的连接结果中包含了关系R （左边关系）中不满足连接条件的元组，在这些元组对 应关系S属性上的值为空值\n右外连接 左外连接的连接结果中包含了关系S（右边关系）中不满足连接条件的元组，在这些元组对 应关系S属性上的值为空值\n全外连接 完全外连接的连接结果中包含了关系R中不满 足连接条件的元组，同时也包含了关系S中不 满足连接条件的元组。即连接结果是左外连接 和右外连接结果的并\n算术比较符\n逻辑运算符\n关系代数表达式 关系代数运算经过有限次复合后形成的式子\n典型关系代数语言 ISBL（Infotmation System Base Language）\n关系演算 元组关系演算 以元组变量作为谓词变元的基本对象\n典型代表：ALPHA\n域关系演算 以域变量作为谓词变元的基本对象\n典型代表：QBE(Query By Example)\nALPHA 检索语句： GET 简单检索 限定检索 排序检索 定额检索 元组变量检索\n元组变量\n含义：表示可以在某一关系范围内变化（也称为范围 变量Range Variable）\n用途：\n简化关系名 操作条件中使用量词时必须用元组变量 定义格式：RANGE 关系名 变量名\n存在量词检索\n全称量词检索 蕴函（Implication）检索 集函数（Aggregation function）或内部函数（Build-in function）\n更新语句：PUT,HOLD,UPDATE,DELETE,DROP 插入：-建立新元组-PUT 修改：-HOLD-修改-UPDATE 宿主语言：开发软件赖以生存的软件环境的程序语言\n删除：-HOLD-DELETE 存储异常 数据冗余 更新异常 插入异常 删除异常 原因：数据库模式没有反映数据间的依赖关系\n规范化 规范化理论用于改造关系模式，通过分解关系模式来消除不合适的数据依赖，以解决存储异常问题。\n函数依赖 定义 设R（U）是一个属性集U上的关系模式，X 和Y是U的子集。若对于R（U）的任意一个 可能的关系r，r中不可能存在两个元组在X 上的属性值相等， 而在Y上的属性值不等 ， 则称 “X函数确定Y” 或 “Y函数依赖于 X”，记作X→Y。\n若X→Y，则X称为这个函数依赖的决定属性集（ Determinant）。\n若X→Y，并且Y→X, 则记为X ←→Y。\n平凡与非平凡函数依赖 在关系模式R（U）中，对于U的子集X和Y ，若X→Y\n且Y不属于X，则称X→Y是非平凡的函数依赖；\n但Y属于X, 则称 X→Y是平凡的函数依赖。\n完全与部分函数依赖 在关系模式R（U）中，如果X→Y，\n并且对于X的任何一个真子集X’，都有X\u0026rsquo;-/\u0026gt;Y, 则称 Y完全函数依赖于X。\n但Y不完全函数依赖于X，则称Y部分函数依赖于X\n传递与直接函数依赖 在关系模式R（U）中，如果X→Y，Y→Z ，\n且Y不属于X，Y-/\u0026gt;X，则称Z传递函数依赖于 X 且X ←→Y，则Z直接依赖于X 键 设K为关系模式R中的属性或属性组合。若K→U（f），则K称为R的一个侯选键 （Candidate Key）。若关系模式R有多个 候选键，则选定其中的一个做为主键（ Primary Key）。\n范式 范式是符合某一种级别要求的关系模式的集合。\n第一范式（NF） 一个关系模式R的所有属性都是不可分的基本数据项\n第一范式实际上是对关系增加了一个约束 ，即关系中元组的每个属性都只能取一个值。第一范式是对关系模式的基本要求， 不满足第一范式的数据库就不是关系数据库。\n第二范式（2NF） 1NF 每一个非主属性都完全函数依赖于R的键 第三范式（3NF） 不存在键X、属性组Y及非主属性Z（Z 不属于Y）, 使得X→Y，Y -/\u0026gt; X，Y→Z成立 ( 关系中每一个非主属性必须只依赖于主键) 在第二范式的基础上，消除非主属性对主键的传递函数依赖可达到3NF。\n若R∈3NF，则R的每一个非主属性既不部 分函数依赖于候选键也不传递函数依赖于 候选键。\nBC范式（BCNF）Boyce Codd Normal Form 通常认为BCNF是修正的第三范式，所以有时也称为扩充的第三范式\n1NF 若X→Y且Y不属于X时X必含有键 （每一个决定属性因素都包含键） 若R∈BCNF，\n– 所有非主属性对每一个键都是完全函数依赖\n– 所有的主属性对每一个不包含它的键，也是完全函数依赖\n– 没有任何属性完全函数依赖于非键的任何一组属性\n多值依赖 ####### 定义\n设R（U）是一个属性集U上的一个关系模式， X、 Y和Z是U的子集，并且Z＝U－X －Y，多值依赖 X→→Y成立当且仅当对R的任一关系r，r在（X，Z）上的每个值对应一组Y的值，这组值仅仅决定于X值而与Z值无关\n若X→→Y，而Z＝φ，则称 X→→Y为平凡的多值依赖，否则称X→→Y为非平凡的多值依赖\n####### 性质\n多值依赖具有对称性，若X→→Y，则X→→Z，其中Z＝U－X - Y 多值依赖具有传递性，若X→→Y，Y→→Z， 则X→→Z –Y 函数依赖是多值依赖的特殊情况，若X→Y，则X→→Y 若X→→Y，X→→Z，则 X→→Y并Z X→→Y交Z X→→Y-Z X→→Y-Z 第四范式（4NF） 1NF 对于R的每个非平凡多值依赖X→→Y（Y不属于X），X都含有候选键 如果R ∈ 4NF\n–不允许有非平凡且非函数依赖的多值依赖\n–允许的非平凡多值依赖是函数依赖\n连接依赖 设R是一个关系模式，R的属性子集为R1， R2，R3，R4，R5，…，当且仅当R的每个合法值都等于R1，R2，R3，R4，R5，…， 的投影连接时，称R满足连接依赖\n第五范式（5NF） R的每一个非平凡连接依赖都被R的候选键所蕴含（4NF中消除非候选键所蕴含的连接依赖)\nArmstrong公理系统 逻辑蕴含 对于满足一组函数依赖 F 的关系模式R ，其任何一个关系r，若函数依赖X→Y都成立, （即r中任意两元组t，s，若t[X]=s[X]，则 t[Y]=s[Y]），则称F逻辑蕴含X →Y\n推理规则 A1. 自反律（Reflexivity）：若Y 属于X 属于 U， 则X →Y为F所蕴含 A2. 增广律（Augmentation）：若X→Y为F所蕴含，且Z属于U，则XZ→YZ为F所蕴含 A3. 传递律（Transitivity）：若X→Y及Y→Z 为F所蕴含，则X→Z为F所蕴含 进而：\n合并规则：由X→Y，X→Z，有X→YZ 伪传递规则：由X→Y，WY→Z，有XW→Z 分解规则：由X→Y及Z属于Y，有X→Z 根据合并规则和分解规则，可得如下引理 X→A1 A2 …Ak成立⇔X→Ai成立（i=1，2， … ，k）\n有效与完备 有效性：指由F出发根据Armstrong公理推导出来的每个函数依赖一定在F所蕴含的函数依赖的全体之中。有效性可由正确性证明\n完备性：F所蕴含的函数依赖的全体中的每一个函数依赖， 必定可以由F根据Armstrong公理导出\n闭包 在R中，X ⊆U，X_F^+= {A | X→A能由F根据 Armstrong公理导出}，称X_F^+为属性集X关于函数依赖集F的闭包。\n计算方法 计算X（i）（i=0，1，…）\n（1）X（0）=X\n（2）X（i+1）=X（i）A 其中A是这样的属性：在F中寻找尚未用过的左边是 X（i）的子集的FD：Y→A，其中Y是X（i）的子集。\n（3）判断X（i+1）=X（i），若是转（4），否则转（2 ）\n（4） 输出X（i），即为X+\n等价与覆盖 等价：F，G为FD集，F+=G+ 覆盖：F，G为FD集，G属于F，称F覆盖G 最小依赖集 满足：\n右部单属性化：F中任一函数依赖X→ A，A必是单属性 不存在多余FD：F中不存在X→ A，使得F与F-｛X→ A｝等价 每个FD左部没有多余属性：F中不存在X→ A，使得X存在真子集Z，使得F与F-｛X→ A｝并｛Z→ A｝（先减后并）等价 模式分解 三种模式分解等价的定义\n分解具有无损连接性（一定能达到4NF） 分解保持函数依赖（一定能达到3NF） 既保持函数依赖又具有无损连接性（一定能达到3NF） 关键字求解 给定关系模式S和FD集F，按F中函数依赖将S中属性分为\nL类：仅在FD左部出现 R类：仅在FD右部出现 LR类：左右均出现 N类：左右均未出现 引理 X为L类或N类属性，则必为主属性（包含在候选键中）\n推论：\nX为S中N类和L类组成的属性集且X+包含S中全部属性，X必为S的唯一候选键 X为S中的R类属性，即X为S的非主属性 设计步骤 需求分析和概念设计独立于任何数据库管理系统\n逻辑设计和物理设计与选用的DBMS密切相关\n需求分析 任务 明确用户的各种需求 确定新系统的功能 充分考虑今后可能的扩充和改变 结构化分析方法（Structured Analysis）\n从最上层的系统组织机构入手，自顶向下、逐层分解分析系统\n系统抽象为\n分解处理功能和数据\n分解处理功能\n将处理功能的具体内容分解为若干子功能\n分解数据\n处理功能逐步分解的同时，逐级分解所用的数据，形成若干层次的数据流图\n表达方法\n处理逻辑：判定表或判定树 数据：数据字典 征取用户认可\n数据字典 ####### 用途\n进行详细的数据收集和数据分析所获得的主要结果\n####### 内容\n数据项\n不可再分的数据单位\n描述=｛数据项名，数据项含义说明，别名，数据类型，长度，取值范围，取值含义，与其他数据项的逻辑关系，数据项之间的联系｝\n数据结构\n反映数据之间的组合关系\n可以由若干个数据项组成，也可以由若干数据结构组成，或若干二者混合\n描述=｛数据结构名，含义说明，组成：｛数据项或数据结构｝｝\n数据流\n数据结构在系统内的传输路径\n描述=｛数据流名，说明，数据流来源，数据流去向，组成：｛数据结构｝，平均流量，高峰期流量｝\n数据存储\n数据结构停留或保存的地方，数据流的来源和去向之一\n描述=｛数据存储名，说明，编号，输入的数据流，输出的数据流，组成：｛数据结构｝，数据量，存取频度，存取方式｝\n处理过程\n一般用判定表或判定树来描述\n描述=｛处理过程名，说明，输入：｛数据流｝，输出：｛数据流｝，处理：｛简要说明｝｝\n数据字典是关于数据库中数据的描述，是元数据，而不是数据本身。\n数据字典在需求分析阶段建立，在数据库设计过程中不断修改、充实、完善。\n概念设计 将需求分析得到的用户需求抽象为信息结构即概念模型的过程就是概念结构设计\n工具 E-R模型\n设计方法 自顶向下：首先定义全局概念结构的框架，然后逐步细化 自底向上：首先定义各局部应用的概念结构，然后将其集成起来 逐步扩张：首先核心概念结构，然后向外扩充 混合策略：自顶向下设计全局概念框架，自底向上设计各局部概念结构 常用策略 自顶向下进行需求分析\n自底向上设计概念结构\n抽象数据并设计局部视图 集成局部视图，得到全局概念结构 数据抽象 分类（Classification）：\n定义某一类概念作为现实世界中一组对象的类型。\n抽象了对象值与型之间的“is member of”的语义\n聚集（Aggregation）：\n定义某一类型的组成成分\n抽象了对象内部类型和成分之间“is part of”的语义\n概括（Generalization）：\n定义类型只见那的一种子集联系\n抽象了类型之间的“is subset of”的语义\n局部视图设计 在多层的数据流图中选择一个适当层次的数据流图，作为设计分E-R图的出发点\n通常以中层数据流图作为设计分E-R图的依据\n####### 准则\n属性不能再具有需要描述的性质，即属性必须是不可分的数据项 属性不能与其他实体具有联系，联系只发生在实体之间 集成 ####### 消除冲突\n属性冲突 属性域冲突 属性值的类型 取值范围 取值集合不同 属性取值单位冲突 命名冲突 同名异义：不同意义的对象在不同的局部应用中具有相同的名字 异名同义：同一意义的对象在不同的局部应用中具有不同名字 结构冲突 同一对象在不同应用中具有不同的抽象。 同一实体在不同分E-R图中所包含的属性个数 和属性排列次序不完全相同。 实体之间的联系在不同局部视图中呈现不同的 类型。 ####### 消除冗余\n冗余的数据是指可由基本数据导出的数据\n冗余的联系是指可由其他联系导出的联系\n冗余数据和冗余联系容易破坏数据库的完整性，给数据库维护增加困难\n方法：\n求 F L的最小覆盖 GL ，差集为D = F L-G L。逐一 考察 D中的函数依赖，确定是否是冗余的联系， 若是，就把它去掉\n冗余的联系一定在D中，而 D中的联系不一定是冗余的 当实体之间存在多种联系时要将实体之间的联系在形式上加以区分 逻辑设计 把概念结构设计阶段设计好的基本E-R图转换 为与选用DBMS产品所支持的数据模型相符合 的逻辑结构\n步骤 将概念结构转化为一般的关系、网状、层次模型 将转换来的关系、网状、层次模型向特定DBMS支持下的数据模型转换 对数据模型进行优化 设计用户子模式 ER图转关系模型 将实体、实体的属性和实体之间的联系转换为关系模式\n一个1:1联系可以转换为一个独立的关系模式，也可以与任意一端对应的关系模式合并。 一个1:n联系可以转换为一个独立的关系模式，也可以与n端对应的关系模式合并。 一个m:n联系转换为一个关系模式。 三个或三个以上实体间的一个多元联系转 换为一个关系模式。 具有相同键的关系模式可合并 优化 确定数据依赖 消除冗余联系 确定所属范式 确定合并或分解 ####### 水平分解\n把（基本）关系的元组分为若干子集合，定义每个子集合为一个子关系，以提高系统的效率\n满足“80/20原则”的应用 并发事务经常存取不相交的数据 ####### 垂直分解\n把关系模式R的属性分解为若干子集合，形成若干子关系模式\n设计用户子模式 使用更符合用户习惯的别名。 针对不同级别的用户定义不同的外模式，以满足系统对安全性的要求。 简化用户对系统的使用。 物理设计 数据库在物理设备上的存储结构与存取方法称为数据库的物理结构，依赖于选定的数据库管理系统\n为一个给定的逻辑数据模型选取一个最适合应用环境的物理结构的过程，就是数据库的物理设计\n对物理结构进行评价，评价的重点是时间和空间效率\n内容 为关系模式选择存取方法（建立存取路径） 索引方法：B+树索引方法（最普遍） 局促方法（Cluster） HASH方法 设计关系、索引等数据库文件的物理存储结构 数据库实施 运用DBMS提供的数据库语言及宿主语言，根据逻辑设计和物理设计的结果\n数据库运行和维护 简介 SQL语言的前身是SQUARE（Specifying Queries As Relational Expression）语 言，是1974年由Boyce和Chamberlin提出 的，并在IBM公司的关系数据库系统 System R上实现，后改名为SEQUEL（ Structured English QUEry Language） 语言，SEUQUEL简称SQL\nSQL语言集\n数据定义语言DDL 数据操纵语言DML 数据控制语言DCL 的功能于一体\n数据查询：SELECT\n数据定义：CREATE、DROP、ALTER\n数据操纵：INSERT、UPDATE、DELETE\n数据控制：GRANT、REVOKE\n系统结构 SQL语言支持数据库的三级模式结构，在SQL语言中，关系模式称为基本表，外模式称为视图，内模式称为索引\n基本表\n本身独立存在的表，SQL中一个关系对应一个基本表，基本表对应存储文件，一个表可以携带若干索引\n视图\n从一个或多个基本表导出的“虚表”，可在视图上再定义视图，只存放定义而不存放数据\n数据定义 模式 实际上定义了一个命名空间，在空间中可定义该模式包含的数据库对象如基本表、视图、索引等。\n创建模式\nCREATE SCHEMA \u0026lt;schemaName\u0026gt; AUTHORIZATION \u0026lt;userName\u0026gt; [ \u0026lt;tableDefinition\u0026gt;|\u0026lt;viewDefinition\u0026gt;|\u0026lt;authorizationDefiinition\u0026gt; ] CREATE SCHEMA \u0026#34;S-T\u0026#34; AUTHORIZATION SHERRY; 删除模式\nDROP SCHEMA \u0026lt;name\u0026gt; \u0026lt;CASCADE|RESTRICT\u0026gt; CASCADE（级联）\n删除模式同时把该模式中所有的数据库对象全部删除\nRESTRICT（限制）\n如该模式中定义了下属的数据库对象（表、视图等），则拒绝该删除语句执行，仅当该模式中没有任何下属对象时才能执行\n基本表 （模式） 每一个基本表都属于某一个模式,一个模式包含多个基本表\n创建表\nCREATE TABLE \u0026lt;name\u0026gt; ( \u0026lt;colName\u0026gt; \u0026lt;dataType\u0026gt; [\u0026lt;constraint\u0026gt;] [,\u0026lt;colName\u0026gt; \u0026lt;dataType\u0026gt; [\u0026lt;constraint\u0026gt;]] ); CREATE TABLE Student (Sno CHAR(5)PRIMARY KEY, /* 列级完整性约束条件*/ Sname CHAR(20) UNIQUE, /* Sname取唯一值*/ Ssex CHAR(2), Sage SMALLINT, Sdept CHAR(20)); CREATE TABLE Course (Cno CHAR(4)PRIMARY KEY, Cname CHAR(40), Cpno CHAR(4), Ccredit SMALLINT, FOREIGN KEY (Cpno)REFERENCES Course(Cno)); 删除表\nDROP TABLE \u0026lt;name\u0026gt;［ RESTRICT | CASCADE］； 修改表定义\nALTER TABLE \u0026lt;name\u0026gt; [ADD \u0026lt;colname\u0026gt; \u0026lt;datatype\u0026gt;[\u0026lt;constriant\u0026gt;]] [DROP \u0026lt;colname\u0026gt; [CASCADE|RESTRICT]] [ALTER\u0026lt;colname\u0026gt; \u0026lt;datatype\u0026gt;] ALTER TABLE Student ADD S_entrance DATE； ALTER TABLE Student ADD S_entrance DATE； ALTER TABLE Course ADD UNIQUE（Cname）； 视图 （外模式） 定义视图（外模式）\n创建视图\n删除视图\n间接修改视图定义：删除+创建\n创建索引\nCREATE [UNIQUE] [CLUSTER] INDEX ON (\n)\n删除索引\n间接修改索引定义：删除+创建\n索引（内模式） 索引是关系数据库的内部实现技术，属于内模式范畴\n目的：加快查询速度\n创建：DBA或表的属主可以建立索引，DBMS一般会自动建立PRIMARY KEY和UNIQUE列上的索引\n使用：DBMS自动维护索引，自动选择是否使用、使用哪个索引\n方法：RDBMS中索引一般采用B+树、HASH索引\nB+树索引具有动态平衡的优点 HASH索引具有查找速度快的特点 类别：唯一索引、非唯一索引、聚簇索引\n​\t在最经常查询的列上建立聚簇索引以提高查询效率\n​\t一个基本表上最多只能建立一个聚簇索引\n​\t经常更新的列不宜建立聚簇索引\n创建索引\nCREATE [UNIQUE][CLUSTER] INDEX \u0026lt;indexName\u0026gt; ON \u0026lt;tableName\u0026gt; (\u0026lt;colName\u0026gt;[\u0026lt;order\u0026gt;][,\u0026lt;colName\u0026gt;[order]]...); CREATE UNIQUE INDEX Stusno ON Student（Sno）； CREATE UNIQUE INDEX Coucno ON Course（Cno）； CREATE UNIQUE INDEX SCno ON SC（Sno ASC，Cno DESC）； UNIQUE : 相当于对列添加了一个UNIQUE约束 CLUSTER：建立聚簇索引后，基表中数据也需要按指定的 聚簇属性值的升序或降序存放。也即聚簇索引 的索引项顺序与表中记录的物理顺序一致 ORDER：ASC,DESC 删除索引\nDROP INDEX \u0026lt;indexName\u0026gt;; 数据操纵 数据查询 SELECT [ALL|DISTINCT][,]\nFROM [,]\n[WHERE ]\n[GROUP BY [HAVING ]]\n[ORDER BY [ASC|DESC]];\n单表查询 SELECT [ALL|DISTINCT] \u0026lt;colExpression\u0026gt;[，\u0026lt;colExpression\u0026gt;] … FROM \u0026lt;tableOrViewName\u0026gt;[， \u0026lt;tableOrViewName\u0026gt; ] … [ WHERE \u0026lt;condition\u0026gt; ] [ GROUP BY \u0026lt;列名1\u0026gt; [ HAVING \u0026lt;condtion\u0026gt; ] ] [ ORDER BY \u0026lt;列名2\u0026gt; [ ASC|DESC ] ]； 查询指定列\nSELECT Sno,Sname FROM Student; 查询全部列\nSELECT * FROM Student; 查询经过计算的值\nSELECT Sname,1996-Sage /*假定当年的年 份为1996年*/ FROM Student; SELECT Sname,\u0026#39;Year of Birth:\u0026#39;,1996- Sage,ISLOWER(Sdept) FROM Student; SELECT Sname NAME,\u0026#39;Year of Birth: \u0026#39; BIRTH,1996-Sage BIRTHDAY,LOWER(Sdept) DEPARTMENT FROM Student; 取消取值中的重复行\nDISTINCT短语的作用范围是所有目标列\nSELECT Sno FROM SC; =\tSELECT ALL Sno FROM SC; SELECT DISTINCT Sno FROM SC;#去掉重复行 SELECT DISTINCT Cno,Grade FROM SC; 查询满足条件的元组\n字符匹配：\n[NOT] LIKE ‘\u0026lt;匹配串\u0026gt;’ [ESCAPE ‘ \u0026lt;换码字符\u0026gt;’] SELECT Sname FROM Student WHERE Sdept=\u0026#39;CS\u0026#39;; #between SELECT Sname,Sdept,Sage FROM Student WHERE Sage BETWEEN 20 AND 23; SELECT Sname,Sdept,Sage FROM Student WHERE Sage NOT BETWEEN 20 AND 23; #in SELECT Sname,Ssex FROM Student WHERE Sdept IN (\u0026#39;IS\u0026#39;,\u0026#39;MA\u0026#39;,\u0026#39;CS\u0026#39;); SELECT Sname,Ssex FROM Student WHERE Sdept NOT IN(\u0026#39;IS\u0026#39;,\u0026#39;MA\u0026#39;,\u0026#39;CS\u0026#39;); #like SELECT Sname,Sno,Ssex FROM Student WHERE Sname LIKE \u0026#39;刘%\u0026#39;; SELECT Sname FROM Student WHERE Sname LIKE \u0026#39;欧阳__\u0026#39;; SELECT Sname,Sno FROM Student WHERE Sname LIKE \u0026#39;__阳%\u0026#39;; SELECT Sname,Sno,Ssex FROM Student WHERE Sname NOT LIKE \u0026#39;刘%\u0026#39;; #查询以\u0026#34;DB_\u0026#34;开头，且倒数第3个字符为 i的课程的详细情况。 SELECT * FROM Course WHERE Cname LIKE \u0026#39;DB\\_%i_ _\u0026#39; ESCAPE \u0026#39;\\\\\u0026#39;;(只有一个\\) SELECT Sno,Cno From SC WHERE GRADE IS NULL; SELECT Sno,Cno FROM SC WHERE Grade IS NOT NULL; ORDER BY\n升序：ASC；降序：DESC；缺省值为升序\nASC：排序列为空值的元组最后显示\nDESC：排序列为空值的元组最先显示\nSELECT Sno,Grade FROM SC WHERE Cno=\u0026#39;3\u0026#39; ORDER BY Grade DESC; SELECT * FROM Student ORDER BY Sdept,Sage DESC; 聚集函数\n计数\nCOUNT（ *）\nCOUNT（[DISTINCT|ALL] \u0026lt;列名\u0026gt;）\n计算总和\nSUM（[DISTINCT|ALL] \u0026lt;列名\u0026gt;）\n计算平均值\nAVG（[DISTINCT|ALL] \u0026lt;列名\u0026gt;）\n最大最小值\nMAX（[DISTINCT|ALL] \u0026lt;列名\u0026gt;）\nMIN（[DISTINCT|ALL] \u0026lt;列名\u0026gt;）\nSELECT COUNT(*) FROM Student; SELECT AVG(Grade) FROM SC WHERE Cno=\u0026#39;1\u0026#39;; Group By\n细化聚集函数的作用对象，未对查询结果分组，聚集函数将作用于整个查询结果，对查询结果分组后，聚集函数将分别作用于每个组\n使用GROUP BY子句后，SELECT子句的列名列表中只能出现分组属性和聚集函数\n使用HAVING短语筛选最终输出结果，只有满足HAVING短语指定条件的组才输出\nWHERE子句作用于基表或视图，从中选择满足条件的元组 HAVING短语作用于组，从中选择满足条件的组 SELECT Sno FROM SC GROUP BY Sno HAVING COUNT(*) \u0026gt;3; SELECT Sno,COUNT(*) FROM SC WHERE Grade\u0026gt;=90 GROUP BY Sno HAVING COUNT(*)\u0026gt;=3; 连接查询 同时涉及多个表的查询\n[\u0026lt;表名1\u0026gt;.]\u0026lt;列名1\u0026gt; \u0026lt;比较运算符\u0026gt; [\u0026lt;表名2\u0026gt;.]\u0026lt;列名2\u0026gt; [\u0026lt;表名1\u0026gt;.]\u0026lt;列名1\u0026gt; BETWEEN [\u0026lt;表名2\u0026gt;.]\u0026lt;列名2\u0026gt; AND [\u0026lt;表名2\u0026gt;.]\u0026lt;列名3\u0026gt; 连接条件中的各连接字段类型必须是可比的，但名字不必是相同的\n方法：\n嵌套循环法 (NESTED-LOOP)\n首先在表1中找到第一个元组，然后从头开始扫描表2，逐一查找满足连接条件的元组，找到后就将表1中的第一个元组与该元组拼接起来，形成结果表中一个元组\n表2全部查找完后，再找表1中第二个元组，然后再从头开始扫描表2，逐一查找满足连接条件的元组，找到后就将表1中的第二个元组与该元组拼接起来，形成结果表中一个元组，\n重复上述操作，直到表1中的全部元组都处理完毕\n排序合并法 (SORT-MERGE)\n常用于=连接\n对表1的第一个元组，从头开始扫描表2，顺序查找满足连接条件的元组，找到后就将表1中的第一个元组与该元组拼接起来，形成结果表中一个元组。当遇到表2中第一条大于表1连接字段值的元组时，对表2的查询不再继续\n找到表1的第二条元组，然后从刚才的中断点处继续顺序扫描表2，查找满足连接条件的元组，找到后就将表1中的第二个元组与该元组拼接起来，形成结果表中一个元组。直接遇到表2中大于表1连接字段值的元组时，对表2的查询不再继续。\n重复上述操作，直到表1或表2中的全部元组都处理完毕为止\n索引连接（INDEX-JOIN）\n对表2按连接字段建立索引。 对表1中的每个元组，依次根据其连接字段值查询表2的索引，从中找到满足条件的元组，找到后就将表1中的第一个元组与该元组拼接起来，形成结果表中一个元组。\n类型：\n等值连接\nSELECT Student.*,SC.* FROM Student,SC WHERE Student.Sno = SC.Sno; 自然连接\nSELECT Student.Sno,Sname,Ssex,Sage,Sdept,Cno,Grade FROM Student,SC WHERE Student.Sno = SC.Sno; 自身连接\nSELECT FIRST.Cno,SECOND.Cpno FROM Course FIRST,Course SECOND WHERE FIRST.Cpno = SECOND.Cno; 嵌套查询 一个SELECT-FROM-WHERE语句称为一个查询块 。\n将一个查询块嵌套在另一个查询块的WHERE子句或HAVING短语的条件中的查询称为嵌套查询\n子查询的限制:不能使用ORDER BY子句\n不相关子查询\n子查询的查询条件不依赖于父查询\n由里向外逐层处理。即每个子查询在上一级查询处理之前求解，子查询的结果用于建立其父查询的查找条件\n相关子查询\n首先取外层查询中表的第一个元组，根据它与内层查询相关的属性值处理内层查询，若 WHERE子句返回值为真，则取此元组放入结 果表。\n然后再取外层表的下一个元组。\n重复这一过程，直至外层表全部检查完为止。\nSELECT Sno,Sname,Sdept FROM Student WHERE Sdept IN (SELECT Sdept FROM Student WHERE Sname=\u0026#39;刘晨\u0026#39;); SELECT Sno,Cno FROM SC x WHERE Grade \u0026gt;=(SELECT AVG(Grade) FROM SC y WHERE y.Sno=x.Sno); 带有EXISTS谓词的子查询不返回任何数据 ，只产生逻辑真值“true”或逻辑假值 “false”。\n– 若内层查询结果非空，则外层的WHERE子句返回真值\n– 若内层查询结果为空，则外层的WHERE子句返回假值\n由EXISTS引出的子查询，其目标列表达式 通常都用* ，因为带EXISTS的子查询只返回真值或假值，给出列名无实际意义\nSQL语言中没有全称量词，可以把带有全称量词的谓词转换为等价的带有存在量词的谓词\nSQL语言中没有蕴涵（Implication）逻辑运算，可以利用谓词演算对逻辑蕴涵谓词进行等价转换\nSELECT Sname FROM Student WHERE EXISTS(SELECT * FROM SC /*相关子查询*/ WHERE Sno=Student.Sno AND Cno=\u0026#39;1\u0026#39;); SELECT Sno,Sname,Sdept FROM Student S1 WHERE EXISTS (SELECT * FROM Student S2 WHERE S2.Sdept = S1.Sdept AND S2.Sname =\u0026#39;刘晨\u0026#39;); SELECT Sname FROM Student WHERE NOT EXISTS (SELECT * FROM Course WHERE NOT EXISTS (SELECT * FROM SC WHERE Sno= Student.Sno AND Cno= Course.Cno));## 查询选修了全部课程的学生姓名 SELECT DISTINCT Sno FROM SC SCX WHERE NOT EXISTS (SELECT * FROM SC SCY WHERE SCY.Sno =\u0026#39;95002\u0026#39; AND NOT EXISTS (SELECT * FROM SC SCZ WHERE SCZ.Sno= SCX.Sno AND SCZ.Cno= SCY.Cno));## 查询至少选修了学生95002选修的全部课程的学生号码 集合查询 \u0026lt;queryBlock\u0026gt; UNION|INTERSECT|EXCEPT \u0026lt;queryBlock\u0026gt; 并\nSELECT * FROM Student WHERE Sdept=\u0026#39;CS\u0026#39; UNION SELECT * FROM Student WHERE Sage\u0026lt;=19; 交\nSELECT * FROM Student WHERE Sdept=\u0026#39;CS\u0026#39; INTERSECT SELECT * FROM Student WHERE Sage\u0026lt;=19; 差\nSELECT * FROM Student WHERE Sdept=\u0026#39;CS\u0026#39; EXCEPT SELECT * FROM Student WHERE Sage\u0026lt;=19; 数据更新 插入\nINSERT INTO \u0026lt;tableName\u0026gt; [（\u0026lt;col1\u0026gt;[，\u0026lt;col2 \u0026gt;，…]）] VALUES （\u0026lt;value1\u0026gt; [，\u0026lt;value2\u0026gt;] … ）; INSERT INTO Student VALUES (\u0026#39;95020\u0026#39;,\u0026#39;陈冬\u0026#39;,\u0026#39;男\u0026#39;,18,\u0026#39;IS\u0026#39;); INSERT INTO Deptage(Sdept,Avgage) SELECT Sdept,AVG(Sage) FROM Student GROUP BY Sdept; 修改\nUPDATE \u0026lt;tableName\u0026gt; SET \u0026lt;col\u0026gt;=\u0026lt;Expression\u0026gt;[，\u0026lt;col\u0026gt;=\u0026lt;Expression\u0026gt;]… [WHERE \u0026lt;condition\u0026gt;]； UPDATE Student SET Sage=22 WHERE Sno=\u0026#39;95001\u0026#39;; 删除\nDELETE FROM \u0026lt;tableName\u0026gt; [WHERE \u0026lt;condition\u0026gt;]； DELETE FROM Student WHERE Sno=\u0026#39;95019\u0026#39;; DELETE FROM SC; DELETE FROM SC WHERE \u0026#39;CS\u0026#39;= (SELECT Sdept FROM Student WHERE Student.Sno=SC.Sno); 视图 虚表，从一个或几个基本表（或视图）导出的表\n只存放视图的定义，不会出现数据冗余\n基本表中数据发生改变时，从视图中查询出的数据也随之改变\n创建视图 CREATE VIEW \u0026lt;viewName\u0026gt;[(\u0026lt;colName\u0026gt;[,\u0026lt;colName\u0026gt;]...)] AS \u0026lt;subQuery\u0026gt; [WITH CHECK OPTION]; 形式 行列子集视图\nCREATE VIEW IS_Student AS SELECT Sno,Sname,Sage FROM Student WHERE Sdept=\u0026#39;IS\u0026#39;; WITH CHECK OPTION的视图\n要求修改和插入时仍满足只有IS系学生\nCREATE VIEW IS_Student AS SELECT Sno,Sname,Sage FROM Student WHERE Sdept=\u0026#39;IS\u0026#39; WITH CHECK OPTION; 修改、删除时\nDBMS自动加上条件 Sdept=\u0026lsquo;IS‘\n插入时\nDBMS自动检查Sdept是否为\u0026rsquo;IS\u0026rsquo;，不是则拒绝插入，未提供则自动补充为\u0026rsquo;IS\u0026rsquo;\n基于多个基表的视图\nCREATE VIEW IS_S1(Sno,Sname,Grade) AS SELECT student.Sno,Sname,Grade FROM Student,SC WHERE Sdept=\u0026#39;IS\u0026#39; AND sTUDENT.Sno=SC.Sno AND SC.Cno=\u0026#39;1\u0026#39;; 基于视图的视图\nCREATE VIEW IS_S2 AS SELECT Sno,Sname,Grade FROM IS_S1 WHERE Grade\u0026gt;=90; 带表达式的视图\nCREATE VIEW BT_S(Sno,Sname,Sbirth) AS SELECT Sno,Sname,1996-Sage /*假定当年的年份为 1996年*/ FROM Student 分组视图\nCREATE VIEW S_G(Sno,Gavg) AS SELECT Sno,AVG(Grade) FROM Sno,AVG(Grade) FROM SC GROUP BY Sno; 删除视图 DROP VIEW \u0026lt;viewName\u0026gt;; 由该视图导出的视图定义仍在数据字典中，但已不能使用，必须显式删除\n删除基表时，由该基表导出的所有视图定义都必须显式删除\n查询视图 从用户角度，查询视图与查询基本表相同\n视图实体化法(View Materialization) 有效性检查：检查所查询视图是否存在 执行试图定义，将视图临时实体化，生成临时表 查询临时表 查询完毕删除被实体化的视图 视图消解法（View Resolution） 有效性检查：检查所查询视图是否存在 将视图定义中的子查询与用户查询结合，转换成等价的对基本表的查询 执行修正后的查询 更新视图 同样有实体化法和消解法\n有些视图不可更新\n数据控制 亦称为数据保护，包括数据的\n安全性控制\n保护数据库，防止不合法的使用所造成的数据泄露和破坏\nGRANT、REVOKE\n完整性控制\n数据库的完整性是指数据库中数据的正确性与相容性\n键 取值唯一的列 参照完整性 其他约束条件 并发控制\n当多个用户并发地对数据库进行操作时，对他们加以控制、协调，以保证并发操作正确执行，保持数据库的一致性。\n恢复\n当发生各种类型的故障导致数据库处于不一致状态时，将数据库恢复到一致状态的功能\n回滚、重做（UNDO、REDO）\n安全性 授权:DBA与表的属主可以建立\nGRANT \u0026lt;authority1\u0026gt;[,\u0026lt;authority2\u0026gt;]... [ON \u0026lt;targetType\u0026gt; \u0026lt;targetName\u0026gt;] TO \u0026lt;userName\u0026gt;[,\u0026lt;userName\u0026gt;]... [WITH GRANT OPTION]; #case GRANT SELECT ON TABLE Student TO U1; GRANT ALL PRIVILEGES ON TABLE Student,Course TO U2,U3; GRANT SELECT ON TABLE SC TO PUBLIC;(all users) GRANT UPDATE(Sno),SELECT ON TABLE Student TO U4; GRANT CREATETAB ON DATABASE S-T TO U8; WITH GRANT OPTION：被授权对象可以传播权限 收回\nREVOKE \u0026lt;authority1\u0026gt;[,\u0026lt;authority2\u0026gt;]... [ON \u0026lt;targetType\u0026gt; \u0026lt;targetName\u0026gt;] FROM \u0026lt;userName\u0026gt;[,\u0026lt;userName\u0026gt;]...; REVOKE UPDATE(Sno) ON TABLE Student FROM U4; REVOKE SELECT ON TABLE SC FROM PUBLIC; 嵌入式SQL 嵌入式SQL是将SQL语句嵌入程序设计语言中，被嵌入的程序设计语言，如C、C++、Java，称为宿主语言，简称主语言\nEXEC SQL \u0026lt;SQLStatement\u0026gt;; SQL通信区\nSQLCA: SQL Communication Area\n主变量\nHost Variable\n游标\n游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果\nEXEC SQL SELECT [ALL|DISTINCT] \u0026lt;expr\u0026gt;[,\u0026lt;expr\u0026gt;]... INTO \u0026lt;host_var\u0026gt;[\u0026lt;indicator\u0026gt;] [,\u0026lt;host_var\u0026gt;[\u0026lt;indicator\u0026gt;]]... FROM \u0026lt;tableorview\u0026gt;[,\u0026lt;tableorview\u0026gt;] ... [WHERE \u0026lt;condition\u0026gt;] [GROUP BY \u0026lt;colname1\u0026gt; [HAVING \u0026lt;condition\u0026gt;]] [ORDER BY \u0026lt;colname2\u0026gt; [ASC|DESC]]； EXEC SQL SELECT Sno, Sname, Ssex, Sage, Sdept INTO :Hsno, :Hname, :Hsex, :Hage, :Hdept FROM Student WHERE Sno=:givensno； EXEC SQL SELECT Sno, Sname, Ssex, Sage, Sdept INTO :Hsno, :Hname, :Hsex, :Hage, :Hdept FROM Student WHERE Sno=:givensno； EXEC SQL DELETE FROM SC WHERE Sno= （SELECT Sno FROM Student WHERE Sname=:stdname）; 指示变量只能用于INTO子句中 如果INTO子句中主变量后面跟有指示变量，则当查询得出的某个数据项为空值时，系统会自动将相应主变量后面的指示变量置为负值，但不向该主变量执行赋值操作，即主变量值仍保持执行SQL语句之前的值 当发现指示变量值为负值时，不管主变量为何值，均应认为主变量值为NULL 游标 定义\nEXEC SQL DECLARE \u0026lt;cursorName\u0026gt; CURSOR FOR \u0026lt;SELECTStatement\u0026gt;； 说明性语句，并不执行查询\n打开\nEXEC SQL OPEN \u0026lt;cursorName\u0026gt;； 打开游标实际上是执行相应的SELECT语句，把所有满足查询条件的记录从指定表取到缓冲区中 –这时游标处于活动状态，指针指向查询结果集中第一条记录之前\n推动\nEXEC SQL FETCH [[NEXT（default）|PRIOR|FIRST|LAST] FROM] \u0026lt;cursorName\u0026gt; INTO \u0026lt;host_var\u0026gt;[\u0026lt;indicator\u0026gt;][,\u0026lt;host_var\u0026gt;[\u0026lt;indicator\u0026gt;]]...; 指定方向推动游标指针，然后将缓冲区中的当 前记录取出来送至主变量供主语言进一步处理\n关闭\nEXEC SQL CLOSE \u0026lt;cursorName\u0026gt;； 关闭游标，释放结果集占用的缓冲区及其他资源\n游标被关闭后，就不再和原来的查询结果集相联系，被关闭的游标可以再次被打开，与新的查询结果相联系\n查询处理 查询处理是指从数据库中提取数据所涉及的处理过程，包括将用户提交的查询语句转变为数据库的查询计划，并且执行这个查询计划获得查询结果\n连接操作实现\n嵌套循环法（Nested Loop） 索引连接法（Index Join） 排序合并法（Sort-merge Join Merge Join） 散列合并法（Hash Join） 查询优化 代数优化 通过对关系代数表达式的等价变换来提高查询效率\n启发式规则：\n选择运算应尽可能先做。 把投影运算和选择运算同时进行 把投影同其前或其后的双目运算结合起来 把某些选择同在它前面要执行的笛卡尔积结合起来成为一个连接运算 找出公共子表达式 物理优化 代数优化改变查询语句中操作的次序和组合，不涉及底层的存取路径。 对于一个查询语句有许多存取方案，它们的执行效率不同， 仅仅进行代数优化是不够的。 物理优化就是要选择高效合理的操作算法或存取路径，求得优化的查询计划\n基于存取路径 选择操作 连接操作 基于代数估算 两者结合 安全性 数据库的安全性是指保护数据库，防止因用户非法使用数据库造成数据泄露、更改或破坏\n安全技术 访问控制技术\n存取控制技术\n自主存取控制（Discretionary Access Control ，简称DAC） C2级、灵活 同一用户对于不同的数据对象有不同的存取权限 不同的用户对同一对象也有不同的权限 用户还可将其拥有的存取权限转授给其他用户 强制存取控制（Mandatory Access Control，简称 MAC） B1级、严格 每一个数据对象被标以一定的密级 每一个用户也被授予某一个级别的许可证 对于任意一个对象，只有具有合法许可证的用户才 可以存取 数据加密技术\n数据库审计\n在数据库系统运行期间，记录数据库的访问情况，以利用审计数据分析数据库是否受到非法存取\n在用户数量比较大的情况下，为了便于权限管理，需要引入角色的概念。\n数据库角色：被命名的一组与数据库操作相关的权限。 – 角色是权限的集合 – 可以为一组具有相同权限的用户创建一个角色 – 简化授权的过程\nCREATE ROLE \u0026lt;roleName\u0026gt;; GRANT \u0026lt;authority\u0026gt; [,\u0026lt;authority\u0026gt;]... ON \u0026lt;targetType\u0026gt; TargetName TO \u0026lt;roleName\u0026gt;[,\u0026lt;roleName\u0026gt;]... GRANT \u0026lt;role1\u0026gt;[，\u0026lt;role2\u0026gt;]… TO \u0026lt;role3\u0026gt;[，\u0026lt;user1\u0026gt;]… [WITH ADMIN OPTION] REVOKE \u0026lt;authority\u0026gt; [,\u0026lt;authority\u0026gt;]... ON \u0026lt;targetType\u0026gt; TargetName FROM \u0026lt;roleName\u0026gt;[,\u0026lt;roleName\u0026gt;]... 强制存取 强制存取控制（MAC）是指系统为保证更高程度的安全性，按照TDI/TCSEC标准中安全策略的要求，所采取的强制存取检查手段\n在MAC中，DBMS所管理的全部实体被分为主体和客体两大类\n主体是系统中的活动实体 – DBMS所管理的实际用户 – 代表用户的各进程 客体是系统中的被动实体，是受主体操纵的 – 文件 – 基表 – 索引 – 视图 对于主体和客体，DBMS为它们每个实例（值）指派一个敏感度标记（Label）。 敏感度标记分成若干级\n绝密（Top Secret） 机密（Secret） 可信（Confidential） 公开（Public） – 主体的敏感度标记称为许可证级别（Clearance Level）。 – 客体的敏感度标记称为密级（Classification Level）。 – MAC机制就是通过对比主体的Label和客体的Label，最终确定主体是否能够存取客\n仅当主体的许可证级别大于或等于客体的密级时，该主体才能读取相应的客体。 仅当主体的许可证级别等于客体的密级时，该主体才能写相应的客体 修正规则 – 主体的许可证级别 ≤客体的密级 主体能写客体 – 用户可为写入的数据对象赋予高于自己的许可证级别的密级。 – 一旦数据被写入，该用户自己也不能再读该数据对象。\n审计 审计日志（Audit Log）将用户对数据库的所有操作记录在上面\n用户级审计 针对自己创建的数据库表或视图进行审计记录所有用户对这些表或视图的一切成功和（或）不成功的访问要求以及各种类型的SQL操作 系统级审计 DBA设置监测成功或失败的登录要求监测GRANT和REVOKE操作以及其他数据库级权限下的操作 AUDIT(设置审计功能) AUDIT ALTER，UPDATE ON SC； NOAUDIT(取消审计功能) NOAUDIT ALTER，UPDATE ON SC； 完整性 数据库的完整性是指数据的正确性、有效性和相容性，其主要目的是防止错误的数据进入数据库。\n所谓正确性是指数据的合法性。 所谓有效性是指数据是否属于所定义的有效范围。 相容性是指表示同一事实的两个数据应相同，不一致就是不相容 实现方法 约束 默认值 规则 触发器 列级约束 主要是对属性的数据类型、数据格式和取值范围、精度等的约束。具体包括\n对数据类型的约束，包括数据类型、长度、精度等的约束。例如学生姓名的数据类型是字符型，长度是20。 对数据格式的约束，例如规定日期的格式为YYYY/MM/DD。 对取值域的约束，例如学生成绩的取值范围必须是0～100。 对空值的约束 。 元组约束 一个元组是由若干个属性组成的，元组级约束就是元组中各个属性之间的约束关系。例如订货关系中发货日期不能小于订货日期，发货量不得超过订货量等\n关系约束 关系约束是指一个关系的各个元组之间、或者多个关系之间存在的各种联系或约束。常见的关系约束有\n实体完整性约束\n主键的值不能取空值 主键的值唯一 关系模型的实体完整性用PRIMARY KEY定义 单属性构成的主键有两种说明方法：定义为列级约束条件、定义为表级约束条件 对多个属性构成的主键只有一种说明方法：定义为表级约束条件 PRIMARY KEY约束可以作为表定义的一部分在定义表时定义，也可以在表创建之后再添加\n参照完整性约束\n参照完整性给出了数据表之间互相关联的基本要求，其核心是不允许引用不存在的元组，也就是说，外键字段的取值要么为空值，要么就是被引用关系中元组的对应值。\n参照完整性在CREATE TABLE中用FOREIGN KEY和REFERENCES定义\n– FOREIGN KEY短语指明表中的哪些字段是外键，REFERENCES短语指明该表中的外键所参照的数据表名和字段名。 – FOREIGN KEY约束可以作为表定义的一部分在创建表时创建，也可以在已有表中创建FOREIGN KEY约束\n用户自定义完整性约束\n用户定义的完整性就是针对某一具体应用的数据必须满足的语义要求\n列值非空（NOT NULL） 列值唯一（UNIQUE） 检查列值是否满足一个布尔表达式（CHECK） CONSTRAINT \u0026lt;constraintName\u0026gt; [PRIMARY KEY Statement |FOREIGN KEY Statement |CHECK Statement] CREATE TABLE Student （Sno NUMERIC（6） CONSTRAINT C1 CHECK （Sno BETWEEN 90000 AND 99999）, Sname CHAR（20） CONSTRAINT C2 NOT NULL， Sage NUMERIC（3） CONSTRAINT C3 CHECK （Sage \u0026lt; 30）， Ssex CHAR（2） CONSTRAINT C4 CHECK （Ssex IN （‘男’，‘女’））， CONSTRAINT StudentKey PRIMARY KEY（Sno） ）； ALTER TABLE Student DROP CONSTRAINT C1； ALTER TABLE Student ADD CONSTRAINT C1 CHECK （Sno BETWEEN 900000 AND 999999）； ALTER TABLE Student DROP CONSTRAINT C3； ALTER TABLE Student ADD CONSTRAINT C3 CHECK （Sage \u0026lt; 40）; 函数依赖约束\n统计约束\n触发器 触发器是一种特殊类型的存储过程，在对表或视图发出 UPDATE、INSERT 或DELETE 语句时自动执行\nCREATE TRIGGER \u0026lt;triggerName\u0026gt; { BEFORE | AFTER} \u0026lt;triggerEvent\u0026gt; ON \u0026lt;tableName\u0026gt; FOR EACH {ROW | STATEMENT} [WHEN \u0026lt;triggerCondition\u0026gt;] \u0026lt;triggerBody\u0026gt; CREATE TRIGGER Insert_Or_Update_Sal BEFORE INSERT OR UPDATE ON Teacher /*触发事件是插入或更新操作*/ FOR EACH ROW /*行级触发器*/ AS BEGIN /*定义触发动作体，是PL/SQL过程块*/ IF （new.Job=‘教授’）AND （new.Sal \u0026lt; 4000） THEN new.Sal :=4000; END IF； END； CREATE TRIGGER Insert_Sal AFTER INSERT ON Teacher /*触发事件是INSERT*/ FOR EACH ROW AS BEGIN INSERT INTO Sal_log VALUES（ new.Eno，new.Sal，CURRENT_USER， CURRENT_TIMESTAMP）； END； CREATE TRIGGER Update_Sal AFTER UPDATE ON Teacher /*触发事件是UPDATE */ FOR EACH ROW AS BEGIN IF （new.Sal \u0026lt;\u0026gt; old.Sal） THEN INSERT INTO Sal_log VALUES（ new.Eno，new.Sal，CURRENT_USER， CURRENT_TIMESTAMP）； END IF； END； DROP TRIGGER \u0026lt;triggerName\u0026gt; ON \u0026lt;t\u0026gt;； BEFORE 表示在触发事件进行以前，判断触发条件是否满足若满足条件则先执行触发动作部分的操作然后再执行触发事件的操作 AFTER 表示在触发事件完成之后，判断触发条件是否满足若满足条件则执行触发动作部分的操作如果触发事件因错误（如违反约束或语法错误）而失败，触发器将不会执行 行级触发器（FOR EACH ROW） 对每一个修改的元组都会触发触发器的检查和执行 语句级触发器（FOR EACH STATEMENT） 只在SQL语句执行时候进行触发条件的检查和触发器的执行 恢复 事务（Transaction） 用户定义的一个数据库操作序列，是一个不可分割的工作单位\n事务是恢复和并发控制的基本单位\n显式定义\nBEGIN TRANSACTION SQL Statement1 SQL Statement2 … … … … COMMIT|ROLLBACK COMMIT：事务正常结束 隐式定义\n当用户没有显式地定义事务时，DBMS按缺省 规定自动划分事务\n四个特征（ACID特性）\n原子性（Atomicity）\n事务是数据库操作的逻辑工作单位，不可分割\n一致性（Consistency）\n事务执行的结果必须从一个状态转换到另一个状态时保持一致\n隔离性（Isolation）\n一个事务的执行不能影响到另一个事务，即一个事务的内部操作相对于外部事务是隔离的，并发执行事务时事务间不能相互干扰\n事务状态 活动状态（active）\n初始状态，事务执行时处于活动状态\n部分提交状态（partially committed）\n事务最后一条语句执行完毕后进入部分提交状态，此时事务中对数据的操作已经全部完成，但结果数据还驻留在内存中\n失败状态（failed）\n事务不能正常执行，必须回滚\n中止状态（aborted）\n事务回滚并且数据库已经恢复到事务执行前的状态\n提交状态（committed）\n数据库系统将事务中对数据的更改完全写入磁盘时，写入一条事务日志信息，标志着事务成功完成\n故障 某个事务在运行过程中由于种种原因未运行至正常终止点就夭折了\n系统故障\n整个系统的正常运行突然被破坏，所有正在运行的事务都非正常终止，内存中数据库缓冲区的信息全部丢失，外部存储设备上的数据未受影响\n恢复：\n反向UNDO处理Undo队列事务\n正向REDO处理Redo队列事务\n事务内部的故障\n存储设备故障\n硬件故障使存储在外存中的数据部分丢失或全部丢失\n恢复：\n装入数据库发生介质故障前某个时刻的数据副本\n重做自此时始的所有成功事务，将结果重新记入数据库\n其他原因\n故障恢复 由恢复子系统利用日志文件撤销（UNDO）此事务已对数据库进行的修改\n关键问题 如何建立冗余数据 数据转储（backup） 登记日志文件（logging） 如何利用冗余数据进行数据库恢复 转储 DBA将整个数据库复制到磁带或另一个磁盘上保存起来的过程，备用的数据称为后备副本或者后援副本\n转储状态\n静态转储\n在系统中无运行事务时进行转储，转储期间不允许对数据库进行任何的存取、修改活动\n动态转储\n转储操作与用户事务并发进行，转储期间允许对数据库进行存取或修改\n不能保证副本中的数据正确有效，需加上日志文件才能把数据库恢复至某一时刻的正确状态\n转储方式\n海量转储\n每次转储全部数据库\n增量转储\n只转储上次转储后更新过的数据\n日志文件 用来记录事务对数据库的更新操作的文件\n格式 以记录为单位的日志文件 以数据块为单位的日志文件 内容 各事务开始标记（BEGIN TRANSACTION） 各事务结束标记（COMMIT|ROLLBACK） 各事务所有更新操作 与事务有关的内部更新操作 日志文件中的一个日志记录（log record） 基于记录\n事务标识 操作类型（插入、删除、修改） 操作对象（记录ID、BLOCK NO.） 更新前数据的旧值 更新后数据的新值 基于数据块\n事务标识 操作对象（记录ID、BLOCK NO.） 更新前数据所在整个数据块的值 更新后整个数据块的值 规则 为保证数据库可恢复，登记日志文件时必须遵循\n登记的次序严格按照并行事务执行的时间次序 必须先写日志文件，后写数据库（把标识修改的日志记录写至日志文件，把对数据的修改写至数据库） 检查点（CheckPoint） 在日志文件中增加检查点记录（checkpoint）\n增加重新开始文件\n恢复子系统在登记日志文件期间动态地维护日志\n记录：\n建立检查点时刻所有正在执行的事务清单 这些事务最近的一个日志记录的地址 数据库镜像恢复 介质故障是对系统影响最为严重的一种故障，严重影响数据库的可用性\n数据库镜像：\n在不同的设备上同时存在两份数据库，一个是主设备，一个是镜像设备，每当主设备的数据库发生更新是，系统自动更新镜像设备的数据，使得两个设备的数据始终保持一致\n并发 不同的事务执行方式\n事务串行执行\n每个时刻只有一个事务运行，不能充分利用系统资源\n交叉并发方式（Interleaved Concurrency）\n在单处理机系统中，事务的并行执行是这些并行事务的并行操作轮流交叉进行，没有真正并行运行，但能提高系统效率\n同时并发方式（simultaneous concurrency）\n多处理机系统中，每个处理机运行一个事务，多个处理机同时运行多个事务\n异常 丢失修改（Lost Update） 不可重复读（Non-Repeatable Read） 读”脏“数据（Dirty Read） 可串行化调度 调度（Schedule）：并发事务的操作顺序\n可串行化调度：多个事务并发执行的结果与按某一次序穿行执行的结果相同\n可串行性：并发事务正确调度的准则\n冲突操作：不同事务对同一个数据的读写操作和写写操作\n冲突等价：一个调度S能通过一系列非冲突操作执行顺序的交换变成调度S1，称S与S1冲突等价\n不可交换：\n冲突操作 同一事务内的两个操作 冲突可串行化调度：调度Sc可在保证冲突操作的次序不变的情况下通过交换两事务不冲突操作的次序得到可串行化调度\n数据库状态等价性：在数据库初始状态相同时，等价的调度能产生同样的数据库状态\n调度状态可串行：该调度等价于一个串行调度\n冲突可串行化调度是可串行化调度的充分条件\n可串行化测试：\nTi在Tj Read(A)或者Write(A)之前Write(A)或Read(A) Ti在Tj Write(A)之前Write(A) 则连Ti-\u0026gt;Tj\n若结果出现回路则不是冲突可串行，否则是冲突可串行\n封锁 事务T在对某个数据对象（表、记录）操作之前，先向系统发出请求，对其加锁，在事务T释放锁之前，其他的事务不能更新此数据对象\n排它锁（Exclusive Locks）X锁 （写锁）\n只允许T读取和修改A，其他任何事务不能再对A加任何类型的锁\n共享锁（Share Locks) S锁 （读锁）\n其他事务只能对A再加S锁，不能加X锁，可以读A，但不能对A做任何修改\n锁的正确使用要保证\n事务的一致性 调度的合法性 一级封锁协议 事务T在修改数据之前必须先加X锁，直到事务结束释放\n可防止丢失修改\n二级封锁协议 一级+读数据前加S锁，读完即可释放S锁\n可防止丢失修改、读”脏“数据\n三级封锁协议 一级+读数据前加S锁，事务结束才释放\n可防止丢失修改，读”脏“数据、不可重复读\n活锁 请求不断被后来者覆盖\n先来先服务可解决\n死锁 两个封锁者等待彼此释放\n防止 一次封锁法：要求每个事务一次将所有要使用的数据全部加锁\n降低并发度\n难以事先精确确定封锁对象\n顺序封锁法：余线对数据对象规定封锁顺序\n维护成本高\n难以实现\n事务重试法：回滚其中一个事务\n检测 超时法 事务等待图法：形成事务等待图，周期性检测事务等待图寻找回路 两阶段封锁协议（Two-Phase Locking 2PL） 所有事务分两个阶段对数据项加锁和解锁，在读写之前获得锁，释放锁之后不再申请和获得其他封锁\n并行执行的所有事务均遵守两阶段封锁协议，则对这些事务的所有并行调度策略都是可串行化的\n遵循第三级封锁协议必然遵守两阶段封锁协议\n锁表 锁管理器为目前已加所的数据项维护一个记录链表，每个锁请求为一个记录，按请求到达顺序排序\n多粒度封锁 封锁粒度（Granularity）：封锁对象的大小\n在一个系统中同时支持多种封锁粒度供不同的事务选择\n多粒度封锁协议 允许多粒度树中的贝格节点被独立地加锁\n对一个结点加锁意味着对所有后裔加同样的锁\n一个数据对象可能以两种方式封锁：显式封锁和隐式封锁\n意向锁 目的：提高对某个数据对象加锁时系统的检查效率\n内容：对任一结点加基本锁必须先对上层结点加意向锁，对任一结点加意向锁说明下层结点正在被封锁\n意向共享锁（Intent Share Lock) IS锁 意向排它锁（Intent Exclusive Lock）IX锁 共享意向排它锁（Share Intent Exclusive Lock） SIX锁=S+IX 锁的强度：对其他锁的排斥程度\n具有意向锁的多粒度封锁：\n申请时自上向下 释放时自下向上 ","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0/","title":"【数据库】笔记"},{"content":"图形系统 光栅图像 也称为图像、位图、点阵图、像素图，以像素数组的形式存贮在帧缓冲区中\n基本图元 点 设置点大小 glPointSize(Glfloat size); 设置点绘制模式 glBegin(GL_POINTS); glEnd(); 设置点位置 glVertex3f(Glfloat x, Glfloat y, Glfloat z); 设置点颜色 glColor3f(Glfloat r, Glfloat g, Glfloat b); 光滑（反走样） glEnable (GL_POINT_SMOOTH); glEnable (GL_BLEND); glBlendFunc (GL_SRC_ALPHA,GL_ONE_MINUS_SRC_ALPHA); 直线 设置线的宽度 glLineWidth(Glfloat width)\n设置直线绘制模式 glBegin(GL_LINES); 两两连线 glBegin(GL_LINE_STRIP)；持续连线 glBegin(GL_LINE_LOOP)；首尾也相连\n点画线(虚线) glEnable(GL_LINE_STIPPLE); glLineStipple(GLint factor, GLushort pattern);\n光滑（反走样）\nglEnable (GL_LINE_SMOOTH); glEnable (GL_BLEND); glBlendFunc (GL_SRC_ALPHA,GL_ONE_MINUS_SRC_ALPHA);\n三角形 设置三角型绘制模式\nglBegin(GL_TRIANGLES);\nGL_TRIANGLE_STRIP； 三角带\nGL_TRIANGLE_FAN； 三角扇\n着色模式 glShadeModel(GL_FLAT); glShadeModel(GL_SMOOTH);\n背面消除 glEnable(GL_CULL_FACE); glFrontFace(GL_CCW) GL_CW\n多边形模式 glPolygonMode(GL_BACK,GL_LINE); GL_BACK/GL_FRONT/GL_FRONT_AND_BACK GL_LINE/GL_POINT/GL_FILL\n坐标系 使用OpenGL方式定义坐标系：右手坐标系\n世界坐标系\n最大的坐标系，描述其他坐标系的参考框架\n物体（模型）坐标系\n与特定物体相关\n随着物体运动而运动\n惯性坐标系\n物体坐标系到世界坐标系转换的中间阶段\n位置与物体坐标系一致，方向与世界坐标系一致\n物体坐标系转世界坐标系：以惯性坐标系为媒介，旋转至惯性坐标系后平移至世界坐标系\n深度缓冲 显示模式设置为深度缓冲区 glutInitDisplayMode(GLUT_DEPTH); 设置深度缓冲区允许位 glEnable(GL_DEPTH_TEST); 每帧绘制前清除深度缓冲区 glClear(GL_DEPTH_BUFFER_BIT); 四种变换 模型变换和视点变换本质上是一致的,对一个的变换相当于对另一个反方向变换\n模型视点变换：通过模型变换从局部坐标系转换为世界坐标系，再通过视点变换从世界坐标系转换为视口坐标系\n控制模型 glTranslatef(mx,my,mz); glRotatef(rx,1,0,0); glRotatef(ry,0,1,0); glRotatef(rz,0,0,1); glScalef(sx,sy,sz); 控制视点 glRotatef(-rz,0,0,1); glRotatef(-ry,0,1,0); glRotatef(-rx,1,0,0); glTranslatef(-mx,-my,-mz); 控制 控制键盘\nglutKeyboardFunc(\u0026amp;myKeyboardFunc);\n矩阵 列向量只可以右乘矩阵（结果仍为列向量），用在OpenGL、多数图形学书籍中。\n向量可由三个线性无关的基向量的线性组合表示\n由基向量构成的矩阵就是坐标系\n空间变换 两种变换等价，将物体变换一个量相当于将原始坐标系变换一个相反的量\n在实际的3D对象绘制过程中，实质上是进行的坐标系的变换，以节省大量的计算时间。\n变换物体：物体上所有点变换到新位置，坐标改变\n变换坐标系：物体上的点没有移动，在新的坐标系中描述（计算量小，速度快）\n坐标系变换理解\n正向： 逆向： 在坐标系M下的a向量表示为 Ma 其在坐标系N下表示为 b= N-1Ma（Ma=Nb）\n旋转 旋转的正方向:由右手定则判定，拇指向上，四指弯曲。拇指方向是坐标轴正方向，四指弯曲方向则为旋转正方向\n方位 通过与相对已知方位（源方位）的旋转来表述，旋转的量称作角位移\n初始方位：自旋转角度为0，方向为（0,0,-1）（z轴负向），此时欧拉角为（0，0，0）\n欧拉角 角位移分解为绕三个相互垂直的轴的三个旋转组成的序列\nheading/yaw 偏航 h (y轴) pitch 俯仰/ 纵摇 p(x轴) bank/roll 倾侧/翻滚 b(z轴) 初始方位角为（0，0，-1），转动为0\n取值范围限定 h,b在-180到180，p在-90到90\n四元数 包括一个标量和一个3D向量，代表一角位移：绕n转θ角，q与-q代表相同的角位移\n单位四元数\n共轭四元数：向量部分变负\nq*=[w v]*=[w -v]\n四元数的逆：共轭除以模的平方\nqq-1=[1,0]\n四元数乘法\n满足结合律，不满足交换律\n四元数乘的模等于模的乘积，单位四元数的乘仍然是单位四元数\n四元数乘积的逆等于各四元数逆反顺序相乘\n扩展3d点(x,y,z)到四元数空间为p=[0,(x,y,z)]\n设q为单位化四元数，n为旋转轴,θ为旋转角。则p绕n旋转θ公式为：\n若q为一般形式的四元数，可通过标准化转换求得旋转轴和旋转角度\n连续旋转：先转a,再转b，等于执行ba旋转\n新乘法\n在新四元数乘法定义下：先a旋转，再b旋转，等于执行一次ab旋转。乘法顺序与旋转顺序相同。\n四元数的差：一个方位到另一个方位的角位移\n四元数点乘：对于单位四元数a,b有-1\u0026lt;=a*b\u0026lt;=1，其点乘越大，则a、b所代表的角位移越相似\nθ为a、b之差d的夹角，则\n四元数对数\n记α为θ/2,定义：\n四元数指数\n四元数数乘：标量值直接乘以四元数中每个分量，满足交换律\n四元数的幂：表示角位移的t倍\n四元数插值 Slerp插值 开始四元数q0和结束四元数q1,差值参数t，0≤t ≤1，slerp(q0,q1,t)返回q0到q1间的方位\n简化\n四元数加法（实部实部相加，虚部虚部相加），要考虑两个四元数w方向是否相同。如果相反则改为相减。 k0+k1≠1，因此插值后的四元数模不为1，需要单位化后使用 Squad插值 实现多个方位角之间的平滑序列，不介绍\n视点控制 glRotated(-m_hpb[2],0,0,1);\t//b\t绕z轴转 glRotated(-m_hpb[1],1,0,0);\t//p\t绕x轴转 glRotated(-m_hpb[0],0,1,0);\t//h\t绕y轴转 glTranslated(-m_pos.x,-m_pos.y,-m_pos.z); 视点位置记录在m_pos,视点方位角在m_hpb\n包围盒 AABB（axially aligned bounding box)\n对变换后物体重新计算AABB 对AABB做和物体相同变换(不保证轴对齐，非AABB) 光照 启动光照 glEnable(GL_LIGHTING); glEnable(GL_LIGHT0); 创建光源 glLight{if}[v](GLenum light , GLenum pname, TYPE param) Pname: GL_AMBIENT\t环境反射 GL_DIFFUSE\t漫反射 GL_SPECULAR\t镜面反射 GL_POSITION\t当第w位为0时表示平行光源，为1表示点光源 纹理 特殊效果 OpenGL内部矩阵 GL_MODELVIEW（模型视图矩阵） 把空间点从模型坐标系最终变换到眼坐标系下，包括视图矩阵和模型矩阵的两个矩阵的乘积，在绘制对象的时候执行变换，把glBegin和glEnd之间的顶点乘以当前模型视图矩阵得到眼坐标系下的空间坐标。\nGL_PROJECTION（投影矩阵） 把眼空间坐标系下的视锥体内点转变到矩形规范体（裁剪坐标系）内的点坐标。便于后续操作。转换后的坐标范围是(-1,-1,-1)~(1,1,1)。范围外点不在视锥体内，要被裁剪掉，包括透视投影或者平行投影两类。\n裁剪坐标=投影矩阵*眼坐标 投影结果：x∈[-1,1] y∈[-1,1] z∈[-1,1] z转换到0~1区间后存入深度缓冲：z’=(z+1)*0.5\nGL_TEXTURE（纹理矩阵） OpenGL函数 glReadPixels glutInitDisplayMode(unsigned int mode); 设置初始显示模式\n值 对应宏定义 意义 GLUT_RGB 0x0000 指定 RGB 颜色模式的窗口 GLUT_RGBA 0x0000 指定 RGBA 颜色模式的窗口 GLUT_INDEX 0x0001 指定颜色索引模式的窗口 GLUT_SINGLE 0x0000 指定单缓存窗口 GLUT_DOUBLE 0x0002 指定双缓存窗口 GLUT_ACCUM 0x0004 窗口使用累加缓存 GLUT_ALPHA 0x0008 窗口的颜色分量包含 alpha 值 GLUT_DEPTH 0x0010 窗口使用深度缓存 GLUT_STENCIL 0x0020 窗口使用模板缓存 GLUT_MULTISAMPLE 0x0080 指定支持多样本功能的窗口 GLUT_STEREO 0x0100 指定立体窗口 GLUT_LUMINANCE 0x0200 窗口使用亮度颜色模型 glClearColor(r,b,g,a); 背景颜色\nglClear(GLbitfield mask); GLbitfield：可以使用 | 运算符组合不同的缓冲标志位，表明需要清除的缓冲，例如glClear（GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT）表示要清除颜色缓冲以及深度缓冲，可以使用以下标志位\nGL_COLOR_BUFFER_BIT: 颜色缓冲 GL_DEPTH_BUFFER_BIT: 深度缓冲 GL_ACCUM_BUFFER_BIT: 累积缓冲 GL_STENCIL_BUFFER_BIT: 模板缓冲 也就是用glClearColor或者glClearDepth、glClearIndex、glClearStencil、glClearAccum等函数所指定的值来清除指定的缓冲区\nglFlush()/glFinish() 强制刷新缓冲，保证绘图命令将被执行\nglSwapBuffer() glBegin(); glBegin表示一组用于定义一个或者多个图元的顶点的开始\nGL_POINTS 单个顶点集 GL_LINES 多组双顶点线段,如果顶点数为奇数，最后一个顶点就会被忽略。 GL_LINE_STRIP 不闭合折线 GL_LINE_LOOP 闭合折线 GL_POLYGON 单个简单填充凸多边形 GL_TRIANGLES 多组独立填充三角形 GL_TRIANGLE_STRIP 线型连续填充三角形串 GL_TRIANGLE_FAN 扇形连续填充三角形串 GL_QUADS 多组独立填充四边形 GL_QUAD_STRIP 连续填充四边形串 glTranslatef()等此类几何转换接口在glBegin()和glEnd()之间是无效的。因此，如果想对模型的位置进行转换，要在调用glBegin()和glEnd()接口对之前行处理。\n在glBegin()和glEnd()之间可调用的函数如下： glVertex()：设置顶点坐标 glColor()：设置当前颜色 glIndex()：设置当前颜色表 glNormal()：设置法向坐标 glEvalCoord()：产生坐标 glCallList()、glCallLists()：执行显示列表 glTexCoord()：设置纹理坐标 glEdgeFlag()：控制边界绘制 glMaterial()：设置材质　glEnd(); glEnd表示一组用于定义一个或者多个图元的顶点的结束\nglEnable(): 用于启用各种功能。功能由参数决定\n类型 值 说明 GL_ALPHA_TEST 4864 跟据函数glAlphaFunc的条件要求来决定图形透明的层度是否显示 GL_AUTO_NORMAL 3456 执行后，图形能把光反射到各个方向 GL_BLEND 3042 启用颜色混合。例如实现半透明效果 GL_CLIP_PLANE0 ~ GL_CLIP_PLANE5 12288 ~ 12283 根据函数glClipPlane的条件要求 启用图形切割管道。这里指六种缓存管道 GL_COLOR_LOGIC_OP 3058 启用每一像素的色彩为位逻辑运算 GL_COLOR_MATERIAL 2930 执行后，图形（材料）将根据光线的照耀进行反射 反射要求由函数glColorMaterial进行设定 GL_CULL_FACE 2884 根据函数glCullFace要求启用隐藏图形材料的面 GL_DEPTH_TEST 2929 启用深度测试 根据坐标的远近自动隐藏被遮住的图形（材料） GL_DITHER 3024 启用抖动 GL_FOG 2912 雾化效果 例如距离越远越模糊 GL_INDEX_LOGIC_OP 3057 逻辑操作 GL_LIGHT0 ~ GL_LIGHT7 16384 ~ 16391 启用0号灯到7号灯(光源) 光源要求由函数glLight函数来完成 GL_LIGHTING 2896 启用灯源 GL_LINE_SMOOTH 2848 执行后，过虑线段的锯齿 GL_LINE_STIPPLE 2852 执行后，画虚线 GL_LOGIC_OP 3057 逻辑操作 GL_MAP1_COLOR_4 3472 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 生成RGBA曲线 GL_MAP1_INDEX 3473 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 生成颜色索引曲线 GL_MAP1_NORMAL 3474 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 生成法线 GL_MAP1_TEXTURE_COORD_1 3475 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 生成文理坐标 GL_MAP1_TEXTURE_COORD_2 3476 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 生成文理坐标 GL_MAP1_TEXTURE_COORD_3 3477 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 生成文理坐标 GL_MAP1_TEXTURE_COORD_4 3478 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 生成文理坐标 GL_MAP1_VERTEX_3 3479 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 在三维空间里生成曲线 GL_MAP1_VERTEX_4 3480 根据函数Map1对贝赛尔曲线的设置， 启用glEvalCoord1,glEvalMesh1,glEvalPoint1 在四维空间里生成法线 GL_MAP2_COLOR_4 3504 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 生成RGBA曲线 GL_MAP2_INDEX 3505 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 生成颜色索引 GL_MAP2_NORMAL 3506 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 生成法线 GL_MAP2_TEXTURE_COORD_1 3507 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 生成纹理坐标 GL_MAP2_TEXTURE_COORD_2 3508 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 生成纹理坐标 GL_MAP2_TEXTURE_COORD_3 3509 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 生成纹理坐标 GL_MAP2_TEXTURE_COORD_4 3510 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 生成纹理坐标 GL_MAP2_VERTEX_3 3511 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 在三维空间里生成曲线 GL_MAP2_VERTEX_4 3512 根据函数Map2对贝赛尔曲线的设置， 启用glEvalCoord2,glEvalMesh2,glEvalPoint2 在三维空间里生成曲线 GL_NORMALIZE 2977 根据函数glNormal的设置条件，启用法向量 GL_POINT_SMOOTH 2832 执行后，过虑线点的锯齿 GL_POLYGON_OFFSET_FILL 32823 根据函数glPolygonOffset的设置，启用面的深度偏移 GL_POLYGON_OFFSET_LINE 10754 根据函数glPolygonOffset的设置，启用线的深度偏移 GL_POLYGON_OFFSET_POINT 10753 根据函数glPolygonOffset的设置，启用点的深度偏移 GL_POLYGON_SMOOTH 2881 过虑图形（多边形）的锯齿 GL_POLYGON_STIPPLE 2882 执行后，多边形为矢量画图 GL_SCISSOR_TEST 3089 根据函数glScissor设置，启用图形剪切 GL_STENCIL_TEST 2960 启用模板测试 GL_TEXTURE_1D 3552 启用一维文理 GL_TEXTURE_2D 3553 启用二维文理 GL_TEXTURE_GEN_Q 3171 根据函数glTexGen，启用纹理处理 GL_TEXTURE_GEN_R 3170 根据函数glTexGen，启用纹理处理 GL_TEXTURE_GEN_S 3168 根据函数glTexGen，启用纹理处理 GL_TEXTURE_GEN_T 3169 根据函数glTexGen，启用纹理处理 glDisable() 用来关闭的。与glDisable()参数一致\nglShadeModel() 参数mode可以是GL_SMOOTH（默认值）或GL_FLAT。采用恒定着色时（即GL_FLAT），使用图元中某个顶点的颜色来渲染整个图元。 在使用光滑着色时（即GL_SMOOTH），独立的处理图元中各个顶点的颜色。对于线段图元，线段上各点的颜色将根据两个顶点的颜色通过差值得到。对于多边形图元，多边形内部区域的颜色将根据所有顶点的颜色差值得到。\nglFrontFace(GLenum mode) 作用是控制多边形的正面是如何决定的\nGL_CCW 表示窗口坐标上投影多边形的顶点顺序为逆时针方向的表面为正面。\nGL_CW 表示顶点顺序为顺时针方向的表面为正面。\n在默认情况下，mode是GL_CCW\nglPolygonMode(GLenum face,GLenum mode) face这个参数确定显示模式将适用于物体的哪些部分，控制多边形的正面和背面的绘图模式：\nGL_FRONT表示显示模式将适用于物体的前向面（也就是物体能看到的面）\nGL_BACK表示显示模式将适用于物体的后向面（也就是物体上不能看到的面）\nGL_FRONT_AND_BACK表示显示模式将适用于物体的所有面\nmode这个参数确定选中的物体的面以何种方式显示（显示模式）：\nGL_POINT表示显示顶点，多边形用点显示\nGL_LINE表示显示线段，多边形用轮廓显示\nGL_FILL表示显示面，多边形采用填充形式\nglTranslatef(x,y,z) 沿X轴正方向平移x个单位(x是有符号数)\n沿Y轴正方向平移y个单位(y是有符号数)\n沿Z轴正方向平移z个单位(z是有符号数)\nglScalef(x,y,z) glRotatef(angle,a,b,c) 绕过原点与（a,b,c）的轴正方向旋转angle度\nglLoadIdentity() 重置当前指定的矩阵为单位矩阵\nglViewport(GLint x,GLint y,GLsizei width, GLsizei height);\n左下角坐标，宽，高\nglClipPlane(GLenum plane,Const GLdouble *equation); 定义一个裁剪平面，quation参数指向平面方程Ax + By + Cz + D = 0的4个系数。\ngluUnProject() int WINAPI gluUnProject( GLdouble winx, GLdouble winy, GLdouble winz, const GLdouble modelMatrix[16], const GLdouble projMatrix[16], const GLint viewport[4], GLdouble *objx, GLdouble *objy, GLdouble *objz );\n(winx,winy,winz)为要映射的窗口坐标\n(objx,objy,objz)为计算的 x\nmodelview 矩阵从 glGetDoublev 调用\nprojMatrix投影矩阵从 glGetDoublev 调用\n视区从 glGetIntegerv 调用\ngluLookAt() gluLookAt(GLdouble eyex,GLdouble eyey,GLdouble eyez,GLdouble centerx,GLdouble centery,GLdouble centerz,GLdouble upx,GLdouble upy,GLdouble upz); 第一组eyex, eyey,eyez 相机在世界坐标的位置\n第二组centerx,centery,centerz 相机镜头对准的物体在世界坐标的位置\n第三组upx,upy,upz 相机向上的方向在世界坐标中的方向\n你把相机想象成为你自己的脑袋：\n第一组数据就是脑袋的位置\n第二组数据就是眼睛看的物体的位置\n第三组就是头顶朝向的方向（因为你可以歪着头看同一个物体）\ngluPerspective(GLdouble fovy, GLdouble aspect, GLdouble zNear, GLdouble zFar ); fovy 上平面与下平面的夹角° aspect 屏幕的宽高比 zNear 近视点距离 zFar 远视点距离\nvoid glOrtho( GLdouble left, GLdouble right, GLdouble bottom, GLdouble top, GLdouble zNear, GLdouble zFar ); 正交投影\nglPushMatrix() 将当前矩阵保存入堆栈顶(保存当前矩阵)\nglPopMatrix() 当经过一系列的变换后，栈顶矩阵被修改，此时调用glPopMatrix()时，栈顶矩阵被弹出，且又会恢复为原来的状态。glPushMatrix()和glPopMatrix()的配对使用可以消除上一次的变换对本次变换的影响。使本次变换是以世界坐标系的原点为参考点进行。\nglMatrixMode() mode 指定哪一个矩阵堆栈是下一个矩阵操作的目标,可选值:\nGL_MODELVIEW,对模型视图矩阵堆栈应用随后的矩阵操作。可以在执行此命令后，输出自己的物体图形了。 GL_PROJECTION,对投影矩阵堆栈应用随后的矩阵操作。可以在执行此命令后，为我们的场景增加透视。 GL_TEXTURE,对纹理矩阵堆栈应用随后的矩阵操作。可以在执行此命令后，为我们的图形增加纹理贴图。 glLoadMatrixf(M) replace the current matrix with an arbitrary matrix M\nglMultMatrixf(M) 将当前矩阵乘以任意矩阵\nglKeybordFunc(unsigned char key,int x,int y) 键盘回调函数\nglutTimerFunc(unsigned int millis, void (*func)(int value), int value) 调用毫秒数，回调Timer，Timer的标识值\n其真实意义是在millis毫秒后回调Timer,因此应在回调Timer中重新调用该函数以形成循环\n","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E7%AC%94%E8%AE%B0/","title":"【计算机图形学】笔记"},{"content":"Hint chapter 1 存储程序概念\nchapter 2 重点IEEE754\nchapter 3 w9 c1\n3.4.10.13.14.18.22\n拓展操作码-设计\nEA计算\n一定要画出指令结构图\n标清B、H、Q\n位数写够\nchapter 4 一位定点数的乘除法\nw12 c3\n6.8.10 (2) (4) .12\n均n次移位\n补乘 辅助位$y_{n+1}-y_{n}$\n原除 变补\nchapter 5 主存储器容量扩展-设计\ncache计算\nw10 12:13\n例题\nw14 c1\n10.13.14.16.17.18. 25\n画图标注！\nRom不连 WE非！\n芯片规格、译码器规格、高低有效电平\ncache\n主存：区号+组号+块号+块内地址\n组相联映象\n主存和Cache按同样大小划分成块， 还按同样大小划分成组。\n从主存的组到Cache的组之间采用直接映象方式。 在两个对应的组内部采用全相联映象（1对1）方式。\nchapter 6 微指令编码法-设计\n6,7,9,12,13,20\n参数计算公式\n微命令设计\n结构和命令\n结构画出来、阐述\n命令一条条写出来\nchapter 8、9 数字磁记录方式-画图\n2.3.5\n数字磁记录方式 X 7\n计算\n4.29.25\n接口、端口\n中断与子程序区别\nCPU响应中断的条件\n中断隐指令\n允许、禁止中断\n中断全过程\nChapter 2 数据的机器层次表示 浮点表示法 $$ N=M\\times r^E $$\nr为阶码的底，与尾数的基数相同，通常 $r=2$\nE为阶码（纯整数）\n常用原码或补码\nM为尾数（纯小数）\n常用移码或补码\n假设用补码表示\n最大正数\n$(1-2^{-N})\\times 2^{2^K-1}$\n最小正数\n$(1-2^{-N})\\times 2^{-2^K}$\n绝对值最大负数\n$-2^{2^K-1}$\n规格化浮点数：尾数最高数位为有效值 $$ \\frac{1}{r}\\le|M|\\lt 1 $$\n规格化最小正数\n$2^{-1}\\times 2^{-2^K}$\n规格化绝对值最小负数\n$(-2^{-1}-2^{-N})\\times 2^{-2^K}$\n移码 同一数值的移码和补码除最高位相反外，其他各位相同\nIEEE754 非数值数据 字符串\n向量法 串表法 国标码\n区位码\n机内码\n汉字字形码\nUnicode\n16-bit\nUnicode的实现方式不同于编码方式。一个字符的Unicode 编码是确定的，但是在实际传输过程中，由于不同系统平台的设计不一定一致，以及出于节省空间的目的，对Unicode编码的实现方式有所不同。Unicode的实现方式称为Unicode转换格式（Unicode Translation Format，简称为UTF）\nChapter 3 指令系统 指令格式 操作码：指明性质及功能 地址码：指明地址（特殊情况下也可能直接给出操作数本身） 长度可以等于机器字长，也可以大于或小于机器字长\n双操作数指令除操作码外：\nA1：第一操作数地址 A2：第二操作数地址 A3：操作结果存放地址 A4：下条将要执行指令的地址 显地址：明显给出\n隐地址：依照约定用隐含的方式给出\n操作码 唯一标识\n规整型（定长）\n简化硬件设计，减少指令译码时间，存在信息冗余\n非规整型（变长）\n寻址技术 寻找操作数或下一条将要执行的指令地址\n包括编址方式和寻址方式\n编址方式 需要编址的设备：\nCPU中的通用寄存器 主存储器 输入输出设备 编址单位\n字编址：编址单位=访问单位 字节编址：编址单位\u0026lt;访问单位 位编址 地址码位数与主存容量和最小寻址单位（编址单位）有关\n寻址\n数据寻址\n指令寻址\n寻找下一条执行指令\n顺序寻址\n程序计数器PC+1\n跳跃寻址\n直接（绝对）寻址 相对寻址 间接寻址 数据寻址 寻找操作数地址\n立即寻址\nop|立即数\n寄存器寻址\n优点\n快 地址码短 直接寻址\n地址码给出的地址即为操作数地址\n操作数地址不能修改，与程序本身所在位置无关，又称为绝对寻址\n间接寻址\n地址码给出的地址指向操作数的存放地址\n通常在指令格式中划出一位@为标志位\n@=0 直接寻址\n@=1 间接寻址\n多级间接寻址\n0 找到有效地址\n1 继续间接寻址\n寄存器间接寻址\n变址寻址\n把指令给出的形式地址A与变址寄存器Rx的内容相加\n面向用户，访问字符串、向量和数组等批数据\n基址寻址\n把基址寄存器Rb的内容与位移量相加\n位移量可正可负\n面向系统，主要用于逻辑地址和物理地址的变换，用以解决程序在主存中的再定位和扩大寻址空间等问题\n页面寻址\n将整个主存空间分成若干个大小相同的区，每个区成为一页\n页面地址：页编号\n业内地址：页内单元编号\n基页（零页）寻址 当前页寻址 页面地址等于程序计数器pc的高位部分 页寄存器寻址 区分寻址 显式\nOP|MOD|A\n隐式\nOP|A\n堆栈 寄存器堆栈（硬堆栈）\n寄器组相互连接，栈顶固定，具有对应位自动推移功能\n存储器堆栈（软堆栈）\n栈底固定，栈顶浮动，高地址向低地址生长，SP指针\n指令类型 数据传送类指令\n实现寄存器之间，寄存器与主存单元之间，主存单元之间数据\n一般传送 堆栈操作 数据交换 运算类指令\n算术运算 逻辑运算 移位 算术移位 逻辑移位 循环移位 程序控制类\n无条件转移\nJMP\n子程序调用\nCALL\nRET\n输入输出类\n独立编址\n把外设端口和主存单元分别独立编址\n统一编址\n把外设寄存器和主存单元统一编址\n发展 复杂指令系统计算机 CISC 精简指令系统计算机 RISC Chapter 4 数值的机器运算 加法器 （不要求？） 加法器是由全加器再配以其他必要的逻辑电路组成的\n$$ S_i=A_i\\oplus B_i\\oplus C_{i-1} \\ C_i=A_iB_i+(A_i\\oplus B_i)C_{i-1} $$\n串行加法器\n只有一个全加器。数据逐位串行送入加法器进行运算\n并行加法器\n由多个全加器组成。提高其速度的关键是尽量加快进位的产生和传递的速度。\n若不考虑$G_i$、$P_i$的形成时间，从$C_0-\u0026gt;C_n$的最长延迟时间为 $2nty$\n并行快速进位 $$ C_i=A_iB_i+(A_i\\oplus B_i)C_{i-1} $$\n令\n$G_i=A_iB_i$，表示若本位的两个输入均为1，必然要向高位产生进位\n$P_i=A_i\\oplus B_i$，表示当两个有入中一个为1，低位进位$C_{i-1}$将超越本位向更高位传送 $$ C_i=G_i+P_iC_{i-1} $$ 由此可得并行加法器的快速进位 $$ G_0=C_0 \\ C_i=\\sum_{j=0}^iG_i\\Pi_{k=j+1}^iP_{k} $$ 分组并行进位 把n位字长分为若干小组，在组内各位之间实行并行快速进位，组间采用串行进位或并行快速进位\n单级先行进位方式 组内并行，组间串行\nCLA: Carry-Lookahead Adder\n若不考虑$G_i$、$P_i$的形成时间，从$C_0-\u0026gt;C_n$的最长延迟时间为 $2kty$，其中k为组数\n多级先行进位方式 组内并行，组间并行\nBCLA: Blocked Carry-Lookahead Adder\n若不考虑$G_i$、$P_i$的形成时间，从$C_0-\u0026gt;C_n$的最长延迟时间为 $6ty$\n定点乘法 寄存器\nA寄存器：部分积与最后乘积的高位部分，初值为0。\nB寄存器：被乘数X。\nC寄存器：乘数Y，运算后C寄存器中不再需要保留乘数，改为存放乘积的低位部分。\n原码一位乘法 规则\n参加运算的操作数取其绝对值 令乘数的最低位为判断位，若为 “1”，加被乘数，若为“0”，不加被乘数 （加0）； 累加后的部分积右移一位； 重复n次（前提为n个数值位，1个符号位）2.和3.； 符号位单独处理，同号为正，异号为负。 summary\n右移n次，变乘为加\n补码一位乘法 规则\n参加运算的数用补码表示； 符号位参加运算； 乘数最低位后面增加一位附加位 $Y_{n+1}$，其初值为0； 由于每求一次部分积要右移一位， 所以乘数的最低两位$Y_n$、$Y_{n+1}$的值决定了每次应执行的操作； 移位按补码右移规则进行 共需做n+1次累加，n次移位，第 n+1次不移位。 summary\n永远右移，$Y_{i+1}-Yi$\n$\\gt0$：加X（补）\n$\\lt0$：减X（补）\n定点除法 寄存器\nA寄存器：存放被除数X，最后A寄存器中剩下的是扩大了若干倍的余数。运算过程中A寄存器的内容将不断地发生变化 B寄存器：存放除数Y C寄存器：存放商Q，它的初值为0 原码加减交替法 规则\n被除数减去除数\n当余数为正时，商为“1”，余数左移一位，减除数\n当余数为负时，商为“0”，余数左移一位，加除数\n重复步骤2.，做n+1步加减运算(n步移位)\n若余数为负回正（第n+2步）\n补码加减交替法 规则\n被除数与除数同号，被除数减去除数；被除数与除数异号，被除数加上除数。\n余数和除数同号，商上1，余数左移一位，下次减除数\n余数和除数异号，商上0，余数左 移一位，下次加除数\n重复步骤2.，包括符号位在内，共做n+1步加减运算(n步移位)。\n商的末位恒置1\n规格化浮点数运算 $$ A=M_A\\times2^{E_A} \\ B=M_B\\times2^{E_B} $$\n仅要求加减\n对阶：小阶向大阶看齐，尾数右移\n尾数加/减\n尾数结果规格化\n左规\n若干次\n右规\n假溢出 1次\nChapter 5 存储系统 由几个容量、速度和价格各不相同的存储器构成的系统\n从CPU的角度来看，n种不同的存储器 （$M_1$～$M_n$）在逻辑上是一个整体，整个存储系统具有 接近于$M_1$的速度 相等或接近$M_n$的容量 接近于$M_n$的位价格 Cache存储系统是为解决主存速度不足而提出来的\n它对系统 程序员和应用程序员都是透明的\n虚拟存储系统是为解决主存容量不足而提出来的\n虚拟存储系统需要通过操作系统调度，因此对系统程序员是不透明的， 但对应用程序员是透明的\n存储器 按作用分类\n高速缓冲存储器\n主存与CPU之间\n主存储器\nCPU可直接随机访问\n辅助存储器\nCPU不能直接访问\n按存取方式分类\n随机存取存储器RAM\nCPU可随机读写访问，存取时间相同\n只读存储器ROM\n可视为特殊只读RAM\n顺序存取存储器SAM\n只能按某种顺序存取，存取时间与物理位置有关\n直接存取存储器DAM\n按存储介质分类\n磁芯存储器\n不会因断电丢失；破坏性读出\n半导体存储器\n双极型（TTL或ECL电路）和MOS型\n磁表面存储器\n光存储器\n按信息的可保存性\n易失性存储器\n断电后存储信息即消失\n非易失性存储器\n断电后信息仍保存\n主存 存储体\n核心，存放程序和数据\n地址译码驱动电路\n包含\n译码器\n将地址总线输入的地址码转换成与之对应的译码输出线上的有 效电平，以表示选中了某一单元\n驱动器\n提供驱动电流去驱动相应的读、写电路， 完成对被选中单元的读、写操作\nI/O和读写电路\n放大器 写入电路 读/写控制电路 字节编址\n大端方案\n高字节存放在低地址\n小端方案\n高字节存放在高地址\n技术指标 存储容量\n主存所能容纳的二进制信息总量\n存取速度\n存取时间\n从启动以此存储器操作到完成该操作所经历的时间\n存取周期\n存储器进行以此完整的读写操作所需要的全部时间，即连续两次访问存储器操作之间所需要的最短时间\n在读写操作之后， 总要有一段恢复内部状态的复原时间\n主存带宽\n数据传输率 Bps,bps,Wps\nBm =主存等效工作频率×主存位宽 ÷8\n​\t=内存时钟频率×倍增系数×主存位数 ÷ 8（不做要求）\n提高措施\n缩短存取周期 增加存储字长 增加存储体 半导体随机存储器和只读存储器 主存储器通常分为RAM和ROM两大部分\nRAM 记忆单元\n存放一个二进制位的物理器件，是存储器的最基本构件\n记忆单元可以由各种材料制成，但最常见的由MOS电路组成。MOS型存储器根据记忆单元的结构又可分为静态RAM和动态RAM两种。\n静态RAM，即SRAM（Static RAM），其存储电路以双稳态触发器为基础； 动态RAM，即DRAM（Dynamic RAM），其存储电路以电容为基础。 存储单元\n由地址码相同的多个记忆单元构成\nDRAM 刷新\n为了维持MOS型动态记忆单元的存储信息，每隔一定时间对存储体中的所有记忆单元的栅极电容补充电荷的过程\n一般选定MOS型动态存储器允许的最大刷新间隔为2ms，也就是说，应在 2ms内，将全部存储体刷新一遍。\n刷新 vs. 重写（再生）\n重写是随机的，一般按存储单元进行的，某个存储单元只有在破坏性读出之后才需要重写 刷新是定时的，通常以存储体矩阵中的一行为单位进行，即使许多记忆单元长期未被访问，若不及时补充电荷的话，信息也会丢失。 刷新方式\n集中式\n在允许的最大刷新间隔内，按照存储芯片容量的大小集中安排若干个刷新周期， 刷新时停止读写操作。\n刷新时间=存储体矩阵行数×刷新周期\n这里刷新周期是指刷新一行所需要的时间，由于刷新过程就是“假读”的过程， 所以刷新周期就等于存取周期。\n优点：读/写操作时不 受刷新工作的影响，因此系统的存取速度比较高。\n缺点：在集中刷新期间必须停止 读/写，这一段时间称为“死区”，而且存储容量越大，死区就越长。\n分散式\n把刷新操作分散到每个存取周期内进行，此时系统的存取周期被分为两部分，前一部分时间进行读/写操作或保持，后一部分时间进行刷新操作。一个系统存取周期内刷新存储矩阵中的一行。\n优点：没有死区\n缺点：\n加长了系统的存取周期\n刷新过于频繁，尤其是当存储容量比较小的情况下，没有充分利用所允许的最大刷新间隔（2ms）\n异步式\n结合集中式和分散式，它充分利用了最大刷新间隔时间， 把刷新操作平均分配到整个最大刷新间隔时间内进行\n相邻两行的刷新间隔=最大刷新间隔时间/行数\n刷新控制\n对CPU是透明的。 刷新通常是一行一行地进行的，每一行中各记忆单元同时被刷新，故刷新操作时仅需要行地址，不需要列地址 刷新操作类似于读出操作 因为所有芯片同时被刷新，所以在考虑刷新问题时，应当从单个芯片的存储容量着手，而不是从整个存储器的容量着手 RAM芯片分析 存储芯片通过地址线、数据线和控制线与外部连接。\n地址线\n单向输入的，其数目与芯片容量有关。如容量为1024×4时，地址线 有10根；容量为64K×1时，地址线有16根\n数据线\n双向的，既可输入，也可输出，其数目与数据位数有关。如1024×4的芯片，数据线有4根；64K×1的芯片，数据线只有1根。\n控制线\n主要有读/写控制线（或写允许线）和片选线两种，读/写控制线是用来决定芯片是进行读操作还是写操作，片选线是用来决定 该芯片是否被选中的\n地址复用\n由于DRAM芯片集成度高，容量大，为了减少芯片引脚数量，DRAM芯片把地址线分成相等的两部分，分两次从相同的引脚送入。两次输入的地址分别称为行地址和列地址，行地址由行地址选通信号送入存储芯片，列地址由列地址选通信号送入存储芯片。由于采用了地址复用技术， 因此，DRAM芯片每增加一条地址线，实际上是增加了两位地址，也即增加了4倍的容量。\n地址译码\n地址译码电路把地址线送来的地址信号翻译成对应存储单元的选择信号\n单译码\n又称字选法，它所对应的存储器结构是字结构的，容量为M个字的存储器（M个字，每字b位），排列成M行×b列的矩阵，矩阵的每一行对应一个字， 有一条公用的选择线wi （字线）。字线选 中某一行时，同一行中的各位就都被选中， 由读写电路对被选中的各位实施读出或写 入操作。\n优点：结构简单\n缺点：使用的外围电路多，成本昂贵。更严重的是，当 字数大大超过位数时，存储器会形成纵向很长而横向很窄的不合理结构，所以这种方式只适用于容量不大的存储器。\n双译码\n又称为重合法。通常是把K位地址码分成接近相等的两段，一段用于水平方向作X地址线，供X地址译码器译码； 一段用于垂直方向作Y地址线，供Y地址译码器译码。X和Y两个方向的选择线在存储体内部的一个记忆单元上交叉，以选择相应的记忆单元\n双译码方式对应的存储芯片结构可以是位结构的，则在Z方向上重叠b个芯片，也可以是字段结构的。\n以Intel 2114为例\n存储容量为1K×4，由4096个六管记忆单元电路组成，它们排成64×64的矩阵， 采用字段结构。CPU送来的地址总线10位 （A0～A9），其中6位作为行选择电路的输入，经行地址译码器产生64条行选择线； 另4位作为列选择电路的输入，经列地址译码器产生16条列选择线，每条列选择线再经列I/O电路把4条数据线（I/O1～I/O4）连接到各字段的相应位上\nCS：Chip Select\nWE：write\nROM 具有非易失性，即使电源断电，ROM中存储的信息也不会丢失\n我们把向ROM写入数据的过程称为对ROM进行编程，根据编程方法的不同，ROM通常可以分为以下几类：\n掩膜式ROM（MROM）\n内容是由半导体生产厂家按用户提出的要求在芯片的生产过程中直接写入的， 写入后任何人都无法改变其内容。\n一次可编程ROM（PROM）\nPROM允许用户利用专门的设备（编程器或写入器）写入自己的程序，但一旦写入后便无法改变，因此它是一种一次性可编程的ROM。\n通常，生产厂家提供的PROM芯片初始内容为全“0”，用户根据自编的程序， 使用编程器外加足够大的电压（或电流）， 将“1”写入相应位，PROM的编程是逐位进行的。常见的PROM根据写入原理可分为两类：结破坏型和熔丝型。\n可擦除可编程ROM（EPROM）\n内容不仅可以由用户利用编程器写入，而且可以对其内容进行多次改写。但要注意的是：在+5V的电源条件下只能读出不能写入，用编程器写入信息时必须用+25V的高压。与前两种ROM相 比，EPROM使用起来最为方便，因此应用非常广泛\nUVEPROM（紫外线擦除）\n用紫外线灯进行擦除的，所以只能对整个芯片擦除，而不能对芯片中个别需要改写的存储单元单独擦除和重写。\nEEPROM（电擦除）\n用电气方法来进行擦除的，它在联机 条件下可以用字擦除方式擦除，也可以用数据块擦除方式擦除。以字擦除方式操作时，能够只擦除被选中的那个存储单元的内容；在数据块擦除方式操作时，可擦除数据块内所有单元的内容。\n选择题\n闪速存储器（flash memory）\n一种快擦写型存储器，它的主要特点 是：既可在不加电的情况下长期保存信息， 又能在线进行快速擦除与重写，兼备了 EEPROM和RAM的优点。 目前，大多数微机的主板采用闪速存储器来存储BIOS（基本输入/输出系统）程序。闪速存储器除了具有ROM的一般特性外，还有低电压改写的特点，便于用户自动升级BIOS\n主存储器的连接与控制 主存容量的扩展 要组成一个主存，首先要考虑选片的问题，然后就是如何把芯片连接起来的问题。 根据存储器所要求的容量和选定的存储芯片的容量，就可以计算出总的芯片数，即\n总片数＝ 总容量 /芯片容量\n位拓展\n在位数方向扩展（加大字长），而芯片的字数和存储器的字数是一致的。\n位扩展的连接方式是将各存储芯片的地址线、片选线和读/写线相应地并联起来， 而将各芯片的数据线单独列出\n当CPU访问该存储器时，其发出的地址和控制信号同时传给8个芯片，选中每个芯片的同一单元，其单元的内容被同时读至数据总线的相应位，或将数据总线上的内容分别同时写入相应单元。\n字拓展\n仅在字数方向扩展，而位数不变。字扩展将芯片的地址线、数据线、 读/写线并联，由片选信号来区分各个芯片。\n字和位同时扩展\n当构成一个容量较大的存储器时，往往需要在字数方向和位数方向上同时扩展， 这是前两种扩展的组合。\n10周 例题x2\n重点：设计、cache计算\n地址分配和片选 CPU要实现对存储单元的访问，首先要选择存储芯片，即进行片选；然后再从选中的芯片中依地址码选择出相应的存储单元，以进行数据的存取，这称为字选。\n片内的字选是由CPU送出的N条低位地址线完成的，地址线直接接到所有存储芯片的地址输入端（N由片内存储容量$2^N$决定）， 而片选信号则是通过高位地址得到的。 实现片选的方法可分为3种：\n线选法(CPU地址信号远高于设计需求，仅需直连)\n是用除片内寻址外的高位地址线直接（或经反相器）分别接至各个存储芯片的片选端，当某地址线信息为“0”时，就选中与之对应的存储芯片。\n优点：不需要地址译码器，线路简单，选择芯片不需要外加逻辑电路\n缺点：\n仅适用于连接存储芯片较少的场合。\n不能充分利用系统的存储器空间，且把地址空间分成了相互隔离的区域，给编程带来了一定的困难\n全译码法（CPU地址信号等于设计需求）\n全译码法将片内寻址外的全部高位地址线作为地址译码器的输入，把经译码器译码后的输出作为各芯片的片选信号，将它们分别接到存储芯片的片选端，以实现对存储芯片的选择。 全译码法的优点是每片（或组）芯片的地址范围是唯一确定的，而且是连续的， 也便于扩展，不会产生地址重叠的存储区， 但全译码法对译码电路要求较高。\n部分译码法\n所谓部分译码即用片内寻址外的高位地址的一部分来译码产生片选信号。 如用4片2K×8的存储芯片组成8K×8存 储器，需要四个片选信号，因此只要用两位地址线来译码产生。\n设地址总线有20位 （A19～A0），则寻址8K×8存储器时，无论 A19 ～A13 取何值，只要A12 =A11 =0，而均选 第一片，只要A12 =0，A11 =1，均选中第 二片，……。也就是说，8K RAM中的任一个存储单元，都对应有2(20-13) =27 个地址，这种一个存储单元出现多个地址的现象称地址重叠\n令未用到的高位地址全为0，这样确定的存储器地址称为基本地址，本例中 8K×8存储器的基本地址即00000H～ 01FFFH。\n部分译码法较全译码法简单， 但存在地址重叠区\n主存与CPU连接 硬连接\n主存与CPU的硬连接有三组连线：\n地址总线（AB） 数据总线（DB） 控制总线（CB） 此时，我们把主存看作一个黑盒子，存储器地址寄存器（MAR）和存储器数据寄存器（MDR）是主存和CPU之间的接口。\nMAR可以接受来自程序计数器的指令地址或来自运算器的操作数地址，以确定要访问的单元。\nMDR是向主存写入数 据或从主存读出数据的缓冲部件。\n基本操作\nCPU与主存的硬连接是两个部件之间联系的物理基础，而两个部件之间还有软连接，即CPU向主存发出的读或写命令， 这才是两个部件之间有效工作的关键。\nCPU对主存进行读/写操作时，首先 CPU在地址总线上给出地址信号，然后发出相应的读写命令，并在数据总线上交换信息。\n读\n从CPU送来的地址所指定的存储单元中取出信息，再送给CPU\n地址→MAR→AB CPU\n将地址信号送至地址总线\nRead\nCPU发读命令\nWait for MFC\n等待存储器工作完成信号\n((MAR))→DB→MDR\n读出信息经数据总线送至CPU\n写\n写操作是指将要写入的信息存入CPU 所指定的存储单元中\n地址→MAR→AB\nCPU将地址信号送至地址总线\n数据→MDR→DB\nCPU将要写入的数据送至数据总线\nWrite\nCPU发写命令\nWait for MFC\n等待存储器工作完成信号\n由于CPU和主存的速度存在着差距，所以两者之间的速度匹配是很关键的，通常有两种匹配方式：\n同步存储器读取 异步存储器读取 上面给出的读/写基本操作是以异步存储器读取来考虑的，CPU和主存间没有统一的时钟，由存储器工作完成信号 （MFC）通知CPU存储器工作已完成。 对于同步存储器读取，CPU和主存采用统一时钟，因为主存速度较慢，所以CPU与 之配合必须放慢速度。\n高速缓冲存储器 程序的局部性原理\n程序的局部性包括：\n时间局部性\n如果一个存储单元被访问，则可能该单元会很快被再次访问，这是因为程序存在着循环\n空间局部性\n空间局 部性是指如果一个存储单元被访问，则该单元邻近的单元也可能很快被访问，这是因为程序中大部分指令是顺序存储、顺序执行的，数据一般也是以向量、数组、树 、表等形式簇聚地存储在一起的\n高速缓冲技术就是利用程序的局部性原理，把程序中正在使用的部分存放在一 个高速的容量较小的Cache中，使CPU的访存操作大多数针对Cache进行，从而使程序的执行速度大大提高\n基本结构\nCache和主存都被分成若干个大小相等的块，每块由若干字节组成。由于Cache的容量远小于主存的容量，它保存的信息只是主存中最活跃的若干块的副本\n读写操作\n读\n当CPU发出读请求时，如果Cache命中 ，就直接对Cache进行读操作，与主存无关 ；如果Cache不命中，则仍需访问主存，并把该块信息一次从主存调入Cache内。若此时Cache已满，则需根据某种替换算法，用 这个块替换掉Cache中原来的某块信息\n写\n当CPU发出写请求时，如果Cache命中 ，会遇到如何保持Cache与主存中的内容一致的问题，处理的方法有：\n写直达法 写回法 如果Cache不命中，就直接把信息写入主存，而与Cache无关\n命中率\n在一个程序执行期间 ，设$N_c$表示Cache完成存取的总次数，$N_m$表示主存完成存取的总次数，$h$定义为命中率。则有: $$ h=\\frac{N_c}{N_c+N_m} $$ 效率\nCache –主存系统的效率 $e$： $$ e=\\frac{访问cache时间}{平均访问时间}\\times 100% $$ $e$与Cache命中率有关，设Cache命中率为$h$，访问 Cache 的时间为$t_c$， 访问主存的时间为$t_m$ 则: $$ e =\\frac{t_c}{h\\times t_c+(1-h)\\times t_m} × 100% $$ 地址变换\n当程序已经装入到Cache之后，在实际运行过程中，把主存地址变换成Cache地址\n地址映象\n把存放在主存中的程序按照某种规则装入到Cache中，并建立主存地址与Cache地址之间的对应关系\n全相联映象\n主存中的任意一块都可以映象到Cache中的任意一块。如果Cache的块数为Cb，主存的块数为 Mb，映象关系共有：Cb×Mb种,用硬件实现非常复杂\n优点：\n块冲突概率比较小 Cache的利用率高 缺点:\n需要一个相联存储器，其代价很高。 相联比较所花费的时间将影响Cache的访问速度 直接映象\n主存中一块只能映象到 Cache的一个特定的块中\nb＝B mod Cb，其中： b为Cache的块号， B是主存的块号， Cb是Cache的块数。\n整个Cache地址与主存地址的低位部分完全相同。\n优点：\n硬件实现很简单, 不需要相联访问存储器。\n访问速度也比较快, 实际上不做地址变换。\n缺点：\n块的冲突率较高。 提高Cache速度的一种方法： 把区号存储器与Cache合并成一个存储器\n组相联映象\n组相联映像实际上是全相联映像和直接映像的折衷方案\n主存和Cache按同样大小划分成块， 还按同样大小划分成组。 从主存的组到Cache的组之间采用直接映象方式。 在两个对应的组内部采用全相联映象方式。\n优点：\n块的冲突概率比较低； 块的利用率大幅度提高； 块失效率明显降低。 缺点：\n实现难度和造价要比直接映象方式高 (地址变换过程自己看，不要求）\n选取地址映象方法的主要依据：\n地址变换的硬件要容易实现 地址变换的速度要快 主存空间利用率要高 发生块冲突的概率要小 （虚存类似，跳过？）\nChapter 6 CPU 功能和组成 计算机的工作过程就是程序的运行过程，也 就是在控制器的控制下逐条执行程序中各指令的过程。在程序运行过程中，计算机的各部件在控制器的控制下有条不紊地工作，在各部件之间流动的指令和数据形成了指令流和数据流。\n从程序运行的角度来看，控制器的基本功能是对指令流和数据流在时间与空间上实施正确的控制。\n对指令流的控制 指令流出的控制 指令分析与执行的控制 指令流向的控制 对数据流的控制 对数据的流入与流出的控制 对数据变换、加工等操作的控制 \u0026lt;/less important\u0026gt;\n对于冯·诺依曼结构的计算机而言， 数据流是根据指令流的操作而形成的，也就是说数据流是由指令流来驱动的\n主要寄存器 用来暂时保存在运算和控制过程中的中间结果、最终结果以及控制、状态信息的，又可分为通用寄存器和专用寄存器两种\n通用寄存器\n可用来存放原始数据和运算结果，有的还可以作为变址寄存器、计数器、地址指针等。\n一般可以由CPU直接访问\n专用寄存器\n专门用来完成某一种特殊功能的寄存器。\nCPU中至少要有五个专用的寄存器。它们是：程序计数器（PC）、 指令寄存器（IR）、存储器地址寄存器 （MAR）、存储器数据寄存器（MDR）、 状态标志寄存器（PSWR）\n程序计数器\n程序计数器用来存放正在执行的指令地址或接着要执行的下条指令地址。\n对于顺序执行的情况，PC的内容应不断地增量（加“1”），以控制指令的顺序执行。\n在遇到需要改变程序执行顺序的情况时，将转移的目标地址送往PC，即可实现 、程序的转移。在有些情况下除需要改变PC 的内容外，还需要保留PC过去的内容，以便返回时使用\n指令寄存器\n指令寄存器用来存放从存储器中取出的指令。当指令从主存取出暂存于指令寄存器之后，在执行指令的过程中，指令寄存器的内容不允许发生变化，以保证实现指令的全部功能。\n存储器数据寄存器\n存储器数据寄存器用来暂时存放由主存储器读出的一条指令或一个数据字；反之，当向主存存入一条指令或一个数据字时，也暂时将它们存放在存储器数据寄存器中。\n存储器地址寄存器\n存储器地址寄存器用来保存当前CPU 所访问的主存单元的地址。由于主存和 CPU之间存在着操作速度上的差别，所以必须使用地址寄存器来保持地址信息，直到主存的读/写操作完成为止。 当CPU和主存进行信息交换，无论是 CPU向主存存/取数据时，还是CPU从主存 中读出指令时，都要使用存储器地址寄存器和数据寄存器。\n状态标志寄存器\n状态标志寄存器用来存放程序状态字的。 程序状态字的各位表征程序和机器运行的状态，是参与控制程序执行的重要依据之一。\n它主要包括两部分内容：\n状态标志，如： 进位标志（C）、结果为零标志（Z）等， 大多数指令的执行将会影响到这些标志位 控制标志，如：中断标志、陷阱标志等。 8086的状态标志寄存器共16位，包括九个标志位，其中六个为状态标志，三个为控制标志\n功能 控制器的主要功能：\n⑴从主存中取出一条指令，并指出下一条指 令在主存中的位置。\n⑵对指令进行译码或测试，产生相应的操作控制信号，以便启动规定的动作。\n⑶指挥并控制CPU、主存和输入/输出设备之间的数据流动方向。\n运算器的主要功能有：\n⑴执行所有的算术运算\n⑵执行所有的逻辑运算，并进行逻辑测试\n主要技术参数 （不关注，但要注意参数，公式 ）\n字长\n单位时间内同时处理的二进 制数据的位数。\nCPU按照其处理信息的字长可以分为：8位CPU、16位CPU、32位CPU以及64位CPU等。\n内部工作频率\n又称为内频或主频，它是衡量 CPU速度的重要参数。CPU的主频表示在CPU内数字脉冲信号震荡的速度，与CPU实际的运算能 力并没有直接关系。因此主频仅是CPU性能表现的一个方面，而不代表CPU的整体性能。\n内部时钟频率的倒数是时钟周期，这是 CPU中最小的时间元素。每个动作至少需要一个时钟周期\n外部工作频率\nCPU除了主频之外，还有另一种工作频率，称为外部工作频率，它是由主板为CPU提供的基准时钟频率\n早期， CPU 的内频就等于外频。\n例如： 80486DX-33 的内频是33MHz ，它的外频也是33MHz。也就是说，80486DX-33以33MHz的速度在内部进行运算，也同样以33MHz的速度与外界沟通。\n目前，CPU的内频越来越高，相比之下主存的速度还很缓慢，如果外频设计得跟内频同步， 则主存都将无法跟上CPU的速度。所以现在外频 跟内频不再只是一比一的同步关系，从而出现了 所谓的内部倍频技术，导致了“倍频”的出现。 内频、外频和倍频三者之间的关系是：\n内频=外频×倍频\n理论上倍频是从1.5一直到无限，以0.5为一个间隔单位\n前端总线频率\n前端总线（Front Side Bus），通常用FSB表示，它是CPU和外界交换数据的最主要通道，主要连接主存、显卡等数据吞吐率高的部件，因此前端总线的数据传输能力对计算机整体性能作用 很大。\n在Pentium 4出现之前，前端总线频率与外频是相同的，因此往往直接称前端总线频率为外 频。随着计算机技术的发展，需要前端总线频率高于外频，因此采用了QDR（Quad Date Rate） 技术或者其他类似的技术，使得前端总线频率成为外频的2倍、4倍甚至更高\nQPI数据传输速率（less important）\n取代FSB的基于包传输的高速点到点连接技术 QPI（快速通道互联）。 QPI抛弃了FSB易混淆的单位MHz，而使用GT/s 、MT/s，明确的表示总线实际的数据传输速率，而不是时钟频率。T/s即transfers per second，表示每秒数据传输的次数。QPI总线采用的是2:1比率，即实际的数据传输速率两倍于实际的总线时钟频率。 QPI 的时钟频率基于2.4 GHz、3.2 GHz，则QPI的数据传输速率为4.8GT/s、6.4GT/s，如时钟频率2.4GHz 的QPI的数据传输速率是2.4 GHz × 2 = 4.8 GT/s。\n一个基本的QPI数据包是80bit，需要四次传输完成每次整个数据包的传输，每次传输的20位数据中 ，有16位为有效数据，其余4位用于循环冗余校验。 由于QPI是双向的，在发送的同时也可以接收另一端传输来的数据，这样，每个QPI总线总带宽=每秒传输次数（即QPI速率）×每次传输的有效数据（即 16bit/8=2Byte）×双向。所以QPI速率为4.8GT/s的总带宽=4.8 GT/s × 2Byte × 2 = 19.2 GB/s，QPI速率为 6.4GT/s 的 总带宽= 6.4 GT/s × 2 Byte × 2 = 25.6 GB/s。不难发现，目前的QPI比以前最宽最快的FSB，还要快一倍。\n片内Cache的容量\n片内Cache又称CPU Cache，它的容量和工作速率对提高计算机的速度起着关键的作用。CPU Cache可以分为L1 Cache、L2 Cache，部分高端 CPU还具有L3 Cache。\nL1 Cache，位于CPU内核的旁边，是与CPU结合最为紧密的CPU缓存。一般来说，一级缓存可以分为\n一级数据缓存（Data Cache，D-Cache） 一级指令缓存（Instruction Cache，I-Cache） 大多数CPU的一级数据缓存和一级指令缓存具有 相同的容量，例如D-Cache和I-Cache各为64KB， 总容量为128KB\nL2 Cache是影响CPU性能的关键因素之一 ，在CPU核心不变化的情况下，增加L2 Cache的容量能使性能大幅度提高，而同一核心CPU的高低端之分往往也是在L2 Cache上有差异。目前CPU的L2 Cache一般为1MB， 最大可达4MB～8MB。\nL3 Cache是为读取L2 Cache后未命中的数据设计的—种缓存，在拥有L3 Cache的CPU中，只有约5%的数据需要从内存中调用，这进一步提高了CPU的效率。L3 Cache的容量一般从8MB～16MB不等。\n工作电压\nCPU正常工作所需的电压\n地址总线宽度\n决定了CPU可以访问的最大的物理地址空间，简单地说就是CPU到底能够使用多大容量的主存。\n例如，Pentium 有32位地址线，可寻址的最大容量为$2^{32}$＝ 4096MB（4GB），Itantium有44位地址线， 可寻址的最大容量为$2^{44}$＝16TB。\n数据总线宽度\n决定了CPU与外部Cache、 主存以及输入输出设备之间进行一次数据传输的信息量。\n如果数据总线为32位，每次最多可以读写主存中的32位；\n如果数据总线为 64位，每次最多可以读写主存中的64位。\n数据总线和地址总线是互相独立的，数据总线宽度指明了芯片的信息传递能力，而地址总线宽度说明了芯片可以访问多少个主存单元\n制造工艺\n线宽是指芯片内电路与电路之间的距离，可以用线宽来描述制造工艺。线宽越小，意味着芯片上包括的晶体管数目越多。\nPentium Ⅱ的线宽是0.35μm，晶体管数达 到7.5兆个；\nPentium Ⅲ的线宽是0.25μm， 晶体管数达到9.5兆个；\nPentium 4的线宽是 0.18μm，晶体管数达到42兆个。\n近年来线宽已由0.15μm、0.13μm、90nm、65nm一 直发展到目前主流的45nm和32nm，而 22nm的制造工艺将是新一代CPU的发展目标。\n控制器的组成和实现 计算机系统的指挥中心，它把运算器、存储器、输入/输出设备等部件组成一个有机的整体，然后根据指令的要求指挥全机的工作。\n组成\n指令部件\n完成取指令并分析指令\n程序计数器（PC）\n指令寄存器（IR）\n指令译码器（ID）\n指令译码器又称操作码译码器或指令功能分析解释器。暂存在指令寄存器中的指令只有在其操作码部分经过译码之后才能识别出这是一条什么样的指令，并产生相应的控制信号提供给 微操作信号发生器\n地址形成部件\n根据指令的不同寻址方式，用来形成操作数的有效地址，在微、小型机中，一般不设专门的地址形成部件，而是利用运算器来进行有效地址的计算。\n时序部件\n能产生一定的时序信号，以保证机器的各功能部件有节奏地进行信息传送、加工及信息存储\n脉冲源\n用来产生具有一定频率和宽度的时钟脉冲信号，为整个机器提供基准信号\n启停控制逻辑\n根据计算机的需要，可靠地开放或封锁脉冲，控制时序信号的发生或停止，实现对整个机器的正确启动或停止。启停控制逻辑保证启动时输出的第一个脉冲和停止时输出的最后一 个脉冲都是完整的脉冲。\n节拍信号发生器\n又称脉冲分配器。脉冲源产生的脉冲信号，经过节拍信号发生器后产生出各个机器周期中的节拍信号， 用以控制计算机完成每一步微操作。\n微操作信号发生器\n一条指令的取出和执行可以分解成很多最基本的操作，这种最基本的不可再分割的操作称为微操作。微操作信号发生器也称为控制单元（CU）。不同的机器指令具有不同的微操作序列\n中断控制逻辑\n控制中断处理的硬件逻辑\n硬件实现方法\n控制器的输入是机器指令代码，输出是微操作控制信号，因此微操作信号发生器是控制器的核心。根据产生微操作控制信号的方式不同，控制器可分为3种，它们的根本区别在于微操作信号发生器的实现方法不同，而控制器中的其它部分基本上是大同小异的。\n组合逻辑型\n称为常规控制器或硬布线控制器，它是采用组合逻辑技术来实现的， 其微操作序列形成部件是由门电路组成的复杂树形网络。 组合逻辑控制器的最大优点是速度快， 但是微操作信号发生器的结构不规整，使得设计、调试、维修较困难，难以实现设计自动化。一旦微操作信号发生器构成之后，要想增加新的控制功能是不可能的。\n存储逻辑型\n这种控制器称为微程序控制器，它是采用存储逻辑来实现的，也就是把微操作信号代码化，使每条机器指令转化成为一段微程序并存入一个专门的存储器（控制存储器） 中，微操作控制信号由微指令产生。 微程序控制器的设计思想和组合逻辑设计思想截然不同。它具有设计规整、调试、 维修以及更改、扩充指令方便的优点，易于实现自动化设计，已成为当前控制器的主流。 但是，由于它增加了一级控制存储器，所以指令执行速度比组合逻辑控制器慢\n组合逻辑和存储逻辑结合型\n这种控制器称为PLA控制器，它是组合逻辑技术和存储逻辑技术结合的产物， 它克服了两者的缺点，是一种较有前途的方法\n时序系统与控制方式 时序系统 机器周期\n机器周期通常又称CPU周期，通常把一条指令划分为若干个机器周期（取指令，分析指令，执行指令），每个机器周期完成一个基本操作。\n指令周期\n取指令、分析指令到执行完该指令所需的全部时间。\n由于各种指令的操作功能不同，有的简单，有的复杂， 因此各种指令的指令周期不尽相同。\n指令周期＝i×机器周期\n通常，每个机器周期都有一个与之对应的周期状态触发器。机器运行在不同的机器周期，其对应的周期状态触发器被置 “1”，显然，在机器运行的任何时刻只能建立一个周期状态，因此，有一个且仅有一个触发器被置“1” 。\n节拍\n在一个机器周期内，要完成若干个微操作。这些微操作有的可以同时执行，有的需要按先后次序串行执行。因而需要把一个机器周期分为若干个相等的时间段， 每一个时间段对应一个电位信号，称为节拍电位信号。 节拍的宽度取决于CPU完成一次基本操作的时间\n由于不同的机器周期内需要完成的微操作内容和个数是不同的，因此，不同机器周期内所需要的节拍数也不相同。\n节拍的选取一般有以下几种方法:\n统一节拍法\n以最复杂的机器周期为准定出节拍数， 每一节拍时间的长短也以最繁的微操作作为标准。这种方法采用统一的、具有相等时间间隔和相同数目的节拍，使得所有的机器周期长度都是相等的，因此称为定长CPU周期\n分散节拍法\n按照机器周期的实际需要安排节拍数， 需要多少节拍，就发出多少节拍，这样可以避免浪费，提高时间利用率。由于各机器周期长度不同，又称为不定长CPU周期\n延长节拍法\n在照顾多数机器周期要求的情况下， 选取适当的节拍数，作为基本节拍，如果在某个机器周期内统一的节拍数无法完成该周期的全部微操作，则可以延长节拍。\n时钟周期插入 （目前方法）\n在一些微型机中，时序信号中不设置节拍，而直接使用时钟周期信号。一个机器周期中含有若干个时钟周期，时钟周期的数目取决于机器周期内完成微操作的多少及相应功能部件的速度。一个机器周期的基本时钟周期数确定之后，还可以不断插入等待时钟周期。\n工作脉冲\n在节拍中执行的有些操作需要同步定时脉冲，为此，在一个节拍内常常设置一个或几个工作脉冲，作为各种同步脉冲的来源。工作脉冲的宽度只占节拍电位宽度的1/n，并处于节拍的末尾，只要能保证所有触发器都可靠、稳定地翻转就可以了。 在只设置机器周期和时钟周期的微型机中，一般不再设置工作脉冲，因为时钟周期既可以作为电位信号，其前后沿又可以作为脉冲触发信号\n多级时序系统\n小型机中常采用机器周期、节拍、工作脉冲三级时序系统。每个机器周期M中包括若干节拍，每个节拍内有一个脉冲。 在机器周期间、节拍电位间、工作脉冲间既不允许有重叠交叉，也不允许有空隙， 应该是一个接一个的准确连接。\n控制方式 同步控制方式\n即固定时序控制方式， 各项操作都由统一的时序信号控制，在每个机器周期中产生统一数目的节拍电位和工作脉冲。由于不同的指令，操作时间长短不一致，同步控制方式应以最复杂指令 的操作时间作为统一的时间间隔标准。这种控制方式设计简单，容易实现， 但是对于许多简单指令来说会有较多的空闲时间，造成较大数量的时间浪费，从而影响了指令的执行速度。 在同步控制方式中，各指令所需的时序由控制器统一发出，所有微操作都与时钟同步，所以又称为集中控制方式或中央控制方式\n异步控制方式\n即可变时序控制方式。 各项操作不采用统一的时序信号控制，而根据指令或部件的具体情况决定，需要多少时间，就占用多少时间。 异步控制采用不同时序，没有时间上的浪费，因而提高了机器的效率，但是控制比较复杂由于这种控制方式没有统一的时钟， 而是由各功能部件本身产生各自的时序信 号自我控制，故又称为分散控制方式或局部控制方式。\n联合控制方式\n同步控制和异步控制相结合。实际上现代计算机中几乎没有完全采用同步或完全采用异步的控制方式，大多数是采用联合控制方式。\n通常的设计思想是：在功能部件内部采用同步方式或以同步方式为主的控制方式，在功能部件之间采用异步方式。\n指令执行过程 一条指令执行过程可以分为三个阶段： 取指令阶段、分析取数阶段和执行阶段。\n取指令阶段\n将现行指令从主存中取出来并送至指令寄存器中去。\n具体的操作为：\n(PC)→MAR\n将程序计数器（PC）中的内容送至存储器地址寄存器（MAR），并送地址总线（AB）\nRead\n由控制单元（ CU ）经控制总线（CB）向主存发读命令\nM(MAR) →MDR\n从主存中取出的指令通过数据总线 （DB）送到存储器数据寄存器（MDR）。\n(MDR)→IR\n将MDR的内容送至指令寄存器（IR） 中\n(PC)＋1→PC\n将PC的内容递增，为取下一条指令 做好准备\n以上这些操作对任何一条指令来说都是必须要执行的操作，所以称为公共操作\n分析取数阶段\n取出指令后，机器立即进入分析指令阶段，指令译码器ID可识别和区分不同的指令类型及各种获取操作数的方法。由于各条指令功能不同，寻址方式也不同，所以分析取数阶段的操作是各不相同的\n执行阶段\n执行阶段完成指令规定的各种操作， 形成稳定的运算结果，并将其存储起来\n指令的微操作序列 控制器在实现一条指令的功能时，总要把每条指令分解成为一系列时间上先后有序的最基本、最简单的微操作，即微操作序列。微操作序列是与CPU的内部数据通路密切相关的，不同的数据通路就有不同的微操作序列\n微程序控制原理 微程序设计技术的实质是将程序设计技术和存储技术相结合，即用程序设计的思想方法来组织操作控制逻辑，将微操作控制信号按一定规则进行信息编码（代码化），形成控制字（微指令），再把这些微指令按时间先后排列起来，存放在一个只读存储器中\n微操作\n计算机中最基本的、不可再分解的操作\n一条机器指令可以分解成一个微操作序列\n微命令\n控制计算机各部件完成某个基本微操作的命令。\n微命令与微操作一一对应，微命令是微操作的控制信号，微操作是微命令的操作过程。\n微命令有兼容性和互斥性之分，兼容性微命令是指那些可以同时产生，共同完成某一些微操作的微命令；而互斥性微命令是指在机器中不允许同时出现的微命令。\n微指令\n控制存储器中的一个单元的内容，即控制字，它是若干个微命令的集合\n一条微指令通常至少包含两大部分信息：\n操作控制字段，又称微操作码字段， 用以产生某一步操作所需的各微操作控制信号。 顺序控制字段，又称微地址码字段， 用以控制产生下一条要执行的微指令地址。 微指令有垂直型和水平型之分，垂直型微指令接近于机器指令的格式，每条微指令只能完成一个基本操作。水平型微指令则具有良好的并行性，每条微指令可以完成较多的基本操作\n微地址\n存放控制字的控制存储器的单元地址就称为微地址。\n微周期\n从控制存储器中读取一条微指令并执行相应的微命令所需的全部时间称为微周期\n微程序\n一系列微指令的有序集合就是微程序\n一条机器指令对应于一段微程序\n微程序和程序是两个不同的概念。微程序是由微指令组成的，它用于描述机器指令，实际上是机器指令的实时解释器，它是由计算机的设计者事先编制好并存放在控制存储器中的。对于程序员来说，计算机系统中微程序一级的结构和功能是透明的。而程序则最终由机器指令组成，它是由软件设计人员事先编制好并存放在主存或辅存中的。\n所以说，微程序控制的计算机涉及到两个层次：一个是机器语言或汇编语言程序员所看到的传统机器层，包括：机器指令、 工作程序、主存储器；另一个是机器设计者看到的微程序层，包括：微指令、微程序和控制存储器\n微指令编码法 微指令编码法指的就是操作控制字段的编码方法。\n直接控制法（不译码法）\n操作控制字段中的各位分别可以直接控制计算机，不需要进行译码。操作控制字段的每一个独立的二进制位代表一个微命令，该位为“1”表示这个微命令有效， 为“0”表示这个微命令无效。每个微命令对应并控制数据通路中的一个微操作\n结构简单，并行性强，操作速度快， 微指令字太长，若微命令的总数为N个，则微指令字的操作控制字段就要有N位。另外，在N个微命令中，有许多是互斥的，不允许并行操作，将它们安排在一条微指令中是毫无意义的，只会使信息的利用率下降。 最短编码法\n使得微指令字最短。这种方法将所有的微命令统一编码，每条微指令只定义一个微命令。\n若微命令的总数为N， 操作控制字段的长度为L，则：$L\\ge log_2N$\n字段编码法\n（仅要求字段直接编码法）\n这种方法将操作控制字段分为若干个小段，每段内采用最短编码法，段与段之间采用直接控制法。这种方法又可进一步分为字段直接编码法和字段间接编码法。\n字段直接编码法\n各字段都可以独立地定义本字段的微命 令，而和其他字段无关，因此又称为显式编 码或单重定义编码方法\n字段间接编码法\n操作控制字段的分段原则\n把互斥性的微命令分在同一段内， 兼容性的微命令分在不同段内。这样不仅有助于提高信息的利用率，缩短微指令字长，而且有助于充分利用硬件所具有的并行性，加快执行的速度 与数据通路结构相适应 每个小段中包含的信息位不能太多， 否则将增加译码线路的复杂性和译码时间 一般每个小段还要留出一个状态，表示本字段不发出任何微命令。因此当某字段的长度为三位时，最多只能表示七个互斥的微命令，通常用000表示不操作 微程序控制器的组成和工作过程 微地址形成部件\n产生初始微地址和后继微地址。\n微地址寄存器（ $\\mu$MAR ）\n接受微地址形成部件送来的微地址， 为在CM中读取微指令作准备。\n控制存储器（CM）\n微程序控制器的核心部件，用来存放微程序。\n微指令寄存器（$\\mu$IR ）\n存放从CM取出的正在执行的微指令\n微程序控制器的工作过程实际上就是在微程序控制器的控制下，计算机执行机器指令的过程。\n执行取指令公操作。\n取指令的公共操作通常由一段取指微程序来完成，这 个取指微程序也可能仅由一条微指令组成。 具体的执行是：在机器开始运行时，自动将取指微程序的入口微地址送$\\mu$MAR，并从CM中读出相应的微指令送入$\\mu$IR。微指令的操作控制字段产生有关的微命令，用来控制实现取机器指令的公共操作。\n取指微程序的入口地址一般为CM的0号单元， 当取指微程序执行完后，从主存中取出的机器指令就已存入指令寄存器IR中了\n由机器指令的操作码字段通过微地址形成部件产生出该机器指令所对应的微程序的入口地址，并送入$\\mu$MAR。\n从CM中逐条取出对应的微指令并执行\n执行完对应于一条机器指令的一段微程序后又回到取指微程序的入口地址，继续第⑴步，以完成取下条机器指令的公共操作。\n以上是一条机器指令的执行过程，如此周而复始，直到整个程序执行完毕为止。\n入口地址形成\n当公用的取指微程序从主存中取出机器指令之后， 由机器指令的操作码字段指出各段微程序的入口地址（初始微地址）。这是一种多分支 （或多路转移）的情况，由机器指令的操作码转换成初始微地址的方式主要有三种。\n一级功能转换\n如果机器指令操作码字段的位数和位置固定，可以直接使操作码与入口地址码的部分位相对应。\n二级功能转换\n当同类机器指令的操作码字段的位数和位置固定，而不同类机器指令的操作码的位数和位置不固定时，就不能再采用一级功能转换的方法。所谓二级功能转换是指第一次先按指令类型标志转移，以区分出指令属于哪一类，如：是单操作数指令， 还是双操作数指令等。因为每一类机器指令中操作码字段的位数和位置是固定的， 所以第二次即可按操作码区分出具体是哪条指令，以便找出相应微程序的入口微地址。\n通过PLA电路实现功能转换\n当机器指令的操作码位数和位置都不固定时，可以采用PLA电路将每条机器指令的操作码翻译成对应的微程序入口地址。这种方法对于变长度、变位置的操作码显得更有效，而且转换速度较快\n后续地址形成\n后继微地址的形成方法对微程序编制的灵活性影响很大，它主要有两大基本类型：\n增量方式\n这种方式和机器指令的控制方式很类似， 它也有顺序执行、转移和转子之分。顺序执行时后继微地址就是现行微地址加上一个增量（通常为1）；转移或转子时，由微指令的顺序控制字段产生转移微地址。因此，在微程序控制器中应当有一个微程序计数器 （ $\\mu$PC），为了降低成本，一般情况下都是将微地址寄存器$\\mu$MAR改为具有计数功能的寄存器，以代替$\\mu$PC\n断定方式\n断定方式的后继微地址可由微程序设计者指定，或者根据微指令所规定的测试结果直接决定后继微地址的全部或部分值。 这是一种直接给定与测试断定相结合的方式，其顺序控制字段一般由两部分组成： 非测试段和测试段。\n非测试段，可由设计者指定，一般是微地址的高位部分，用来指定后继微地址在CM中的某个区域内。\n测试段，根据有关状态的测试结果确定其地址值，一般对应微地址的低位部分。这相当于在指定区域内断定具体的分支。所依据的测试状态可能是指定的开关状态、指令操作码、状态字等。\n测试段如果只有一位，则微地址将产生两个分支，若有两位，则最多可产生四个分支，依此类推，测试段为n位最多可产生$2^n$个分支\n微程序设计 水平型微指令及水平型微程序设计\n水平型微指令是指一次能定义并能并行执行多个微命令的微指令。它的并行操作能力强，效率高，灵活性强，执行一条机器指令所需微指令的数目少，执行时间短；但微指令字较长，增加了控存的横向容量，同时微指令和机器指令的差别很大， 设计者只有熟悉了数据通路，才有可能编制出理想的微程序，一般用户不易掌握。\n垂直型微指令和垂直型微程序设计\n垂直型微指令是指一次只能执行一个微命令的微指令。它的并行操作能力差， 一般只能实现一个微操作，控制一、二个信息传送通路，效率低，执行一条机器指令所需的微指令数目多，执行时间长；但是微指令与机器指令很相似，所以容易掌握和利用，编程比较简单，不必过多地了解数据通路的细节，且微指令字较短。\n指令流水线 （不做计算方面要求，仅考察概念）\n流水线基本原理\n流水线技术是一种显著提高指令执行速度与效率的技术。方法是：指令取指完成后，不等该指令执行完毕即可取下一条指令。 如果把一条指令的解释过程进一步细分，例如 ，把分析、执行两个过程分成取指FI、译码DI、计算操作数地址CO、取操作数FO、执行指令EI和写回寄存器WO六个子过程，并用六个子部件分别处理这六个子过程。\n这样只需在上一指令的第一子过程处理完毕进入第二子过程处理时，在第一子部件中就开始对第二条指令的第一子过程进行处理。随着时间推移，这种重叠操作最后可达到六个子部件同时对六条指令的子过程进行操作。\n影响流水线性能的因素\n在流水线中会出现三种相关，影响流水线的畅通流动：\n结构相关\n当多条指令进入流水线后，硬件资源满足不了指令重叠执行的要求时产生。\n数据相关\n指令在流水线中重叠执行时，当后继指令需要用到前面指令的执行结果时发生。\n控制相关\n当流水线遇到分支指令和其他改变PC值的指令时引起\n性能\n通常用吞吐率、加速比和效率3项指标来衡量\n吞吐率\n单位时间内流水线所完成指令或输出结果的数量\n加速比\nm 段的流水线的速度与等功能的非流水线的速度之比\n效率\n指流水线中各功能段的利用率，由于流水线有建立时间和排空时间，因此各功能段的设备不可能一直处于工作状态\n超标量\n通过重复设置多个功能部件，并让这些功能部件同时工作起来来提高指令的执行速度在超标量的处理器结构中，整数和浮点数运算 、装入、存储以及条件转移等普通操作指令可以同时启动并独立执行。 超标量流水CPU是指集成了多条流水线结构的 CPU，当流水线满载时，每个时钟周期可以完成一条以上的指令。\n流水线分类\n流水线按功能可分成单功能流水线和多功能流水线两种。\n单功能流水线只完成一种功能。如浮点加法或乘法流水线。 多功能流水线则可完成多种功能，它允许在不同时间，甚至同一时间内在流水线内连接不同功能段的子集来实现不同功能 流水线按工作方式可分为静态流水线和动态流水线两种。\n在静态流水线中，同一时间内它只能以一种功能方式工作。它可以是单功能的，也可以是多功能的。当是多功能流水线时，则从一种功能方式变为另一种功能方式时，必须先排空流水线，然后为另一种功能设置初始条件后方可使用。显然，不希望这种功能的转换频繁的发生，否则将严重影响流水线的处理效率。 动态流水线则允许在同一时间内将不同的功能段连接成不同的功能子集（前提条件是功能部件的使用不发生冲突），以完成不同的运算功能。 显然，动态流水线必是多功能流水线，而单功能流水线则必是静态的。 Chapter 7 总线 总线是一组能为多个部件服务的公共信息传送线路，它能分时地发送与接收各部件的信息。\n总线结构的优点：\n大大减少信息传送线的数目 提高计算机扩充内存及外部设备的灵活性 分类 按功能层次分：\n片内总线：CPU内部的总线 系统总线：计算机各部件之间的信息传输线 数据总线：双向，与机器字长、存储字长有关 地址总线：单向，与存储地址、 I/O地址有关 控制总线：部分出部分入，控制器控制所有部件 通信总线：用于计算机系统之间、计算机系统与其他系统（如控制仪表、移动通信等）之间的通信 按数据线的多少分为：\n并行总线：含有多条数据线的总线，可以实现一个数据的多位同时传输。 串行总线：含有一条双向数据线或两条单向数据线的总线，可以实现一个数据的各位按照一定的速度和顺序依次传输。 结构 单总线结构\n单总线结构是将CPU、主存、I/O设备（通过 I/O接口）都挂在一组总线上。\nCPU与主存、CPU与外设之间可以直接进行信息交换，主存与外设、外设与外设之间也可以直接进行信息交换，而不需经过CPU的干预。\n双总线结构 这种结构保持了单总线结构的优点，又在CPU和主存之间专门设置了一组存储总线。 优缺点：由于CPU和主存之间需要频繁地交换信息，采用专用的存储总线后，存取速度将大大提高，同时也减轻了系统总线的负担，但是硬件的代价随之有所增加\n三总线结构 在双总线系统上增加I/O总线\n性能指标 总线宽度：数据总线的根数\n总线带宽：数据传输率\n总线带宽定义为总线的最大数据传输率，即每秒传输的字节数。在同步通信中，总线的带宽与总线时钟密不可分，总线时钟频率的高低决定了总线带宽的大小。 $$ B=W\\times\\frac{F}{N} $$ 其中，W为数据总线宽度，通常以字节为单位；F为总线的时钟频率；N为完成一次数据传送所用的时钟周期数。\n时钟同步/异步：总线上的数据与时钟同步的称为同步总线，与时钟不同步的称为异步总线\n信号线数：地址总线、数据总线和控制总线三种总线数的总和。\n总线仲裁 由于总线上连接着多个部件，何时由哪个部件发送信息，如何定时，如何防止信息丢失，如何避免多个设备同时发送，如何规定接收部件等一系列问题都需要总线控制器统一管理。 为了保证同一时刻只有一个申请者使用总线，总线控制机构中设置有总线判优和仲裁控制逻辑。 总线判优按其仲裁控制机构的设置可分为集中式控制和分布式控制两种。\n集中式 链式查询方式 链式查询方式的总线控制器使用三根控制线与所有部件和设备相连。 三根控制线是：\n总线请求（BR）：该线有效，表示至少有一个部件或设备要求使用总线 总线忙（BS）：该线有效，表示总线正在被某部件或设备使用 总线批准（BG）：该线有效，表示总线控制器响应总线请求 当一个或多个设备同时发出总线使用请求信号BR时，中央仲裁器发出的总线授权信号BG沿着菊花链串行的从一个设备依次传送到下一个设备，到达离出发点最近的发出总线请求的设备之后就不再往下传。\n计数器定时查询方式 计数定时查询方式的总线上的每个部件可以通过公共的BR 线发出请求，总线控制器收到请求之后，在BS为“0”的情况下， 让计数器开始计数，定时地查询各个部件以确定是谁发出的请求。 当查询线上的计数值与发出请求的部件号一致时，该部件就使BS线置“1”，获得了总线使用权，并中止计数查询，直至该部件完成数据传送之后，撤消BS信号。\n独立请求方式 在独立请求方式中， 每一个共享总线的部件均有一对控制线：总线请求BRi和总线批准BGi。当某个部件请求使用总线时，便发出BRi，总线控制器中有一排队电路，根据一定的优先次序决定首先响应哪个部件的请求BRi，然后给该部件送回批准信号BGi。 独立请求方式的优点是响应时间快，然而这是以增加控制线数和硬件电路为代价的。此方式对优先次序的控制也是相当灵活的，它可以预先固定，也可以通过程序来改变优先次序。\n分布式 同集中式仲裁相比，分布式仲裁不需要中央仲裁器，而是让各个主设备功能模块都有自己的仲裁号和仲裁电路。 需要使用总线时，各个设备的功能模块将自己唯一的仲裁号发送到共享的总线上，各自的仲裁电路再将从仲裁总线上获得的仲裁号和自己的仲裁号相对比，获胜的仲裁号将保留在仲裁总线上，相应设备的总线请求获得响应。 多个设备提出总线请求时，一般采用优先级或公平策略进行仲裁\nChapter 8 外部设备 除主机以外，而又围绕着主机而设置的各种硬件装置叫做外部设备或外围设备\n输入/输出设备 从计算机的角度出发，向计算机输入信息的外部设备称为输入设备；接受计算机输出信息的外部设备称为输出设备。 辅助存储器 辅助存储器即外存储器，它是指主机以外的存储装置，又称为后援存储器。 终端设备 终端设备由输入设备、输出设备和终端控制器组成，通常通过通信线路与主机相连。终端设备具有向计算机输入和接收计算机输出的能力，具有与通信线路连接的通信控制能力，有些还具有一定的数据处理能力。 过程控制设备 模/数、数/模转换设备均是过程控制设备，有关的检测设备也属于过程控制设备。 磁介质存储器 读写过程 磁记录介质 在磁介质存储器中，信息是记录在一薄层磁性材料的表面上，这个薄层称为磁层。磁层与所附着的载体被称为记录介质或记录媒体。 磁头 磁头是磁记录设备的关键部件之一，它是一种电磁转换元件。写磁头实现把电脉冲表示的二进制代码转换成磁记录介质上的磁化状态，即电－磁转换；读磁头实现把磁记录介质上的磁化状态转换成电脉冲，即磁－电转换。在读写过程中，记录介质与磁头之间相对运动，一般是记录介质运动而磁头不动 写入过程\n在写磁头线圈中通以一定方向的写电流，于是在磁头下方的一个局部区域被磁化，形成一个磁化单元或称记录单元。当这部分介质移出磁头作用区后，仍将留下足够强的剩磁。在写磁头中通以正、负两个不同方向的写电流，就会产生两种不同的剩磁状态，正好对应二进制信息的“1”和“0”。\n读出过程\n读出过程\n读出时，读出线圈不外加电流。当某一磁化单元运动到读磁头下方时，使得磁头中流过的磁通有很大的变化，于是在读出线圈两端产生感应电动势e。感应电动势e经放大、检波、限幅、整形和选通后，获得符合要求的信号。\n技术指标 记录密度 记录密度是指磁介质存储器单位长度或单位面积磁层表面所能存储的二进制信息量。通常以道密度和位密度表示，也可用两者的乘积面密度来表示。\n道密度 道密度又叫横向密度，是指垂直于磁道方向上单位长度中的磁道数目，道密度 的单位是道/英寸（TPI）或道/毫米（TPM）。磁道指的是磁头写入磁场在记录介质表面上形成的磁化轨迹。 位密度 位密度又叫纵向密度，是指沿磁道方向上单位长度中所能记录的二进制信息的位数，位密度的单位为位/英寸（bpi）或位/毫（bpm）。 存储容量 存储容量是指整个磁介质存储器所能存储的二进制信息的总量，一般用位或字节为单位表示，它与存储介质尺寸和记录密度直接相关。磁介质存储器的存储容量有非格式化容量和格式化容量两种指标。非格式化容 量是指磁记录表面上可全部利用的磁化单元数；格式化容量是指用户实际可以使用的存储容量。格式化容量一般约为非格式化容量的60～80％左右。\n平均存取时间 当磁头接到读/写命令，从原来的位置移动到指定位置，并完成读/写操作的时间叫存取时间。对于磁盘存储器来说，存取时间主要包括4部分：\n第一部分是指磁头从原先位置移动到目的磁道所需要的时间，称为定位时间或寻道时间；\n第二部分是指在到达目的磁道以后，等待被访问的记录块旋转到磁头下方的等待时间，称为旋转时间或等待时间；\n第三部分是信息的读写操作时间，也称为传输时间；\n最后是磁盘控制器的开销。\n由于寻找不同磁道和等待不同记录块所花的时间不同，所以通常取它们的平均值。传输时间和控制器的开销相对平均寻道时间$T_s$和平均等待时间$T_w$来说要小得多，所以磁盘的平均存取时间Ta约等于 $$ T_a=T_s+T_w=\\frac{T_{smax}+T_{smin}}{2}+\\frac{T_{wmax}+T_{wmin}}{2} $$\n数据传送率 磁介质存储器在单位时间内向主机传送数据的位数或字节数，称为数据传送率Dr ，单位为位/秒或字节/秒。\n误码率 误码率是衡量磁介质存储器出错概率的参数，它等于读出的出错信息位数和读出总的信息位数之比。\n数字磁记录方式 为了提高磁介质存储器的性能，扩大存储容量，加快存取速度，除了要不断改善磁头和记录介质的电磁性能和机械性能之外，选用高性能的数字磁记录方式对提高记录密度和可靠性也是很重要的。磁记录方式是一种编码方式，即按照某种规律将一连串的二进制数字信息变换成记录介质上相应磁化翻转形式\n直接记录方式 当记录密度较低时，可以不编码，直接按记录信息的“0”、“1”排序记录。\n归零制（RZ） 记录“1”时，磁头线圈中通以正向脉冲电流；记录“0”时，通以反向脉冲电流。由于脉冲电流均要回到零，故称为归零制。 归零制的两个脉冲之间有一段间隔没有电流，相应的这段磁层未被磁化\n不归零制（NRZ）\n记录“1”时，磁头线圈以中通正向电流；记录“0”时，通以反向电流。磁头中电流不回到零。如果记录的相邻两位信息相同（即连续录“1”或“0”）时，写电流方向不变；只有当记录的相邻两位信息不相同（即“0”和“1”交替）时，写电流才改变方向，所以又称为异码变化或“见变就翻”的不归零制\n不归零－1制（NRZ-1） 这是一种改进的不归零制，记录“1”时，磁头线圈中写电流改变方向，使磁层磁化翻转；而记录“0”时，写电流方向维持不变，保持原来的磁化状态，所以称之为见“1”就翻的不归零制。\n以上各种记录方式，目前已很少应用，但不归零制是编码方式的基础，无论哪一种编码方式，只要数据序列变换成记录序列之后，均按照NRZ-1制规则记录到磁层上。\n按位编码记录方式 调相制（PE） 调相制又称相位编码方式，它采用0°和180°相位的不同分别示“1”或“0”。它的编码规则是：\n记录“1”时，写电流在位周期中间由负变正； 记录 “0” 时，写电流在位周期中间由正变负。 当连续出现两个或两个以上 “1” 或 “0” 时，为了维持上述原则，在位周期的边界上也要翻转一次。 这种记录方式常用于磁带机中。\n调频制（FM） 调频制是根据写电流的频率来区分记录“1”或“0”的。\n记录“1”时，写电流在位周期中间和边界各改变一次方向； 记录“0”时，写电流仅在位周期边界改变一次方向。 因此，记录“1”的磁化翻转频率为记录“0”时的两倍，故又称倍频制。若以T0表示位周期，则调频制的磁化翻转间距为0.5T0和T0。\n改进的调频制（MFM） MFM制是在FM制基础上改进的一种记录方式，又称为延迟调制码或密勒码。其编码规则为：\n记录“1”时，写电流在位周期中间改变方向； 记录独立的一个“0”，写电流不改变方向； 记录连续的两个“0”，写电流在位周期边界改变方向。 改进的调频制的磁化翻转间距有三种：T0、1.5T0、2T0 ，对应于三种不同的频率，所以又称为三频制。\n改进的改进型调频制（$M^2FM$） $M^2FM$制的编码规则为：\n记录“1”时，写电流在位周期中间改变方向； 记录独立的一个“0”，写电流不改变方向； 记录连续的两个“0”，写电流在位周期边界处改变方向，产生磁化翻转； 记录连续两个以上的“0”，写电流在前两个“0”的位周期边界处改变方向，以后每隔两个“0”的位周期边界处，写电流再改变一次方向，产生翻转翻转。 改进的改进型调频制的磁化翻转间距有四种：T0、1.5T0、2T0、2.5T0，对应于四种不同的频率，所以又称为四频制。\n磁盘存储器 基本结构 硬盘存储器的硬件包括硬盘控制器（适配器）、硬盘驱动器以及连接电缆。硬盘控制器对硬盘进行管理，并在主机和硬盘之间传送数据。硬盘驱动器内有多个盘片，它们被叠装在主轴上，构成一个盘组，每个盘片的两面都可用作记录面。\n硬盘存储器可分为温彻斯特盘和非温彻斯特盘两类。温彻斯特盘的主要特点是磁头、盘片、磁头定位机构、主轴，甚至连读/写驱动电路等都被密封在一个盘盒内，构成一个头－盘组合体，这个组合体不可随意拆卸，它的防尘性能好，可靠性高，对使用环境要求不高。而非温式磁盘的磁头和盘片等不是密封的，因此要求有超净使用环境。\n硬盘存储器还可分为固定头硬盘和活动头硬盘两类。固定头硬盘机中，每个磁道对应一个磁头，省去了磁头找磁道的时间，存取速度快。但由于磁头太多，使磁盘的道密度不可能很高。移动头硬盘机中，每个盘面上只有一个读写头，安装在读写臂上，当需要在不同磁道上读写时，要驱动读写臂沿盘面作径向移动。由于增加了寻道时间，所以其存取时间比固定头硬盘机要长。\n硬盘驱动器\n磁头 接触启停式浮动磁头是指读/写操作时磁头浮空，不与盘面记录区相接触，以免划伤记录区。但在启动前和停止后，磁头将仍与盘面接触。具体的做法是：在盘面记录区与轴心之间有一段空白区，被当作启停区或着陆区。未启动前及停止后，磁头停在启停区，与盘面接触。当盘片旋转并达到额定转速时，气垫浮力使磁头浮起并达到所需的浮动高度，然后将磁头向外移至0号磁道，准备寻道 磁头定位系统 步进电机定位机构 整个定位机构是开环控制。根据现行磁道号与目的的磁道号之差，求得步进脉冲数，每发一个步进脉冲，脉冲移动一个道距。步进电机定位机构的结构紧凑、控制简单，但定位精度比较低。 音圈电机定位机构 音圈电机可以直接驱动磁头作直线运动，整个定位系统是一个带有速度和位置反馈的闭环调节自动控制系统，其特点是寻道速度快，定位精度高。 信息分布 在硬盘中信息分布呈如下层次：记录面、圆柱面、磁道、扇区\n记录面 硬盘驱动器中可有多个盘片（数量为1~20片），每个盘片有两个记录面，每个记录面对应一个磁头\n磁道 在读/写时，磁头固定不动，盘片高速旋转，磁化区构成一个闭合圆环，称为磁道。在盘面上，一条条磁道形成一组同心圆，最外圈的磁道为0号，往内则磁道号逐步增加。（每个盘片可有500~2500条磁道）。\n圆柱面 在一个盘组中，各记录面上相同编号（位置）的诸磁道构成一个圆柱面。例如，某驱动器有4片8面，则8个0号磁道构成0号圆柱面，8个1号磁道构成1号圆柱面。硬盘的圆柱面数等于一个记录面上的磁道数，圆柱面号即对应的磁道号。引入圆柱面的概念是为了提高硬盘的存储速度。\n扇区 通常将一条磁道划分为若干个段，每个段称为一个扇区或扇段，每个扇区存放一个定长信息块（如512个字节）。一条磁道划分多少扇区，每个扇区可存放多少字节，一般由操作系统决定。磁道上的扇区编号从1号开始\n磁盘地址\n主机向磁盘控制器送出有关寻址信息，磁盘地址一般表示为： 驱动器号、圆柱面（磁道）号、记录面（磁头）号、扇区号\n技术参数 硬盘容量 硬盘容量当然是越大越好。\n主轴转速 从理论上来说，转速越快，硬盘的速度越快。\n道密度 硬盘驱动器在介质上的道密度可达38 000TPI或更高\n平均存取时间 平均存取时间近似等于平均寻道时间和平均等待时间之和。 缓存 缓存存在的目的是为了解决硬盘内部与接口数据之间速度不匹配的问题，它可以提高硬盘的读写速度。\n数据传输率 硬盘的数据传输率分为内部数据传输率和外部数据传输率。内部数据传输率主要依赖于硬盘的旋转速度，因硬盘的品牌及型号不同而有着较大的差异。外部数据传输率指的是系统总线与硬盘缓冲区之间的数据传输率，外部数据传输率与硬盘接口类型和缓存大小有关。\n非格式化容量＝最大位密度×最内圈 磁道周长×总磁道数\n新的磁盘在使用之前需先进行格式化， 格式化实际上就是在磁盘上划分记录区， 写入各种标志信息和地址信息，这些信息占用了磁盘的存储空间，故格式化之后的有效存储容量要小于非格式化容量。\n格式化容量＝每道扇区数×扇区容量 ×总磁道数\n分区域记录 传统硬盘驱动器的每个磁道上记录的扇区数是相同的，因而存储的信息量也是相同的，这意味着在磁盘上位密度是变化的。因为内圈磁道的周长短，外圈磁道的周长长，所以内圈磁道的位密度高，外圈磁道的位密度低，最内圈磁道的位密度（最大位密度）决定了磁盘驱动器的容量。 又因为每个磁道记录的信息量及转速是相同的，所以它们的数据传送率也是相同的。\n由于外圈磁道比内圈磁道更长一些，但存储的信息量却相同，所以外圈磁道上明显地存在着浪费。 采用分区域记录（等位密度）技术可以增加硬盘驱动器的容量。由于外圈磁道有更长的周长，所以外层磁道要比内层磁道包含更多的扇区，即外圈磁道上保存的信息比内圈磁道多。\n分区域记录的另一个影响是数据传输率随磁头所处的区域而变化。分区域驱动器还是以恒定速度旋转，可是，由于外层区域每磁道有更多的扇区，所以数据传输速度要更快一些。这就是当今驱动器标注最小和最大连续传输速率的原因，因为传输速率取决于磁头读/写的位置。分区域记录技术的使用，大大地提高了硬盘利用率，与采用每磁道固定扇区的硬盘比较，使驱动器增加了20～50％的硬盘容量。实际上，现在所有的IDE和SCSI都采用分区域记录\nChapter 9 输入输出系统 输入输出接口 主机和外设的连接方式有辐射型连接、总线型连接等。输入/输出接口（I/O接口）是主机和外设之间的交接界面，通过接口可以实现主机和外设之间的信息交换。 主机和外设各自具有自己的工作特点，它们在信息形式和工作速度上具有很大的差异，接口正是为了解决这些差异而设置的。\n主机和外设之间需要交换的信息有：\n数据信息 这类信息可以是通过输入设备送到计算机的输入数据，也可以是经过计算机运算处理和加工后，送到输出设备的结果数据。传送可以是并行的，也可以是串行的。\n控制信息 这是CPU对外设的控制信息或管理命令，如外设的启动和停止控制、 输入或输出操作的指定、工作方式的选择、中断功能的允许和禁止等\n状态信息 这类信息用来标志外设的工作状态，比如，输入设备数据准备好标志，输出设备忙闲标志等。CPU在必要时可通过对它的查询来决定下一步的操作。\n联络信息 这是主机和外设间工作的时间配合信息，它与主机和外设间的信息交换方式密切相关。通过联络信息可以决定不同工作速度的外设和主机之间交换信息的最佳时刻，以保证整个计算机系统能统一协调地工作。\n外设识别信息 这是I/O寻址的信息，使CPU能从众多的外设中寻找出与自己进行信息交换的唯一外部设备\n功能 实现主机和外设的通信联络控制\n接口中的同步控制电路用来解决主机与外设的时间配合问题。\n进行地址译码和设备选择\n当CPU送来选择外设的地址码后，接口必须对地址进行译码以产生设备选择信息，使主机能和指定外设交换信息\n实现数据缓冲\n在接口电路中，一般设置有一个或几个数据缓冲寄存器，用于数据的暂存，以避免因速度不一致而丢失数据。在传送过程中，先将数据送入数据缓冲寄存器中，然后再送到输出设备或主机中去。\n数据格式的变换\n在输入或输出操作过程中，为了满足主机或外设的各自要求，接口电路中必须具有完成各类数据相互转换的功能。\n传递控制命令和状态信息 当CPU要启动某一外设时，通过接口中的控制命令寄存器向外设发出启动命令；当外设准备就绪时，则有状态信息送回接口中的状态寄存器，为CPU 提供反馈信息，告诉CPU，I/O设备已经具备和CPU交换数据的条件。当外设向CPU提出中断请求和DMA请求时，CPU也有相应的响应信号反馈给外设。\n组成 接口中要分别传送数据信息、控制信息和状态信息，数据信息、控制信息和状态信息都通过数据总线来传送。大多数计算机都把I/O设备的状态信息视为输入数据，而把控制信息看成输出数据，并在接口中分设各自相应的寄存器，赋以不同的端口地址，各种信息分时地使用数据总线传送到各自的寄存器中。\n端口是指接口电路中可以进行读/写的寄存器，若干个端口加上相应的控制逻辑电路组成接口。\n通常，一个接口中包含有数据端口、控制端口和状态端口。存放数据信息的寄存器称为数据端口，存放控制命令的端口称为命令端口，存放状态信息的寄存器称为状态端口。CPU通过输入指令可以从有关端口中读出信息，通过输出指令可以把信息写入有关端口。对状态端口只进行输入操作，将设备状态标志送到CPU 中去；对命令端口只进行输出操作，CPU将向外设发送各种控制命令。因此，在有的接口电路中状态信息和控制信息共用一个寄存器，称之为设备的控制状态寄存器。\n类型 按数据传送方式分类 有串行接口和并行接口。这里所说的数据传送方式指的是外设和接口一侧的传送方式，而在主机和接口一侧，数据总是并行传送的。\n按主机访问I/O设备的控制方式分类 可分为程序查询式接口、中断接口、DMA接口、通道接口等。\n按功能选择的灵活性分类 有可编程接口和不可编程接口。\n按通用性分类 有通用接口和专用接口。\n按输入/输出的信号分类 有数字接口和模拟接口。\n按应用来分类 ①运行辅助接口。 ② 用户交互接口。 ③ 传感接口。 ④ 控制接口。\n外设的识别与端口寻址 外设识别是通过地址总线和接口电路中的外设识别电路来实现的，I/O端口地址就是主机与外设直接通信的地址，CPU可以通过端口发送命令、读取状态和传送数据。 端口地址编址方式 I/O端口编址方式有两种：一种是I/O映射方式，即把I/O端口地址与主存单元地址分别进行独立的编址；另一种是存储器映射方式，即把端口地址与主存单元地址统一编址。\n独立编址 主存地址空间和I/O端口地址空间是相对独立的，分别单独编址。比如，在8086中，其主存地址范围是从00000H～FFFFFH连续的1MB，其I/O端口的地址范围从0000H～ FFFFH，它们互相独立，互不影响。CPU访问主存时，由主存读/写控制线控制；访问外设时，由I/O读/写控制线控制，所以在指令系统中必须设置专门的I/O指令。当CPU使用 I/O指令时，其指令的地址字段直接或间接的指示出端口地址。\n统一编址 I/O端口地址和主存单元的地址是统一编址的，把I/O接口中的端口作为主存单元一样进行访问，不设置专门的I/O指令。 每个外设至少有两个寄存器：控制状态寄存器和数据缓冲寄存器，外设寄存器的地址码是连续的。在PDP-11中，把主存的高4KB地址空间留给外设接口寄存器和CPU内部寄存器使用，这4KB存储空间不允许用户再存放其他内容\n信息传送控制方式 程序查询方式 程序查询方式是一种程序直接控制方式，这是主机与外设间进行信息交换的最简单方式，输入和输出完全是通过CPU执行程序来完成的。 这种方式控制简单，但外设和主机不能同时工作，各外设之间也不能同时工作，系统效率很低，因此，仅适用于外设的数目不多，对I/O处理的实时要求不那么高，CPU的操作任务比较单一，并不很忙的情况\n程序中断方式 外设在作好输入/输出准备时，向主机发中断请求，主机接到请求后就暂时中止原来执行的程序，转去执行中断服务程序对外部请求进行处理，在中断处理完毕后返回原来的程序继续执行。 程序中断不仅允许主机和外设同时并行工作，并且允许一台主机管理多台外设。但是完成一次程序中断需要许多辅助操作，可能使CPU应接不暇；对于一些高速外设，可能会造成信息丢失，因此，它主要适用于中、低速外设\n直接存储器存取（DMA）方式 DMA方式是在主存储器和外部设备之间开辟直接的数据通路，可以进行基本上不需要CPU介入的主存和外设之间的信息传送，这样不仅能保证CPU的高效率，而且能满足高速外设的需要。 DMA方式只能进行简单的数据传送操作，在数据块传送的起始和结束时还需CPU及中断系统进行预处理和后处理。\nI/O通道控制方式 通道是一个具有特殊功能的处理器，它能独立地执行通道程序，产生相应的控制信号，实现对外设的统一管理和外设与主存之间的数据传送。但它不是一个完全独立的处理机，它要在CPU的I/O指令指挥下才能启动、停止或改变工作状态，是从属于CPU的一个专用处理器。 一个通道执行输入/输出过程全部由通道按照通道程序自行处理，不论交换信息多少，只打扰CPU两次（启动和停止时）。\n程序查询方式 由CPU执行一段输入、输出程序来实现主存与外设之间的数据传送方式，叫做程序直接控制方式。根据外设的不同性质，这种传送方式又可分为无条件传送和程序查询方式两种。\n在无条件传送方式中，I/O接口总是准备好接收主机的输出数据，或总是准备好向主机输入的数据，因而CPU无需查询外设的工作状态，而默认外设始终处于准备就绪状态。\n许多外设的工作状态是很难事先预知的，为了保证数据传送的正确进行，就要求CPU在程序中查询外设的工作状态，如果外设尚未准备就绪，CPU就等待，只有外设已作好准备，CPU才能执行I/O指令，这就是程序查询方式。\n工作流程 预置传送参数 在传送数据之前，由CPU执行一段程序，预置传送参数。传送参数包括存取数据的主存缓冲区首地址和传送数据的个数。\n向I/O接口发命令字 当CPU选中某台外设时，执行输出指令向I/O接口发出命令字，启动外设，为接收数据或发送数据的操作做准备。\n从I/O接口取回状态字 CPU执行输入指令，从I/O接口中取回状态字并进行测试，判断数据传送是否可以进行。\n查询外设标志 CPU不断查询状态标志，如果外设没有准备就绪，CPU就踏步进行等待，一直到这个外设准备就绪，并发出“准备就绪”信号为止。\n传送数据 只有外设准备好，才能实现主机与外设间的一次数据传送。输入时，CPU执行输入指令，从I/O接口的数据缓冲寄存器中接收数据；输出时，CPU执行输出指令，将数据写入I/O接口的数据缓冲寄存器。\n修改传送参数 每进行一次数据传送，需要修改传送参数，其中包括主存缓冲区地址加1，传送个数减1。\n判断传送是否结束\n如果传送个数不为0，则转第3步，继续传送，直到传送结束为止。\n程序中断方式 中断 自愿中断和强迫中断 自愿中断又称程序自中断，它不是随机产生的中断，而是在程序中安排的有关指令，这些指令可以使机器进入中断处理的过程，如：指令系统中的软件中断指令等。 强迫中断是随机产生的中断，不是程序中事先安排好的。当这种中断产生后，由中断系统强迫计算机中止现行程序并转入中断服务程序。\n程序中断和简单中断 程序中断就是我们前面提到的中断，主机在响应中断请求后，通过执行一段中断服务程序来处理更紧迫的任务。 简单中断就是外设与主存间直接进行信息交换的方法，即DMA方式。这种“中断”不去执行中断服务程序，故不破坏现行程序的状态。主机发现有简单中断请求（也就是DMA请求）时，将让出一个或几个存取周期供外设与主存交换信息，然后继续执行程序\n内中断和外中断 内中断是指由于CPU内部硬件或软件原因引起的中断。 外中断是指CPU以外的部件引起的中断。\n向量中断和非向量中断 向量中断是指那些中断服务程序的入口地址是由中断事件自己提供的中断。中断事件在提出中断请求的同时，通过硬件向主机提供中断服务程序入口地址，即向量地址。\n非向量中断的中断事件不能直接提供中断服务程序的入口地址，而由CPU 查询之后得到。\n单重中断和多重中断 单重中断在CPU执行中断服务程序的过程中不能被再打断。 多重中断在执行某个中断服务程序的过程中，CPU 可去响应级别更高的中断请求，又称为中断嵌套。\n中断源 中断源是指中断的来源，即任何引起计算机中断的事件\n可采用具有存储功能的触发器来记录中断源，称为中断请求触发器。当某一个中断源有中断请求时，其相应的中断请求触发器置成“1”状态，此时，该中断源向CPU发出中断请求信号。\n多个中断请求触发器构成一个中断请求寄存器，其中每一位对应一个中断源，中断请求寄存器的内容称为中断字或中断码，中断字中为“1”的位就表示对应的中断源有中断请求。\n中断请求信号 (1)独立请求线\n每个中断源单独设置中断请求线，将中断请求信号直接送往CPU，这种方式的特点是CPU在接到中断请求的同时也就知道了中断源是谁，其中断服务程序的入口地址在哪里。 (2) 公共请求线\n多个中断源共有一根公共请求线，这种方式的特点是在负载允许的情况下，中断源的数目可随意扩充，但CPU在接到中断请求后，必须通过软件或硬件的方法来识别中断源，然后再找出中断服务程序的入口地址。 (3) 二维结构 将中断请求线连成二维结构，同一优先级别的中断源， 采用一根公共的请求线，不同请求线上的中断源优先级别不同，这种方式综合了前两种方式的优点，在中断源较多的系统中常采用这种方式。\n中断优先级 当多个中断源同时发出中断请求时，CPU在任何瞬间只能接受一个中断源的请求。通常，把全部中断源按中断的性质和处理的轻重缓急安排优先级，并进行排队。 确定中断优先级的原则是：对那些提出中断请求后需要立刻处理，否则就会造成严重后果的中断源规定最高的优先级；而对那些可以延迟响应和处理的中断源规定较低的优先级。如故障中断一般优先级较高，接着才是I/O设备中断。而在I/O设备中又可以根据各个设备的速度来决定优先级。\n每个中断源均有一个为其服务的中断服务程序，每个中断服务程序都有与之对应的优先级别。另外，CPU正在执行的程序也有优先级。只有当某个中断源的优先级别高于CPU现在的优先级时，才能中止CPU执行现在的程序。\n软件判优法 软件判优法，就是用程序来判别优先级，这是最简单的中断判优方法\n当CPU接到中断请求信号后，就执行查询程序，逐个检测中断请求寄存器的各位状态，检测顺序是按优先级的大小排列的，最先检测的中断源具有最高的优先级，其次检测的中断源具有次高优先级，如此下去，最后检测的中断源具有最低的优先级。\n显然，软件判优是与识别中断源结合在一起的，当查询到中断请求信号的发出者，也就是找到了中断源，程序立即可以转入对应的中断服务程序中去\n硬件判优电路 采用硬件实现中断优先级判定可节省CPU时间，而且速度快，但是成本较高。根据中断请求信号的传送方式不同，有不同的优先排队电路，常见的有以下几种方案。\n独立请求线的优先排队电路\n公共请求线的优先排队电路\n中断响应 CPU响应中断的条件 CPU接收到中断请求信号 首先中断源要发出中断请求，同时C PU还要接收到这个中断请求信号。\nCPU允许中断 CPU允许中断即开中断。CPU内部有一个中断允许触发器，只有当其被置位时，CPU才可能响应中断源的中断请求（中断开放）。如其被复位，CPU处于不可中断状态，即使中断源有中断请求，CPU也不响应（中断关闭）。\n一条指令执行完毕\n一般情况下，CPU在一条指令执行完毕，且没有更紧迫的任务时才能响应中断请求。\n中断隐指令 CPU响应中断之后，经过某些操作，转去执行中断服务程序。 这些操作是由硬件直接实现的，我们把它称为中断隐指令。\n中断隐指令并不是指令系统中的一条真正的指令，它没有操作码，所以中断隐指令是一种不允许、也不可能为用户使用的特 殊指令。\n其所完成的操作主要有：\n保存断点\n将原来程序的断点（即程序计数器PC的内容）保存起来。\n暂不允许中断\n为了在用软件保护中断现场（即CPU的主要寄存器状态）时，不被新的中断所打断，从而保证被中断的程序在中断服务程序执行完毕之后能接着正确地执行下去。\n引出中断服务程序\n引出中断服务程序的实质就是取出中断服务程序的入口地址送程序计数器。\n中断周期 将特定地址“0”送至存储器地址寄存器，记作0→MAR； 将PC的内容（断点）送至MDR，记作(PC)→MDR； 向主存发写命令，启动存储器做写操作，记作Write； 将MDR的内容通过数据总线写入到MAR所指示的主存单元（ 0 号）中，记作MDR→M(MAR)； 向量地址形成部件的输出送至PC，为进入中断服务程序作准备，记作向量地址→PC； 关中断，将中断允许触发器清0，记作0→EINT。 如果断点存入堆栈，只需将上述⑴改为堆栈指针SP→MAR。\n进入中断服务程序 识别中断源在于转入为该中断源专门设置的中断服务程序。 向量中断时，中断源向CPU发出中断请求信号之后，CPU经过一定的判优处理，若决定响应这个中断请求，则向中断源发出中断响应信号。中断源接到中断响应信号后就通过自己的向量地址发生器向CPU发送向量地址。\n向量地址通常有两种情况：\n(1) 向量地址是中断服务程序的入口地址如果向量地址就是中断服务程序的入口地址，则CPU 不需要再经过处理就可以进入相应的中断服务程序。\nPC←8×NNN 转中断服务程序入口地址 由此可见，中断服务程序的入口地址 依次是00H、08H、10H、……、 38H。\n(2) 向量地址是中断向量表的指针\n如果向量地址是中断向量表的指针，则向量地址指向一个中断向量表，从中断向量表的相应单元中再取出中断服务程序的入口地址，此时中断源给出的向量地址是中断服务程序入口地址的地址。\n中断现场的保护和恢复 中断现场指的是发生中断时CPU的主要状态，其中最重要的是断点，另外还有一些通用寄存器的状态。\n之所以需要保护和恢复现场的原因是因为CPU要先后执行两个完全不同的程序（现行程序和中断服务程序），必须进行两种程序运行状态的转换。一般来说，在中断隐指令中，CPU硬件将自动保存断点，有些计算机还自动保存程序状态寄存器的内容。但是，在许多应用中，要保证中断返回后原来的程序能正确地继续运行，仅保存这一、二个寄存器的内容是不够的。\n为此，在中断服务程序开始时，应由软件去保存那些硬件没有保存，而在中断服务程序中又可能用到的寄存器（如某些通用寄存器）的内容，在中断返回之前，这些内容还应该被恢复。\n现代计算机一般都先采用硬件方法来自动快速的保护和恢复部分重要的现场，其余寄存器的内容再由软件完成保护和恢复，这种方法的硬件支持是堆栈。\n允许和禁止中断 允许中断还是禁止中断是用CPU中的中断允许触发器控制的，当中断允许触发器被置“1”，则允许中断，当中断允许触发器被置“0”，则禁止中断。 允许中断即开中断，下列情况时应开中断： ⑴ 在中断服务程序执行完毕，恢复中断现场之后； ⑵ 在多重中断的情况下，保护中断现场之后。\n禁止中断即关中断，下列情况时应关中断： ⑴ 当响应某一级中断请求，不再允许被其他中断请求打断时； ⑵ 在中断服务程序的保护和恢复现场之前。\n中断屏蔽 中断源发出中断请求之后，这个中断请求并不一定能真正送到CPU去，在有些情况下，可以用程序方式有选择地封锁部分中断，这就是中断屏蔽\n如果给每个中断源都相应地配备一个中断屏蔽触发器MASK，则每个中断请求信号在送往判优电路之前，还要受到屏蔽触发器的控制。\n当MASK=1，表示对应中断源的请求被屏蔽（封锁其中断源的请求），可见中断请求触发器和中断屏蔽触发器是成对出现的，只有当 INTRi=1（中断源有中断请求），MASKi=0（该级中断未被屏蔽），才允许对应的中断请求送往CPU。\n在中断接口电路中，多个屏蔽触发器组成一个屏蔽寄存器，其内容称为屏蔽字或屏蔽码，由程序来设置。屏蔽字某一位的状态将成为本中断源能否真正发出中断请求信号的必要条件之一。\n这样，就可实现CPU对中断处理的控制，使中断能在系统中合理协调地进行。\n中断屏蔽寄存器的作用：\n用程序设置的方法将屏蔽寄存器中的某一位置“1”，则对应的中断请求被封锁，无法去参加排队判优；若屏蔽寄存器中的某一位置“0”，才允许对应的中断请求送往CPU\n中断升级 中断屏蔽字的另一个作用是可以改变中断优先级，将原级别较低的中断源变成较高的级别，我们称之为中断升级。这实际上是一种动态改变优先级的方法。 这里所说的改变优先次序是指改变中断的处理次序。中断处理次序和中断响应次序是两个不同的概念，中断响应次序是由硬件排队电路决定的，无法改变。但是，中断处理次序是可以由屏蔽码来改变的，故把屏蔽码看成软排队器。中断处理次序可以不同于中断响应次序。\n横着看\n第1级中断源的优先级别最高，它禁止本级和更低级的中断请求；第4级中断源的优先级别最低，它仅禁止本级的中断请求，而对其他高级的中断请求全部开放。\n中断全过程 中断全过程是指从中断源发出中断请求开始，CPU响应这个请求，现行程序被中断，转至中断服务程序，直至中断服务程序执行完毕，CPU 再返回原来的程序继续执行的整个过程。 中断全过程分为五个阶段： 中断请求、中断判优、中断响应、中断处理、中断返回。\n其中中断处理就是执行中断服务程序，中断服务程序基本上由三部分组成，\n第一部分为准备部分，其基本功能是保护现场，对于非向量中断方式则需要确定中断源，最后开放中断，允许更高级的中断请求打断低级的中断服务程序。\n第二部分为处理部分，即真正执行为某个中断源服务的中断服务程序。\n第三部分为结尾部分，首先要关中断，以防止在恢复现场过程中被新的中断打断，接着恢复现场，然后开放中断，以便返回原来的程序后可响应其它的中断请求\nDMA方式 通道控制方式 ","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E7%AC%94%E8%AE%B0/","title":"【计算机组成原理】笔记"},{"content":"Hint 非功能需求及其含义\n产品为了满足用户的业务需求而必须具备的除功能之外的特性，它与功能需求是互补的\n性能 可扩展性/伸缩性 可维护性 可用性 安全性 业务质量(cost, schedule, staff, resources) 编程的三个层次\n面向过程 面向对象 面向事件 面向对象设计的三大原则（特性）\n封装\n就是将客观事物抽象为逻辑实体，实体的属性和功能相结合，形成一个有机的整体。并对实体的属性和功能实现进行访问控制，向信任的实体开放，对不信任的实体隐藏。，通过开放的外部接口即可访问，无需知道功能如何实现。\n继承\n在继承机制下形成有层级的类，使得低层级的类可以延用高层级类的特征和方法\n多态\n是指一个类的同名方法，在不同情况下的实现细节不同\nDesign Pattern 作用 可重用性 容易被他人理解 代码可靠性 原则 开闭原则（Open Closed Principle，OCP）\n对扩展开放，对修改关闭\n单一职责原则（Single Responsibility Principle, SRP）\n一个类只负责一个功能领域中的相应职责\n依赖倒转原则（Dependency Inversion Principle，DIP）\n依赖于抽象，不能依赖于具体实现\n里氏代换原则（Liskov Substitution Principle，LSP）\n所有引用基类的地方必须能透明地使用其子类的对象\n接口隔离原则（Interface Segregation Principle，ISP）\n类之间的依赖关系应该建立在最小的接口上\n合成/聚合复用原则（Composite/Aggregate Reuse Principle，CARP）\n尽量使用合成/聚合，而不是通过继承达到复用的目的\n最少知识原则（Least Knowledge Principle，LKP）或者迪米特法则（Law of Demeter，LOD）\n一个软件实体应当尽可能少的与其他实体发生相互作用\n创建型模式 关注点是如何创建对象，其核心思想是要把对象的创建和使用相分离，这样使得两者能相对独立地变换\nSingleton 单例\n提供访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。\nabstract\nobject Singleton { ... } implementation\nobject Singleton { fun service(): Unit { println(\u0026#34;I am singleton\u0026#34;) println(\u0026#34;I offer some general service\u0026#34;) } } fun demo() { Singleton.service() } Factory 工厂\n不对外暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。\nSimple Factory 一个Factory返回多个Product子类，新增Product子类时需修改Factory类\nabstract\ninterface Product interface ProductType interface Factory { fun getProduct(productType: ProductType): Product } implementation\nenum class OrganismType: ProductType { AQUATIC, TERRESTRIAL } class Aquatic: Organism class Terrestrial: Organism //水生生物与陆生生物 object OrganismFactory: Factory { override fun getProduct(productType: ProductType): Organism { println(productType as OrganismType) return when (productType as OrganismType) { OrganismType.AQUATIC -\u0026gt; Aquatic() OrganismType.TERRESTRIAL -\u0026gt; Terrestrial() } } } fun simpleFactoryDemo() { val organism = OrganismFactory.getProduct(OrganismType.TERRESTRIAL) } Factory Method 每个Product子类均有各自对应的Factory\nabstract\ninterface Product interface ProductType interface Factory { fun getProduct(productType: ProductType): Product } implementation\ninterface Organism: Product interface OrganismFactories: Factory { override fun getProduct(productType: ProductType): Organism } class Aquatic: Organism class Terrestrial: Organism enum class OrganismType: ProductType { AQUATIC, TERRESTRIAL } object AquaticFactory: OrganismFactories { override fun getProduct(productType: ProductType): Organism { if (productType as OrganismType != OrganismType.AQUATIC){ println(\u0026#34;wrong type\u0026#34;) } println(\u0026#34;aquatic\u0026#34;) return Aquatic() } } object TerrestrialFactory: OrganismFactories { override fun getProduct(productType: ProductType): Organism { if (productType as OrganismType != OrganismType.TERRESTRIAL){ println(\u0026#34;wrong type\u0026#34;) } println(\u0026#34;terrestrial\u0026#34;) return Terrestrial() } } enum class OrganismType: ProductType { AQUATIC, TERRESTRIAL } fun factoryMethodDemo() { val aquatic = AquaticFactory.getProduct(OrganismType.AQUATIC) val terrestrial = AquaticFactory.getProduct(OrganismType.TERRESTRIAL) } Abstract Factory 抽象工厂\n工厂方法的进一步深化，围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂\n当存在多个Product族时使用，类似于简单工厂与工厂方法的结合，从Product族层面看为工厂方法，从某一Product族中的子Product层面看为简单工厂\nabstract\ninterface Product interface ProductType interface ProductFamilyType interface AbstractFactory { fun getProduct(productFamilyType: ProductFamilyType, productType: ProductType): Product } implementation\ninterface Substance: Product interface Organic: Substance interface Inorganic: Substance interface SubstanceType: ProductType enum class SubstanceFamilyType: ProductFamilyType { ORGANIC, INORGANIC } enum class OrganicType: SubstanceType { FAT, PROTEIN, CARBOHYDRATE } enum class InorganicType: SubstanceType { WATER, INORGANICSALT } class Water: Inorganic class InorganicSalt: Inorganic class Fat: Organic class Protein: Organic class Carbohydrate: Organic //有机物与无机物 object InorganicFactory: AbstractFactory { override fun getProduct(productFamilyType: ProductFamilyType, productType: ProductType): Inorganic { if(productFamilyType as SubstanceFamilyType!=SubstanceFamilyType.INORGANIC) throw java.lang.Exception() val substance = when (productType as InorganicType) { InorganicType.WATER -\u0026gt; Water() InorganicType.INORGANICSALT -\u0026gt; InorganicSalt() } println(\u0026#34;$productType of $productFamilyType family is produced\u0026#34;) return substance } } object OrganicFactory: AbstractFactory { override fun getProduct(productFamilyType: ProductFamilyType, productType: ProductType): Organic { if(productFamilyType as SubstanceFamilyType!=SubstanceFamilyType.ORGANIC) throw java.lang.Exception() val substance = when (productType as OrganicType) { OrganicType.FAT -\u0026gt; Fat() OrganicType.CARBOHYDRATE -\u0026gt; Carbohydrate() OrganicType.PROTEIN -\u0026gt; Protein() } println(\u0026#34;$productType of $productFamilyType family is produced\u0026#34;) return substance } } fun abstractFactoryDemo() { try { OrganicFactory.getProduct(SubstanceFamilyType.ORGANIC,InorganicType.INORGANICSALT) } catch (e: Exception) { println(\u0026#34;wrong productFamilyType or ProductType\u0026#34;) } } 应该不考 Builder 建造者\n建造者模式（Builder Pattern）使用多个简单的对象一步一步构建成一个复杂的对象，将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。\n适用于：\n1、需要生成的对象具有复杂的内部结构。\n2、需要生成的对象内部属性本身相互依赖。\nvs. Factory Pattern ：建造者模式更加关注与零件装配的顺序。\nvs. Bridge Pattern\nabstract\ninterface Product interface Builder { fun build(): Product } implementation\ndata class Intelligence(val wisdom: Int = 0) : Product data class Courage(val courageous: Int = 0) : Product data class Spirit(val intelligence: Intelligence = Intelligence(), val courage: Courage = Courage()) : Product data class Skelecton(val solidity: Int = 0) : Product data class Muscle(val strength: Int = 0) : Product data class Flesh(val skelecton: Skelecton = Skelecton(), val muscle: Muscle = Muscle()) : Product data class Life(val spirit: Spirit, val flesh: Flesh) : Product class LifeBuilder: Builder { private var spirit: Spirit = Spirit() private var flesh: Flesh = Flesh() fun soul(wisdom: Int, courageous: Int): LifeBuilder { spirit = Spirit(Intelligence(wisdom), Courage(courageous)) println(\u0026#34;intelligence $wisdom\u0026#34;) println(\u0026#34;courageous $courageous\u0026#34;) println(\u0026#34;a beautiful soul was infused\u0026#34;) return this } fun body(solidity: Int, strength: Int): LifeBuilder { flesh = Flesh(Skelecton(solidity), Muscle(strength)) println(\u0026#34;solidity $solidity\u0026#34;) println(\u0026#34;strength $strength\u0026#34;) println(\u0026#34;a strong body is formed\u0026#34;) return this } override fun build(): Life { println(\u0026#34;spirit is infused with flesh\u0026#34;) return Life(spirit,flesh) } } Prototype 原型\n用于创建重复的对象，利用已有的一个原型对象，通过复制快速地生成和原型对象一样的实例，同时又能保证性能，当直接创建对象的代价比较大时，则可采用这种模式。\nabstract\ninterface Content { fun copy(): Content } interface Prototype { val content: Content fun clone(): Content } implementation\n//仿生人 class Android: Content { override fun copy(): Content { println(\u0026#34;new android had been manufactured\u0026#34;) return Android() } } class AndroidPrototype(override val content: Android) : Prototype { override fun clone(): Content { return content.copy() } } fun prototypeDemo() { val androidPrototype = AndroidPrototype(Android()) val nAndroid: Android = androidPrototype.clone() as Android } 行为型模式 主要涉及算法和对象间的职责分配，描述一组对象应该如何通过使用对象组合协作来完成一个整体任务。\nObserver vs. MVC（时序图）vs. Reactor（时序图） Observer 观察者\n当一个对象被修改时，自动通知依赖它的对象\nvs. Reactor\nabstract\ninterface Observer { fun update(observableState: ObservableState) } interface ObservableState interface Observable { var state: ObservableState val observers: ArrayList\u0026lt;Observer\u0026gt; fun attachObserver(observer: Observer) { observers.add(observer) } fun updateState(state: ObservableState) { this.state = state } fun notifyObservers() { observers.forEach { it -\u0026gt; it.update(state) } } } implementation\ndata class LoginState(val on: Boolean = false) : ObservableState class User(override var state: ObservableState = LoginState(), override val observers: ArrayList\u0026lt;Observer\u0026gt; = ArrayList()) : Observable class Portrait: Observer { override fun update(observableState: ObservableState) { when( (observableState as LoginState).on ) { true -\u0026gt; println(\u0026#34;portrait became light\u0026#34;) false -\u0026gt; println(\u0026#34;portrait become dark\u0026#34;) } } } class Friends: Observer { override fun update(observableState: ObservableState) { when( (observableState as LoginState).on ) { true -\u0026gt; println(\u0026#34;user is online\u0026#34;) false -\u0026gt; println(\u0026#34;user is offline\u0026#34;) } } } fun observerDemo() { val user = User() user.attachObserver(Friends()) user.attachObserver(Portrait()) user.notifyObservers() user.updateState(LoginState(true)) } Reactor 基于事件驱动的设计模式，拥有一个或多个并发输入源，有一个服务处理器和多个请求处理器，服务处理器会同步的将输入的请求事件以多路复用的方式分发给相应的请求处理器\nabstract\ninterface Event interface Reactor { val handlers: ArrayList\u0026lt;EventHandler\u0026gt; fun dispatch(event: Event) } interface EventHandler { fun handle(event: Event) } implementation\nclass ClickedEvent: Event class EnteredEvent: Event class ClickedEventHandler: EventHandler { override fun handle(event: Event) { println(\u0026#34;clicked event is handled\u0026#34;) } } class EnteredEventHandler: EventHandler { override fun handle(event: Event) { println(\u0026#34;entered event is handled\u0026#34;) } } class EventDispatcher(override val handlers: ArrayList\u0026lt;EventHandler\u0026gt; = ArrayList()) : Reactor { override fun dispatch(event: Event) { when (event) { is ClickedEvent -\u0026gt; ClickedEventHandler().handle(event) is EnteredEvent -\u0026gt; EnteredEventHandler().handle(event) } } } fun reactorDemo() { val eventDispatcher = EventDispatcher() eventDispatcher.dispatch(ClickedEvent()) eventDispatcher.dispatch(EnteredEvent()) } MVC 模型（Model） 用于封装与应用程序的业务逻辑相关的数据以及对数据的处理方法。“ Model ”有对数据直接访问的权力，例如对数据库的访问。“Model”不依赖“View”和“Controller”，也就是说， Model 不关心它会被如何显示或是如何被操作。但是 Model 中数据的变化一般会通过一种刷新机制被公布。为了实现这种机制， View 或事先在此 Model 上注册，从而，View 可以了解在数据 Model 上发生的改变（观察者模式）或在Controller操控下更新View **视图（View）**能够实现数据有目的的显示（理论上，这不是必需的）。在 View 中一般没有程序上的逻辑。为了实现 View 上的刷新功能，View 需要访问它监视的数据模型（Model），因此应该事先在被它监视的数据那里注册。 **控制器（Controller）**起到不同层面间的组织作用，用于控制应用程序的流程。它处理事件并作出响应。“事件”包括用户的行为和数据 Model 上的改变。 abstract\ninterface Model { fun update() fun saveToDatabase() fun notifyView(view: View) } interface Controller { val model: Model val view: View fun updateModel() } interface View { fun updateGUI() } implementation\nclass ViewImpl: View { override fun updateGUI() { println(\u0026#34;gui is updated\u0026#34;) } } class ControllerImpl(override val model: Model = ModelImpl(), override val view: View = ViewImpl()) : Controller { override fun updateModel() { println(\u0026#34;controller react to user behavior\u0026#34;) model.update() model.notifyView(view) } } class ModelImpl : Model { override fun update() { println(\u0026#34;model is updated\u0026#34;) saveToDatabase() } override fun saveToDatabase() { println(\u0026#34;change to model is saved to database\u0026#34;) } override fun notifyView(view: View) { println(\u0026#34;notify view\u0026#34;) view.updateGUI() } } fun MVCDemo() { val controller = ControllerImpl() controller.updateModel() } Interpreter 解释器\n给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子\n常被用在 SQL 解析、符号处理引擎\nabstract\ninterface Expression { fun interpret(context: String): Boolean } implementation\nclass AndExpression(val exp1: Expression, val exp2: Expression): Expression { override fun interpret(context: String): Boolean { return exp1.interpret(context)\u0026amp;\u0026amp;exp2.interpret(context) } } class OrExpression(val exp1: Expression, val exp2: Expression): Expression { override fun interpret(context: String): Boolean { return exp1.interpret(context)||exp2.interpret(context) } } class NotExpression(val exp: Expression): Expression { override fun interpret(context: String): Boolean { return !exp.interpret(context) } } class MetaExpression(val data: String): Expression { override fun interpret(context: String): Boolean { return context.contains(data) } } fun interpretDemo() { val rick = MetaExpression(\u0026#34;rick\u0026#34;) val morty = MetaExpression(\u0026#34;morty\u0026#34;) println(NotExpression(rick).interpret(\u0026#34;rick\u0026#34;)) println(AndExpression(rick,morty).interpret(\u0026#34;rick and morty\u0026#34;)) println(AndExpression(rick,NotExpression(morty)).interpret(\u0026#34;rick and morty\u0026#34;)) // false,true,f } Strategy 策略\n创建定义一系列的算法的各种策略对象和一个所执行算法随着策略对象改变而改变的 Context 对象，使得算法可以自由切换，且可独立于它的使用者变化\nvs. State Pattern\nabstract\ninterface Strategy { fun action() } interface StrategyChoice interface StrategyConsumer { var strategy: Strategy fun updateStrategy(strategyChoice: StrategyChoice) fun action() } implementation\nenum class Enemy: StrategyChoice{ ATTACK, MOVE, DEFENSE } //三种策略 class DefenseStrategy: Strategy { override fun action() { println(\u0026#34;enemy is attacking, take the defense strategy\u0026#34;) } } class MoveStrategy: Strategy { override fun action() { println(\u0026#34;enemy is defensing, take the move strategy\u0026#34;) } } class AttackStrategy: Strategy { override fun action() { println(\u0026#34;enemy is moving, take the attack strategy\u0026#34;) } } //战士根据敌人的动作切换策略 class Fighter(override var strategy: Strategy = DefenseStrategy()) : StrategyContext { override fun action(strategyChoice: StrategyChoice) { strategy = when (strategyChoice as Enemy) { Enemy.ATTACK -\u0026gt; DefenseStrategy() Enemy.DEFENSE -\u0026gt; MoveStrategy() Enemy.MOVE -\u0026gt; AttackStrategy() } strategy.action() println() } } fun strategyDemo() { val fighter = Fighter() fighter.action(Enemy.DEFENSE) fighter.action(Enemy.MOVE) fighter.action(Enemy.ATTACK) } Visitor 访问者\n为了访问比较复杂的数据结构，不去改变数据结构，而是把对数据的操作抽象出来，在“访问”的过程中以回调形式在访问者中处理操作逻辑。如果要新增一组操作，那么只需要增加一个新的访问者。\nabstract\ninterface Element { fun accept(visitor: Visitor): Unit { println(\u0026#34;universal element output\u0026#34;) visitor.visit(this) println(\u0026#34;\u0026#34;) } } interface Visitor { fun visit(element: Element): Unit { println(\u0026#34;universal visitor output\u0026#34;) } } implementation\nclass Meat: Element class Fruits: Element class Vegetable: Element class Vegetarian: Visitor { override fun visit(element: Element) { super.visit(element) println(\u0026#34;I am a vegetarian\u0026#34;) when(element){ is Meat -\u0026gt; println(\u0026#34;I hate meat\u0026#34;) is Fruits -\u0026gt; println(\u0026#34;I enjoy fruits\u0026#34;) is Vegetable -\u0026gt; println(\u0026#34;I love vegetable\u0026#34;) } } } class Carnivore: Visitor { override fun visit(element: Element) { super.visit(element) println(\u0026#34;I am a carnivore\u0026#34;) when(element){ is Meat -\u0026gt; println(\u0026#34;I love meat\u0026#34;) is Fruits -\u0026gt; println(\u0026#34;Fruits is Ok\u0026#34;) is Vegetable -\u0026gt; println(\u0026#34;I hate vegetable\u0026#34;) } } } fun demo() { val meat: Meat = Meat() val fruits: Fruits = Fruits() val vegetable: Vegetable = Vegetable() val vegetarian: Vegetarian = Vegetarian() val carnivore: Carnivore = Carnivore() meat.accept(carnivore) meat.accept(vegetarian) fruits.accept(carnivore) fruits.accept(vegetarian) vegetable.accept(carnivore) vegetable.accept(vegetarian) } 应该不考 Command 命令模式\n数据驱动\n以命令（Command）的形式包裹请求，并传给调用对象（Invoker）。调用对象寻找可以处理该命令的合适的对象（Receiver）来执行命令。\nabstract\ninterface Command { fun execute(receiver: Receiver) { receiver.exec() } } interface Receiver { fun exec() } interface Invoker { val commands: ArrayList\u0026lt;Command\u0026gt; fun addCommand(command: Command) { commands.add(command) } fun executeCommands() } implementation\nclass Gardening: Command class Cleaning: Command class Cleaner: Receiver { override fun exec() { println(\u0026#34;The house had been cleaned.\u0026#34;) } } class Gardener: Receiver { override fun exec() { println(\u0026#34;The garden had been taken care of.\u0026#34;) } } class Butler(override val commands: ArrayList\u0026lt;Command\u0026gt;) : Invoker { private val gardener = Gardener() private val cleaner = Cleaner() override fun executeCommands() { commands.forEach{ command -\u0026gt; when(command) { is Gardening -\u0026gt; command.execute(gardener) is Cleaning -\u0026gt; command.execute(cleaner) } println() } commands.clear() } } fun commandDemo() { val butler = Butler(ArrayList()) butler.addCommand(Gardening()) butler.addCommand(Cleaning()) butler.executeCommands() } State 将各状态对应逻辑分拆到不同的状态类中，以达到易于拓展新状态的目的。\n存在各种状态State的和一个管理状态切换的 Context\nabstract\ninterface State { fun action() } interface Context { var currentState: State fun action() { currentState.action() } } implementation\nclass QianJiSan(override var currentState: State) : Context class ShieldForm: State { override fun action() { println(\u0026#34;ShieldForm\u0026#34;) println(\u0026#34;defense\u0026#34;) } } class SwordForm: State { override fun action() { println(\u0026#34;SwordForm\u0026#34;) println(\u0026#34;fight\u0026#34;) } } fun stateDemo() { val swordForm = SwordForm() val shieldForm = ShieldForm() val qianJiSan = QianJiSan(swordForm) qianJiSan.action() qianJiSan.currentState = shieldForm qianJiSan.action() } Chain-of-responsibility 责任链\n使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。\n责任链模式（Chain of Responsibility）是一种处理请求的模式，它让多个处理器都有机会处理该请求，直到其中某个处理成功为止。责任链模式把多个处理器串成链，然后让请求在链上传递\n责任链模式（Chain of Responsibility Pattern）为请求创建了一个接收者对象的链。这种模式给予请求的类型，对请求的发送者和接收者进行解耦。这种类型的设计模式属于行为型模式。\n在这种模式中，通常每个接收者都包含对另一个接收者的引用。如果一个对象不能处理该请求，那么它会把相同的请求传给下一个接收者，依此类推。\nabstract\ninterface Handler { val nextHandler: Handler? val authority: Priority fun handle(priority: Priority) } interface Priority implementation\nenum class Vampire: Priority { Baron, Vicomte, Comte, Marquess, Duke, Infante } interface Priest: Handler { val name: String override val authority: Vampire override val nextHandler: Priest? override fun handle(priority: Priority) { if ((priority as Vampire)\u0026lt;=authority) { println(\u0026#34;$priority wad purified\u0026#34;) println() } else { println(\u0026#34;request ${nextHandler?.name?:\u0026#34;god\u0026#34;} to process\u0026#34;) nextHandler?.handle(priority) } } } //三种神职人员 class Monk(override val nextHandler: Priest, override val name: String = \u0026#34;monk\u0026#34;, override val authority: Vampire = Vampire.Vicomte) : Priest {} class Bishop(override val nextHandler: Priest, override val name: String = \u0026#34;bishop\u0026#34;, override val authority: Vampire = Vampire.Marquess) : Priest {} class Pope(override val name: String = \u0026#34;pope\u0026#34;, override val authority: Vampire = Vampire.Duke, override val nextHandler: Priest? = null) : Priest {} fun chainOfResponsibilityDemo(){ val pope = Pope() val bishop = Bishop(pope) val monk = Monk(bishop) monk.handle(Vampire.Duke) monk.handle(Vampire.Infante) } Iterator 迭代器\n提供一种无需对外暴露集合对象的内部表示就能就遍历集合对象元素的方法\n分离了集合对象的遍历行为，把遍历元素责任交给迭代器，而不是集合对象\nabstract\ninterface Iterator { fun hasNext(): Boolean fun next(): Any } interface Container { fun iterator(): Iterator } implementation\nclass Virtue: Container { val virtues: ArrayList\u0026lt;String\u0026gt; = arrayListOf(\u0026#34;Chastity\u0026#34;,\u0026#34;Diligence\u0026#34;,\u0026#34;Charity\u0026#34;,\u0026#34;Humility\u0026#34;,\u0026#34;Patience\u0026#34;,\u0026#34;Temperance\u0026#34;) inner class VirTueIterator: Iterator { var cur: Int = 0 override fun hasNext(): Boolean { return cur\u0026lt;virtues.size } override fun next(): String { return virtues[cur++] } } override fun iterator(): Iterator { return VirTueIterator() } } Mediator 中介\n用一个中介对象来处理不同类/对象之间的通信，把多方会谈变成双方会谈，以降低多个类之间的通信复杂性。\n中介者使各个类不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。\nabstract\ninterface Mediator { val communicators: ArrayList\u0026lt;Communicator\u0026gt; fun deliver(message: Message) } interface Message interface Communicator { val mediator: Mediator fun sendMessage(message: Message, receiver: Communicator) fun receiveMessage(message: Message, sender: Communicator) = {} } implementation\ndata class ChatMessage(val content: String) : Message class ChatRoom(override val communicators: ArrayList\u0026lt;Communicator\u0026gt; = ArrayList()) : Mediator { override fun deliver(message: Message) { println((message as ChatMessage).content) println() } } class ChatUser(val name: String, override val mediator: ChatRoom) : Communicator { override fun sendMessage(message: Message, receiver: Communicator) { println(\u0026#34;$name send a message to ${(receiver as ChatUser).name}\u0026#34;) mediator.deliver(message) } } Memento 备忘录\n在不破坏封装性的前提下，在某对象之外保存该对象的内部状态，以便在适当的时候恢复对象\nabstract\ninterface Originator { var states: States fun save(): Memonto fun load(memonto: Memonto) } interface States {} interface Memonto { val states: States fun load(): States {return states} } implementation\ndata class GameStates(val level: Int, val stage: Int): States class GameArchive(override val states: States) : Memonto class Game(override var states: States = GameStates(1,1)) : Originator { override fun save(): Memonto { println(\u0026#34;game archive is saved\u0026#34;) return GameArchive(states) } override fun load(memonto: Memonto) { states = memonto.states println(\u0026#34;game archive is loaded\u0026#34;) } } fun memontoDemo() { val game = Game() val archi = game.save() game.load(archi) } Template-method 模板方法\n一个抽象类/接口公开定义了执行它的方法的模板“骨架”。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。\nabstract\nabstract class Template { protected abstract fun first() protected abstract fun second() protected abstract fun last() final fun steps() { first() second() last() } } implementation\nclass Human: Template() { override fun first() { println(\u0026#34;birth\u0026#34;) } override fun second() { println(\u0026#34;aging\u0026#34;) } override fun last() { println(\u0026#34;death\u0026#34;) } } fun templateDemo() { val human = Human() human.steps() } 结构型模式 不仅仅简单地使用继承，而更多地通过组合与运行期的动态组合来实现更好、更灵活的结构与功能\nAdapter 适配器\n作为两个不兼容的接口之间的桥梁，将一个类的接口转换成客户希望的另外一个接口以结合了两个独立接口的功能，使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。\nabstract\ninterface Destination { fun func1() } interface Source { fun func2() } interface Adapter: Destination { val source: Source fun fun1() } implementation\nclass Wizard: Source { override fun func2() { println(\u0026#34;Attack Power\u0026#34;) } } open class Warrior: Destination { override fun func1() { println(\u0026#34;Attack Damage\u0026#34;) } } class WizardWarrior(override val source: Source) : Warrior(),Adapter { override fun func1() { super.func1() source.func2() } } Proxy 代理\n创建具有某对象的代理对象，以便代表改对象向外界提供功能接口。\nabstract\npublic interface Original { void action(); } interface Proxy: Original{ val original: Original override fun action(): Unit { original.action() } } implementation\nclass HttpService: Original { override fun action() { println(\u0026#34;HTTP Service offered\u0026#34;) } } class EvilHttpService(override val original: HttpService) : Proxy { private fun check() :Boolean { return (1..2).random()\u0026lt;2 } override fun action() { println(\u0026#34;HTTP request is censored\u0026#34;) val youAreAGoodPerson = check() if (youAreAGoodPerson) { original.action() }else { println(\u0026#34;your access is denied!\u0026#34;) } println() } } fun demo() { val httpService = EvilHttpService(HttpService()) httpService.action() } Decorator 装饰器\n作为现有的类的一个包装，向一个现有的对象添加新的功能，同时又不改变其结构，装饰和数据源类实现同一接口， 从而能在客户端代码中相互替换。\nabstract\ninterface Original { fun action(): Unit } interface Decorator: Original{ val original: Original fun before(): Unit {} fun after(): Unit {} override fun action(): Unit { before() original.action() after() println() } } implementation\nclass Mage: Original { override fun action() { println(\u0026#34;I am a mage\u0026#34;) } } class FireMage(override val original: Original): Decorator{ override fun after() { println(\u0026#34;A FireMage to be exact\u0026#34;) println(\u0026#34;Incendio\u0026#34;) } } class IceMage(override val original: Original): Decorator{ override fun after() { println(\u0026#34;A IceMage to be exact\u0026#34;) println(\u0026#34;freezing charm\u0026#34;) } } fun demo() { val fireMage = FireMage(Mage()) val iceMage = IceMage(Mage()) fireMage.action() iceMage.action() } 代理模式，注重对对象某一功能的流程把控和辅助。 它可以控制对象做某些事，重心是为了借用对象的功能完成某一流程，而非对象功能如何。\n装饰模式，注重对对象功能的扩展，它不关心外界如何调用，只注重对对象功能的加强，装饰后还是对象本身。\n应该不考 Bridge 桥接\n又称为柄体(Handle and Body)模式或接口(Interfce)模式\n将抽象接口作为抽象化和实现化之间的桥接结构，把抽象化与实现化解耦，使得实体类的功能独立于接口实现类，使得二者可以独立变化。避免有多种可能会变化的情况下直接继承带来的子类爆炸问题，提高扩展的灵活性。\nabstract\ninterface Bridge { fun func() } interface Entity { val bridge: Bridge fun func() } implementation\nabstract class Mecha(override val bridge: EnergyCore) : Entity { internal abstract fun statement() override fun func() { statement() bridge.func() println() } } //不同机甲型号可搭配不同能源核心，使得二者均易于拓展 class MechaI(core: EnergyCore): Mecha(bridge = core) { override fun statement() { println(\u0026#34;MechaI\u0026#34;) } } class MechaV(core: EnergyCore): Mecha(bridge = core) { override fun statement() { println(\u0026#34;MechaV\u0026#34;) } } class MechaX(core: EnergyCore): Mecha(bridge = core) { override fun statement() { println(\u0026#34;MechaX\u0026#34;) } } interface EnergyCore: Bridge class SolarEnergyCore: EnergyCore { override fun func() { println(\u0026#34;solar energy is transforming into power\u0026#34;) } } class WindEnergyCore: EnergyCore { override fun func() { println(\u0026#34;wind energy is transforming into power\u0026#34;) } } fun bridgeDemo() { MechaI(WindEnergyCore()).func() MechaI(SolarEnergyCore()).func() MechaV(WindEnergyCore()).func() MechaV(SolarEnergyCore()).func() MechaX(WindEnergyCore()).func() MechaX(SolarEnergyCore()).func() } Composite 组合\n又称为部分整体模式，依据树形结构来组合对象，创建了一个包含自己对象组的类，把一组相似的对象当作一个单一的对象处理。使得用户对单个对象和组合对象的使用具有一致性。\nabstract\ninterface Composite { val composites: ArrayList\u0026lt;Composite\u0026gt; } implementation\ninterface HTMLElement: Composite { val content: String fun toHTML() { composites.forEach { it -\u0026gt; (it as HTMLElement).toHTML() } } } class HTML(override val content: String = \u0026#34;\u0026#34;,override val composites: ArrayList\u0026lt;Composite\u0026gt; = ArrayList()) : HTMLElement { override fun toHTML() { println(\u0026#34;\u0026lt;html\u0026gt;\u0026#34;) super.toHTML() println(\u0026#34;\u0026lt;/html\u0026gt;\u0026#34;) } } class Body(override val content: String = \u0026#34;\u0026#34;,override val composites: ArrayList\u0026lt;Composite\u0026gt; = ArrayList()) : HTMLElement { override fun toHTML() { println(\u0026#34;\u0026lt;body\u0026gt;\u0026#34;) super.toHTML() println(\u0026#34;\u0026lt;/body\u0026gt;\u0026#34;) } } class H1(override val content: String = \u0026#34;\u0026#34;,override val composites: ArrayList\u0026lt;Composite\u0026gt; = ArrayList()) : HTMLElement { override fun toHTML() { println(\u0026#34;\u0026lt;h1\u0026gt; $content \u0026lt;/h1\u0026gt;\u0026#34;) } } class P(override val content: String = \u0026#34;\u0026#34;,override val composites: ArrayList\u0026lt;Composite\u0026gt; = ArrayList()) : HTMLElement { override fun toHTML() { println(\u0026#34;\u0026lt;p\u0026gt; $content \u0026lt;/p\u0026gt;\u0026#34;) } } fun compositeDemo() { val html = HTML() val body = Body() body.composites.add(H1(\u0026#34;header\u0026#34;)) body.composites.add(P(\u0026#34;a paragraph\u0026#34;)) body.composites.add(P(\u0026#34;another paragraph\u0026#34;)) html.composites.add(body) html.toHTML() } Facade 外观\n对外隐藏具有诸多子系统的系统的复杂性，只提供一个可以访问系统的接口\nabstract\ninterface Facade { val subsystems: ArrayList\u0026lt;Subsystem\u0026gt; fun react() } interface Subsystem{ fun react() } implementation\nclass ForeignAffairsMinistry: Subsystem{ override fun react() { println(\u0026#34;The Ministry of Foreign Affairs intervened\u0026#34;) } } class NationalDefenseMinistry: Subsystem{ override fun react() { println(\u0026#34;The Ministry of Defense conducts military mobilization\u0026#34;) } } class Government(override val subsystems: ArrayList\u0026lt;Subsystem\u0026gt; = ArrayList()) : Facade { private val foreignAffairsMinistry: ForeignAffairsMinistry = ForeignAffairsMinistry() private val nationalDefenseMinistry: NationalDefenseMinistry = NationalDefenseMinistry() override fun react() { if((1..2).random()\u0026lt;2){ foreignAffairsMinistry.react() }else nationalDefenseMinistry.react() } } Flyweight 通过工厂方法尝试重用现有的同类对象以减少创建对象的数量，以减少内存占用和提高性能，如果未找到匹配的对象，则创建新对象\n在实际应用中，享元模式主要应用于缓存\nvs. Singleton\n单例模式是类级别的，一个类只能有一个对象实例；\n享元模式是对象级别的，可以有多个对象实例，并允许多个变量引用同一个对象实例；\nabstract\ninterface FlyWeight interface FlyWeightEnum interface FlyWeightFactory { val map: Map\u0026lt;FlyWeightEnum,FlyWeight\u0026gt; fun getFlyWeight(flyWeightEnum: FlyWeightEnum): FlyWeight } implementation\nenum class FaithOfTheSeven: FlyWeightEnum { Father, Mother, Warrior, Smith, Maiden, Crone, Stranger } class Deity(private val priesthood: FaithOfTheSeven = FaithOfTheSeven.Father): FlyWeight { fun answerPrayer() { println(\u0026#34;$priesthood answered the prayer\u0026#34;) println() } } class Church(override val map: HashMap\u0026lt;FlyWeightEnum, Deity\u0026gt; = HashMap()) : FlyWeightFactory { override fun getFlyWeight(flyWeightEnum: FlyWeightEnum): Deity { if (!map.containsKey(flyWeightEnum)) { map[flyWeightEnum] = Deity(flyWeightEnum as FaithOfTheSeven) println(\u0026#34;$flyWeightEnum was awakened\u0026#34;) } return map.getOrDefault(flyWeightEnum, Deity()) } fun pray(flyWeightEnum: FlyWeightEnum) { println(\u0026#34;The believers prayed\u0026#34;) getFlyWeight(flyWeightEnum).answerPrayer() } } Design Pattern in Power Point duck 1004 strategy coffee 1025 decorator Software architecture 管道过滤器风格 pipes and filters\nFilter\nComponent\nPipe\nConnector\n主程序/子程序风格 Main program and subroutine\nHierarchical decomposition: Based on definition-use relations Definition-call Call-return Single thread of control Supported directly by programming languages 面向对象风格 Object-Oriented Organization\nEach object contains data and operations on these data.\nData within an object can only be accessed by methods of its inclosing object.\nObject is probably instantiated from a template (Class)\nfeature\nEncapsulation\nRestrict access to certain information\nPolymorphism\nChoose the method at run-time\nInheritance\ntake over (or inherit) attributes and behavior of the pre-existing classes\n分层的风格 Call and return systems\nMain program and subroutine OO Systems Hierarchical layer 基于共享数据的事件风格 Individual components announce event to communicate with others (Publish) Other components may register an interest in the kind of events (subscribe). They are invoked when the event appear PowerPoint 软件体系结构＝（元素，形态，基本理论）\n软件体系结构层次的设计主要包括以下方面：\n组成系统的构件描述 构件之间的交互 指导构件交互的模式 施加在模式上的约束 一个软件系统的体系结构定义了组成系统的\n构件（components）\n构件用于实施计算和保存状态\n连接件（connectors）\n连接件用于表达构件之间的关系\n和它们之间的匹配\n构件和连接件之间的匹 配表示了系统的拓扑结构\nComponents:\ndefine the locus of computation\nExamples: filters, databases, objects, clients/servers\nConnectors:\nmediate component interactions\nExamples: procedure call, pipes, event broadcast\nBatch Sequential Systems\nProcessing steps are independent programs Each step runs to completion before next step starts Data transmitted as a whole between steps Typical applications: classical data processing program development data flow System\nA data flow system is one in which the availability of data controls the computation the structure of the design is dominated by orderly motion of data from process to process the pattern of data flow is explicit In a pure data flow system, there is no other interaction between processes ","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%AC%94%E8%AE%B0/","title":"【设计模式】笔记"},{"content":"黑盒测试 黑盒测试是在程序接口进行的测试，它只检查程序功能是否能按照规格说明书的规定正常使用，程序是否能适当地接收输入数据产生正确的输出信息，并且保持外部信息的完整性。\n等价类划分 等价类是输入域的某个子集合，而所有的等价类的并集是整个输入域。在子集合中 ，各个输入数据对于揭露程序中的错误是等效的。\n边界值分析 边界值分析是等价类划分的一种变体和改良\n边界值分析利用输入变量的最小值（min）、略大于最小值（ min+）、输入值域内的任意值（nom）、略小于最大值（max- ）和最大值（max）来设计测试用例\n对于一个含有n个变量的程序，保留其中一个变量，让其 余的变量取正常值，被保留的变量依次取min、min+、 nom、max-、max值，对每个变量都重复进行，这样对于 一个n变量的程序，边界值分析测试程序会产生4n+1个测 试用例。\n健壮性测试\n健壮性测试是边界值分析测试的一种扩展，除了取5个边界值外 ，还需要考虑采用一个略超过最大值（max+）以及略小于最小 值（min-）的取值，检查超过极限值时系统的情况\n最坏情况测试\n拒绝边界值分析采用的可靠性理论的单缺陷假设，关心当多个变量取极值时会出现什么情况。 对每个变量，首先进行包含最小值、略高于最小值、正常值、略低于最大值和最大值五元素集合的测试，然后对这些集合进行笛卡尔积计算，以生成测试用例\n健壮最坏情况测试\n使用健壮性测试的七元素集合的笛卡尔积生成测试用例\n错误猜测法 依据经验猜测错误\n判定表 条件桩（Condition Stub）：列出了问题的所有条件。通常认为列出的条件的次序无关紧要。 动作桩（Action Stub）：列出了问题规定可能采取的操作。这些操作的排列顺序没有约束。 条件项（Condition Entry）：列出针对它所列条件的取值，在所有可能情况下的真假值。 动作项（Action Entry）：列出在条件项的各种取值情况下应采取的动作 简化\n因果图法 等价类划分方法和边界值分析法都是着重考虑输入条件， 并没有考虑到输入情况的各种组合，也没考虑到各个输入情况之间的相互制约关系；而因果图是一种挑选高效测试用例以检查组合输入条件的系统方法\n白盒测试 白盒测试是知道产品内部工作过程，可通过测试来检测产品内部动作是否按照规格说明书的规定正常进行\n静态白盒测试 代码检查\n人工+工具\n静态结构分析\n主要由软件工具自动进行\n软件质量度量\n主要由软件工具自动进行\n动态白盒测试 功能确认与接口分析 性能与效率分析 内存分析 代码跟踪 逻辑覆盖率分析 ","date":"2023-06-06T00:00:00Z","permalink":"https://dyhes.github.io/p/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E7%AC%94%E8%AE%B0/","title":"【软件测试】笔记"},{"content":"IPA short vowel\nlong vowel\nwhen native speakers talk, they don\u0026rsquo;t pause between words.\nLinking Linking two consonants when one word ends with a consonant sound and the next word starts with the same consonant, we link the sounds. But don\u0026rsquo;t link /tf/ and /dg/\nlinking similar consonants（voiced and unvoiced pair）\ndon\u0026rsquo;t change the mouse position\nLinking consonants to vowels link like consanant-vowel patten\npretend the consonants are the start of the second word\nLinking two vowels add /w/ or /j/ between the two vowel, choose the easiest\nStress words Syllable a syllable has one vowel sound (and only one vowel sound) and one or more consonants sounds.\nstressed If a word has two syllables or more, one syllable is always stressed: it has a strong, clear pronunciation.\nthe stressed syllable should be louder the stressed syllable should be a little higher the stressed syllable should be a little longer in time unstressed quieter, lower and shorter\nthe vowel sound in the unstressed syllable are reduced ti schwa sounds or short /I/ sounds.\nSentences unstressed auxiliary words（助动词）, articles（冠词） and prepositions（介词）\ngrammar words\nstressed nouns, verbs, adjectives（形容词）and adverbs（副词）\ncontext words\nWeak Form vowels of some grammar words became schwa\nthe last word of a sentence can\u0026rsquo;t be weak\nalways connected with words before and after it\nIntonation pattens Rising Falling Rising-falling Falling-rising Flat High Low rules Rule1\nFalling tone= end of sentence\nRising tone= sentence not finished\nRule2\nFalling tone = \u0026rsquo;new\u0026rsquo; information Rising tone = \u0026lsquo;old\u0026rsquo; infomation Rule3(Questions)\nFalling tone= you don\u0026rsquo;t know the answer Rising tone = you think you know the answer Rule4\nFalling tone = make a comment Rising tone = criticizing someone Rule5\nFalling tone= make a suggestion Rising tone = make a indirect suggestion Accent American \u0026lsquo;r\u0026rsquo; is pronounced when you have \u0026rsquo;t\u0026rsquo; sound between two vowel sounds, the \u0026rsquo;t\u0026rsquo; sound changes to a \u0026rsquo;d\u0026rsquo; sound. British words written with a vowel + \u0026lsquo;r\u0026rsquo;, the \u0026lsquo;r\u0026rsquo; is not normally pronounced. add \u0026lsquo;j\u0026rsquo; when \u0026lsquo;u\u0026rsquo; comes after certain consonants（\u0026rsquo;t\u0026rsquo;,‘d’,\u0026rsquo;n\u0026rsquo;） Contraction will \u0026rsquo;ll\nam \u0026rsquo;m\nare \u0026rsquo;re\nbefore vowel /shewa\nis \u0026rsquo;s has \u0026rsquo;s\nmost sound \u0026rsquo;s /z/\n/t/ /p/ /k/ /f/ /theta/ \u0026rsquo;s /s/\nwould \u0026rsquo;d had \u0026rsquo;d\nvowel+\u0026rsquo;d /d/\nname vowel or consonant+\u0026rsquo;d /schwa d/\nhave \u0026rsquo;ve\nvowel+\u0026rsquo;ve /v/\nconsonant+\u0026rsquo;ve /schwa v/\nnot n\u0026rsquo;t /nt/\nunwritten\ndid-\u0026gt; ‘d\nseveral continuous contraction\nthey\u0026rsquo;ll\u0026rsquo;ve\nElision vowel some vowels between consonants are not pronounced.\nconsonant in some cases, consonant sounds especially /t/ or /d/ are changed or not pronounced.\nif a word ends in a consonant plus \u0026rsquo;t\u0026rsquo; or \u0026rsquo;d\u0026rsquo;, and the next word start with a consonant, then the \u0026rsquo;t\u0026rsquo; or \u0026rsquo;d \u0026rsquo; either disappears or becomes glottal(put your tongue to the position but never release it) /h/ at the start of a sentence sentence remove \u0026lsquo;I\u0026rsquo;, \u0026lsquo;you\u0026rsquo;, \u0026lsquo;he\u0026rsquo; or auxiliary verbs from the start of a sentence or question\nSilence Letters k in kn p in ps h in honor b in mb n in mn Plural voiced /z/ unvoiced /s/ special case（[s z xu ju qu dj]） /Iz/ Passive voice voiced consonant, vowel, diphthong /d/ unvoiced /t/ /t/ or /d/ /Id/ ","date":"2023-04-26T00:00:00Z","permalink":"https://dyhes.github.io/p/ipapronounciation/","title":"【IPA】Pronounciation"},{"content":"left recursive grammar The biggest thing is the new adaptive parsing strategy, which lets us accept any grammar we care to write. That gives us a huge productivity boost because we can now write much more natural expression rules (which occur in almost every grammar).\nANTLR v4 will also take left recursive grammar now, translating it secretly to a non-left recursive version.\nGrammar Lexicon identifier Token names always start with a capital letter and so do lexer rules.\nParser rule names always start with a lowercase letter (those that fail Character.isUpperCase). The initial character can be followed by uppercase and lowercase letters, digits, and underscores.\nHere are some sample names:\nID, LPAREN, RIGHT_CURLY // token names/lexer rules expr, simpleDeclarator, d2, header_file // parser rule names literal ANTLR does not distinguish between character and string literals as most languages do. All literal strings one or more characters in length are enclosed in single quotes such as ';', 'if', '\u0026gt;=', and '\\'' (refers to the one-character string containing the single quote character).\nLiterals never contain regular expressions.\nLiterals can contain Unicode escape sequences of the form '\\uXXXX' (for Unicode code points up to 'U+FFFF') or '\\u{XXXXXX}' (for all Unicode code points), where 'XXXX' is the hexadecimal Unicode code point value.\nANTLR also understands the usual special escape sequences: '\\n' (newline), '\\r' (carriage return), '\\t' (tab), '\\b' (backspace), and '\\f' (form feed). You can use Unicode code points directly within literals or use the Unicode escape sequences.\nActions Actions are code blocks written in the target language.\nYou can use actions in a number of places within a grammar, but the syntax is always the same:\narbitrary text surrounded by curly braces.\nYou don’t need to escape a closing curly character if it’s in a string or comment\nIf the curlies are balanced, you also don’t need to escape }.\nOtherwise, escape extra curlies with a backslash: \\{ or \\}.\nEmbedded code can appear in: @header and @members named actions, parser and lexer rules, exception catching specifications, attribute sections for parser rules (return values, arguments, and locals), and some rule element options (currently predicates).\nThe only interpretation ANTLR does inside actions relates to grammar attributes; see Token Attributes and Chapter 10, Attributes and Actions.\nActions embedded within lexer rules are emitted without any interpretation or translation into generated lexers.\nKeywords Here’s a list of the reserved words in ANTLR grammars:\nimport, fragment, lexer, parser, grammar, returns, locals, throws, catch, finally, mode, options, tokens Also, although it is not a keyword, do not use the word rule as a rule name. Further, do not use any keyword of the target language as a token, label, or rule name. For example, rule if would result in a generated function called if. That would not compile obviously.\nGrammar Structure A grammar is essentially a grammar declaration followed by a list of rules, but has the general form:\n/** Optional javadoc style comment */ grammar Name; options {...} import ... ; tokens {...} channels {...} // lexer only @actionName {...} rule1 // parser and lexer rules, possibly intermingled ... ruleN You can specify options, imports, token specifications, and actions in any order.\nAll of those elements are optional except for the header grammar Name and at least one rule. Rules take the basic form:\nruleName : alternative1 | ... | alternativeN ; Grammars defined without a prefix on the grammar header are combined grammars that can contain both lexical and parser rules.\nOnly lexer grammars can contain mode specifications.\nOnly lexer grammars can contain custom channels specifications\nchannels { WHITESPACE_CHANNEL, COMMENTS_CHANNEL } Those channels can then be used like enums within lexer rules\nGrammar import Grammar imports let you break up a grammar into logical and reusable chunks. ANTLR treats imported grammars very much like object-oriented programming languages treat super-classes. A grammar inherits all of the rules, tokens specifications, and named actions from the imported grammar. Rules in the “main grammar” override rules from imported grammars to implement inheritance.\nTokens Section The purpose of the tokens section is to define token types needed by a grammar for which there is no associated lexical rule. The basic syntax is:\ntokens { Token1, ..., TokenN } Most of the time, the tokens section is used to define token types needed by actions in the grammar.\n// explicitly define keyword token types to avoid implicit definition warnings tokens { BEGIN, END, IF, THEN, WHILE } @lexer::members { // keywords map used in lexer to assign token types Map\u0026lt;String,Integer\u0026gt; keywords = new HashMap\u0026lt;String,Integer\u0026gt;() {{ put(\u0026#34;begin\u0026#34;, KeywordsParser.BEGIN); put(\u0026#34;end\u0026#34;, KeywordsParser.END); ... }}; } Action at the grammar level Currently there are only two defined named actions (for the Java target) used outside of grammar rules: header and members. The former injects code into the generated recognizer class file, before the recognizer class definition,\nand the latter injects code into the recognizer class definition, as fields and methods.\nFor combined grammars, ANTLR injects the actions into both the parser and the lexer. To restrict an action to the generated parser or lexer, use @parser::name or @lexer::name.\ngrammar Count; @header { package foo; } @members { int count = 0; } Parser Rules Parsers consist of a set of parser rules either in a parser or a combined grammar. A Java application launches a parser by invoking the rule function, generated by ANTLR, associated with the desired start rule.\nRules can also have alternatives separated by the |\noperator: stat: retstat | \u0026#39;break\u0026#39; \u0026#39;;\u0026#39; | \u0026#39;continue\u0026#39; \u0026#39;;\u0026#39; ; Alternatives are either a list of rule elements or empty. For example, here’s a rule with an empty alternative that makes the entire rule optional:\nsuperClass : \u0026#39;extends\u0026#39; ID | // empty ; Alternative labels Labeling Rule Alternatives for Precise Event Methods, we can get more precise parse-tree listener events by labeling the outermost alternatives of a rule using the ## operator. All alternatives within a rule must be labeled, or none of them.\nHere are two rules with labeled alternatives.\ngrammar T; stat: \u0026#39;return\u0026#39; e \u0026#39;;\u0026#39; ## Return | \u0026#39;break\u0026#39; \u0026#39;;\u0026#39; ## Break ; e : e \u0026#39;*\u0026#39; e ## Mult | e \u0026#39;+\u0026#39; e ## Add | INT ## Int ; Alternative labels do not have to be at the end of the line and there does not have to be a space after the ## symbol.\nANTLR generates a rule context class definition for each label. For example, here is the listener that ANTLR generates:\npublic interface AListener extends ParseTreeListener { void enterReturn(AParser.ReturnContext ctx); void exitReturn(AParser.ReturnContext ctx); void enterBreak(AParser.BreakContext ctx); void exitBreak(AParser.BreakContext ctx); void enterMult(AParser.MultContext ctx); void exitMult(AParser.MultContext ctx); void enterAdd(AParser.AddContext ctx); void exitAdd(AParser.AddContext ctx); void enterInt(AParser.IntContext ctx); void exitInt(AParser.IntContext ctx); } There are enter and exit methods associated with each labeled alternative. The parameters to those methods are specific to alternatives.\nYou can reuse the same label on multiple alternatives to indicate that the parse tree walker should trigger the same event for those alternatives.\ne : e \u0026#39;*\u0026#39; e ## BinaryOp | e \u0026#39;+\u0026#39; e ## BinaryOp | INT ## Int ; Rule Context Objects ANTLR generates methods to access the rule context objects (parse tree nodes) associated with each rule reference. For rules with a single rule reference, ANTLR generates a method with no arguments. Consider the following rule.\ninc : e \u0026#39;++\u0026#39; ; ANTLR generates this context class:\npublic static class IncContext extends ParserRuleContext { public EContext e() { ... } // return context object associated with e ... } ANTLR also provide support to access context objects when there is more than a single reference to a rule:\nfield : e \u0026#39;.\u0026#39; e ; ANTLR generates a method with an index to access the ith element as well as a method to get context for all references to that rule:\npublic static class FieldContext extends ParserRuleContext { public EContext e(int i) { ... } // get ith e context public List\u0026lt;EContext\u0026gt; e() { ... } // return ALL e contexts ... } If we had another rule, s, that references field, an embedded action could access the list of e rule matches performed by field:\ns : field { List\u0026lt;EContext\u0026gt; x = $field.ctx.e(); ... } ; Rule Element Labels = operator You can label rule elements using the = operator to add fields to the rule context objects:\nstat: \u0026#39;return\u0026#39; value=e \u0026#39;;\u0026#39; ## Return | \u0026#39;break\u0026#39; \u0026#39;;\u0026#39; ## Break ; Here value is the label for the return value of rule e, which is defined elsewhere. Labels become fields in the appropriate parse tree node class.\nIn this case, label value becomes a field in ReturnContext because of the Return alternative label:\npublic static class ReturnContext extends StatContext { public EContext value; ... } += operator It’s often handy to track a number of tokens, which you can do with the += “list label” operator. For example, the following rule creates a list of the Token objects matched for a simple array construct:\narray : \u0026#39;{\u0026#39; el+=INT (\u0026#39;,\u0026#39; el+=INT)* \u0026#39;}\u0026#39; ; ANTLR generates a List field in the appropriate rule context class:\npublic static class ArrayContext extends ParserRuleContext { public List\u0026lt;Token\u0026gt; el = new ArrayList\u0026lt;Token\u0026gt;(); ... } These list labels also work for rule references:\nelist : exprs+=e (\u0026#39;,\u0026#39; exprs+=e)* ; ANTLR generates a field holding the list of context objects:\npublic static class ElistContext extends ParserRuleContext { public List\u0026lt;EContext\u0026gt; exprs = new ArrayList\u0026lt;EContext\u0026gt;(); ... } Rule Elements Rule elements specify what the parser should do at a given moment just like statements in a programming language. The elements can be rule, token, string literal like expression, ID, and \u0026lsquo;return\u0026rsquo;. Here’s a complete list of the rule elements (we’ll look at actions and predicates in more detail later):\nSyntax Description T Match token T at the current input position. Tokens always begin with a capital letter. \u0026rsquo;literal\u0026rsquo; Match the string literal at the current input position. A string literal is simply a token with a fixed string. r Match rule r at current input position, which amounts to invoking the rule just like a function call. Parser rule names always begin with a lowercase letter. r [«args»] Match rule r at current input position, passing in a list of arguments just like a function call. The arguments inside the square brackets are in the syntax of the target language and are usually a comma-separated list of expressions. {«action»} Execute an action immediately after the preceding alternative element and immediately before the following alternative element. The action conforms to the syntax of the target language. ANTLR copies the action code to the generated class verbatim, except for substituting attribute and token references such as $x and $x.y. {«p»}? Evaluate semantic predicate «p». Do not continue parsing past a predicate if «p» evaluates to false at runtime. Predicates encountered during prediction, when ANTLR distinguishes between alternatives, enable or disable the alternative(s) surrounding the predicate(s). . Match any single token except for the end of file token. The “dot” operator is called the wildcard. When you want to match everything but a particular token or set of tokens, use the ~ “not” operator. This operator is rarely used in the parser but is available.\nSubrules A rule can contain alternative blocks called subrules (as allowed in Extended BNF Notation: EBNF). A subrule is like a rule that lacks a name and is enclosed in parentheses. Subrules can have one or more alternatives inside the parentheses. Subrules cannot define attributes with locals and returns like rules can. There are four kinds of subrules (x, y, and z represent grammar fragments):\nSyntax Description (x|y|z). Match any alternative within the subrule exactly once. Example: ` returnType : (type (x|y|z)? Match nothing or any alternative within subrule. Example: classDeclaration : 'class' ID (typeParameters)? ('extends' type)? ('implements' typeList)? classBody ; (x|y|z)* Match an alternative within subrule zero or more times. Example: annotationName : ID ('.' ID)* ; (x|y|z)+ Match an alternative within subrule one or more times. Example: annotations : (annotation)+ ; You can suffix the ?, *, and + subrule operators with the nongreedy operator, which is also a question mark: ??, *?, and +?.\nAs a shorthand, you can omit the parentheses for subrules composed of a single alternative with a single rule element reference. For example, annotation+ is the same as (annotation)+ and ID+ is the same as (ID)+. Labels also work with the shorthand. ids+=INT+ make a list of INT token objects.\nCatching Exceptions When a syntax error occurs within a rule, ANTLR catches the exception, reports the error, attempts to recover (possibly by consuming more tokens), and then returns from the rule. Every rule is wrapped in a try/catch/finally statement:\nvoid r() throws RecognitionException { try { rule-body } catch (RecognitionException re) { _errHandler.reportError(this, re); _errHandler.recover(this, re); } finally { exitRule(); } } We can use a strategy object to alter ANTLR’s error handling. Replacing the strategy changes the strategy for all rules, however. To alter the exception handling for a single rule, specify an exception after the rule definition:\nr : ... ; catch[RecognitionException e] { throw e; } That example shows how to avoid default error reporting and recovery. r rethrows the exception, which is useful when it makes more sense for a higher-level rule to report the error. Specifying any exception clause, prevents ANTLR from generating a clause to handle RecognitionException.\nYou can specify other exceptions as well:\nr : ... ; catch[FailedPredicateException fpe] { ... } catch[RecognitionException e] { ... } The code snippets inside curly braces and the exception “argument” actions must be written in the target language; Java, in this case. When you need to execute an action even if an exception occurs, put it into the finally clause:\nr : ... ; // catch blocks go first finally { System.out.println(\u0026#34;exit rule r\u0026#34;); } The finally clause executes right before the rule triggers exitRule before returning. If you want to execute an action after the rule finishes matching the alternatives but before it does its cleanup work, use an after action.\nHere’s a complete list of exceptions:\nException name Description RecognitionException The superclass of all exceptions thrown by an ANTLR-generated recognizer. It’s a subclass of RuntimeException to avoid the hassles of checked exceptions. This exception records where the recognizer (lexer or parser) was in the input, where it was in the ATN (internal graph data structure representing the grammar), the rule invocation stack, and what kind of problem occurred. NoViableAltException Indicates that the parser could not decide which of two or more paths to take by looking at the remaining input. This exception tracks the starting token of the offending input and also knows where the parser was in the various paths when the error occurred. LexerNoViableAltException The equivalent of NoViableAltException but for lexers only. InputMismatchException The current input Token does not match what the parser expected. FailedPredicateException A semantic predicate that evaluates to false during prediction renders the surrounding alternative nonviable. Prediction occurs when a rule is predicting which alternative to take. If all viable paths disappear, parser will throw NoViableAltException. This predicate gets thrown by the parser when a semantic predicate evaluates to false outside of prediction, during the normal parsing process of matching tokens and calling rules. Rule Attribute Definitions There are a number of action-related syntax elements associated with rules to be aware of. Rules can have arguments, return values, and local variables just like functions in a programming language. ANTLR collects all of the variables you define and stores them in the rule context object. These variables are usually called attributes.\nHere’s the general syntax showing all possible attribute definition locations:\nrulename[args] returns [retvals] locals [localvars] : ... ; The attributes defined within those [\u0026hellip;] can be used like any other variable. Here is a sample rule that copies parameters to return values:\n// Return the argument plus the integer value of the INT token add[int x] returns [int result] : \u0026#39;+=\u0026#39; INT {$result = $x + $INT.int;} ; The args, locals, and return [...] are generally in the target language but with some constraints. The [...] string is a comma-separated list of declarations either with prefix or postfix type notation or no-type notation. The elements can have initializer such as [int x = 32, float y] but don\u0026rsquo;t go too crazy as we are parsing this generic text manually in ScopeParser.\nrule-level named actions As with the grammar level, you can specify rule-level named actions. For rules, the valid names are init and after. As the names imply, parsers execute init actions immediately before trying to match the associated rule and execute after actions immediately after matching the rule.\nANTLR after actions do not execute as part of the finally code block of the generated rule function. Use the ANTLR finally action to place code in the generated rule function finally code block.\nThe actions come after any argument, return value, or local attribute definition actions. The row rule preamble from Section 10.2, Accessing Token and Rule Attributes illustrates the syntax nicely: actions/CSV.g4\n/** Derived from rule \u0026#34;row : field (\u0026#39;,\u0026#39; field)* \u0026#39;\\r\u0026#39;? \u0026#39;\\n\u0026#39; ;\u0026#34; */ row[String[] columns] returns [Map\u0026lt;String,String\u0026gt; values] locals [int col=0] @init { $values = new HashMap\u0026lt;String,String\u0026gt;(); } @after { if ($values!=null \u0026amp;\u0026amp; $values.size()\u0026gt;0) { System.out.println(\u0026#34;values = \u0026#34;+$values); } } : ... ; Rule row takes argument columns, returns values, and defines local variable col. The “actions” in square brackets are copied directly into the generated code. The generated rule functions also specify the rule arguments as function arguments, but they are quickly copied into the local RowContext object:\npublic class CSVParser extends Parser { ... public static class RowContext extends ParserRuleContext { public String [] columns; public Map\u0026lt;String,String\u0026gt; values; public int col=0; ... public final RowContext row(String [] columns) throws RecognitionException { RowContext _localctx = new RowContext(_ctx, 4, columns); enterRule(_localctx, RULE_row); ... } } ... } ANTLR tracks nested [...] within the action so that String[] columns is parsed properly. It also tracks angle brackets so that commas within generic type parameters do not signify the start of another attribute. Map\u0026lt;String,String\u0026gt; values is one attribute definition.\nThere can be multiple attributes in each action, even for return values. Use a comma to separate attributes within the same action:\na[Map\u0026lt;String,String\u0026gt; x, int y] : ... ; ANTLR interprets that action to define two arguments, x and y:\npublic final AContext a(Map\u0026lt;String,String\u0026gt; x, int y) throws RecognitionException { AContext _localctx = new AContext(_ctx, 0, x, y); enterRule(_localctx, RULE_a); ... } Start Rules and EOF A start rule is the rule engaged first by the parser; it’s the rule function called by the language application. Any rule in the grammar can act as a start rule.\nStart rules don’t necessarily consume all of the input. They consume only as much input as needed to match an alternative of the rule. For example, consider the following rule that matches one, two, or three tokens, depending on the input.\ns : ID | ID \u0026#39;+\u0026#39; | ID \u0026#39;+\u0026#39; INT ; Upon a+3, rule s matches the third alternative. Upon a+b, it matches the second alternative and ignores the final b token. Upon a b, it matches the first alternative, ignoring the b token. The parser does not consume the complete input in the latter two cases because rule s doesn’t explicitly say that end of file must occur after matching an alternative of the rule.\nOn the other hand, rules that describe entire input files should reference special predefined-token EOF. If they don’t, you might scratch your head for a while wondering why the start rule doesn’t report errors for any input no matter what you give it. Here’s a rule that’s part of a grammar for reading configuration files:\nconfig : element*; // can \u0026#34;match\u0026#34; even with invalid input. Invalid input would cause config to return immediately without matching any input and without reporting an error. Here’s the proper specification:\nfile : element* EOF; // don\u0026#39;t stop early. must match all input Left-recursive rules The most natural expression of some common language constructs is left recursive. Unfortunately, left recursive specifications of arithmetic expressions are typically ambiguous but much easier to write out than the multiple levels required in a typical top-down grammar. Here is a sample ANTLR 4 grammar with a left recursive expression rule:\nstat: expr \u0026#39;=\u0026#39; expr \u0026#39;;\u0026#39; // e.g., x=y; or x=f(x); | expr \u0026#39;;\u0026#39; // e.g., f(x); or f(g(x)); ; expr: expr \u0026#39;*\u0026#39; expr | expr \u0026#39;+\u0026#39; expr | expr \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; // f(x) | id ; In straight context free grammars, such a rule is ambiguous because 1+2*3 it can interpret either operator as occurring first, but ANTLR rewrites that to be non-left recursive and unambiguous using semantic predicates:\nexpr[int pr] : id ( {4 \u0026gt;= $pr}? \u0026#39;*\u0026#39; expr[5] | {3 \u0026gt;= $pr}? \u0026#39;+\u0026#39; expr[4] | {2 \u0026gt;= $pr}? \u0026#39;(\u0026#39; expr[0] \u0026#39;)\u0026#39; )* ; The predicates resolve ambiguities by comparing the precedence of the current operator against the precedence of the previous operator. An expansion of expr[pr] can match only those subexpressions whose precedence meets or exceeds pr.\nFormal rules The formal 4.0, 4.1 ANTLR left-recursion elimination rules were changed (simplified) for 4.2 and are laid out in the ALL(*) tech report:\nBinary expressions are expressions which contain a recursive invocation of the rule as the first and last element of the alternative. Suffix expressions contain a recursive invocation of the rule as the first element of the alternative, but not as the last element. Prefix expressions contain a recursive invocation of the rule as the last element of the alternative, but not as the first element. There is no such thing as a \u0026ldquo;ternary\u0026rdquo; expression\u0026ndash;they are just binary expressions in disguise.\nThe right associativity specifiers used to be on the individual tokens but it\u0026rsquo;s done on alternative basis anyway so the option is now on the individual alternative; e.g.,\ne : e \u0026#39;*\u0026#39; e | e \u0026#39;+\u0026#39; e |\u0026lt;assoc=right\u0026gt; e \u0026#39;?\u0026#39; e \u0026#39;:\u0026#39; e |\u0026lt;assoc=right\u0026gt; e \u0026#39;=\u0026#39; e | INT ; If your 4.0 or 4.1 grammar uses a right-associative ternary operator, you will need to update your grammar to include \u0026lt;assoc=right\u0026gt; on the alternative operator. To smooth the transition, \u0026lt;assoc=right\u0026gt; is still allowed on token references but it is ignored.\nActions and Attributes Actions are blocks of text written in the target language and enclosed in curly braces. The recognizer triggers them according to their locations within the grammar. For example, the following rule emits \u0026ldquo;found a decl\u0026rdquo; after the parser has seen a valid declaration:\ndecl: type ID \u0026#39;;\u0026#39; {System.out.println(\u0026#34;found a decl\u0026#34;);} ; type: \u0026#39;int\u0026#39; | \u0026#39;float\u0026#39; ; Most often, actions access the attributes of tokens and rule references:\ndecl: type ID \u0026#39;;\u0026#39; {System.out.println(\u0026#34;var \u0026#34;+$ID.text+\u0026#34;:\u0026#34;+$type.text+\u0026#34;;\u0026#34;);} | t=ID id=ID \u0026#39;;\u0026#39; {System.out.println(\u0026#34;var \u0026#34;+$id.text+\u0026#34;:\u0026#34;+$t.text+\u0026#34;;\u0026#34;);} ; Token Attributes All tokens have a collection of predefined, read-only attributes. The attributes include useful token properties such as the token type and text matched for a token. Actions can access these attributes via $label.attribute where label labels a particular instance of a token reference (a and b in the example below are used in the action code as $a and $b). Often, a particular token is only referenced once in the rule, in which case the token name itself can be used unambiguously in the action code (token INT can be used as $INT in the action). The following example illustrates token attribute expression syntax:\nr : INT {int x = $INT.line;} ( ID {if ($INT.line == $ID.line) ...;} )? a=FLOAT b=FLOAT {if ($a.line == $b.line) ...;} ; The action within the (...)? subrule can see the INT token matched before it in the outer level.\nBecause there are two references to the FLOAT token, a reference to $FLOAT in an action is not unique; you must use labels to specify which token reference you’re interested in.\nToken references within different alternatives are unique because only one of them can be matched for any invocation of the rule. For example, in the following rule, actions in both alternatives can reference $ID directly without using a label:\nr : ... ID {System.out.println($ID.text);} | ... ID {System.out.println($ID.text);} ; To access the tokens matched for literals, you must use a label:\nstat: r=\u0026#39;return\u0026#39; expr \u0026#39;;\u0026#39; {System.out.println(\u0026#34;line=\u0026#34;+$r.line);} ; Most of the time you access the attributes of the token, but sometimes it is useful to access the Token object itself because it aggregates all the attributes. Further, you can use it to test whether an optional subrule matched a token:\nstat: \u0026#39;if\u0026#39; expr \u0026#39;then\u0026#39; stat (el=\u0026#39;else\u0026#39; stat)? {if ( $el!=null ) System.out.println(\u0026#34;found an else\u0026#34;);} | ... ; $T and $L evaluate to Token objects for token name T and token label L. $ll evaluates to List\u0026lt;Token\u0026gt; for list label ll. $T.attr evaluates to the type and value specified in the following table for attribute attr:\nAttribute Type Description text String The text matched for the token; translates to a call to getText. Example: $ID.text. type int The token type (nonzero positive integer) of the token such as INT; translates to a call to getType. Example: $ID.type. line int The line number on which the token occurs, counting from 1; translates to a call to getLine. Example: $ID.line. pos int The character position within the line at which the token’s first character occurs counting from zero; translates to a call to getCharPositionInLine. Example: $ID.pos. index int The overall index of this token in the token stream, counting from zero; translates to a call to getTokenIndex. Example: $ID.index. channel int The token’s channel number. The parser tunes to only one channel, effectively ignoring off-channel tokens. The default channel is 0 (Token.DEFAULT_CHANNEL), and the default hidden channel is Token.HIDDEN_CHANNEL. Translates to a call to getChannel. Example: $ID.channel. int int The integer value of the text held by this token; it assumes that the text is a valid numeric string. Handy for building calculators and so on. Translates to Integer.valueOf(text-of-token). Example: $INT.int. Parser Rule Attributes ANTLR predefines a number of read-only attributes associated with parser rule references that are available to actions. Actions can access rule attributes only for references that precede the action. The syntax is $r.attr for rule name r or a label assigned to a rule reference. For example, $expr.text returns the complete text matched by a preceding invocation of rule expr:\nreturnStat : \u0026#39;return\u0026#39; expr {System.out.println(\u0026#34;matched \u0026#34;+$expr.text);} ; Using a rule label looks like this:\nreturnStat : \u0026#39;return\u0026#39; e=expr {System.out.println(\u0026#34;matched \u0026#34;+$e.text);} ; You can also use $ followed by the name of the attribute to access the value associated with the currently executing rule. For example, $start is the starting token of the current rule.\nreturnStat : \u0026#39;return\u0026#39; expr {System.out.println(\u0026#34;first token \u0026#34;+$start.getText());} ; $r and $rl evaluate to ParserRuleContext objects of type RContext for rule name r and rule label rl. $rll evaluates to List\u0026lt;RContext\u0026gt; for rule list label rll. $r.attr evaluates to the type and value specified in the following table for attribute attr:\nAttribute Type Description text String The text matched for a rule or the text matched from the start of the rule up until the point of the $text expression evaluation. Note that this includes the text for all tokens including those on hidden channels, which is what you want because usually that has all the whitespace and comments. When referring to the current rule, this attribute is available in any action including any exception actions. start Token The first token to be potentially matched by the rule that is on the main token channel; in other words, this attribute is never a hidden token. For rules that end up matching no tokens, this attribute points at the first token that could have been matched by this rule. When referring to the current rule, this attribute is available to any action within the rule. stop Token The last nonhidden channel token to be matched by the rule. When referring to the current rule, this attribute is available only to the after and finally actions. ctx ParserRuleContext The rule context object associated with a rule invocation. All of the other attributes are available through this attribute. For example, $ctx.start accesses the start field within the current rules context object. It’s the same as $start. parser Parser The parser itself. This attribute can be used, for example, to invoke a method defined in the parser\u0026rsquo;s @members section from a semantic predicate. Dynamically-Scoped Attributes You can pass information to and from rules using parameters and return values, just like functions in a general-purpose programming language. Programming languages don’t allow functions to access the local variables or parameters of invoking functions, however. For example, the following reference to local variable x form a nested method call is illegal in Java:\nvoid f() { int x = 0; g(); } void g() { h(); } void h() { int y = x; // INVALID reference to f\u0026#39;s local variable x } Variable x is available only within the scope of f, which is the text lexically delimited by curly brackets. For this reason, Java is said to use lexical scoping. Lexical scoping is the norm for most programming languages. Languages that allow methods further down in the call chain to access local variables defined earlier are said to use dynamic scoping. The term dynamic refers to the fact that a compiler cannot statically determine the set of visible variables. This is because the set of variables visible to a method changes depending on who calls that method.\nIt turns out that, in the grammar realm, distant rules sometimes need to communicate with each other, mostly to provide context information to rules matched below in the rule invocation chain. (Naturally, this assumes that you are using actions directly in the grammar instead of the parse-tree listener event mechanism.) ANTLR allows dynamic scoping in that actions can access attributes from invoking rules using syntax $r::x where r is a rule name and x is an attribute within that rule. It is up to the programmer to ensure that r is in fact an invoking rule of the current rule. A runtime exception occurs if r is not in the current call chain when you access $r::x.\nTo illustrate the use of dynamic scoping, consider the real problem of defining variables and ensuring that variables in expressions are defined. The following grammar defines the symbols attribute where it belongs in the block rule but adds variable names to it in rule decl. Rule stat then consults the list to see whether variables have been defined.\ngrammar DynScope; prog: block ; block /* List of symbols defined within this block */ locals [ List\u0026lt;String\u0026gt; symbols = new ArrayList\u0026lt;String\u0026gt;() ] : \u0026#39;{\u0026#39; decl* stat+ \u0026#39;}\u0026#39; // print out all symbols found in block // $block::symbols evaluates to a List as defined in scope {System.out.println(\u0026#34;symbols=\u0026#34;+$symbols);} ; /** Match a declaration and add identifier name to list of symbols */ decl: \u0026#39;int\u0026#39; ID {$block::symbols.add($ID.text);} \u0026#39;;\u0026#39; ; /** Match an assignment then test list of symbols to verify * that it contains the variable on the left side of the assignment. * Method contains() is List.contains() because $block::symbols * is a List. */ stat: ID \u0026#39;=\u0026#39; INT \u0026#39;;\u0026#39; { if ( !$block::symbols.contains($ID.text) ) { System.err.println(\u0026#34;undefined variable: \u0026#34;+$ID.text); } } | block ; ID : [a-z]+ ; INT : [0-9]+ ; WS : [ \\t\\r\\n]+ -\u0026gt; skip ; Here’s a simple build and test sequence:\n$ antlr4 DynScope.g4 $ javac DynScope*.java $ grun DynScope prog =\u0026gt; { =\u0026gt; int i; =\u0026gt; i = 0; =\u0026gt; j = 3; =\u0026gt; } =\u0026gt; EOF \u0026lt;= undefined variable: j symbols=[i] There’s an important difference between a simple field declaration in a @members action and dynamic scoping. symbols is a local variable and so there is a copy for each invocation of rule block. That’s exactly what we want for nested blocks so that we can reuse the same input variable name in an inner block. For example, the following nested code block redefines i in the inner scope. This new definition must hide the definition in the outer scope.\n{ int i; int j; i = 0; { int i; int x; x = 5; } x = 3; } Here’s the output generated for that input by DynScope:\n$ grun DynScope prog nested-input symbols=[i, x] undefined variable: x symbols=[i, j] Referencing $block::symbols accesses the symbols field of the most recently invoked block’s rule context object. If you need access to a symbols instance from a rule invocation farther up the call chain, you can walk backwards starting at the current context, $ctx. Use getParent to walk up the chain.\nParse Tree Listeners By default, ANTLR-generated parsers build a data structure called a parse tree or syntax tree that records how the parser recognized the structure of the input sentence and component phrases.\nThe interior nodes of the parse tree are phrase names that group and identify their children. The root node is the most abstract phrase name. The leaves of a parse tree are always the input tokens.\nParse trees sit between a language recognizer and an interpreter or translator implementation. They are extremely effective data structures because they contain all of the input and complete knowledge of how the parser grouped the symbols into phrases. Better yet, they are easy to understand and the parser generates them automatically (unless you turn them off with parser.setBuildParseTree(false)).\nBecause we specify phrase structure with a set of rules, parse tree subtree roots correspond to grammar rule names.\nANTLR has a ParseTreeWalker that knows how to walk these parse trees and trigger events in listener implementation objects that you can create. The ANTLR tool generates listener interfaces for you also, unless you turn that off with a commandline option. You can also have it generate visitors.\nFor example from a Java.g4 grammar, ANTLR generates:\npublic interface JavaListener extends ParseTreeListener\u0026lt;Token\u0026gt; { void enterClassDeclaration(JavaParser.ClassDeclarationContext ctx); void exitClassDeclaration(JavaParser.ClassDeclarationContext ctx); void enterMethodDeclaration(JavaParser.MethodDeclarationContext ctx); ... } where there is an enter and exit method for each rule in the parser grammar.\nANTLR also generates a base listener with empty implementations of all listener interface methods, in this case called JavaBaseListener. You can build your listener by subclassing this base and overriding the methods of interest.\nAssuming you\u0026rsquo;ve created a listener object called MyListener, here is how to call the Java parser and walk the parse tree:\nJavaLexer lexer = new JavaLexer(input); CommonTokenStream tokens = new CommonTokenStream(lexer); JavaParser parser = new JavaParser(tokens); JavaParser.CompilationUnitContext tree = parser.compilationUnit(); // parse a compilationUnit MyListener extractor = new MyListener(parser); ParseTreeWalker.DEFAULT.walk(extractor, tree); // initiate walk of tree with listener in use of default walker Listeners and visitors are great because they keep application-specific code out of grammars, making grammars easier to read and preventing them from getting entangled with a particular application.\nThe biggest difference between the listener and visitor mechanisms is that listener methods are called independently by an ANTLR-provided walker object, whereas visitor methods must walk their children with explicit visit calls. Forgetting to invoke visitor methods on a node’s children, means those subtrees don’t get visited.\nListening during the parse We can also use listeners to execute code during the parse instead of waiting for a tree walker walks the resulting parse tree. Let\u0026rsquo;s say we have the following simple expression grammar.\ngrammar CalcNoLR; s : expr EOF ; expr:\tadd ((MUL | DIV) add)* ; add : atom ((ADD | SUB) atom)* ; atom : INT ; INT : [0-9]+; MUL : \u0026#39;*\u0026#39;; DIV : \u0026#39;/\u0026#39;; ADD : \u0026#39;+\u0026#39;; SUB : \u0026#39;-\u0026#39;; WS : [ \\t]+ -\u0026gt; channel(HIDDEN); We can create a listener that executes during the parse by implementing the listener interface as before:\nclass CountListener extends CalcNoLRBaseListener { public int nums = 0; public boolean execExitS = false; @Override public void exitS(CalcNoLRParser.SContext ctx) { execExitS = true; } @Override public void exitAtom(CalcNoLRParser.AtomContext ctx) { nums++; } } And then passing it to addParseListener():\nString input = \u0026#34;2 + 8 / 2\u0026#34;; CalcNoLRLexer lexer = new CalcNoLRLexer(new ANTLRInputStream(input)); CalcNoLRParser parser = new CalcNoLRParser(new CommonTokenStream(lexer)); CountListener counter = new CountListener(); parser.addParseListener(counter); // Check that the purses valid first CalcNoLRParser.SContext context = parser.s(); String parseTreeS = context.toStringTree(parser); assertEquals(\u0026#34;(s (expr (add (atom 2) + (atom 8)) / (add (atom 2))) \u0026lt;EOF\u0026gt;)\u0026#34;, parseTreeS); assertEquals(3, counter.nums); assertEquals(true, counter.execExitS); One should not do very complicated work during the parse because the parser is throwing exception to handle syntax errors. If you\u0026rsquo;re complicated code throws different kind of exception it will screw up the parsing and things will go nuts. If you want to catch and properly handle exceptions in your listener code during the parse, you should override this method from Parser:\nprotected boolean listenerExceptionOccurred = false; /** * Notify any parse listeners of an exit rule event. * * @see #addParseListener */ @override protected void triggerExitRuleEvent() { if ( listenerExceptionOccurred ) return; try { // reverse order walk of listeners for (int i = _parseListeners.size() - 1; i \u0026gt;= 0; i--) { ParseTreeListener listener = _parseListeners.get(i); _ctx.exitRule(listener); listener.exitEveryRule(_ctx); } } catch (Throwable e) { // If an exception is thrown in the user\u0026#39;s listener code, we need to bail out // completely out of the parser, without executing anymore user code. We // must also stop the parse otherwise other listener actions will attempt to execute // almost certainly with invalid results. So, record the fact an exception occurred listenerExceptionOccurred = true; throw e; } } Now, if you throw an exception inside one of the listener methods:\n// Now throw an exception in the listener class ErrorListener extends CalcNoLRBaseListener { public boolean execExitS = false; public boolean execExitAtom = false; @Override public void exitS(CalcNoLRParser.SContext ctx) { execExitS = true; } @Override public void exitAtom(CalcNoLRParser.AtomContext ctx) { execExitAtom = true; throw new NullPointerException(\u0026#34;bail out\u0026#34;); } } then the exception will properly cause the parser to bailout and the exception will not be thrown out:\njava.lang.NullPointerException: bail out at org.antlr.v4.test.runtime.java.api.TestParseListener$2ErrorListener.exitAtom(TestParseListener.java:102) at org.antlr.v4.test.runtime.java.api.CalcNoLRParser$AtomContext.exitRule(CalcNoLRParser.java:311) at org.antlr.v4.runtime.Parser.triggerExitRuleEvent(Parser.java:412) at org.antlr.v4.runtime.Parser.exitRule(Parser.java:654) at org.antlr.v4.test.runtime.java.api.CalcNoLRParser.atom(CalcNoLRParser.java:336) at org.antlr.v4.test.runtime.java.api.CalcNoLRParser.add(CalcNoLRParser.java:261) at org.antlr.v4.test.runtime.java.api.CalcNoLRParser.expr(CalcNoLRParser.java:181) at org.antlr.v4.test.runtime.java.api.CalcNoLRParser.s(CalcNoLRParser.java:123) Semantic Predicates Semantic predicates, {...}?, are boolean expressions written in the target language that indicate the validity of continuing the parse along the path \u0026ldquo;guarded\u0026rdquo; by the predicate. Predicates can appear anywhere within a parser rule just like actions can, but only those appearing on the left edge of alternatives can affect prediction (choosing between alternatives). This section provides all of the fine print regarding the use of semantic predicates in parser and lexer rules. Let\u0026rsquo;s start out by digging deeper into how the parser incorporates predicates into parsing decisions.\nMaking Predicated Parsing Decisions ANTLR\u0026rsquo;s general decision-making strategy is to find all viable alternatives and then ignore the alternatives guarded with predicates that currently evaluate to false. (A viable alternative is one that matches the current input.) If more than one viable alternative remains, the parser chooses the alternative specified first in the decision.\nConsider a variant of C++ where array references also use parentheses instead of square brackets. If we only predicate one of the alternatives, we still have an ambiguous decision in expr:\nexpr: ID \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; // array reference (ANTLR picks this one) | {istype()}? ID \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; // ctor-style typecast | ID \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; // function call ; In this case, all three alternatives are viable for input x(i). When x is not a type name, the predicate evaluates to false, leaving only the first and third alternatives as possible matches for expr. ANTLR automatically chooses the first alternative matching the array reference to resolve the ambiguity. Leaving ANTLR with more than one viable alternative because of too few predicates is probably not a good idea. It\u0026rsquo;s best to cover n viable alternatives with at least n-1 predicates. In other words, don\u0026rsquo;t build rules like expr with too few predicates.\nSometimes, the parser finds multiple visible predicates associated with a single choice. No worries. ANTLR just combines the predicates with appropriate logical operators to conjure up a single meta-predicate on-the-fly.\nFor example, the decision in rule stat joins the predicates from both alternatives of expr with the || operator to guard the second stat alternative:\nstat: decl | expr ; decl: ID ID ; expr: {istype()}? ID \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; // ctor-style typecast | {isfunc()}? ID \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; // function call ; The parser will only predict an expr from stat when istype()||isfunc() evaluates to true. This makes sense because the parser should only choose to match an expression if the upcoming ID is a type name or function name. It wouldn\u0026rsquo;t make sense to just test one of the predicates in this case. Note that, when the parser gets to expr itself, the parsing decision tests the predicates individually, one for each alternative.\nIf multiple predicates occur in a sequence, the parser joins them with the \u0026amp;\u0026amp; operator. For example, consider changing stat to include a predicate before the call toexpr:\nstat: decl | {java5}? expr ; Now, the parser would only predict the second alternative if java5\u0026amp;\u0026amp;(istype()||isfunc()) evaluated to true.\nTurning to the code inside the predicates themselves now, keep in mind the following guidelines.\nEven when the parser isn\u0026rsquo;t making decisions, predicates can deactivate alternatives, causing rules to fail. This happens when a rule only has a single alternative. There is no choice to make, but ANTLR evaluates the predicate as part of the normal parsing process, just like it does for actions. That means that the following rule always fails to match.\nprog: {false}? \u0026#39;return\u0026#39; INT ; // throws FailedPredicateException ANTLR converts {false}? in the grammar to a conditional in the generated parser:\nif ( !false ) throw new FailedPredicateException(...); So far, all of the predicates we\u0026rsquo;ve seen have been visible and available to the prediction process, but that\u0026rsquo;s not always the case.\nFinding Visible Predicates The parser will not evaluate predicates during prediction that occur after an action or token reference. Let\u0026rsquo;s think about the relationship between actions and predicates first.\nANTLR has no idea what\u0026rsquo;s inside the raw code of an action and so it must assume any predicate could depend on side effects of that action. Imagine an action that computed value x and a predicate that tested x. Evaluating that predicate before the action executed to create x would violate the implied order of operations within the grammar.\nMore importantly, the parser can\u0026rsquo;t execute actions until it has decided which alternative to match. That\u0026rsquo;s because actions have side effects and we can\u0026rsquo;t undo things like print statements. For example, in the following rule, the parser can\u0026rsquo;t execute the action in front of the {java5}? predicate before committing to that alternative.\n@members {boolean allowgoto=false;} stat: {System.out.println(\u0026#34;goto\u0026#34;); allowgoto=true;} {java5}? \u0026#39;goto\u0026#39; ID \u0026#39;;\u0026#39; | ... ; If we can\u0026rsquo;t execute the action during prediction, we shouldn\u0026rsquo;t evaluate the {java5}? predicate because it depends on that action.\nThe prediction process also can\u0026rsquo;t see through token references. Token references have the side effect of advancing the input one symbol. A predicate that tested the current input symbol would find itself out of sync if the parser shifted it over the token reference. For example, in the following grammar, the predicates expect getCurrentToken to return an ID token.\nstat: \u0026#39;{\u0026#39; decl \u0026#39;}\u0026#39; | \u0026#39;{\u0026#39; stat \u0026#39;}\u0026#39; ; decl: {istype(getCurrentToken().getText())}? ID ID \u0026#39;;\u0026#39; ; expr: {isvar(getCurrentToken().getText())}? ID ; The decision in stat can\u0026rsquo;t test those predicates because, at the start of stat, the current token is a left curly. To preserve the semantics, ANTLR won\u0026rsquo;t test the predicates in that decision.\nVisible predicates are those that prediction encounters before encountering an action or token. The prediction process ignores nonvisible predicates, treating them as if they don\u0026rsquo;t exist.\nIn rare cases, the parser won\u0026rsquo;t be able to use a predicate, even if it\u0026rsquo;s visible to a particular decision. That brings us to our next fine print topic.\nUsing Context-Dependent Predicates A predicate that depends on a parameter or local variable of the surrounding rule, is considered a context-dependent predicate. Clearly, we can only evaluate such predicates within the rules in which they\u0026rsquo;re defined. For example, it makes no sense for the decision in prog below to test context-dependent predicate {$i\u0026lt;=5}?. That $i local variable is not even defined in prog.\nprog: vec5 | ... ; vec5 locals [int i=1] : ( {$i\u0026lt;=5}? INT {$i++;} )* // match 5 INTs ; ANTLR ignores context-dependent predicates that it can\u0026rsquo;t evaluate in the proper context. Normally the proper context is simply the rule defining the predicate, but sometimes the parser can\u0026rsquo;t even evaluate a context-dependent predicate from within the same rule! Detecting these cases is done on-the-fly at runtime during adaptive LL(*) prediction.\nFor example, prediction for the optional branch of the else subrule in stat below \u0026ldquo;falls off\u0026rdquo; the end of stat and continues looking for symbols in the invoking prog rule.\nprog: stat+ ; // stat can follow stat stat locals [int i=0] : {$i==0}? \u0026#39;if\u0026#39; expr \u0026#39;then\u0026#39; stat {$i=5;} (\u0026#39;else\u0026#39; stat)? | \u0026#39;break\u0026#39; \u0026#39;;\u0026#39; ; The prediction process is trying to figure out what can follow an if statement other than an else clause. Since the input can have multiple stats in a row, the prediction for the optional branch of the else subrule reenters stat. This time, of course, it gets a new copy of $i with a value of 0, not 5. ANTLR ignores context-dependent predicate {$i==0}? because it knows that the parser isn\u0026rsquo;t in the original stat call. The predicate would test a different version of $i so the parser can\u0026rsquo;t evaluate it.\nThe fine print for predicates in the lexer more or less follow these same guidelines, except of course lexer rules can\u0026rsquo;t have parameters and local variables. Let\u0026rsquo;s look at all of the lexer-specific guidelines in the next section.\nPredicates in Lexer Rules In parser rules, predicates must appear on the left edge of alternatives to aid in alternative prediction. Lexers, on the other hand, prefer predicates on the right edge of lexer rules because they choose rules after seeing a token\u0026rsquo;s entire text. Predicates in lexer rules can technically be anywhere within the rule. Some positions might be more or less efficient than others; ANTLR makes no guarantees about the optimal spot. A predicate in a lexer rule might be executed multiple times even during a single token match. You can embed multiple predicates per lexer rule and they are evaluated as the lexer reaches them during matching.\nLoosely speaking, the lexer\u0026rsquo;s goal is to choose the rule that matches the most input characters. At each character, the lexer decides which rules are still viable. Eventually, only a single rule will be still viable. At that point, the lexer creates a token object according the rule\u0026rsquo;s token type and matched text.\nSometimes the lexer is faced with more than a single viable matching rule. For example, input enum would match an ENUM rule and an ID rule. If the next character after enum is a space, neither rule can continue. The lexer resolves the ambiguity by choosing the viable rule specified first in the grammar. That\u0026rsquo;s why we have to place keyword rules before an identifier rule like this:\nENUM : \u0026#39;enum\u0026#39; ; ID : [a-z]+ ; If, on the other hand, the next character after input enum is a letter, then only ID is viable.\nPredicates come into play by pruning the set of viable lexer rules. When the lexer encounters a false predicate, it deactivates that rule just like parsers deactivate alternatives with false predicates.\nLike parser predicates, lexer predicates can\u0026rsquo;t depend on side effects from lexer actions. That said, the predicate can depend on a side effect of an action that occured during the recognition of the previous token. That\u0026rsquo;s because actions can only execute after the lexer positively identifies the rule to match. Since predicates are part of the rule selection process, they can\u0026rsquo;t rely on action side effects created by actions in currently-prospective rules. Lexer actions must appear after predicates in lexer rules. As an example, here\u0026rsquo;s another way to match enum as a keyword in the lexer:\nENUM: [a-z]+ {getText().equals(\u0026#34;enum\u0026#34;)}? {System.out.println(\u0026#34;enum!\u0026#34;);} ; ID : [a-z]+ {System.out.println(\u0026#34;ID \u0026#34;+getText());} ; The print action in ENUM appears last and executes only if the current input matches [a-z]+ and the predicate is true. Let\u0026rsquo;s build and test Enum3 to see if it distinguishes between enum and an identifier:\n$ antlr4 Enum3.g4 $ javac Enum3.java $ grun Enum3 tokens =\u0026gt; enum abc =\u0026gt; EOF \u0026lt;= enum! ID abc That works great, but it\u0026rsquo;s really just for instructional purposes. It\u0026rsquo;s easier to understand and more efficient to match enum keywords with a simple rule like this:\nENUM : \u0026#39;enum\u0026#39; ; Here\u0026rsquo;s another example of a predicate. It\u0026rsquo;s important to note that the predicate is evaluated before the action because actions are only executed if the lexer rule matches. The actions are not executed in line; they are collected and executed en mass later.\nINDENT : [ \\t]+ {System.out.println(\u0026#34;INDENT\u0026#34;)\u0026gt;} {this.getCharPositionInLine()==0}? ; For more information on how actions and predicates operate in the lexer, see Lexer actions and semantic predicates are executed out of order and Lexer.getCharIndex() return value not behaving as expected. The lexer rule that will not work as expected is:\nStuff : ( \u0026#39;a\u0026#39;+ {count++;} | \u0026#39;b\u0026#39;) \u0026#39;c\u0026#39; \u0026#39;d\u0026#39; {count == 3}? ; The count++ code we\u0026rsquo;ll not execute until after Stuff has been recognized (assuming count!=3).\n华为云 一般如果语法非常复杂，会基于Lexer和Parser写到两个不同的文件中（例如Java，可参考：https://github.com/antlr/grammars-v4/tree/master/java/java8），如果语法比较简单，可以只写到一个文件中（例如Lua，可参考：https://github.com/antlr/grammars-v4/blob/master/lua/Lua.g4）。\nAntlr4整体结构如下：\n/** Optional javadoc style comment */ grammar Name; options {...} import ... ; tokens {...} channels {...} // lexer only @actionName {...} rule1 // parser and lexer rules, possibly intermingled ... ruleN 替代标签\n通过在语句后面，添加 #替代标签，可以将语句转换为这些替代标签，从而加以区分\n结合性\n默认情况下，ANTLR从左到右结合运算符，然而某些像指数群这样的运算符则是从右到左。可以使用选项assoc手动指定运算符记号上的相关性。\n实际上，Antlr4 已经对一些常用的操作符的优先级进行了处理，例如加减乘除等，这些就不需要再特殊处理\n通道\n很多信息，例如注释、空格等，是结果信息生成不需要处理的，但是我们又不适合直接丢弃，安全地忽略掉注释和空格的方法是把这些发送给语法分析器的记号放到一个“隐藏通道”中，语法分析器仅需要调协到单个通道即可。我们可以把任何我们想要的东西传递到其它通道中。\n","date":"2023-04-13T00:00:00Z","permalink":"https://dyhes.github.io/p/antlrbasics/","title":"【ANTLR】Basics"},{"content":"What is Docker Docker is an open source platform for building, deploying, and managing containerized applications.\nIt enables developers to package applications into containers—standardized executable components combining application source code with the operating system (OS) libraries and dependencies required to run that code in any environment.\nDevelopers can create containers without Docker, but the platform makes it easier, simpler, and safer to build, deploy and manage containers.\nDocker is essentially a toolkit that enables developers to build, deploy, run, update, and stop containers using simple commands and work-saving automation through a single API.\nContainers are made possible by process isolation and virtualization capabilities built into the Linux kernel.\nDocker tools DockerFile Every Docker container starts with a simple text file containing instructions for how to build the Docker container image. DockerFile automates the process of Docker image creation. It’s essentially a list of command-line interface (CLI) instructions that Docker Engine will run in order to assemble the image.\nDocker images Docker images contain executable application source code as well as all the tools, libraries, and dependencies that the application code needs to run as a container.\nWhen you run the Docker image, it becomes one instance (or multiple instances) of the container.\nMultiple Docker images can be created from a single base image, and they’ll share the commonalities of their stack.\nDocker images are made up of layers, and each layer corresponds to a version of the image. Whenever a developer makes changes to the image, a new top layer is created, and this top layer replaces the previous top layer as the current version of the image. Previous layers are saved for rollbacks or to be re-used in other projects.\nEach time a container is created from a Docker image, yet another new layer called the container layer is created. Changes made to the container—such as the addition or deletion of files—are saved to the container layer only and exist only while the container is running.\nThis iterative image-creation process enables increased overall efficiency since multiple live container instances can run from just a single base image, and when they do so, they leverage a common stack.\nDocker containers Docker containers are the live, running instances of Docker images. While Docker images are read-only files, containers are live, ephemeral, executable content. Users can interact with them, and administrators can adjust their settings and conditions using docker commands.\nDocker Hub Docker Hub is the public repository of Docker images It includes images that have been produced by Docker, Inc., certified images belonging to the Docker Trusted Registry, and many thousands of other images.\nAll Docker Hub users can share their images at will. They can also download predefined base images from the Docker filesystem to use as a starting point for any containerization project.\nDocker daemon Docker daemon is a service running on your operating system This service creates and manages your Docker images for you using the commands from the client, acting as the control center of your Docker implementation.\nDocker registry A Docker registry is a scalable open-source storage and distribution system for docker images. The registry enables you to track image versions in repositories, using tagging for identification. This is accomplished using git, a version control tool.\nDocker Compose If you’re building an application out of processes in multiple containers that all reside on the same host, you can use Docker Compose to manage the application’s architecture. Docker Compose creates a YAML file that specifies which services are included in the application and can deploy and run containers with a single command. Using Docker Compose, you can also define persistent volumes for storage, specify base nodes, and document and configure service dependencies.\nKubernetes To monitor and manage container lifecycles in more complex environments, you’ll need to turn to a container orchestration tool. While Docker includes its own orchestration tool (called Docker Swarm), most developers choose Kubernetes instead.\nKubernetes is an open-source container orchestration platform descended from a project developed for internal use at Google. Kubernetes schedules and automates tasks integral to the management of container-based architectures, including container deployment, updates, service discovery, storage provisioning, load balancing, health monitoring, and more. In addition, the open source ecosystem of tools for Kubernetes—including Istio and Knative—enables organizations to deploy a high-productivity Platform-as-a-Service (PaaS) for containerized applications and a faster on-ramp to serverless computing.\n","date":"2023-03-21T00:00:00Z","permalink":"https://dyhes.github.io/p/dockerintroduction/","title":"【Docker】Introduction"},{"content":"Please Check:\nhttps://www.cnblogs.com/heslin/p/16880765.html\n","date":"2022-11-11T00:00:00Z","permalink":"https://dyhes.github.io/p/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%A6%82%E8%BF%B0/","title":"【知识图谱】概述"},{"content":"Introduction Nginx is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache.\nNginx是可以用于反向代理，负载均衡，邮件代理和HTTP缓存的Web服务器\nInstallation ubuntu:\nsudo apt install nginx 查看状态\nsystemctl status nginx 管理\nsudo systemctl stop nginx sudo systemctl start nginx sudo systemctl restart nginx sudo systemctl reload nginx ##configuration changed sudo nginx -s reload sudo systemctl disable nginx ##disable the behavior of starting automatically when the server boots sudo systemctl enable nginx Terminology context\nevents { ... } directive\nworker_connections 768; context内的键值对\nBugs css text/plain NGINX: CSS was not loaded because its MIME type, \u0026ldquo;text/plain\u0026rdquo;, is not \u0026ldquo;text/css\u0026rdquo;\n修改为\n后仍未加载\n原因为浏览器缓存\nCtrl+Shift+R hard reload 解决问题\n","date":"2022-11-09T00:00:00Z","image":"https://dyhes.github.io/p/nginx%E6%B5%85%E5%B0%9D/cover_hu4946912959379683430.png","permalink":"https://dyhes.github.io/p/nginx%E6%B5%85%E5%B0%9D/","title":"【NGINX】浅尝"},{"content":"RePost from :\n一文教你搞定所有前端鉴权与后端鉴权方案，让你不再迷惘 - 掘金 (juejin.cn)\nTerminology 认证(Identification) ：根据声明者所特有的识别信息，确认声明者的身份。 身份证，用户名和密码等\n授权(Authorization)： 在信息安全领域是指资源所有者委派执行者，赋予执行者指定范围的资源操作权限，以便对资源的相关操作。\n现实生活领域： 银行卡（由银行派发）、门禁卡（由物业管理处派发）、钥匙（由房东派发）\n互联网领域： web 服务器的 session 机制、web 浏览器的 cookie 机制、颁发授权令牌（token）等\n鉴权(Authentication)： 在信息安全领域是指对于一个声明者所声明的身份权利，对其所声明的真实性进行鉴别确认的过程。授权和鉴权是两个上下游相匹配的关系，先授权，后鉴权。\n现实生活领域： 门禁卡需要通过门禁卡识别器，银行卡需要通过银行卡识别器；\n互联网领域： 校验 session/cookie/token 的合法性和有效性\n权限控制(Access/Permission Control)： 将可执行的操作定义为权限列表，然后判断操作是否允许/禁止 对于权限控制，可以分为两部分进行理解：一个是权限，另一个是控制。权限是抽象的逻辑概念，而控制是具体的实现方式。\n现实生活领域： 门禁卡的权限\n互联网领域： 通过 web 后端服务，控制接口访问，允许或拒绝访问请求\n关系 前后依次发生\n使用门禁卡开门： 认证、授权、鉴权、权限控制四个环节一气呵成，在瞬间同时发生\n用户的网站登录： 用户在使用用户名和密码进行登录时，认证和授权两个环节一同完成，而鉴权和权限控制则发生在后续的请求访问中，比如在选购物品或支付时。\nHTTP基本鉴权 在 HTTP 中，基本认证方案（Basic Access Authentication) 是允许客户端（通常指的就是网页浏览器）在请求时，通过用户提供用户名和密码的方式，实现对用户身份的验证。\n几乎所有网站都不会使用此认证方案\n缺点 不安全 无法主动注销 Session-Cookie 鉴权 Session-Cookie 认证是利用服务端的 Session和 客户端 的 Cookie 来实现的前后端通信认证模式。\nCookie HTTP 无状态（对于事务处理没有记忆能力，每次客户端和服务端会话完成时，服务端不会保存任何会话信息）\n为了让服务器区分不同的客户端，服务端就必须主动的去维护一个状态，这个状态用于告知服务端前后两个请求是否来自同一浏览器。而这个状态可以通过 Cookie 去实现。\n特点：\n存储在客户端，可随意篡改，不安全 有大小限制，最大为 4kb 有数量限制，一般一个浏览器对于一个网站只能存不超过 20 个 Cookie，浏览器一般只允许存放 300个 Cookie Android 和 IOS 对 Cookie 支持性不好 不可跨域 Session 无状态协议通信过程中，为了实现中断/继续操作，将用户和服务器之间的交互进行的一种抽象；\n具体来说，是服务器生成的一种具有特定结构的数据，可以通过多种方式保存，如内存、数据库、文件等，大型网站一般有专门的 Session 服务器集群来保存用户会话\n原理流程：\n客户端： 用户向服务器首次发送请求； 服务器： 接收到数据并自动为该用户创建特定的 Session / Session ID，来标识用户并跟踪用户当前的会话过程； 客户端： 浏览器收到响应获取会话信息，并且会在下一次请求时带上 Session / Session ID； 服务器： 服务器提取后会与本地保存的 Session ID进行对比找到该特定用户的会话，进而获取会话状态； 至此客户端与服务器的通信变成有状态的通信； 特点：\nSession 保存在服务器上； 通过服务器自带的加密协议进行； 与 Cookie 的差异：\n安全性： Cookie 由于保存在客户端，可随意篡改，Session 则不同存储在服务器端，无法伪造，所以 Session 的安全性更高； 存取值的类型不同： Cookie 只支持字符串数据，Session 可以存任意数据类型； 有效期不同： Cookie 可设置为长时间保持，Session 一般失效时间较短； 存储大小不同： Cookie 保存的数据不能超过 4K； Session-Cookie ：将 Session 存储在客户端的 Cookie 中\nPros\nCookie 简单易用 Session 数据存储在服务端，相较于 JWT 方便进行管理，也就是当用户登录和主动注销，只需要添加删除对应的 Session 就可以了，方便管理 只需要后端操作即可，前端可以无感等进行操作 Cons\n依赖 Cookie(用户可能在浏览器端禁用 Cookie） 非常不安全，Cookie 将数据暴露在浏览器中，增加了数据被盗的风险（容易被 CSRF 等攻击） Session 存储在服务端（Redis），增大了服务端的开销，用户量大的时候会大大降低服务器性能 对移动端的支持性不友好 前端常用库\n使用 express：express-session 使用 koa：koa-session Token 鉴权 避免Session-Cookie 缺点\nToken Token 是一个令牌，客户端访问服务器时，验证通过后服务端会为其签发一张令牌，之后，客户端就可以携带令牌访问服务器，服务端只需要验证令牌的有效性即可。\n即：访问资源接口（API）时所需要的资源凭证\n一般 Token 的组成：uid (用户唯一的身份标识) + time (当前时间的时间戳) + sign (签名，Token 的前几位以哈希算法压缩成的一定长度的十六进制字符串)\nPros\n服务端无状态化、可扩展性好： Token 机制在服务端不需要存储会话（Session）信息，因为 Token 自身包含了其所标识用户的相关信息，这有利于在多个服务间共享用户状态 支持 APP 移动端设备； 安全性好： 有效避免 CSRF 攻击（因为不需要 Cookie） 支持跨程序调用： 因为 Cookie 是不允许跨域访问的，而 Token 则不存在这个问题 Cons\n配合： 需要前后端配合处理； 占带宽： 正常情况下比 sid 更大，消耗更多流量，挤占更多宽带 性能问题： 虽说验证 Token 时不用再去访问数据库或远程服务进行权限校验，但是需要对 Token 加解密等操作，所以会更耗性能； 有效期短： 为了避免 Token 被盗用，一般 Token 的有效期会设置的较短，需要 Refresh Token Refresh Token 业务接口用来鉴权的 Token，称为 Access Token。\n为了安全， Access Token 有效期一般设置较短，以避免被盗用。但过短的有效期会造成 Access Token 经常过期\n解决办法：\n刷新 Access Token，让用户重新登录获取新 Token，会很麻烦； 引入一个专门生成 Access Token 的 Token，称为 Refresh Token 区别\nAccess Token： 用来访问业务接口，由于有效期足够短，盗用风险小，也可以使请求方式更宽松灵活； Refresh Token： 用来获取 Access Token，有效期可以长一些，通过独立服务和严格的请求方式增加安全性；由于不常验证，也可以如前面的 Session 一样处理； 与 Session-Cookie 的区别 Session-Cookie 和 Token 有很多类似的地方， Token 像是 Session-Cookie 的升级改良版，存在以下区别\n存储地不同： Session 一般是存储在服务端；Token 是无状态的，一般由前端存储； 安全性不同： Session 和 Token 并不矛盾，作为身份认证 Token 安全性比 Session 好，因为每一个请求都有签名还能防止监听以及重放攻击； 支持性不同： Session-Cookie 认证需要靠浏览器的 Cookie 机制实现，如果遇到原生 NativeAPP 时这种机制就不起作用了，或是浏览器的 Cookie 存储功能被禁用，也是无法使用该认证机制实现鉴权的；而 Token 验证机制丰富了客户端类型。 如果用户数据可能需要和第三方共享，或者允许第三方调用 API 接口，则应使用 Token\nJWT（JSON Web Token）鉴权 服务端验证客户端发送过来的 Token 时，还需要查询数据库获取用户基本信息，然后验证 Token 是否有效\n每次请求验证都要查询数据库，增加了查库带来的延迟等性能消耗；\n业界常用解决方案： JWT\nJWT JWT 是 Auth0 提出的通过 对 JSON 进行加密签名来实现授权验证的方案\n就是登录成功后将相关用户信息组成 JSON 对象，然后对这个对象进行某种方式的加密，返回给客户端； 客户端在下次请求时带上这个 Token； 服务端再收到请求时校验 token 合法性，其实也就是在校验请求的合法性。\n组成\n它是一个很长的字符串，中间用点（.）分隔成三个部分\nHeader\n通常包括两部分\ntyp：代表 Token 的类型，这里使用的是 JWT 类型； alg：使用的 Hash 算法，例如 HMAC SHA256 或 RSA. Payload\n包含一些声明 Claim (实体的描述，通常是一个 User 信息，还包括一些其他的元数据) ，用来存放实际需要传递的数据\nJWT 规定了7个官方字段：\niss (issuer)：签发人 exp (expiration time)：过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号 除了官方字段，还可以在这个部分定义私有字段\nSignature\nSignature 部分是对前两部分的签名，防止数据篡改。\n首先，需要指定一个密钥（secret）。这个密钥只有服务器才知道，不能泄露给用户。然后，使用 Header 里面指定的签名算法（默认是 HMAC SHA256）产生签名。\n使用方式\n客户端收到服务器返回的 JWT，可以储存在 Cookie 里面，也可以储存在 localStorage。\n此后，客户端每次与服务器通信，都要带上这个 JWT。\n可以把它放在 Cookie 里面自动发送，但是这样不能跨域，所以更好的做法是放在 HTTP 请求的头信息Authorization字段里面。\nAuthorization: Bearer \u0026lt;token\u0026gt; Pros\n不需要在服务端保存会话信息（RESTful API 的原则之一就是无状态），所以易于应用的扩展，即信息不保存在服务端，不会存在 Session 扩展不方便的情况 JWT 中的 Payload 负载可以存储常用信息，用于信息交换，有效地使用 JWT，可以降低服务端查询数据库的次数 Cons\n加密问题： JWT 默认是不加密，但也是可以加密的。生成原始 Token 以后，可以用密钥再加密一次。 到期问题： 由于服务器不保存 Session 状态，因此无法在使用过程中废止某个 Token，或者更改 Token 的权限。也就是说，一旦 JWT 签发了，在到期之前就会始终有效，除非服务器部署额外的逻辑。 前端常用库\n使用 express：express-jwt 使用 koa：koa-jwt 单点登录（Single Sign On） 避免大型系统内子系统的重复登录，只需要登录一次，就可以访问其他相互信任的应用系统\n例如\n登录天猫，淘宝也会自动登录； 登录百度贴吧，百度网盘也会自动登录； 同域SSO 当百度网站存在两个相同主域名下的贴吧子系统 tieba.baidu.com 和网盘子系统 pan.baidu.com 时，以下为他们实现 SSO 的步骤：\n客户端： 用户访问某个子系统时（例如 tieba.baidu.com），如果没有登录，则跳转至 SSO 认证中心提供的登录页面进行登录； 服务端： 登录认证后，服务端把登录用户的信息存储于 Session 中，并且附加在响应头的 Set-Cookie 字段中，设置 Cookie 的 Domain 为 .baidu.com ； 客户端：再次发送请求时，携带主域名 Domain 下的 Cookie 给服务器，此时服务端就可以通过该 Cookie 来验证登录状态了； 亦即 Session-Cookie 认证\n异域SSO 如天猫 (tmall.com) 和 淘宝 (taobao.com)\n利用 CAS(Central Authentication Service)中央授权服务\n客户端： 开始访问系统 A；\n系统 A： 发现用户未登录，重定向至 CAS 认证服务（sso.com），同时 URL 地址参数携带登录成功后回跳到系统 A 的页面链接\nCAS 认证服务： 发现请求 Cookie 中没有携带登录的票据凭证（TGC），所以 CAS 认证服务判定用户处于 未登录 状态，重定向用户页面至 CAS 的登录界面，用户在 CAS 的登录页面上进行登录操作。\n客户端： 输入用户名密码进行 CAS 系统认证；\nCAS 认证服务： 校验用户信息，并且 生成 TGC 放入自己的 Session 中，同时以 Set-Cookie 形式写入 Domain 为 sso.com 的域下 ；同时生成一个 授权令牌 ST (Service Ticket) ，然后重定向至系统 A 的地址，重定向的地址中包含生成的 ST\n系统 A： 拿着 ST 向 CAS 认证服务发送请求，CAS 认证服务验证票据 (ST) 的有效性。验证成功后，系统 A 知道用户已经在 CAS 登录了（其中的 ST 可以保存到 Cookie 或者本地中），系统 A 服务器使用该票据 (ST) 创建与用户的会话，称为局部会话，返回受保护资源；\n客户端： 开始访问系统 B；\n系统 B： 发现用户未登录，重定向至 SSO 认证服务，并将自己的地址作为参数传递，并附上在 sso.com 域下的 cookie 值是第五步生成的 TGC；\nCAS 认证服务： CAS 认证服务中心发现用户已登录，跳转回系统 B 的地址，并附上票据 (ST) ;\n系统 B： 拿到票据 (ST)，去 CAS 认证服务验证票据 (ST) 的有效性。验证成功后，客户端也可以跟系统 B 交往了 ~\n注意\n如图中流程所示，我们发现 CAS 认证服务 在签发的 授权令牌 ST 后，直接重定向，这样其实是比较容易容易被窃取，那么我们需要在系统 A 或者系统 B 在向 CAS 验证成功 (如图中的第 14 步和第 11 步) 后，再生成另一个新的验证 Token 返回给客户端保存； CAS 一般提供四个接口： /login：登录接口，用于登录到中央授权服务 /logout：登出接口，用于从中央授权服务中登出 /validate：用于验证用户是否登录中央授权服务 /serviceValidate：用于让各个 Service 验证用户是否登录中央授权服务 CAS 生成的票据： TGT（Ticket Grangting Ticket） ：TGT 是 CAS 为用户签发的 登录票据，拥有了 TGT，用户就可以证明自己在 CAS 成功登录过。 TGC：Ticket Granting Cookie： CAS Server 生成TGT放入自己的 Session 中，而 TGC 就是这个 Session 的唯一标识（SessionId），以 Cookie 形式放到浏览器端，是 CAS Server 用来明确用户身份的凭证。 ST（Service Ticket） ：ST 是 CAS 为用户签发的访问某个 Service 的票据。 OAuth 2.0 是一种授权机制。数据的所有者告诉系统，同意授权第三方应用进入系统，获取这些数据。系统从而产生一个短期的进入令牌（Token），用来代替密码，供第三方应用使用。\n令牌与密码的差异：\n令牌（Token） 与 密码（Password） 的作用是一样的，都可以进入系统，但是有三点差异。\n令牌是短期的，到期会自动失效： 用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。 令牌可以被数据所有者撤销，会立即失效。 令牌有权限范围（scope）： 对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。 OAuth 2.0 对于如何颁发令牌的细节，规定得非常详细。具体来说，一共分成四种授权模式 （Authorization Grant） ，适用于不同的互联网场景。\n无论哪个模式都拥有三个必要角色：客户端、授权服务器、资源服务器，有的还有用户（资源拥有者）。\n授权码模式（Authorization Code Grant) 授权码（Authorization Code Grant) 方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。\n这种方式是最常用的流程，安全性也最高，它适用于那些有后端服务的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。\n隐藏式模式（Implicit Grant） 有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。OAuth2.0 就规定了第二种方式，允许直接向前端颁发令牌。这种方式没有授权码这个中间步骤，所以称为（授权码）\u0026ldquo;隐藏式\u0026rdquo;（implicit）。\n用户名密码式模式（Password Credentials Grant） 如果你高度信任某个应用，OAuth 2.0 也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为\u0026quot;密码式\u0026quot;（password）\n客户端模式（Client Credentials Grant） 客户端模式指客户端以自己的名义，而不是以用户的名义，向授权服务器 进行认证。\n主要适用于没有前端的命令行应用。\n选型 联合登录和信任登录 联合登录\n指同时包含多种凭证校验的登录服务，同时，也可以理解为使用第三方凭证进行校验的登录服务。\nexample： 对于两个网站 A 和 B，在登录 A 网站的时候用 B 网站的帐号密码，或者登录 B 网站的时候使用 A 网站的帐号密码\n这样的概念其实与上面所讲的 OAuth2.0 的 用户名密码式模式 认证方式类似。\n最经典的莫过于 APP 内嵌 H5 的使用场景，当用户从 APP 进入内嵌的 H5 时，我们希望 APP 内已登录的用户能够访问到 H5 内受限的资源，而未登录的用户则需要登录后访问。\n这里思路主要有两种，一种是原生跳转内嵌 H5 页面时，将登录态 Token 附加在 URL 参数上，另一种则是内嵌 H5 主动通过与原生客户端制定的协议获取应用内的登录状态。\n信任登录\n指所有不需要用户主动参与的登录，例如建立在私有设备与用户之间的绑定关系，凭证就是私有设备的信息，此时不需要用户再提供额外的凭证。信任登录又指用第三方比较成熟的用户库来校验凭证，并登录当前访问的网站。\nexample： 在 A 网站有登录状态的时候，可以直接跳转到 B 网站而不用登录，就是 信任登录。\n目前比较常见的第三方信任登录帐号如：QQ 号淘宝帐号、支付宝帐号、微博帐号等。\n不难发现 OAtuth 2.0 其实就是信任登录的缩影，因为正是有了OAuth，我们的信任登录才得以实现。\n唯一登录 实现用户只能在一个设备上登录，禁止用户重复登录\n扫码登录 无需在网页上输入任何账号和密码，只需要让移动端 APP (如微信、淘宝、QQ等等) 中已登录用户主动扫描 二维码 ，再确认登录，以使 PC 端的同款应用得以快速登录，需要三端 (PC端、手机端、服务端) 来进行配合才能达到登录成功的效果\n二维码\n二维码 又称二维条码，常见的二维码为 QR Code，QR 全称 Quick Response，是一个近几年来移动设备上超流行的一种编码方式，它比传统的Bar Code条形码能存更多的信息，也能表示更多的数据类型。\n一键登录（适用于原生APP） 传统账号密码登录缺点\n要求用户要记住自己的账号和密码，存在记忆成本。用户为了降低记忆成本，很可能会在不同平台使用同一套账号密码。从安全角度考虑，一旦某个平台的账号密码泄露了，会连累到该用户使用的其他平台。 账号和个人身份无关，意味着同一个用户可以注册多个不同的账号，可能出现恶意注册情况。 随着无线互联的发展以及手机卡实名制的推广，手机号俨然已成为特别的身份证明，与账号密码相比，手机号可以更好地验证用户的身份，防止恶意注册。\n但是手机号注册还是需要一系列繁琐的操作：输入手机号、等待短信验证码、输入验证码、点击登录。整个流程少说二十秒，而且如果收不到短信，也就登录补了，这类问题有可能导致潜在的用户流失。\n从安全角度考虑，还存在验证码泄漏的风险。如果有人知道了你的手机号，并且窃取到了验证码，那他也能登录你的账号了。\n一键登录。\n直接获取到当前手机使用的手机卡号进行登录\n取决于运营商是否开放相关服务；随着运营商开放了相关的服务，我们现在已经能够接入运营商提供的 SDK 并付费使用相关的服务\n三大运营商开放平台：\n移动 - 互联网能力开放平台 电信 - 天翼账号开放平台 联通 - WO+ 开放平台 由于国内三大运营商各自有独立的 SDK，所以会导致兼容方面的工作会特别繁琐。如果要采用一键登录的方案，不妨采用第三方提供了号码认证服务，下列几家供应商都拥有手机号码认证能力：\n阿里 - 号码认证服务 创蓝 - 闪验 极光 - 极光认证 mob - 秒验 注意：\n在认证过程中，需要用户打开蜂窝网络，如果手机设备没有插入 SIM 卡、或者关闭蜂窝网络的情况下，是无法完成认证的。所以就算接入一键登录，还是要兼容传统的登录方式，允许用户在失败的情况下，仍能正常完成登录流程。\n总结 HTTP 基本认证适用于内部网络，或者对安全要求不是很高的网络；\nSession-Cookie 适用于一般中大型的网站（移动端 APP 除外）；\nToken 和 JWT 都适用于市面上大部分的企业型网站，JWT 效能会优于 Token；\n单点登录 适用于子系统较多的大型企业网站；\nOAuth 2.0适用于需要快速注册用户型的网站；\n扫码登录 适用于已完成部署了三端的企业；\n一键登录 适用于原生 APP；\n","date":"2022-11-09T00:00:00Z","image":"https://dyhes.github.io/covers/cover3.png","permalink":"https://dyhes.github.io/p/%E9%89%B4%E6%9D%83authentication%E6%A6%82%E8%A7%88/","title":"【鉴权】Authentication概览"},{"content":"History When Linus Torvalds was studying at the University of Helsinki, he was using a version of the UNIX operating system called \u0026lsquo;Minix\u0026rsquo;. Linus and other users sent requests for modifications and improvements to Minix\u0026rsquo;s creator, Andrew Tanenbaum, but he felt that they weren\u0026rsquo;t necessary. That\u0026rsquo;s when Linus decided to create his own operating system that would take into account users\u0026rsquo; comments and suggestions for improvements.\nIn 1991, ideal conditions existed that would create Linux. In essence, Linus Torvalds had a kernel but no programs of his own, Richard Stallman and GNU had programs but no working kernel. Late in 1991, Linus Torvalds had his kernel and a few GNU programs wrapped around it so it would work well enough to show other people what he had done.\nDistributions The three most popular Desktop distributions of Linux are;\nFedora Ubuntu Linux Mint The four most popular Server versions of Linux are;\nDebian\nThe Debian Project was founded by Ian Murdock in 1993. Debian gets its name from the combination of Ian Murdock and his wife Debra\u0026rsquo;s name (Deb-Ian) .Despite its not-for-profit status, Debian is an extremely versatile distribution. It forms the base of many user-friendly distributions like Ubuntu, Linspire and Xandros.\nmv sources.list.old sources.list\nCentOS (Linux.org runs on a CentOS Linux VPS)\nOpenSUSE\nSlackware\ninstall new software dpkg dselect rpm yum File System / This will get you into the \u0026lsquo;root\u0026rsquo; or main directory, not /root\nroot root user\u0026rsquo;s home directory\nhome directory that contains non-root user\u0026rsquo;s home directory\nbin bin/ is one of the most important directories in Linux. You\u0026rsquo;ll find all of the most used commands there. Right now you should be seeing a lot of red (or green, depending on your version of Linux). Those are programs.\nsbin This directory is like /bin in that it has frequently used programs in it, but they\u0026rsquo;re only meant to be used by root.\netc This houses most of the configuration files for Linux.\ndev These are the devices that your system uses or can use.\nboot where the Linux kernel usually is.\ntmp /tmp is a directory that is used to store temporary files, as the name may suggest.\nvar /var is a directory for certain files that may change their size (i.e. variable size) For example, there are a few excellent databases for Linux. One is called MySQL. Normally, MySQL keeps its data in a subdirectory of /var called /var/mysql/.\nusr unix system resources\ncontaining dynamically combined programs, user files and manually-installed programs\nThe usr/ directory contains files and programs meant to be used by all of the users on the system.\nlib /lib is for library files. That\u0026rsquo;s where the name /lib comes from.\nother directories Most installations of Linux will also provide these directories:\n/mnt /cdrom /floppy\nThese shouldn\u0026rsquo;t contain anything. Later on, we\u0026rsquo;ll explain in more detail what these are for. Let\u0026rsquo;s just say that in Linux, if you want to see what\u0026rsquo;s on a floppy disk or a CD, you\u0026rsquo;re not going to be able to just click on an \u0026lsquo;a:\u0026rsquo; icon or a \u0026rsquo;d:\u0026rsquo; icon. You\u0026rsquo;re going to do\nFile Permission Let\u0026rsquo;s look at what these symbols mean:\nstart with -(d for directories)\nthree entity types owner group world permission symbol r:read 4 w:write 2 x:execute 1 File Backup tar an archiving utility\ntar -cvf(create verbose file) tarname files tar -czvf(create zip verbose file) tar.gzName files \u0026rsquo;tar\u0026rsquo; just assembles the files together into only one file. There is no reduction in the size of these files (the tarball might even be bigger!) Now we would have to do one more thing in order to reduce this file into a more manageable size: use \u0026lsquo;gzip\u0026rsquo;.\ntar -zxvpf my_tar_file.tar.gz -z - unzip the file first -x - extract the files from the tarball -v - \u0026amp;quot;verbose\u0026amp;quot; (i.e tar tells you what files it\u0026rsquo;s extracting) -p - preserves dates, permissions of the original files -f - use the file in question (if you don\u0026rsquo;t specify this, tar just sort of sits around doing nothing)\ngzip gzip tar_file.tar the file extension becomes .tar.gz\nText Editor vi The most popular text editor for Linux is called \u0026lsquo;vi\u0026rsquo;. This is a program that comes from UNIX. There is a more recent version called \u0026lsquo;vim\u0026rsquo; which means \u0026lsquo;vi improved\u0026rsquo;.\nESC + i start editing ESC + :w save ESC + ：wq save and quit ESC + :q quit without saving ESC + :q! quit without saving and warning joe Ctrl+k+h pico \u0026hellip;\nUser adduser create a new user or update default new user information\nuserdel delete a user\npasswd change user password\nPackage slackware installpkg file.tgz removepkg file.tgz upgradepkg file.tgz debian dpkg file.deb (advanced package tool) apt-setup apt-get update apt-get install package.deb apt-get remove package.deb apt-get upgrade [p] Commands shutdown shutdown -h(halt) now shutdown -r(reboot) now shutdown -r now shutdown -h +5 (5 minutes) man This command will show the manual for a command or program. The manual is a file that shows you how to use the command and list the different options for the command in question.\nman mkdir info Typing info [command name] will get you more information on a command and is more current than most man files and perhaps a little more readable. In fact, some \u0026lsquo;man\u0026rsquo; files will actually tell you to consult the \u0026lsquo;info\u0026rsquo; file. The \u0026lsquo;info\u0026rsquo; files are not always installed automatically. so you may want to consult your own version of Linux about these files.\nwhatis display one-line manual page descriptions\napropos search the manual page names and descriptions\nchmod Change access permissions\nchmod 644 myDoc.txt chown Change file owner and group\nchown owner.group filename chown bob.bob example.txt last show a listing of last logged in users\ndf report file system disk space usage\nfree Display amount of free and used memory in the system\ndu du -b filename ps report a snapshot of the current processes\nkill kill PID kill a process\nmkdir make directories\nrmdir remove empty directories\nrm remove files or directories\nmv move or rename files\nmv tonyd/(dir) my_friends/(dir) cp copy files and directories\ntouch change file timestamps\nfind search for files\ngrep prints lines that contain a match for one or more patterns\nwho prints information about users who are currently logged on.\ntee read from standard input and write to standard output and files\n\u0026gt; does the same without |\nls -l | tee directory_listing date \u0026gt; directory_listing date \u0026gt;\u0026gt; directory_listing(append) whereis locate the binary,source, and manual page files for a command\nwhich locate a command\necho display a line of text\nwc print newline, word, and byte counts for each file\ndir list directory contents\npwd print working directory\ncal displays a calendar and the adte of Easter\n\u0026lt;TAB\u0026gt; auto complete\nup arrow show last command\ndown arrw most recent commands\n","date":"2022-11-07T00:00:00Z","permalink":"https://dyhes.github.io/p/linuxbasics/","title":"【Linux】Basics"},{"content":"smart contract A contract in the sense of Solidity is a collection of code (its functions) and data (its state) that resides at a specific address on the Ethereum blockchain.\nTransactions A transaction is a message that is sent from one account to another account (which might be the same or empty, see below). It can include binary data (which is called “payload”) and Ether.\nIf the target account contains code, that code is executed and the payload is provided as input data.\nIf the target account is not set (the transaction does not have a recipient or the recipient is set to null), the transaction creates a new contract. As already mentioned, the address of that contract is not the zero address but an address derived from the sender and its number of transactions sent (the “nonce”). The payload of such a contract creation transaction is taken to be EVM bytecode and executed. The output data of this execution is permanently stored as the code of the contract. This means that in order to create a contract, you do not send the actual code of the contract, but in fact code that returns that code when executed.\nGas Upon creation, each transaction is charged with a certain amount of gas that has to be paid for by the originator of the transaction (tx.origin). While the EVM executes the transaction, the gas is gradually depleted according to specific rules. If the gas is used up at any point (i.e. it would be negative), an out-of-gas exception is triggered, which ends execution and reverts all modifications made to the state in the current call frame.\nStorage, Memory and Stacks The Ethereum Virtual Machine has three areas where it can store data: storage, memory and the stack.\nEach account has a data area called storage, which is persistent between function calls and transactions. Storage is a key-value store that maps 256-bit words to 256-bit words. It is not possible to enumerate storage from within a contract, it is comparatively costly to read, and even more to initialise and modify storage. Because of this cost, you should minimize what you store in persistent storage to what the contract needs to run. Store data like derived calculations, caching, and aggregates outside of the contract. A contract can neither read nor write to any storage apart from its own.\nThe second data area is called memory, of which a contract obtains a freshly cleared instance for each message call. Memory is linear and can be addressed at byte level, but reads are limited to a width of 256 bits, while writes can be either 8 bits or 256 bits wide. Memory is expanded by a word (256-bit), when accessing (either reading or writing) a previously untouched memory word (i.e. any offset within a word). At the time of expansion, the cost in gas must be paid. Memory is more costly the larger it grows (it scales quadratically).\nThe EVM is not a register machine but a stack machine, so all computations are performed on a data area called the stack. It has a maximum size of 1024 elements and contains words of 256 bits. Access to the stack is limited to the top end in the following way: It is possible to copy one of the topmost 16 elements to the top of the stack or swap the topmost element with one of the 16 elements below it. All other operations take the topmost two (or one, or more, depending on the operation) elements from the stack and push the result onto the stack. Of course it is possible to move stack elements to storage or memory in order to get deeper access to the stack, but it is not possible to just access arbitrary elements deeper in the stack without first removing the top of the stack.\n","date":"2022-10-13T00:00:00Z","permalink":"https://dyhes.github.io/p/solidityconcepts/","title":"【Solidity】Concepts"},{"content":"数据加密 对称加密 算法：DES、3DES、DESX、Blowfish、IDEA、RC4、RC5、RC6 和 AES 非对称算法 ：RSA、ECC （移动设备用）、Diffie-Hellman、El Gamal、DSA （数字签名用） 散列（Hash）算法 ：MD2、MD4、MD5、HAVAL、SHA、SHA-1、HMAC、HMAC-MD5、HMAC-SHA1 对称加密 Symmetric Key Encryption\n对称密码算法的加密密钥能够从解密密钥中推算出来，反过来也成立。在大多数对称算法中，加密解密密钥是相同的。它要求发送者和接收者在安全通信之前，商定一个密钥。\n对称算法又可分为两类。\n一次只对明文中的单个位（有时对字节）运算的算法称为序列算法或序列密码。 另一类算法是对明文的一组位进行运算，这些位组称为分组，相应的算法称为分组算法或分组密码。现代计算机密码算法的典型分组长度为64位――这个长度既考虑到分析破译密码的难度，又考虑到使用的方便性。后来，随着破译能力的发展，分组长度又提高到128位或更长。分组加密算法中，一般有ECB,CBC,CFB,OFB这几种算法模式。 DES DES是Data Encryption Standard（数据加密标准）的缩写。它是由IBM公司研制的一种对称密码算法，美国国家标准局于1977年公布把它作为非机要部门使用的数据加密标准，三十年来，它一直活跃在国际保密通信的舞台上，扮演了十分重要的角色。\nDES是一个分组加密算法，典型的DES以64位为分组对数据加密，加密和解密用的是同一个算法。它的密钥长度是56位（因为每个第8位都用作奇偶校验），密钥可以是任意的56位的数，而且可以任意时候改变。其中有极少数被认为是易破解的弱密钥，但是很容易避开它们不用。所以保密性依赖于密钥。\n3DES 3DES，即triple-DES，简单地说就是3次DES加解密的组合。现已不推荐使用\nAES Advanced Encryption Standard，是现行的对称加密标准。目前（2017）如果使用对称加密，应该使用AES\n分组：128bit。\n密钥：128bit、192bit、256bit。\n非对称加密 Asymmetric Key Encryption\n公开密钥加密（英语：public-key cryptography，又译为公开密钥加密），也称为非对称加密（asymmetric cryptography），一种密码学算法类型，在这种密码学方法中，需要一对密钥(其实这里密钥说法不好，就是“钥”)，一个是私人密钥，另一个则是公开密钥。这两个密钥是数学相关，用某用户密钥加密后所得的信息，只能用该用户的解密密钥才能解密。如果知道了其中一个，并不能计算出另外一个。因此如果公开了一对密钥中的一个，并不会危害到另外一个的秘密性质。称公开的密钥为公钥；不公开的密钥为私钥。\nRSA RSA加密算法是一种非对称加密算法，在公开密钥加密和电子商业中被广泛使用。RSA是1977年由罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）一起提出的。当时他们三人都在麻省理工学院工作。RSA就是他们三人姓氏开头字母拼在一起组成的。 这种算法非常可靠，密钥越长，它就越难破解。根据已经披露的文献，目前被破解的最长RSA密钥是768个二进制位。也就是说，长度超过768位的密钥，还无法破解（至少没人公开宣布）。因此可以认为，1024位的RSA密钥基本安全，2048位的密钥极其安全。\nHash Hash 算法特别的地方在于它是一种单向算法，用户可以通过 Hash 算法对目标信息生成一段特定长度的唯一的 Hash 值，却不能通过这个 Hash 值重新获得目标信息。因此 Hash 算法常用在不可还原的密码存储、信息完整性校验等。 单向散列函数一般用于产生消息摘要，密钥加密等，常见的有： MD5 （Message Digest Algorithm 5）：是 RSA 数据安全公司开发的一种单向散列算法，非可逆，相同的明文产生相同的密文； SHA （Secure Hash Algorithm）：可以对任意长度的数据运算生成一个 160 位的数值。 SHA-1 与 MD5 的比较 : 因为二者均由 MD4 导出，SHA-1 和 MD5 彼此很相似。相应的，他们的强度和其他特性也是相似，但还有以下几点不同： 对强行供给的安全性：最显著和最重要的区别是 SHA-1 摘要比 MD5 摘要长 32 位。使用强行技术，产生任何一个报文使其摘要等于给定报摘要的难度对 MD5 是 2128 数量级的操作，而对 SHA-1 则是 2160 数量级的操作。这样，SHA-1 对强行攻击有更大的强度； 对密码分析的安全性：由于 MD5 的设计，易受密码分析的攻击，SHA-1 显得不易受这样的攻击； 速度：在相同的硬件上，SHA-1 的运行速度比 MD5 慢。\n数字签名 Digit Signature\n数字签名是只有信息的发送者才能产生的别人无法伪造的一段数字串，这段数字串同时也是对信息的发送者发送信息真实性的一个有效证明。它是一种类似写在纸上的普通的物理签名，但是在使用了公钥加密领域的技术来实现的，用于鉴别数字信息的方法。一套数字签名通常定义两种互补的运算，一个用于签名，另一个用于验证。数字签名是非对称密钥加密技术与数字摘要技术的应用。\n","date":"2022-10-13T00:00:00Z","permalink":"https://dyhes.github.io/p/%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86%E6%A6%82%E5%BF%B5/","title":"【数据加密】概念"},{"content":"Expressions and Statements Statements are instructions that perform some action and do not return a value. Expressions evaluate to a resulting value.\nOwnship Ownership is a set of rules that governs how a Rust program manages memory.\nAll data stored on the stack must have a known, fixed size. Data with an unknown size at compile time or a size that might change must be stored on the heap instead.\nKeeping track of what parts of code are using what data on the heap, minimizing the amount of duplicate data on the heap, and cleaning up unused data on the heap so you don’t run out of space are all problems that ownership addresses.\nOwnership Rules Each value in Rust has an owner. There can only be one owner at a time. When the owner goes out of scope, the value will be dropped. The ownership of a variable follows the same pattern every time: assigning a value to another variable moves it.\nA reference is like a pointer in that it’s an address we can follow to access the data stored at that address; that data is owned by some other variable.\nModule Systems Packages: A Cargo feature that lets you build, test, and share crates Crates: A tree of modules that produces a library or executable Modules and use: Let you control the organization, scope, and privacy of paths Paths: A way of naming an item, such as a struct, function, or module ","date":"2022-09-12T00:00:00Z","permalink":"https://dyhes.github.io/p/rustbasics/","title":"【Rust】Basics"},{"content":"QApplication The QCoreApplication class and QApplication class and provides an event loop for console Qt applications. Those class are used by applications to provide event loop. For non-GUI application that uses Qt, there should be exactly one QCoreApplication object. For GUI applications, we will use QApplication.\nWhen calling a.exec() the event loop is launched.\nWhen we compile the project, behind the scene, our Qt app is compiled in the following steps:\nqmake parses the .pro file, and generates makefile. The program is build using make (jom on windows). Signals and Slots Signals and slots are used for communication between objects. The signals and slots mechanism is a central feature of Qt and probably the part that differs most from the features provided by other frameworks.\nA signal is emitted when a particular event occurs. Qt\u0026rsquo;s widgets have many predefined signals, but we can always subclass widgets to add our own signals to them. A slot is a function that is called in response to a particular signal. Qt\u0026rsquo;s widgets have many pre-defined slots, but it is common practice to subclass widgets and add your own slots so that you can handle the signals that you are interested in.\nAll classes that inherit from QObject or one of its subclasses (e.g., QWidget) can contain signals and slots.\nJust as an object does not know if anything receives its signals, a slot does not know if it has any signals connected to it. This ensures that truly independent components can be created with Qt. You can connect as many signals as you want to a single slot, and a signal can be connected to as many slots as you need.\nQ_OBJECT Macro The Q_OBJECT macro must appear in the private section of a class definition that declares its own signals and slots or that uses other services provided by Qt\u0026rsquo;s meta-object system.\nNote that when we want to make connection between signal and slot, we actually do it with Object scope:\nQObject::connect(sender, signal, receiver, slot): QString We will use all three methods to find some substrings of a given string.\nout \u0026lt;\u0026lt; str.right(5) \u0026lt;\u0026lt; endl; With the right method, we get five rightmost characters of the str string. The \u0026rsquo;train\u0026rsquo; is printed.\nout \u0026lt;\u0026lt; str.left(9) \u0026lt;\u0026lt; endl; With the left method, we get nine leftmost characters of the str string. The \u0026lsquo;The night\u0026rsquo; is printed.\nout \u0026lt;\u0026lt; str.mid(4, 5) \u0026lt;\u0026lt; endl; With the mid method, we get five characters starting from the 4th position. The \u0026rsquo;night\u0026rsquo; is printed.\nQString str2(\u0026#34;The big apple\u0026#34;); QStringRef sub(\u0026amp;str2, 0, 7); The QStringRef class is a read-only version of a QString. Here we create a QStringRef of a portion of the str2 string. The second parameter is the position and the third is the length of the substring.\nThe Q_OBJECT macro must be included in classes that declare their own signals and slots.\nQMainWindow 是主窗口类，主窗口具有主菜单栏、工具栏和状态栏，类似于一般的应用程序的主窗口； QWidget 是所有具有可视界面类的基类，选择 QWidget 创建的界面对各种界面组件都可以 支持； QDialog 是对话框类，可建立一个基于对话框的界面； ","date":"2022-08-15T00:00:00Z","permalink":"https://dyhes.github.io/p/qtbasics/","title":"【Qt】Basics"},{"content":"The first step in any machine learning project is familiarize yourself with the data. You\u0026rsquo;ll use the Pandas library for this. Pandas is the primary tool data scientists use for exploring and manipulating data.\nThe most important part of the Pandas library is the DataFrame. A DataFrame holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database.\nPandas has powerful methods for most things you\u0026rsquo;ll want to do with this type of data.\n# save filepath to variable for easier access melbourne_file_path = \u0026#39;../input/melbourne-housing-snapshot/melb_data.csv\u0026#39; # read the data and store data in DataFrame titled melbourne_data melbourne_data = pd.read_csv(melbourne_file_path) # print a summary of the data in Melbourne data melbourne_data.describe() Building Your Model You will use the scikit-learn library to create your models. When coding, this library is written as sklearn, as you will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames.\nThe steps to building and using a model are:\nDefine: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too. Fit: Capture patterns from provided data. This is the heart of modeling. Predict: Just what it sounds like Evaluate: Determine how accurate the model\u0026rsquo;s predictions are. from sklearn.tree import DecisionTreeRegressor # Define model. Specify a number for random_state to ensure same results each run melbourne_model = DecisionTreeRegressor(random_state=1) # Fit model melbourne_model.fit(X, y) Many machine learning models allow some randomness in model training. Specifying a number for random_state ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won\u0026rsquo;t depend meaningfully on exactly what value you choose.\nWe now have a fitted model that we can use to make predictions.\nIn practice, you\u0026rsquo;ll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we\u0026rsquo;ll make predictions for the first few rows of the training data to see how the predict function works.\nThe scikit-learn library has a function train_test_split to break up the data into two pieces. We\u0026rsquo;ll use some of that data as training data to fit the model, and we\u0026rsquo;ll use the other data as validation data to calculate mean_absolute_error.\nfrom sklearn.model_selection import train_test_split # split data into training and validation data, for both features and target # The split is based on a random number generator. Supplying a numeric value to # the random_state argument guarantees we get the same split every time we # run this script. train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0) # Define model melbourne_model = DecisionTreeRegressor() # Fit model melbourne_model.fit(train_X, train_y) # get predicted prices on validation data val_predictions = melbourne_model.predict(val_X) print(mean_absolute_error(val_y, val_predictions)) Since we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the (red) validation curve in the figure below.\npipeline Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\nCross-validation In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality.\nDeep Learning Deep learning is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.\nfrom tensorflow import keras from tensorflow.keras import layers # Create a network with 1 linear unit model = keras.Sequential([ layers.Dense(units=1, input_shape=[3]) ]) With the first argument, units, we define how many outputs we want.\nWith the second argument, input_shape, we tell Keras the dimensions of the inputs.\nMany Kinds of Layers A \u0026ldquo;layer\u0026rdquo; in Keras is a very general kind of thing. A layer can be, essentially, any kind of data transformation. Many layers, like the convolutional and recurrent layers, transform data through use of neurons and differ primarily in the pattern of connections they form. Others though are used for feature engineering or just simple arithmetic. There\u0026rsquo;s a whole world of layers to discover \u0026ndash; check them out!\nIt turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.\nWithout activation functions, neural networks can only learn linear relationships. In order to fit curves, we\u0026rsquo;ll need to use activation functions.\nAn activation function is simply some function we apply to each of a layer\u0026rsquo;s outputs (its activations). The most common is the rectifier function max(0,x)max(0,x).\nStochastic Gradient Descent In addition to the training data, we need two more things:\nA \u0026ldquo;loss function\u0026rdquo; that measures how good the network\u0026rsquo;s predictions are. An \u0026ldquo;optimizer\u0026rdquo; that can tell the network how to change its weights. The optimizer is an algorithm that adjusts the weights to minimize the loss.\nVirtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\nSample some training data and run it through the network to make predictions. Measure the loss between the predictions and the true values. Finally, adjust the weights in a direction that makes the loss smaller. Then just do this over and over until the loss is as small as you like (or until it won\u0026rsquo;t decrease any further.)\nEach iteration\u0026rsquo;s sample of training data is called a minibatch (or often just \u0026ldquo;batch\u0026rdquo;), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\nAfter defining a model, you can add a loss function and optimizer with the model\u0026rsquo;s compile method:\nmodel.compile( optimizer=\u0026#34;adam\u0026#34;, loss=\u0026#34;mae\u0026#34;, ) overfitting and underfitting A model\u0026rsquo;s capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.\nYou can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\nother kind of layers beside dense layer dropout layer can help correct overfitting.\nTo break up these conspiracies, we randomly drop out some fraction of a layer\u0026rsquo;s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\nIn Keras, the dropout rate argument rate defines what percentage of the input units to shut off. Put the Dropout layer just before the layer you want the dropout applied to:\nkeras.Sequential([ # ... layers.Dropout(rate=0.3), # apply 30% dropout to the next layer layers.Dense(16), # ... ]) When adding dropout, you may need to increase the number of units in your Dense layers.\nClassification Accuracy is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: accuracy = number_correct / total. A model that always predicted correctly would have an accuracy score of 1.0. All else being equal, accuracy is a reasonable metric to use whenever the classes in the dataset occur with about the same frequency.\nThe problem with accuracy (and most other classification metrics) is that it can\u0026rsquo;t be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in \u0026ldquo;jumps\u0026rdquo;. So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.\nNow, recall that the loss function defines the objective of the network during training. With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.\nFor classification, what we want instead is a distance between probabilities, and this is what cross-entropy provides. Cross-entropy is a sort of measure for the distance from one probability distribution to another.\nReinforcement Learning This idea of using reward to track the performance of an agent is a core idea in the field of reinforcement learning. Once we define the problem in this way, we can use any of a variety of reinforcement learning algorithms to produce an agent.\nComputer Vision A convnet used for image classification consists of two parts: a convolutional base and a dense head.\nThe base is used to extract the features from an image. It is formed primarily of layers performing the convolution operation, but often includes other kinds of layers as well.\nThe head is used to determine the class of the image. It is formed primarily of dense layers, but might include other layers like dropout.\nThe goal of the network during training is to learn two things:\nwhich features to extract from an image (base), which class goes with what features (head). These days, convnets are rarely trained from scratch. More often, we reuse the base of a pretrained model. To the pretrained base we then attach an untrained head. In other words, we reuse the part of a network that has already learned to do 1. Extract features, and attach to it some fresh layers to learn 2. Classify\nThe most commonly used dataset for pretraining is ImageNet, a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its applications module.\nFeature Extraction The feature extraction performed by the base consists of three basic operations:\nFilter an image for a particular feature (convolution) Detect that feature within the filtered image (ReLU) Condense the image to enhance the features (maximum pooling) The weights a convnet learns during training are primarily contained in its convolutional layers. These weights we call kernels. We can represent them as small arrays:\nA kernel operates by scanning over an image and producing a weighted sum of pixel values. In this way, a kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information.\nA kernel acts as a kind of lens.\nKernels define how a convolutional layer is connected to the layer that follows. The kernel above will connect each neuron in the output to nine neurons in the input. By setting the dimensions of the kernels with kernel_size, you are telling the convnet how to form these connections. Most often, a kernel will have odd-numbered dimensions \u0026ndash; like kernel_size=(3, 3) or (5, 5) \u0026ndash; so that a single pixel sits at the center, but this is not a requirement.\nThe kernels in a convolutional layer determine what kinds of features it creates. During training, a convnet tries to learn what features it needs to solve the classification problem. This means finding the best values for its kernels.\nThe activations in the network we call feature maps. They are what result when we apply a filter to an image; they contain the visual features the kernel extracts. Here are a few kernels pictured with feature maps they produced.\nKernels and features.\nFrom the pattern of numbers in the kernel, you can tell the kinds of feature maps it creates. Generally, what a convolution accentuates in its inputs will match the shape of the positive numbers in the kernel. The left and middle kernels above will both filter for horizontal shapes.\nWith the filters parameter, you tell the convolutional layer how many feature maps you want it to create as output.\nNotice that after applying the ReLU function (Detect) the feature map ends up with a lot of \u0026ldquo;dead space,\u0026rdquo; that is, large areas containing only 0\u0026rsquo;s (the black areas in the image). Having to carry these 0 activations through the entire network would increase the size of the model without adding much useful information. Instead, we would like to condense the feature map to retain only the most useful part \u0026ndash; the feature itself.\nThis in fact is what maximum pooling does. Max pooling takes a patch of activations in the original feature map and replaces them with the maximum activation in that patch.\nThe pooling step increases the proportion of active pixels to zero pixels.\nIn fact, the zero-pixels carry positional information. The blank space still positions the feature within the image. When MaxPool2D removes some of these pixels, it removes some of the positional information in the feature map. This gives a convnet a property called translation invariance. This means that a convnet with maximum pooling will tend not to distinguish features by their location in the image.\nWe mentioned in the previous exercise that average pooling has largely been superceeded by maximum pooling within the convolutional base. There is, however, a kind of average pooling that is still widely used in the head of a convnet. This is global average pooling. A GlobalAvgPool2D layer is often used as an alternative to some or all of the hidden Dense layers in the head of the network\nThere are two additional parameters affecting both convolution and pooling layers \u0026ndash; these are the strides of the window and whether to use padding at the image edges. The strides parameter says how far the window should move at each step, and the padding parameter describes how we handle the pixels at the edges of the input.\n.\nIncreasing the stride means that we miss out on potentially valuble information in our summary. Maximum pooling layers, however, will almost always have stride values greater than 1, like (2, 2) or (3, 3), but not larger than the window itself.\nWhat the convolution does with these boundary values is determined by its padding parameter. In TensorFlow, you have two choices: either padding='same' or padding='valid'. There are trade-offs with each.\nWhen we set padding='valid', the convolution window will stay entirely inside the input. The drawback is that the output shrinks (loses pixels), and shrinks more for larger kernels. This will limit the number of layers the network can contain, especially when inputs are small in size.\nThe alternative is to use padding='same'. The trick here is to pad the input with 0\u0026rsquo;s around its borders, using just enough 0\u0026rsquo;s to make the size of the output the same as the size of the input. This can have the effect however of diluting the influence of pixels at the borders. The animation below shows a sliding window with 'same' padding.\nThe VGG model we\u0026rsquo;ve been looking at uses same padding for all of its convolutional layers. Most modern convnets will use some combination of the two.\n. A single round of feature extraction can only extract relatively simple features from an image, things like simple lines or contrasts. These are too simple to solve most classification problems. Instead, convnets will repeat this extraction over and over, so that the features become more complex and refined as they travel deeper into the network.\nIt does this by passing them through long chains of convolutional blocks which perform this extraction.\nThese convolutional blocks are stacks of Conv2D and MaxPool2D layers, whose role in feature extraction we learned about in the last few lessons.\nEach block represents a round of extraction, and by composing these blocks the convnet can combine and recombine the features produced, growing them and shaping them to better fit the problem at hand. The deep structure of modern convnets is what allows this sophisticated feature engineering and has been largely responsible for their superior performance.\n","date":"2022-08-09T00:00:00Z","permalink":"https://dyhes.github.io/p/kagglesome-knowledge/","title":"【Kaggle】Some Knowledge"},{"content":"Kaggle is a cloud computational environment that enables reproducible and collaborative analysis\nTypes of Noteboooks Script Scripts are files that execute everything as code sequentially. To start a script, click on “Create Notebook” and select “Script”. This will open the Scripts editing interface.\nNotebooks The last type is Jupyter notebooks (usually just “notebooks”). Jupyter notebooks consist of a sequence of cells, where each cell is formatted in either Markdown (for writing text) or in a programming language of your choice (for writing code). To start a notebook, click on “Create Notebook”, and select “Notebook”. This will open the Notebooks editing interface.\nDatasets and Competitions Data on Kaggle is available through either Datasets or our Competitions.\nThere are two ways of loading a Dataset in a Notebook. The first is to navigate to a chosen dataset’s landing page, then click on the “New Notebook” button. This will launch a new Notebook session with the dataset in question spun up and ready to go.\nAlternatively, you may wish to add datasets after creating your Notebook. To do that, navigate to the “Data” pane in a Notebook editor and click the “Add Data” button. This will open a modal that lets you select Datasets to add to your Notebook.\nYou will notice that there is a third option in the “Add Data” modal: Notebook Output Files.\nUp to 20 GBs of output from a Notebook may be saved to disk in /kaggle/working. This data is saved automatically and you can then reuse that data in any future Notebook: just navigate to the “Data” pane in a Notebook editor, click on “Add Data”, click on the \u0026ldquo;Notebook Output Files\u0026rdquo; tab, find a Notebook of interest, and then click to add it to your current Notebook.\nBy chaining Notebooks as data sources in this way, it’s possible to build pipelines and generate more and better content than you could in a single notebook alone.\nEditor Kaggle Notebooks may be created and edited via the Notebook editor. On larger screens, the Notebook editor consists of three parts:\nAn editing window A console A settings window Collaboration From your Notebook editor or viewer, public or private, you may navigate to the \u0026lsquo;Share\u0026rsquo; or \u0026lsquo;Sharing\u0026rsquo; button in the Notebook’s menu to expose, among other settings, the Collaborators options. There, use the search box to find and add other users as Notebook collaborators.\nIf your Notebook is private, you may choose between giving Collaborators either viewing privileges (“Can view”) or editing privileges (“Can edit”). If your Notebook is public, Collaborators can only be added with editing privileges (“Can edit”), as anyone can view it already.\nWhen you add a collaborator, they will receive a notification via email.\nYour Notebook collaborators won’t automatically have the same access to any private Datasets as you unless they are explicitly invited to collaborate on the Dataset. Anyone has access to Datasets shared publicly.\nEnvironment Notebooks is more than just a code editor. It’s a versioned computational environment designed to make it easy to reproduce data science work. In the Notebooks IDE, you have access to an interactive session running in a Docker container with pre-installed packages, the ability to mount versioned data sources, customizable compute resources like GPUs, and more.\nModifying a Notebook-specific Environment It is also possible to modify the Docker container associated with the current Notebook image.\nUsing a standard package installer In the Notebook Editor, make sure \u0026ldquo;Internet\u0026rdquo; is enabled in the Settings pane (it will be by default if it\u0026rsquo;s a new notebook).\nFor Python, you can run arbitrary shell commands by prepending ! to a code cell. For instance, to install a new package using pip, run !pip install my-new-package. You can also upgrade or downgrade an existing package by running !pip install my-existing-package==X.Y.Z.\n","date":"2022-08-08T00:00:00Z","permalink":"https://dyhes.github.io/p/kaggleconcepts/","title":"【Kaggle】Concepts"},{"content":"设计模式 设计模式，即Design Patterns，是指在软件设计中，被反复使用的一种代码设计经验。使用设计模式的目的是为了可重用代码，提高代码的可扩展性和可维护性。。\n开闭原则 由Bertrand Meyer提出的开闭原则（Open Closed Principle）是指，软件应该对扩展开放，而对修改关闭。这里的意思是在增加新功能的时候，能不改代码就尽量不要改，如果只增加代码就完成了新功能，那是最好的。\n里氏替换原则 里氏替换原则是Barbara Liskov提出的，这是一种面向对象的设计原则，即如果我们调用一个父类的方法可以成功，那么替换成子类调用也应该完全可以运行。\n学习设计模式，关键是学习设计思想，不能简单地生搬硬套，也不能为了使用设计模式而过度设计，要合理平衡设计的复杂度和灵活性，并意识到设计模式也并不是万能的。\n创建型模式 创建型模式关注点是如何创建对象，其核心思想是要把对象的创建和使用相分离，这样使得两者能相对独立地变换。\n创建型模式包括：\n工厂方法：Factory Method 抽象工厂：Abstract Factory 建造者：Builder 原型：Prototype 单例：Singleton 工厂方法 ┌─────────────┐ ┌─────────────┐ │ Product │ │ Factory │ └─────────────┘ └─────────────┘ ▲ ▲ │ │ ┌─────────────┐ ┌─────────────┐ │ ProductImpl │\u0026lt;─ ─ ─│ FactoryImpl │ └─────────────┘ └─────────────┘ 工厂方法即Factory Method，是一种对象创建型模式。\n工厂方法的目的是使得创建对象和使用对象是分离的，并且客户端总是引用抽象工厂和抽象产品\npublic interface NumberFactory { // 创建方法: Number parse(String s); // 获取工厂实例: static NumberFactory getFactory() { return impl; } static NumberFactory impl = new NumberFactoryImpl(); } 调用方可以完全忽略真正的工厂NumberFactoryImpl和实际的产品BigDecimal，这样做的好处是允许创建产品的代码独立地变换，而不会影响到调用方。\npublic class NumberFactory { public static Number parse(String s) { return new BigDecimal(s); } } 实际上大多数情况下我们并不需要抽象工厂，而是通过静态方法直接返回产品，这种简化的使用静态方法创建产品的方式称为静态工厂方法（Static Factory Method）。\n抽象工厂 抽象工厂模式和工厂方法不太一样，它要解决的问题比较复杂，不但工厂是抽象的，产品是抽象的，而且有多个产品需要创建，因此，这个抽象工厂会对应到多个实际工厂，每个实际工厂负责创建多个实际产品：\n┌────────┐ ─ \u0026gt;│ProductA│ ┌────────┐ ┌─────────┐ │ └────────┘ │ Client │─ ─\u0026gt;│ Factory │─ ─ └────────┘ └─────────┘ │ ┌────────┐ ▲ ─ \u0026gt;│ProductB│ ┌───────┴───────┐ └────────┘ │ │ ┌─────────┐ ┌─────────┐ │Factory1 │ │Factory2 │ └─────────┘ └─────────┘ │ ┌─────────┐ │ ┌─────────┐ ─ \u0026gt;│ProductA1│ ─ \u0026gt;│ProductA2│ │ └─────────┘ │ └─────────┘ ┌─────────┐ ┌─────────┐ └ ─\u0026gt;│ProductB1│ └ ─\u0026gt;│ProductB2│ └─────────┘ └─────────┘ 这种模式有点类似于多个供应商负责提供一系列类型的产品。\n//接口 public interface AbstractFactory { // 创建Html文档: HtmlDocument createHtml(String md); // 创建Word文档: WordDocument createWord(String md); } // Html文档接口: public interface HtmlDocument { String toHtml(); void save(Path path) throws IOException; } // Word文档接口: public interface WordDocument { void save(Path path) throws IOException; } //fast实现 public class FastHtmlDocument implements HtmlDocument { public String toHtml() { ... } public void save(Path path) throws IOException { ... } } public class FastWordDocument implements WordDocument { public void save(Path path) throws IOException { ... } } public class FastFactory implements AbstractFactory { public HtmlDocument createHtml(String md) { return new FastHtmlDocument(md); } public WordDocument createWord(String md) { return new FastWordDocument(md); } } //good实现 public class GoodFactory implements AbstractFactory { public HtmlDocument createHtml(String md) { return new GoodHtmlDocument(md); } public WordDocument createWord(String md) { return new GoodWordDocument(md); } } // 实际产品: public class GoodHtmlDocument implements HtmlDocument { ... } public class GoodWordDocument implements HtmlDocument { ... } //alternative public interface AbstractFactory { public static AbstractFactory createFactory(String name) { if (name.equalsIgnoreCase(\u0026#34;fast\u0026#34;)) { return new FastFactory(); } else if (name.equalsIgnoreCase(\u0026#34;good\u0026#34;)) { return new GoodFactory(); } else { throw new IllegalArgumentException(\u0026#34;Invalid factory name\u0026#34;); } } } 建造者 将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。\n生成器模式（Builder）是使用多个“小型”工厂来最终创建出一个完整对象。\n当我们使用Builder的时候，一般来说，是因为创建这个对象的步骤比较多，每个步骤都需要一个零部件，最终组合成一个完整的对象。\n原型 用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。\n原型模式，即Prototype，是指创建新对象的时候，根据现有的一个原型来创建。\npublic class Student { private int id; private String name; private int score; public Student copy() { Student std = new Student(); std.id = this.id; std.name = this.name; std.score = this.score; return std; } } 单例 保证一个类仅有一个实例，并提供一个访问它的全局访问点。\n单例模式（Singleton）的目的是为了保证在一个进程中，某个类有且仅有一个实例。\n因为这个类只有一个实例，因此，自然不能让调用方使用new Xyz()来创建实例了。所以，单例的构造方法必须是private，这样就防止了调用方自己创建实例，但是在类的内部，是可以用一个静态字段来引用唯一创建的实例的\npublic class Singleton { // 静态字段引用唯一实例: private static final Singleton INSTANCE = new Singleton(); // 通过静态方法返回实例: public static Singleton getInstance() { return INSTANCE; } // private构造方法保证外部无法实例化: private Singleton() { } } 另一种实现Singleton的方式是利用Java的enum，因为Java保证枚举类的每个枚举都是单例，所以我们只需要编写一个只有一个枚举的类即可：\npublic enum World { // 唯一枚举: INSTANCE; private String name = \u0026#34;world\u0026#34;; public String getName() { return this.name; } public void setName(String name) { this.name = name; } } 实际上，很多程序，尤其是Web程序，大部分服务类都应该被视作Singleton，如果全部按Singleton的写法写，会非常麻烦，所以，通常是通过约定让框架（例如Spring）来实例化这些类，保证只有一个实例，调用方自觉通过框架获取实例而不是new操作符，因此，除非确有必要，否则Singleton模式一般以“约定”为主，不会刻意实现它。\n结构型模式 结构型模式主要涉及如何组合各种对象以便获得更好、更灵活的结构。虽然面向对象的继承机制提供了最基本的子类扩展父类的功能，但结构型模式不仅仅简单地使用继承，而更多地通过组合与运行期的动态组合来实现更灵活的功能。\n结构型模式有：\n适配器 桥接 组合 装饰器 外观 享元 代理 适配器 将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。\npublic class Task implements Callable\u0026lt;Long\u0026gt; { private long num; public Task(long num) { this.num = num; } public Long call() throws Exception { long r = 0; for (long n = 1; n \u0026lt;= this.num; n++) { r = r + n; } System.out.println(\u0026#34;Result: \u0026#34; + r); return r; } } Callable\u0026lt;Long\u0026gt; callable = new Task(123450000L); Thread thread = new Thread(callable); // compile error! thread.start(); public class RunnableAdapter implements Runnable { // 引用待转换接口: private Callable\u0026lt;?\u0026gt; callable; public RunnableAdapter(Callable\u0026lt;?\u0026gt; callable) { this.callable = callable; } // 实现指定接口: public void run() { // 将指定接口调用委托给转换接口调用: try { callable.call(); } catch (Exception e) { throw new RuntimeException(e); } } } Callable\u0026lt;Long\u0026gt; callable = new Task(123450000L); Thread thread = new Thread(new RunnableAdapter(callable)); thread.start(); InputStreamReader就是Java标准库提供的Adapter，它负责把一个InputStream适配为Reader。类似的还有OutputStreamWriter。\n桥接 将抽象部分与它的实现部分分离，使它们都可以独立地变化。\n桥接模式就是为了避免直接继承带来的子类爆炸。\npublic abstract class Car { // 引用Engine: protected Engine engine; public Car(Engine engine) { this.engine = engine; } public abstract void drive(); } public interface Engine { void start(); } public abstract class RefinedCar extends Car { public RefinedCar(Engine engine) { super(engine); } public void drive() { this.engine.start(); System.out.println(\u0026#34;Drive \u0026#34; + getBrand() + \u0026#34; car...\u0026#34;); } public abstract String getBrand(); } 这样一来，最终的不同品牌继承自RefinedCar，例如BossCar：\npublic class BossCar extends RefinedCar { public BossCar(Engine engine) { super(engine); } public String getBrand() { return \u0026#34;Boss\u0026#34;; } } 而针对每一种引擎，继承自Engine，例如HybridEngine：\npublic class HybridEngine implements Engine { public void start() { System.out.println(\u0026#34;Start Hybrid Engine...\u0026#34;); } } 客户端通过自己选择一个品牌，再配合一种引擎，得到最终的Car：\nRefinedCar car = new BossCar(new HybridEngine()); car.drive(); 使用桥接模式的好处在于，如果要增加一种引擎，只需要针对Engine派生一个新的子类，如果要增加一个品牌，只需要针对RefinedCar派生一个子类，任何RefinedCar的子类都可以和任何一种Engine自由组合，即一辆汽车的两个维度：品牌和引擎都可以独立地变化。\n组合 将对象组合成树形结构以表示“部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。\n组合模式（Composite）经常用于树形结构，为了简化代码，使用Composite可以把一个叶子节点与一个父节点统一起来处理。\npublic interface Node { // 添加一个节点为子节点: Node add(Node node); // 获取子节点: List\u0026lt;Node\u0026gt; children(); // 输出为XML: String toXml(); } public class ElementNode implements Node { private String name; private List\u0026lt;Node\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); public ElementNode(String name) { this.name = name; } public Node add(Node node) { list.add(node); return this; } public List\u0026lt;Node\u0026gt; children() { return list; } public String toXml() { String start = \u0026#34;\u0026lt;\u0026#34; + name + \u0026#34;\u0026gt;\\n\u0026#34;; String end = \u0026#34;\u0026lt;/\u0026#34; + name + \u0026#34;\u0026gt;\\n\u0026#34;; StringJoiner sj = new StringJoiner(\u0026#34;\u0026#34;, start, end); list.forEach(node -\u0026gt; { sj.add(node.toXml() + \u0026#34;\\n\u0026#34;); }); return sj.toString(); } } public class TextNode implements Node { private String text; public TextNode(String text) { this.text = text; } public Node add(Node node) { throw new UnsupportedOperationException(); } public List\u0026lt;Node\u0026gt; children() { return List.of(); } public String toXml() { return text; } } 装饰器 动态地给一个对象添加一些额外的职责。就增加功能来说，相比生成子类更为灵活。\n装饰器（Decorator）模式，是一种在运行期动态给某个对象的实例增加功能的方法。\n在Java标准库中，InputStream是抽象类，FileInputStream、ServletInputStream、Socket.getInputStream()这些InputStream都是最终数据源。\n现在，如果要给不同的最终数据源增加缓冲功能、计算签名功能、加密解密功能，那么，3个最终数据源、3种功能一共需要9个子类。如果继续增加最终数据源，或者增加新功能，子类会爆炸式增长，这种设计方式显然是不可取的。\nDecorator模式的目的就是把一个一个的附加功能，用Decorator的方式给一层一层地累加到原始数据源上，最终，通过组合获得我们想要的功能。\n┌───────────┐ │ Component │ └───────────┘ ▲ ┌────────────┼─────────────────┐ │ │ │ ┌───────────┐┌───────────┐ ┌───────────┐ │ComponentA ││ComponentB │... │ Decorator │ └───────────┘└───────────┘ └───────────┘ ▲ ┌──────┴──────┐ │ │ ┌───────────┐ ┌───────────┐ │DecoratorA │ │DecoratorB │... └───────────┘ └───────────┘ 装饰器模式实际上实现了核心功能和附加功能的分离，使得两部分都可以独立地扩展，而具体如何附加功能，由调用方自由组合，从而极大地增强了灵活性。\n外观 为子系统中的一组接口提供一个一致的界面。Facade模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。\n如果客户端要跟许多子系统打交道，那么客户端需要了解各个子系统的接口，比较麻烦。如果有一个统一的“中介”，让客户端只跟中介打交道，中介再去跟各个子系统打交道，对客户端来说就比较简单。所以Facade就相当于搞了一个中介。\n// 工商注册: public class AdminOfIndustry { public Company register(String name) { ... } } // 银行开户: public class Bank { public String openAccount(String companyId) { ... } } // 纳税登记: public class Taxation { public String applyTaxCode(String companyId) { ... } } public class Facade { public Company openCompany(String name) { Company c = this.admin.register(name); String bankAccount = this.bank.openAccount(c.getId()); c.setBankAccount(bankAccount); String taxCode = this.taxation.applyTaxCode(c.getId()); c.setTaxCode(taxCode); return c; } } 很多Web程序，内部有多个子系统提供服务，经常使用一个统一的Facade入口，例如一个RestApiController，使得外部用户调用的时候，只关心Facade提供的接口，不用管内部到底是哪个子系统处理的。\n更复杂的Web程序，会有多个Web服务，这个时候，经常会使用一个统一的网关入口来自动转发到不同的Web服务，这种提供统一入口的网关就是Gateway，它本质上也是一个Facade，但可以附加一些用户认证、限流限速的额外服务。\n享元 运用共享技术有效地支持大量细粒度的对象。\n享元（Flyweight）的核心思想很简单：如果一个对象实例一经创建就不可变，那么反复创建相同的实例就没有必要，直接向调用方返回一个共享的实例就行，这样即节省内存，又可以减少创建对象的过程，提高运行速度。\n因此，享元模式就是通过工厂方法创建对象，在工厂方法内部，很可能返回缓存的实例，而不是新创建实例，从而实现不可变实例的复用\n在实际应用中，享元模式主要应用于缓存，即客户端如果重复请求某些对象，不必每次查询数据库或者读取文件，而是直接返回内存中缓存的数据。\n代理 为其他对象提供一种代理以控制对这个对象的访问。\n远程代理 远程代理即Remote Proxy，本地的调用者持有的接口实际上是一个代理，这个代理负责把对接口的方法访问转换成远程调用，然后返回结果。Java内置的RMI机制就是一个完整的远程代理模式。\n虚代理 虚代理即Virtual Proxy，它让调用者先持有一个代理对象，但真正的对象尚未创建。如果没有必要，这个真正的对象是不会被创建的，直到客户端需要真的必须调用时，才创建真正的对象。JDBC的连接池返回的JDBC连接（Connection对象）就可以是一个虚代理，即获取连接时根本没有任何实际的数据库连接，直到第一次执行JDBC查询或更新操作时，才真正创建实际的JDBC连接。\n保护代理 保护代理即Protection Proxy，它用代理对象控制对原始对象的访问，常用于鉴权。\n智能引用 智能引用即Smart Reference，它也是一种代理对象，如果有很多客户端对它进行访问，通过内部的计数器可以在外部调用者都不使用后自动释放它。\n行为型模式 行为型模式主要涉及算法和对象间的职责分配。通过使用对象组合，行为型模式可以描述一组对象应该如何协作来完成一个整体任务。\n行为型模式有：\n责任链 命令 解释器 迭代器 中介 备忘录 观察者 状态 策略 模板方法 访问者 责任链 使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。\n责任链模式（Chain of Responsibility）是一种处理请求的模式，它让多个处理器都有机会处理该请求，直到其中某个处理成功为止。责任链模式把多个处理器串成链，然后让请求在链上传递\n┌─────────┐ │ Request │ └─────────┘ │ ┌ ─ ─ ─ ─ ┼ ─ ─ ─ ─ ┐ ▼ │ ┌─────────────┐ │ │ ProcessorA │ │ └─────────────┘ │ │ │ ▼ │ ┌─────────────┐ │ │ ProcessorB │ │ └─────────────┘ │ │ │ ▼ │ ┌─────────────┐ │ │ ProcessorC │ │ └─────────────┘ │ │ └ ─ ─ ─ ─ ┼ ─ ─ ─ ─ ┘ │ ▼ 在实际场景中，财务审批就是一个责任链模式。假设某个员工需要报销一笔费用，审核者可以分为：\nManager：只能审核1000元以下的报销； Director：只能审核10000元以下的报销； CEO：可以审核任意额度。 用责任链模式设计此报销流程时，每个审核者只关心自己责任范围内的请求，并且处理它。对于超出自己责任范围的，扔给下一个审核者处理，这样，将来继续添加审核者的时候，不用改动现有逻辑。\npublic class Request { private String name; private BigDecimal amount; public Request(String name, BigDecimal amount) { this.name = name; this.amount = amount; } public String getName() { return name; } public BigDecimal getAmount() { return amount; } } public interface Handler { // 返回Boolean.TRUE = 成功 // 返回Boolean.FALSE = 拒绝 // 返回null = 交下一个处理 Boolean process(Request request); } ublic class ManagerHandler implements Handler { public Boolean process(Request request) { // 如果超过1000元，处理不了，交下一个处理: if (request.getAmount().compareTo(BigDecimal.valueOf(1000)) \u0026gt; 0) { return null; } // 对Bob有偏见: return !request.getName().equalsIgnoreCase(\u0026#34;bob\u0026#34;); } } public class HandlerChain { // 持有所有Handler: private List\u0026lt;Handler\u0026gt; handlers = new ArrayList\u0026lt;\u0026gt;(); public void addHandler(Handler handler) { this.handlers.add(handler); } public boolean process(Request request) { // 依次调用每个Handler: for (Handler handler : handlers) { Boolean r = handler.process(request); if (r != null) { // 如果返回TRUE或FALSE，处理结束: System.out.println(request + \u0026#34; \u0026#34; + (r ? \u0026#34;Approved by \u0026#34; : \u0026#34;Denied by \u0026#34;) + handler.getClass().getSimpleName()); return r; } } throw new RuntimeException(\u0026#34;Could not handle request: \u0026#34; + request); } } //也可由handler实行传递 还有一些责任链模式，每个Handler都有机会处理Request，通常这种责任链被称为拦截器（Interceptor）或者过滤器（Filter），它的目的不是找到某个Handler处理掉Request，而是每个Handler都做一些工作，比如：\n记录日志； 检查权限； 准备相关资源； \u0026hellip; 例如，JavaEE的Servlet规范定义的Filter就是一种责任链模式，它不但允许每个Filter都有机会处理请求，还允许每个Filter决定是否将请求“放行”给下一个Filter。这种模式不但允许一个Filter自行决定处理ServletRequest和ServletResponse，还可以“伪造”ServletRequest和ServletResponse以便让下一个Filter处理，能实现非常复杂的功能。\n命令 将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化，对请求排队或记录请求日志，以及支持可撤销的操作。\n命令模式（Command）是指，把请求封装成一个命令，然后执行该命令。\n┌──────┐ ┌───────┐ │Client│─ ─ ─\u0026gt;│Command│ └──────┘ └───────┘ │ ┌──────────────┐ ├─\u0026gt;│ CopyCommand │ │ ├──────────────┤ │ │editor.copy() │─ ┐ │ └──────────────┘ │ │ ┌────────────┐ │ ┌──────────────┐ ─\u0026gt;│ TextEditor │ └─\u0026gt;│ PasteCommand │ │ └────────────┘ ├──────────────┤ │editor.paste()│─ ┘ └──────────────┘ public interface Command { void execute(); } public class CopyCommand implements Command { // 持有执行者对象: private TextEditor receiver; public CopyCommand(TextEditor receiver) { this.receiver = receiver; } public void execute() { receiver.copy(); } } public class PasteCommand implements Command { private TextEditor receiver; public PasteCommand(TextEditor receiver) { this.receiver = receiver; } public void execute() { receiver.paste(); } } TextEditor editor = new TextEditor(); editor.add(\u0026#34;Command pattern in text editor.\\n\u0026#34;); // 执行一个CopyCommand: Command copy = new CopyCommand(editor); copy.execute(); editor.add(\u0026#34;----\\n\u0026#34;); // 执行一个PasteCommand: Command paste = new PasteCommand(editor); paste.execute(); System.out.println(editor.getState()); 实际上，使用命令模式，确实增加了系统的复杂度。如果需求很简单，那么直接调用显然更直观而且更简单。\n解释器 给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。\n解释器模式（Interpreter）是一种针对特定问题设计的一种解决方案\n迭代器 提供一种方法顺序访问一个聚合对象中的各个元素，而又不需要暴露该对象的内部表示。\n迭代器模式（Iterator）实际上在Java的集合类中已经广泛使用了。实际上，因为Iterator模式十分有用，因此，Java允许我们直接把任何支持Iterator的集合对象用foreach循环写出来。\n中介 用一个中介对象来封装一系列的对象交互。中介者使各个对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。\n中介模式（Mediator）又称调停者模式，它的目的是把多方会谈变成双方会谈，从而实现多方的松耦合。\n多方会谈的复杂逻辑：\n┌─────────────────┐ ┌─────────────────┐ │ CheckBox List │\u0026lt;───\u0026gt;│SelectAll Button │ └─────────────────┘ └─────────────────┘ ▲ ▲ ▲ │ └─────────────────────┤ ▼ │ ┌─────────────────┐ ┌────────┴────────┐ │SelectNone Button│\u0026lt;────│ Inverse Button │ └─────────────────┘ └─────────────────┘ 如果我们引入一个中介，把多方会谈变成多个双方会谈，虽然多了一个对象，但对象之间的关系就变简单了：\n┌─────────────────┐ ┌─────\u0026gt;│ CheckBox List │ │ └─────────────────┘ │ ┌─────────────────┐ │ ┌───\u0026gt;│SelectAll Button │ ▼ ▼ └─────────────────┘ ┌─────────┐ │Mediator │ └─────────┘ ▲ ▲ ┌─────────────────┐ │ └───\u0026gt;│SelectNone Button│ │ └─────────────────┘ │ ┌─────────────────┐ └─────\u0026gt;│ Inverse Button │ └─────────────────┘ 使用Mediator模式后，我们得到了以下好处：\n各个UI组件互不引用，这样就减少了组件之间的耦合关系； Mediator用于当一个组件发生状态变化时，根据当前所有组件的状态决定更新某些组件； 如果新增一个UI组件，我们只需要修改Mediator更新状态的逻辑，现有的其他UI组件代码不变。 Mediator模式经常用在有众多交互组件的UI上。为了简化UI程序，MVC模式以及MVVM模式都可以看作是Mediator模式的扩展。\n备忘录 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。\n备忘录模式（Memento），主要用于捕获一个对象的内部状态，以便在将来的某个时候恢复此状态。\n标准的备忘录模式有这么几种角色：\nMemonto：存储的内部状态； Originator：创建一个备忘录并设置其状态； Caretaker：负责保存备忘录。 实际上我们在使用备忘录模式的时候，不必设计得这么复杂，只需要对类似TextEditor的类，增加getState()和setState()就可以了。\npublic class TextEditor { private StringBuilder buffer = new StringBuilder(); public void add(char ch) { buffer.append(ch); } public void add(String s) { buffer.append(s); } public void delete() { if (buffer.length() \u0026gt; 0) { buffer.deleteCharAt(buffer.length() - 1); } } } public class TextEditor { ... // 获取状态: public String getState() { return buffer.toString(); } // 恢复状态: public void setState(String state) { this.buffer.delete(0, this.buffer.length()); this.buffer.append(state); } } 观察者 定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。\n观察者模式（Observer）又称发布-订阅模式（Publish-Subscribe：Pub/Sub）。它是一种通知机制，让发送通知的一方（被观察方）和接收通知的一方（观察者）能彼此分离，互不影响。\n┌─────────┐ ┌───────────────┐ │ Store │─ ─ ─\u0026gt;│ProductObserver│ └─────────┘ └───────────────┘ │ ▲ │ │ ┌─────┴─────┐ ▼ │ │ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ Product │ │ Admin │ │Customer │ ... └─────────┘ └─────────┘ └─────────┘ //not Observer Pattern public class Store { Customer customer; Admin admin; private Map\u0026lt;String, Product\u0026gt; products = new HashMap\u0026lt;\u0026gt;(); public void addNewProduct(String name, double price) { Product p = new Product(name, price); products.put(p.getName(), p); // 通知用户: customer.onPublished(p); // 通知管理员: admin.onPublished(p); } public void setProductPrice(String name, double price) { Product p = products.get(name); p.setPrice(price); // 通知用户: customer.onPriceChanged(p); // 通知管理员: admin.onPriceChanged(p); } } //引用一个ProductObserver接口，任何人想要观察Store，只要实现该接口，并且把自己注册到Store即可 public class Store { private List\u0026lt;ProductObserver\u0026gt; observers = new ArrayList\u0026lt;\u0026gt;(); private Map\u0026lt;String, Product\u0026gt; products = new HashMap\u0026lt;\u0026gt;(); // 注册观察者: public void addObserver(ProductObserver observer) { this.observers.add(observer); } // 取消注册: public void removeObserver(ProductObserver observer) { this.observers.remove(observer); } public void addNewProduct(String name, double price) { Product p = new Product(name, price); products.put(p.getName(), p); // 通知观察者: observers.forEach(o -\u0026gt; o.onPublished(p)); } public void setProductPrice(String name, double price) { Product p = products.get(name); p.setPrice(price); // 通知观察者: observers.forEach(o -\u0026gt; o.onPriceChanged(p)); } } 状态 允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。\n状态模式（State）经常用在带有状态的对象中。\n策略 定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。\n策略模式：Strategy，是指，定义一组算法，并把其封装到一个对象中。然后在运行时，可以灵活的使用其中的一个算法。\n模板方法 定义一个操作中的算法的骨架，而将一些步骤延迟到子类中，使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。\n访问者 表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作。\n访问者模式（Visitor）是一种操作一组对象的操作，它的目的是不改变对象的定义，但允许新增不同的访问者，来定义新的操作。\n设计模式 - 廖雪峰的官方网站 (liaoxuefeng.com)\n","date":"2022-08-06T00:00:00Z","permalink":"https://dyhes.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%A6%82%E8%A7%88/","title":"【设计模式】概览"},{"content":"In Kotlin, code statements do not have to end with a semicolon (;)\nThe println() function is used to output values/print text\nThere is also a print() function, which is similar to println(). The only difference is that it does not insert a new line at the end of the output\nVariables To create a variable, use var or val, and assign a value to it with the equal sign (=):\nThe difference between var and val is that variables declared with the var keyword can be changed/modified, while val variables cannot.\nUnlike many other programming languages, variables in Kotlin do not need to be declared with a specified type\nHowever, it is possible to specify the type if you insist\nvar name: String = \u0026#34;John\u0026#34; // String val birthyear: Int = 1975 // Int You can also declare a variable without assigning the value, and assign the value later. However, this is only possible when you specify the type\nvar name: String name = \u0026#34;John\u0026#34; println(name) You can also use the + character to add a variable to another variable\nname rules The general rule for Kotlin variables are:\nNames can contain letters, digits, underscores, and dollar signs Names should start with a letter Names can also begin with $ and _ (but we will not use it in this tutorial) Names are case sensitive (\u0026ldquo;myVar\u0026rdquo; and \u0026ldquo;myvar\u0026rdquo; are different variables) Names should start with a lowercase letter and it cannot contain whitespace Reserved words (like Kotlin keywords, such as var or String) cannot be used as names Date Type Data types are divided into different groups:\nNumbers\nNumber types are divided into two groups:\nInteger types store whole numbers, positive or negative (such as 123 or -456), without decimals. Valid types are Byte, Short, Int and Long.\nFloating point types represent numbers with a fractional part, containing one or more decimals. There are two types: Float and Double.\nTo convert a numeric data type to another type, you must use one of the following functions: toByte(), toShort(), toInt(), toLong(), toFloat(), toDouble() or toChar()\nCharacters\nThe Char data type is used to store a single character. A char value must be surrounded by single quotes\nBooleans\nStrings\nInstead of concatenation, you can also use \u0026ldquo;string templates\u0026rdquo;, which is an easy way to add variables and expressions inside a string.\nJust refer to the variable with the $ symbol:\nvar firstName = \u0026#34;John\u0026#34; var lastName = \u0026#34;Doe\u0026#34; println(\u0026#34;My name is $firstName $lastName\u0026#34;) Arrays\nCondition if..else:In Kotlin, you can also use if..else statements as expressions (assign a value to a variable and return it):\nInstead of writing many if..else expressions, you can use the when expression(like switch-case), which is much easier to read. The when expression is similar to the switch statement in Java.\nval day = 4 val result = when (day) { 1 -\u0026gt; \u0026#34;Monday\u0026#34; 2 -\u0026gt; \u0026#34;Tuesday\u0026#34; 3 -\u0026gt; \u0026#34;Wednesday\u0026#34; 4 -\u0026gt; \u0026#34;Thursday\u0026#34; 5 -\u0026gt; \u0026#34;Friday\u0026#34; 6 -\u0026gt; \u0026#34;Saturday\u0026#34; 7 -\u0026gt; \u0026#34;Sunday\u0026#34; else -\u0026gt; \u0026#34;Invalid day.\u0026#34; } println(result) for...in\nWith the for loop, you can also create ranges of values with \u0026ldquo;..\u0026rdquo;:\nfor (chars in \u0026#39;a\u0026#39;..\u0026#39;x\u0026#39;) { println(chars) } Arrays Arrays are used to store multiple values in a single variable, instead of creating separate variables for each value.\nTo create an array, use the arrayOf() function, and place the values in a comma-separated list inside it:\nval cars = arrayOf(\u0026#34;Volvo\u0026#34;, \u0026#34;BMW\u0026#34;, \u0026#34;Ford\u0026#34;, \u0026#34;Mazda\u0026#34;) Functions The fun keyword is used to declare a function.\nfun myFunction(x: Int): Int { return (x + 5) } There is also a shorter syntax for returning values. You can use the = operator instead of return without specifying the return type. Kotlin is smart enough to automatically find out what it is:\nfun myFunction(x: Int, y: Int) = x + y fun main() { var result = myFunction(3, 5) println(result) } Class we can use the class name to create objects.\nClass Car{ ... } var car=Car() inheritance Use the open keyword in front of the superclass/parent, to make this the class other classes should inherit properties and functions from.\nTo inherit from a class, specify the name of the subclass, followed by a colon :, and then the name of the superclass.\n// Superclass open class MyParentClass { val x = 5 } // Subclass class MyChildClass: MyParentClass() { fun myFunction() { println(x) // x is now inherited from the superclass } } // Create an object of MyChildClass and call myFunction fun main() { val myObj = MyChildClass() myObj.myFunction() } *unrar x -o- -y dtu_training.rar /home/lab303/PointMVSNet/\n","date":"2022-06-16T00:00:00Z","permalink":"https://dyhes.github.io/p/kotlinbasics/","title":"【Kotlin】Basics"},{"content":"object HelloWorld { def main(args: Array[String]): Unit = { println(\u0026#34;Hello, world!\u0026#34;) } } Static members (methods or fields) do not exist in Scala. Rather than defining static members, the Scala programmer declares these members in singleton objects.\nTo compile the example, we use scalac, the Scala compiler.\nOne of Scala’s strengths is that it makes it very easy to interact with Java code. All classes from the java.lang package are imported by default, while others need to be imported explicitly.\nimport java.util.{Date, Locale} import java.text.DateFormat._ object FrenchDate { def main(args: Array[String]): Unit = { val now = new Date val df = getDateInstance(LONG, Locale.FRANCE) println(df format now) } } This last line shows an interesting property of Scala’s syntax. Methods taking one argument can be used with an infix syntax.\ndf format now df.format(now) Scala is a pure object-oriented language in the sense that everything is an object, including numbers or functions.\nClasses in Scala are declared using a syntax which is close to Java’s syntax. One important difference is that classes in Scala can have parameters.\nclass Complex(real: Double, imaginary: Double) { def re() = real def im() = imaginary } //a better way class Complex(real: Double, imaginary: Double) { def re = real def im = imaginary } class Complex(real: Double, imaginary: Double) { def re = real def im = imaginary override def toString() = \u0026#34;\u0026#34; + re + (if (im \u0026gt;= 0) \u0026#34;+\u0026#34; else \u0026#34;\u0026#34;) + im + \u0026#34;i\u0026#34; } type Environment = String =\u0026gt; Int ","date":"2022-06-13T00:00:00Z","permalink":"https://dyhes.github.io/p/scala%E6%B5%85%E7%AA%A5/","title":"【Scala】浅窥"},{"content":"1. 原型 / 构造函数 / 实例 原型(prototype): 一个简单的对象，用于实现对象的 属性继承。在 Firefox 和 Chrome 中，每个JavaScript对象中都包含一个__proto__ (非标准)的属性指向该对象的原型，可通过obj.__proto__进行访问。 构造函数: 可以通过new来 新建一个对象 的函数。 实例: 通过构造函数和new创建出来的对象，便是实例。 实例通过__proto__指向原型，通过constructor指向构造函数（借助原型）。 // 实例 const instance = new Object() 则此时， 实例为instance, 构造函数为Object，我们知道，构造函数拥有一个prototype的属性指向原型，因此原型为:\n// 原型 const prototype = Object.prototype 三者的关系:\n实例.__proto__ === 原型 原型.constructor === 构造函数 构造函数.prototype === 原型 2.原型链 原型链是由原型对象组成，每个对象都有 __proto__ 属性，指向了创建该对象的构造函数的原型，__proto__ 将对象连接起来组成了原型链。是一个用来实现继承和共享属性的有限的对象链。\n属性查找机制: 当查找对象的属性时，如果实例对象自身不存在该属性，则沿着原型链往上一级查找，找到时则输出，不存在时，则继续沿着原型链往上一级查找，直至最顶级的原型对象Object.prototype，如还是没找到，则输出 undefined； 属性修改机制: 只会修改实例对象本身的属性，如果不存在，则进行添加该属性，如果需要修改原型的属性时，则可以用: b.prototype.x = 2；但是这样会造成所有继承于该对象的实例的属性发生改变。 3. 对象的拷贝 浅拷贝: 以赋值的形式拷贝引用对象，仍指向同一个地址，修改时原对象也会受到影响\nObject.assign 展开运算符(\u0026hellip;) 深拷贝: 完全拷贝一个新对象，修改时原对象不再受到任何影响\nJSON.parse(JSON.stringify(obj)) : 性能最快\n具有循环引用的对象时，报错 当值为函数、undefined、或symbol时，无法拷贝 递归进行逐一赋值\n4. new运算符的执行过程 新生成一个对象 链接到原型: obj.__proto__ = Con.prototype 绑定this: apply 返回新对象(如果构造函数有自己 retrun 时，则返回该值) 5. instanceof原理 能在实例的 原型对象链 中找到该构造函数的prototype属性所指向的 原型对象，就返回true。即:\n// __proto__: 代表原型对象链 instance.[__proto__...] === instance.constructor.prototype // return true 6. 类型转换 -、*、/、% ：一律转换成数值后计算 +： 数字 + 字符串 = 字符串， 运算顺序是从左到右 数字 + 对象， 优先调用对象的valueOf -\u0026gt; toString 数字 + boolean/null -\u0026gt; 数字 数字 + undefined -\u0026gt; NaN [1].toString() === '1' {}.toString() === '[object object]' NaN !== NaN 、`+undefined 为 NaN 7. 防抖与节流 防抖与节流函数是一种最常用的 高频触发优化方式，能对性能有较大的帮助。\n防抖 (debounce): 将多次高频操作优化为只在最后一次执行，通常使用的场景是：用户输入，只需再输入完成后做一次输入校验即可。 function debounce(fn, wait, immediate) { let timer = null return function() { let args = arguments let context = this if (immediate \u0026amp;\u0026amp; !timer) { fn.apply(context, args) } if (timer) clearTimeout(timer) timer = setTimeout(() =\u0026gt; { fn.apply(context, args) }, wait) } } 节流(throttle): 每隔一段时间后执行一次，也就是降低频率，将高频操作优化成低频操作，通常使用场景: 滚动条事件 或者 resize 事件，通常每隔 100~500 ms执行一次即可。 function throttle(fn, wait, immediate) { let timer = null let callNow = immediate return function() { let context = this, args = arguments if (callNow) { fn.apply(context, args) callNow = false } if (!timer) { timer = setTimeout(() =\u0026gt; { fn.apply(context, args) timer = null }, wait) } } } ","date":"2022-05-05T00:00:00Z","permalink":"https://dyhes.github.io/p/jsinterview-preparation/","title":"【JS】Interview Preparation"},{"content":"Intro 云计算是指IT资源的交付和使用模式，通过网络以按需、易扩展的方式获得所需的资源（硬件、平台、软件）。典型的云计算提供商往往提供通用的网络业务应用，可以通过浏览器等软件或者其他Web服务来访问，而软件和数据都存储在远程数据中心的服务器上。用户通过计算机、手机等方式接入数据中心，按自己的需求进行运算。\n提供资源的网络被称为“云”。“云”中的资源在使用者看来是可以无限扩展的，并且可以随时获取、按需使用、随时扩展、按使用付费。\n特点 资源池弹性可扩张 按需提供资源服务 网络化的资源接入 虚拟化 提高可靠性和安全性 分类 按技术路线：\n资源整合型\n这种类型的云计算系统在技术实现方面大多体现为集群架构，通过将大量节点的计算资源和存储资源整合后输出。这类系统通常能实现跨节点弹性化的资源池构建，核心技术为分布式计算和存储技术。MPI、Hadoop、HPCC、Storm等都可以被分类为资源整合型云计算系统。\n资源切分型\n这种类型最为典型的就是虚拟化系统，这类云计算系统通过系统虚拟化实现对单个服务器资源的弹性化切分，从而有效地利用服务器资源，其核心技术为虚拟化技术。KVM、VMware都是这类技术的代表。\n按服务对象（部署模式）:\n公有云\n指服务对象是面向公众的云计算服务，公有云对云计算系统的稳定性、安全性和并发服务能力有更高的要求。\n私有云\n指主要服务于某一组织内部的云计算服务，其服务并不向公众开放，如企业、政府内部的云服务。\n混合云\n是把公有云和私有云结合在一起的方式。在这个模式中，用户通常将非企业关键信息外包，并在公有云上处理，而掌握企业关键服务及数据的内容则放在私有云上处理。\n社区云\n是公有云范畴内的一个组成部分。它由众多利益相仿的组织掌控及使用，其目的是实现云计算的一些优势，例如特定安全要求、共同宗旨等。社区成员共同使用云数据及应用程序。\n按资源封装层次：\n基础设施即服务（Infrastructure-as-a-Service，IaaS）：把单纯的计算和存储资源不经封装地直接通过网络以服务的形式提供的用户使用。\nIaaS主要由计算机硬件、网络、存储设备、平台虚拟化环境、效用计费方法、服务级别协议等组成。\n平台即服务（Platform-as-a-Service，PaaS）：计算和存储资源经封装后，以某种接口和协议的形式提供给用户调用，资源的使用者不再直接面对底层资源。\nPaaS是一种分布式平台服务，为用户提供一个包括应用设计、应用开发、应用测试及应用托管的完整的计算机平台。\nPaaS的主要用户是开发人员，PaaS平台的种类目前较少，比较著名的有\nlForce.com\nlGoogle App Engine\nlWindows Azure\nCloud Foundry\n核心技术：REST技术，多租户技术，并行计算技术，应用服务器，分布式缓存\n软件即服务（Software-as-a-Service，SaaS）：将计算和存储资源封装为用户可以直接使用的应用并通过网络提供给用户，SaaS面向的服务对象为最终用户，用户只是对软件功能进行使用，无需了解任何云计算系统的内部结构，也不需要用户具有专业的技术开发能力。\n契机 云计算是分布式计算、并行计算、效用计算、虚拟化、网络存储、负载均衡、热备份冗余等传统计算机和网络技术发展融合的产物，更是SaaS、SOA等技术混合演进的结果，下图所示的五大契机更是直接促进了云计算的诞生。\nIaaS IaaS所提供的虚拟机通常都会带有一个可以连上网络的操作系统（如Windows、Linux）。用户通过网络可以登录并操作虚拟机并按照虚拟机的资源配置和使用时间来付费。\n由于这些提供给用户的功能都是最基础的计算机功能，所以这种服务形式也就被形象地称为基础设施即服务IaaS服务的基本功能。\nIaaS层一般都具有以下基本功能：\n用户管理\n用户管理主要是管理用户账号、用户的环境配置、用户的使用计费等。\n任务管理\n任务管理主要管理用户请求资源的任务，包括任务的调度、任务的执行、任务的生命周期管理等。任务管理的目的是保证所有的任务都能快速高效地完成。\n资源管理（核心）\n主要包括：\n资源抽象\n据业务逻辑和基础设施层服务接口的需要，基础设施层资源的抽象往往是具有多个层次的。目前业界提出的资源模型中就出现了虚拟机（Virtual Machine）、集群（Cluster）、虚拟数据中心（Virtual Data Center）和云（Cloud）等若干层次分明的资源抽象。资源抽象为上层资源管理逻辑定义了操作的对象和粒度，是构建基础设施层的基础。\n资源监控\n资源部署\n资源分发\n资源调度（等）\n安全管理\n整体架构 基础设施平台分为三层:\n基础设施资源池\n基础设施资源池是实现融合基础设施结构的关键要素，是共享服务器、存储和网络的集合，从而能够更快捷地支持业务需求的变化。实现基础设施资源池的一种有效方法就是服务器虚拟化，它是一种可以在一台物理服务器上运行多个逻辑服务器的技术，每个逻辑服务器被称为一个虚拟机。虚拟化技术是IaaS层的核心技术，主要实现了对底层物理资源的抽象，使其成为一个个可以被灵活生成、调度、管理的基础资源单位。\n资源管理平台\n业务服务管理平台\n服务器虚拟化技术 IaaS具体管理的物理资源可以分为三大类：计算资源（CPU、内存）、存储资源和网络资源。\n从计算资源角度来讲，IaaS软件管理的最小的物理单元为一个物理服务器。根据需求，可以在服务器上创建多个虚拟机（如右图所示）。若干配置相同的物理服务器会组成一个集群，要求配置相同的主要原因是因为需要支持虚拟机动态迁移。通常一些集群还会组成更大规模的区域（Zone）。\n根据虚拟化层实现方式的不同，服务器虚拟化主要有两种类型：\n寄宿虚拟化：虚拟机监视器（Virtual Machine Monitor，VMM）是运行在宿主操作系统之上的应用程序，利用宿主操作系统的功能来实现硬件资源的抽象和虚拟机的管理。这种模式的虚拟化实现起来比较容易，但由于虚拟机对资源的管理需要通过宿主操作系统来完成，因此其性能通常比较低。\n原生虚拟化：在原生虚拟化中，直接运行在硬件之上的不是宿主操作系统，而是虚拟化平台（Hypervisor）。虚拟机运行在虚拟化平台上，虚拟化平台提供指令集和设备接口，以提供对虚拟机的支持。这种实现通常具有较好的性能，但是实现起来更为复杂。\n关键特性 多实例\n在一个物理服务器上可以运行多个虚拟服务器，即可以支持多个客户操作系统\n隔离性\n一个虚拟机与其他虚拟机完全隔离，就如同几个独立的物理服务器一样。\n封装性\n服务器虚拟化将物理机的硬件封装为标准化的虚拟硬件设备，保证兼容性。\n高性能\n服务器虚拟化的高性能是指虚拟机监视器的开销要被控制在可承受的范围之内。\n核心技术 CPU虚拟化\nCPU虚拟化技术把物理CPU抽象成虚拟CPU，**任意时刻一个物理CPU只能运行一个虚拟CPU的指令。每个客户操作系统可以使用一个或多个虚拟CPU。**虚拟CPU的运行相互隔离，互不影响。\n在纯软件的CPU虚拟化中，有全虚拟化和半虚拟化两种不同的软件方案。\n全虚拟化是采用二进制动态翻译技术（Dynamic Binary Translation）来解决客户操作系统的特权指令问题。\n半虚拟化是通过修改客户操作系统来解决虚拟机执行特权指令的问题，即将所有敏感指令替换为对底层虚拟化平台的超级调用。\n内存虚拟化\n内存虚拟化技术把物理机的真实物理内存统一管理，包装成多个虚拟的物理内存分别供若干个虚拟机使用，使得每个虚拟机拥有各自独立的内存空间。\n在内存虚拟化中存在着逻辑内存、“物理”内存和机器内存三种内存类型，这三种内存的地址空间被称为逻辑地址、“物理”地址和机器地址。在内存虚拟化中，逻辑内存与机器内存之间的映射关系是由内存虚拟化管理单元来负责的。\n设备与I/O虚拟化\n设备与I/O虚拟化技术对物理机的真实设备进行统一管理，包装成多个虚拟设备给若干个虚拟机使用，响应每个虚拟机的设备访问请求和I/O请求。目前，主流的设备与I/O虚拟化都是通过软件的方式实现的。虚拟化平台作为在共享硬件与虚拟机之间的平台，为设备与I/O的管理提供了便利，也为虚拟机提供了丰富的虚拟设备功能。在I/O设备中有一种比较特殊的设备—网卡。网卡除了和一般的I/O设备一样作为虚拟机的共享设备外，还要解决虚拟机与外部网络或者虚拟机相互之间的通信问题。\n网卡虚拟化技术主要分为两类：虚拟网卡技术和虚拟网桥技术。\n虚拟网卡是指虚拟机中的网卡，是由模拟器通过软件的方法模拟出来的；\n虚拟网桥是指利用软件方法实现的网桥，其作用是在一台服务器中，使多块共享一块物理网卡的虚拟网卡对外表现为多块独立的网卡。\n网络虚拟化\n网络虚拟化是将多个硬件或软件网络资源及相关的网络功能集成到一个可用软件中统一管控的过程，并且对于网络应用而言，该网络环境的实现方式是透明的。该网络环境称为虚拟网络，形成该虚拟网络的过程称为网络虚拟化。\n假如原本的物理机只有一个网卡，那么它有一个MAC地址，并且可以分配一个IP地址，其他机器就可以通过IP地址访问这个物理主机。当创建多个虚拟机以后，每个虚拟机都需要有独立的网络配置，以便它们可以像物理机一样处理各种网络连接。但是这个时候物理机上依然只有一个网卡，多个虚拟机通过这一个物理网卡都能进行顺畅的网络连接的过程即为网络虚拟化\n网络虚拟化一般是指虚拟专用网。虚拟专用网对网络连接进行了抽象，远程用户可以像物理连接在组织内部网络的用户一样来访问该网络。 虚拟专用网络是通过一个公用网络建立一个临时的、安全的连接，是一条穿过混乱的公用网络的安全、稳定隧道。使用这条隧道可以对数据进行几倍加密达到安全使用互联网的目的。虚拟专用网可以保护网络环境，使用户能够快捷、安全地访问组织内部的网络。\n实时迁移技术\n实时迁移（Live Migration）技术是在虚拟机运行过程中，将整个虚拟机的运行状态完整、快速地从原来所在的宿主机硬件平台迁移到新的宿主机硬件平台上，并且整个迁移过程是平滑的，用户几乎不会察觉到任何差异。\n实时迁移需要虚拟机监视器的协助，即通过源主机和目标主机上虚拟机监视器的相互配合，来完成客户操作系统的内存和其他状态信息的拷贝。\nPaaS 平台层的功能以服务的形式提供给用户，可以作为应用开发测试和运行管理的环境，亦平台即服务（Platform as a Service，PaaS）。平台即服务是云计算平台层的外在表现形式，是云计算平台提供的一类重要的功能集合。\nPaaS将“共享”扩展到更大的范围。与基础设施层所共享的对象不同，PaaS所共享的对象是应用运行所需的资源和基础功能。\nCloud Foundry Cloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题。同时，它本身是一个基于Ruby on Rails的由多个相对独立的子系统通过消息机制组成的分布式系统，使平台在各层级都可水平扩展，既能在大型数据中心里运行，也能在一台计算机中运行，二者使用相同的代码库。\n作为新一代云应用平台，Cloud Foundry专为私有云计算环境、企业级数据中心和公有云服务提供商所打造。\nHadoop Hadoop可以为大数据应用提供一个可编程的、经济的、可伸缩的平台。这个分布式系统由分布式文件存储系统（HDFS）以及计算框架（MapReduce）组成。Hadoop是一个开源项目，能为大量数据集提供批量数据处理能力。Hadoop的设计可以容忍软硬件的不可靠，并且为应用开发者提供一个便于开发分布式应用的平台。Hadoop使用没有特殊硬件或特殊网络基础设施的普通的服务器群来形成一个逻辑上可存储大量数据、进行并发计算的集群，这个集群可以被很多团体和个人共享。\n作为一个顶级项目，Hadoop项目包含许多组件子项目，其中最主要的两个子项目分别为Hadoop分布式文件系统（HDFS）和分布式并行计算框架MapReduce。这两个子项目是对Google特有的GFS和MapReduce的直接实现，它们是一对相互独立而又互补的技术。\nHDFS HDFS是一个可以存储极大数据集的文件系统，它是通过向外扩展方式构建的主机集群。它有着独特的设计和性能特点，特别是，HDFS以时延为代价对吞吐量进行了优化，并且通过副本代替物理冗余达到了高可靠性。\nHDFS（Hadoop Distributed File System）是一个分布式文件系统，HDFS提出了“移动计算能力比移动数据更廉价”的设计理念。\nHDFS是一种主/从模式的系统结构，主服务器，即图中的命名节点（NameNode），管理文件系统命名空间和客户端访问，具体文件系统命名空间操作包括“打开”“关闭”“重命名”等，并负责数据块到数据节点之间的映射。\nHDFS同时给应用程序提供接口以保证处理过程尽量靠近数据的位置，减少中间数据传输的开销。\nHDFS的命名空间存放在命名节点上，为了保证访问效率，命名节点在内存中保存整个文件系统的命名空间和文件的块映射图。\nHDFS提出了数据均衡方案，如果某个数据节点上的空闲空间低于特定的临界点，那么就会启动一个计划自动地将数据从一个数据节点迁移到空闲的数据节点上。\nMapReduce MapReduce是一个数据处理模式，它规范了数据在两个处理阶段（Map和Reduce）的输入和输出，并将其应用于任意规模的大数据集。MapReduce与HDFS紧密结合，确保在任何情况下，MapReduce任务都能直接在存储所需数据的HDFS节点上运行。\nSaaS SaaS平台是基于IaaS和PaaS平台之上的。SaaS平台主要是为SaaS应用提供通用的运行环境或系统部件，使SaaS软件提供商能够专注于客户所需业务的开发。\n一般来说，SaaS应用架构可以有四种类型:\n定制开发\n定制开发是一种最简单的提供SaaS服务的类型。这种模型下，SaaS提供商为每个客户定制一套软件，并为其部署。每个客户使用一个独立的数据库实例和应用服务器实例。\n可配置\n可配置类型通过不同的配置满足不同客户的需求，而不需要为每个客户进行特定定制，以降低定制开发的成本。\n多租户架构\n多租户架构通过运行一个应用实例，为不同租户提供服务，并且通过可配置的元数据，为不同用租户提供不同的功能和用户体验。\n可伸缩性的多租户架构\n可伸缩性的多租户架构通过多个运行实例来分担大量用户的访问，从而可以让应用实现近似无限的水平扩展。\n这四种类型由是否支持可定制、可扩展和多租户三个方面的不同组合而决定。一般而言，同时支持三个方面表明应用的灵活性和可用性更强，因而更成熟。所以，这四种架构也被称为SaaS平台四级成熟度模型。每一级都比前一级增加三种特性中的一种。\n虚拟化技术 虚拟化技术和并行计算、分布式计算、网格计算等的发展促进了云计算技术的产生和发展，通过云计算技术，我们将大量的计算机资源组成资源池来创建高度虚拟化的资源提供给用户，即云计算技术解决方案依靠并利用虚拟化提供服务。\n虚拟化技术主要应用在基础设施即服务的服务模式（IaaS）中，大多资源都可以通过虚拟化技术对其进行统一管理。\n现代计算机系统被分为多个自下而上的层次。从下到上依次是裸机（底层硬件）、操作系统，操作系统提供应用程序编程接口及运行在操作系统之上的各种各样的应用程序。 虚拟化技术可以在这些不同层次之间建立虚拟化层，向上提供与真实层次相同或相近的功能，向下只需知道下层的抽象接口，不需要知道下层的具体实现。虚拟化层的引入，必然给系统带来一定的性能损耗，构建、维护虚拟化层也会增加一定的费用。\n分类 虚拟化类型 虚拟化出的目标对象 所处位置 实例 指令集架构级虚拟化 指令集 指令集架构级 Bochs、VLIW 硬件抽象层虚拟化 计算机的各种硬件 应用层 VMWare、Virtual PC、Xen、KVM 操作系统层虚拟化 操作系统 本地操作系统内核 Virtual Server、Zone、Virtuozzo 编程语言层上的虚拟化 应用层的部分功能 应用层 JVM、CLR 库函数层的虚拟化 应用级库函数的接口 应用层 Wine 指令集架构级虚拟化\n指令集架构级虚拟化是通过纯软件方法，模拟出与实际运行的应用程序（或操作系统）所不同的指令集去执行，采用这种方法构造的虚拟机一般称为模拟器（Emulator）。模拟器是将虚拟平台上的指令翻译成本地指令集，然后在实际的硬件上执行。其是、具有、当前比较典型的模拟器系统有Bochs、VLIW等。\n硬件抽象层虚拟化\n硬件抽象层虚拟化是指将虚拟资源映射到物理资源，并在虚拟机的运算中使用实实在在的硬件。即使用软件来虚拟一台标准计算机的硬件配置，如CPU、内存、硬盘、声卡、显卡、光驱等，成为一台虚拟的裸机。\n高度的隔离性 可以支持与宿主机不同的操作系统及应用程序 易于维护及风险低 操作系统层虚拟化\n操作系统层虚拟化是指通过划分一个宿主操作系统的特定部分，产生一个个隔离的操作执行环境。操作系统层的虚拟化是操作系统内核直接提供的虚拟化，虚拟出的操作系统之间共享底层宿主操作系统内核和底层的硬件资源。\n与硬件虚拟化区别：\n操作系统虚拟化是以原系统为模板，虚拟出的是原系统的副本，而硬件虚拟化虚拟的是硬件环境，然后真实地安装系统。 操作系统虚拟化虚拟出的系统只能是物理操作系统的副本，而硬件虚拟化虚拟出的系统可以为不同的系统。 虚拟出的系统间关系不同，操作系统虚拟化虚拟的多个系统有较强的联系。 性能损耗不同，操作系统虚拟化虚拟出的系统都是虚拟的，性能损耗低，而硬件虚拟化是在硬件虚拟层上实实在在安装的操作系统，性能损耗高。 系统虚拟化 系统虚拟化是指在一台物理计算机系统上虚拟出一台或多台虚拟计算机系统。\n优点：\n硬件无关性\n虚拟机与底层硬件之间是虚拟化层，其与底层硬件之间并没有直接的联系。所以只要另一台计算机提供相同的虚拟硬件抽象层，一个虚拟机就可以无缝地进行迁移。\n隔离性\n使用虚拟机，应用软件可以独立地在虚拟机上运行，不受其他虚拟机的影响。即使其他的虚拟机崩溃，也可以正常运行。\n多实例\n在一台物理机上可以运行多台虚拟机，而一台虚拟机上又可以安装多个操作系统。\n特权功能\n虚拟化层拥有更高的特权体现在：虚拟化层中添加的功能不需要了解客户机的具体语义，实现起来更加容易，并且添加的功能具有较高的特权级，不能被客户机绕过。\n服务器虚拟化 系统虚拟化的最大价值在于服务器虚拟化。服务器虚拟化是将系统虚拟化技术应用于服务器上，将一台或多台服务器虚拟化为若干台服务器使用。\n分类 将一台服务器虚拟成多台服务器，即将一台物理服务器分割成多个相互独立、互不干扰的虚拟环境； 服务器整合，就是多个独立的物理服务器虚拟为一个逻辑服务器，使多台服务器相互协作，处理同一个业务； 服务器先整合、再切分，就是将多台物理服务器虚拟成一台逻辑服务器，然后再将其划分为多个虚拟环境，即多个业务在多台虚拟服务器上运行。 所需技术 CPU虚拟化 内存虚拟化 设备与I/O虚拟化 网络虚拟化 桌面虚拟化 桌面虚拟化依赖于服务器虚拟化，直观上来说就是将计算机的桌面进行虚拟化，是将计算机的桌面与其使用的终端设备相分离。\n桌面虚拟化为用户提供部署在云端的远程计算机桌面环境，用户可以使用不同的终端设备，通过网络来访问该桌面环境，即在虚拟桌面环境服务器上运行用户所需要的操作系统和应用软件。\n优势 更灵活的访问和使用\n更低的用户终端配置\n更便于集中管控终端桌面\n更高的数据安全性\n更低的成本\n解决方案 基于VDI的虚拟桌面解决方案\n是基于服务器虚拟化的，拥有服务器虚拟化的所有优点。其原理是在远程数据中心的服务器上安装虚拟机并在其中部署用户所需要的操作系统及操作系统上的各种应用，此时虚拟桌面就是虚拟机上的操作系统及其上的各种应用。\n基于SBC的虚拟桌面解决方案\n原理是在数据中心内的物理机上直接安装、运行操作系统和应用软件，此时的桌面就是服务器上的物理桌面。\n项目 VDI SBC 服务器性能要求 高，需要能支持服务器虚拟化软件的运行 低，只要能部署操作系统及应用软件 用户支持扩展性 低，与服务器上能同时承载的虚拟机个数有关 高，与服务器上能同时支持的应用软件执行实例有关 方案实施复杂性 高，需要在安装和管理服务器虚拟化软件的前提下提供服务 低，只需要以传统方式安装和部署应用软件就可提供服务 桌面交付兼容性 高，支持Linux桌面、Windows桌面等桌面上的应用 低，只支持Windows上的应用 桌面安全隔离性 高，依赖于虚拟机之间的安全隔离性 低，依赖于Windows操作系统进程之间的安全隔离性 桌面性能隔离性 高，依赖于虚拟机之间的性能隔离性 低，依赖于Windows操作系统进程之间的性能隔离性 终端应用程序兼容性 无，每一个桌面都是一个独立的工作站 有，依赖于操作系统的版本 提供服务的性能 高，在一个刀片上只有一个用户或少数几个用户 低，在一个刀片上的用户数相对较多 云计算平台 Amazon AWS Amazon Web Services（AWS）是Amazon Web服务的总称。Amazon Web Services共包括了12个门类共33种云计算产品与服务。通过AWS的IT基础设施层服务和丰富的平台层服务，用户可以在Amazon公司的云计算平台上构建各种企业级应用和个人应用。\nGoogle GAE 2008年Google公司推出了**Google AppEngine（GAE）**Web运行平台，使用户的业务系统能够运行在Google分布式基础设施上。GAE平台具有易用性、可伸缩性、低成本的特点。另外，Google公司还提供了丰富的云端应用，如Gmail、Google Docs等。Gmail是一个电子邮箱的SaaS平台，Google Docs是一个界面类似于微软Office产品的SaaS平台。\nMicrosoft Azure Windows Azure是Microsoft云平台上的操作系统。Microsoft在云计算的目标不仅仅是提供一个云计算操作系统，而是为开发者提供一个PaaS平台。通常Microsoft Azure是指Mircrosof的Azure服务平台，Windows Azure是指Microsoft的Azure云计算操作系统。\n云存储 云存储（Cloud Storage）的概念与云计算类似，它是指通过集群应用、网络技术或分布式文件系统等功能，将网络中大量各种不同类型的存储设备通过应用软件集合起来协同工作，共同对外提供数据存储和业务访问功能的一个系统。\n用户使用云存储，并不是使用某一个存储设备，而是使用整个云存储系统带来的一种数据访问服务。云存储的核心是应用软件与存储设备相结合，通过应用软件来实现存储设备向存储服务的转变，是一个以数据存储和管理为核心的云计算系统。\n当云计算系统运算和处理的核心是大量数据的存储和管理时，云计算系统中就需要配置大量的存储设备，那么云计算系统就转变成为一个云存储系统，所以云存储是一个以数据存储和管理为核心的云计算系统。\n云存储架构 紧耦合对称（TCS）架构\n传统的存储系统利用紧耦合对称架构，这种架构的设计旨在解决HPC（高性能计算、超级运算）问题，现在其正在向外扩展成为云存储，从而满足快速呈现的市场需求。\n松耦合非对称（LCA）架构\n新的存储系统已经采用了松弛耦合非对称架构，集中元数据和控制操作，这种架构并不非常适合高性能HPC，但是这种设计旨在解决云部署的大容量存储需求。\n云存储类型 块存储\n块存储会把单笔的数据写到不同的硬盘，借以得到较大的单笔读写带宽，适合用在数据库或者需要单笔数据快速读写的应用。它的优点是对单笔数据读写很快，缺点是成本较高，并且无法解决真正海量文件的存储。\nDAS是直接连接于主机服务器的一种储存方式，每一台主机服务器有独立的存储设备，每台主机服务器的存储设备无法互通，需要跨主机存取资料时，必须经过相对复杂的设定，是一种应用较为早的技术实现。 SAN是一种用高速（光纤）网络连接专业主机服务器的一种储存方式，此系统会位于主机群的后端，它使用高速I/O 连接方式，如SCSI、ESCON 及Fibre-Channels。SAN特点是代价高、性能好。 文件存储\n文件存储是基于文件级别的存储，它是把一个文件放在一个硬盘上，即使文件太大拆分时，也放在同一个硬盘上。它的缺点是对单一文件的读写会受到单一硬盘效能的限制，优点是对一个多文件、多人使用的系统，总带宽可以随着存储节点的增加而扩展，它的架构可以无限制地扩容，并且成本低廉。\n对象存储\n与文件系统不同，对象存储系统并非将文件组织成一个目录层次结构，而是在一个扁平化的容器组织中存储文件（在Amazon的S3系统中被称作“桶”），并使用唯一的ID（在S3中被称作“关键字”）来检索它们。其结果是对象存储系统相比文件系统需要更少的元数据来存储和访问文件，并且它们还减少了因存储元数据而产生的管理文件元数据的开销。\n存储虚拟化 基于主机的虚拟化存储的实现，其核心技术是通过增加一个运行在操作系统下的逻辑卷管理软件将磁盘上的物理块号映射成逻辑卷号，并以此实现把多个物理磁盘阵列映射成一个统一的虚拟的逻辑存储空间（逻辑块），实现存储虚拟化的控制和管理。 基于存储设备虚拟化技术依赖于提供相关功能的存储设备的阵列控制器模块，常见于高端存储设备，其主要应用针对异构的SAN存储构架。 基于存储网络虚拟化的技术的核心是在存储区域网中增加虚拟化引擎实现存储资源的集中管理，其具体实施一般是通过具有虚拟化支持能力的路由器或交换机实现。在此基础上，存储网络虚拟化又可以分为带内虚拟化与带外虚拟化两类。 实现层面 主　机 网　络 设　备 优点 支持异构的存储系统；不占用磁盘控制器资源 与主机无关，不占用主机资源；能够支持异构主机、异构存储设备；对不同存储设备构建统一管理平台，可扩展性好 与主机无关，不占用主机资源；数据管理功能丰富；技术成熟度高 缺点 占用主机资源，降低应用性能；存在操作系统和应用的兼容性问题；主机数量越多，管理成本越高 占用交换机资源；面临带内、带外的选择；存储设备兼容性需要严格验证；原有的磁盘阵列的高级存储功能将不能使用 受制于存储控制器接口资源，虚拟化能力较弱；异构厂家存储设备的高级存储功能将不能使用 主要用途 使服务器的存储空间可以跨越多个异构磁盘阵列，常用于在不同磁盘阵列之间做数据镜像保护 异构存储系统整合和统一数据管理（灾备） 异构存储系统整合和统一数据管理（灾备） 适用场景 主机已采用SF卷管理，需要新接多台存储设备；存储系统中包含异构阵列设备；业务持续能力与数据吞吐要求较高 系统包括不同品牌和型号的主机与存储设备；对数据无缝迁移及数据格式转换有较高时间性保证 系统中包括自带虚拟化功能的高端存储设备与若干需要利旧的中低端存储 不适用场景 主机数量大，采用SF会涉及高昂的费用，待迁入系统数据量过大，如果只能采取存储级迁移方式，数据格式转换将耗费大量的时间和人力 对业务持续能力和稳定性要求苛刻 需要新购机头时，费用较高；存在更高端的存储设备 Swift Swift是OpenStack开源云计算项目的子项目之一，被称为对象存储，其提供了强大的扩展性、冗余性和持久性。\nSwift是一个可以存放大量非结构化数据的、支持多租户的、可以高扩展的持久性对象存储系统。Swift通过REST API来存放、检索和删除容器中的对象。开发者可以直接通过Swift API使用Swift服务，也可以通过多种语言的客户库程序中的任何一个进行使用，例如Java、Python、PHP和C#。\nSpark Spark最初由美国加州伯克利大学（UCBerkeley）的AMP实验室于2009年开发，是基于内存计算的大数据并行计算框架，可用于构建大型的、低延迟的数据分析应用程序。2013年Spark加入Apache孵化器项目后发展迅猛，如今已成为Apache软件基金会最重要的三大分布式计算系统开源项目之一（Hadoop、Spark、Storm）。\nScala Scala是一门现代的多范式编程语言，运行于Java平台（JVM，Java 虚拟机），并兼容现有的Java程序。Scala的特性：\nScala具备强大的并发性，支持函数式编程，可以更好地支持分布式系统\nScala语法简洁，能提供优雅的API, Scala兼容Java，运行速度快，且能融合到Hadoop生态圈中\nScala是Spark的主要编程语言，但Spark还支持Java、Python、R作为编程语言；\nScala的优势是提供了REPL（Read-Eval-Print Loop，交互式解释器），提高程序开发效率。\n运行架构 Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor）\n前置概念 RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型\nDAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系\nExecutor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task\nApplication：用户编写的Spark应用程序\nJob：一个Job包含多个RDD及作用于相应RDD上的各种操作\nStage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集\nTask：运行在Executor上的工作单元\n一个Application由一个Driver和若干个Job构成，一个Job由多个Stage构成，一个Stage由多个没有Shuffle关系的Task组成。当执行一个Application时，Driver会向集群管理器申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果会返回给Driver，或者写到HDFS或者其他数据库中\n","date":"2022-05-03T00:00:00Z","permalink":"https://dyhes.github.io/p/%E4%BA%91%E8%AE%A1%E7%AE%97concepts/","title":"【云计算】Concepts"},{"content":"梵语\n梵语，是印欧语系的印度-伊朗语族的印度-雅利安语支的一种语言，是该语系中最古老的语言之一。 和拉丁语一样，梵语已经成为一种属于学术和宗教的专门用语。 虽然现代梵语的母语使用者非常少，但是因为梵文本身的特殊性，在现代佛教、印度教和耆那教的经典中依然可以随处找到梵语\n佛陀\n佛陀为梵语 बुद्ध 的音译，意为觉者\n菩萨\n菩薩具名菩提薩埵（梵語：बोधिसत्त्व，bodhisattva；巴利語：बोधिसत्त，bodhisatta），意譯為道心眾生、覺有情、闓士、開士、高士、大士等。菩提是佛道，薩埵是眾生，眾生發心求入佛道名菩薩。又，上求菩提，下化眾生，故為菩薩[1]。已斷煩惱結，於生死輪迴自在、具救度眾生能力者，稱為大菩薩，即菩提薩埵摩訶薩埵（梵語：bodhisattva mahāsattva），簡稱**菩薩摩訶薩或摩訶薩**。菩萨是因位正在修行的圣者。佛是果位已经圆满的圣者。 一切行菩萨道，修行佛法的众生，在成佛之前都可以称作菩萨。\n五明\n源于古印度婆罗门传统，后被佛教所用，指五门学科。\n声明（梵语：śabda-vidyā；藏文：སྒྲ་རིག་པ་）：语言、语言学、文字学、训诂学、文学等，即语言表达及书写、著述能力。 因明（梵语：hetu-vidyā；藏文：ཚད་མ་རིག་པ་）：逻辑学、论辩术，已证内明的菩萨，可以借此辩驳外道的异论，令未相信佛法的人相信，令已相信的人信仰更加坚定。 内明（梵语：adhyātma-vidyā；藏文：ནང་རིག་པ་）：佛学教理学、哲学，为五明之首；对佛教而言，即经、律、论三藏。对已证实相的菩萨，内明既利自己修行，又可帮助他人开悟。对婆罗门而言，则是指研习四吠陀论。 医方明（梵语：cikitsā-vidyā；藏文：གསོ་རིག་པ་）：医学、药学乃至咒术（陀罗尼）等，可助强身健体，并医治别人的色身病苦，拯救众生。 工巧明（梵语：śilpa-karma-sthāna-vidyā；藏文：བཟོ་རིག་པ་）：艺术、科学、工艺、农业等，即日常生活中所需要的技艺。 五明：\n（1）語文學的聲明。\n（2）工藝學的工巧明。\n（3）醫藥學的醫方明。\n（4）論理學的因明。\n（5）宗教學的內明。\n四吠陀\n古印度婆罗门教的四部根本圣典\n（1）梨俱吠陀—宗教的讚歌。\n（2）沙磨吠陀—祭祀儀式的頌文。\n（3）夜柔吠陀—祭祀儀式的歌詞。\n（4）阿闍婆吠陀—俗世相傳的咒術。\n释迦牟尼\n释迦牟尼（梵语：शाक्यमुनि，Śākyamuni，意为“释迦族之圣者”\n法\n法（梵語：धर्म，dharma，巴利語：धम्म，dhamma），可直译为万法、轨则[1]等，指佛教對世間、出世間的所有義理與修證的開示。世间所有事物和现象称爲世间法或有为法，与之相对的是出世间法或无为法。所有法的集合為一切法。\n因缘\n一物之生，亲与强力者为因，疏添弱力者为缘，萬法是由因緣和合共依存。例如我們的軀體是由父母生育為緣，而生命是自己帶來的業力為因，我們是自己過去所做的無明煩惱業力，所以招感這個身體果報，有了果報身體的生，那麼老、病、死，就不能避免，所以要解脫生老病死的痛苦，只有修道斷除根本煩惱的「無明」。\n業力\n業（梵語：कर्मन्，羅馬化：karma，巴利語：kamma，漢語音譯作羯磨或羯摩等），或稱果報、業力、業報、報應，意為行動、作用或功業。[1] 對於靈性的相信者，該術語也指因果關係的靈性原則，通常被描述為業力原則（英語：principle of karma），其中個人（原因）的意圖和行為會影響該個人的未來。業力描述了一種靈性概念，根據該概念，每一個行動—身體的和心智的—都不可避免地產生後果。\n三皈依\n三皈依即是皈依佛，皈依法，皈依僧。皈依的「皈」字是反黑為白，轉染成淨，把心地住於正念清淨境界；「依」字是依靠的意思。普通皈依二字簡釋為依靠、救度、恭敬等義。如把三皈依淺白地說：皈依佛是我依靠佛陀的指示，皈依法是我依靠教義求得真理，皈依僧是我依靠僧伽引導，正信佛教。換句話說，皈依了三寶可得救度，好像大海中的航船，需要有指南針，迷路的小孩重歸慈母懷抱，便可獲得天倫的溫暖與慰藉。\n皈依三寶是進入佛教的初步儀式，表示正式做一個佛弟子。要知道一個忠實的佛弟子，是要實行佛陀的教理。佛陀在世時告訴門下弟子說：「諸惡莫作，眾善奉行，自淨其意，是諸佛教。」\n佛法总纲\n佛陀法藥，廣說雖有八萬四千，簡約之則不外三大綱領，曰戒、定、慧，此名三無漏學；以能斷煩惱，能了生死，故稱無漏（漏喻煩惱）。這可說是佛法的本體，若離此三，則非真正佛法。\n一、戒即戒律。譬如家有家規，國有國法。修學佛法，則需持佛戒律，這是學佛的根本。因為持戒，才能防非止惡，清淨三業。三藏經典中，律藏屬於戒學，故曰「律規三業」，規範三業，非戒而何。然戒有禁止與勒令，禁止是消極的去惡，勒令是積極的為善，所以戒也包括一切的善法。\n二、定即禪定。禪是梵語「禪那」省稱，定是梵語「三昧」譯名。禪定要義，在於息慮靜緣，內須不打妄想，外要不亂攀緣，（外不取相，內不動心），這卻是入道的樞紐。三藏經典中，經藏屬於定學，故曰「經詮一心」，一心即是定。若能入定，自可引發本性的功德；譬如盆水，擾動則混濁不清，靜寂則光明映照，眾生的心水亦然，所以定也包括一切的神通功德。\n三、慧即智慧。智是決斷，慧是簡擇。這是學佛最終目的，開發智慧，就能去惑證理，所謂破諸煩惱，證真實理。三藏經典中，論藏屬於慧學，故曰「論辨邪正」，辨析邪正，最能開發智慧。然智有實智、權智，達真實理是實智，用之權巧攝眾是權智，所以慧也包括一切的權智。\n須知，三學有其次第，必先持戒而後有定，若能入定自然開慧。故楞嚴經云：「攝心為戒，因戒生定，因定發慧，是則名為三無漏學。」\n缘起\n佛陀在菩提樹下，夜睹明星，證悟一切萬有都是從因緣所生起的，沒有一件事物能夠例外；所以「諸法因緣生，諸法因緣滅」，「緣起性空，性空緣起」，就成為一條永久不變的定律\n五乘教法\n人乘，天乘，聲聞乘，緣覺乘，菩薩乘\n「乘」是載運的意義。即是把眾生從迷岸運至悟岸的工具，這工具乃指教法。故乘是譬喻而指教法—此即指四聖諦的理法，由聞四聖諦可到達（運載）悟道之彼岸。\n六根本烦恼\n一、貪—是貪愛、貪戀、貪著。對於五欲過分追求，或不義的竊取，或對於自我的財物與所學的理法，慳吝不捨，不肯惠施於人。只為私欲，損害他人，貪之為害，罪業深重，經云：「死墮餓鬼，受饑渴報。」\n二、瞋—是瞋恚。對違逆不順之境，不能忍受，生起忿怒，或橫起暴惡，損害他人，或饑渴寒熱到來，無涵養心，而生瞋恨。華嚴經云：「一念瞋心起，八萬障門開。」瞋恚之惱害，死墮地獄，受身相醜陋報。\n三、癡—是愚癡迷昧，智慧閉塞，是非不辨，事理不明，謗無因果，起諸邪見，名為愚癡。因愚癡而妄生貪求，求之不得，則成瞋恚，故愚癡為三毒（貪、瞋、癡）總根，能造傷天害理之事，死墮畜生中，受人宰割。\n四、慢—是我慢貢高，驕傲自大，看不起人，對於他人不如自己的，或超過自己的，乃至與我程度相等的，都表現輕慢態度，造成不能與人和睦共處的苦惱。\n五、疑—疑是懷疑不信。對佛教真理，善惡業力，以及世間一切實事實理，都不能誠信無疑，杜塞善門，滋生邪見，造出是非惡業。\n以上貪、瞋、癡、慢、疑五煩惱，是心著垢染，繫縛不脫，謂之思惑，即思想上的迷惑錯誤。其性質遲鈍，難以制服，尤其無明惑不易斷滅，故稱為五鈍使。\n六、不正見，由思惟不正，產生錯誤的知見，有五不正見，即是：\n（一）、薩迦耶見—華譯身見，或我見。因執此身以為有「我」，其實此身是五蘊（色、受、想、行、識名五蘊）四大（地、水、火、風名四大）和合之身，本來是無常敗壞之物，而無知執之為「一」，為「常」，為「遍」，為「主宰」的實我存在，即是薩迦耶見。\n（二）、邊見—又名邊執見。邊是鄙陋或偏差的意思。即謂所得的知見鄙陋，有所偏蔽，執常執斷，各落一邊。執常的知見，以「常」為恒常不變，人死仍舊為人，牛死仍舊為牛，貧窮永遠是貧窮，富貴永遠是富貴。執斷的知見，以人死一滅永滅，無有生死輪迴升沈之事，則作惡何畏？無善惡因果可論。\n（三）、邪見—不正的知見，不信正法，撥無因果，譭謗聖賢，壞諸善事。\n（四）、見取見—見即指前說的「我見、邊見、邪見」。取是取著不捨，乃對前說三不正見，執以為實。又這見亦名「非果計果」，即是自己還沒有親證聖果，而妄執已得到聖果的錯覺。\n（五）、戒禁取見—戒是戒止殺盜淫妄之類，禁是禁絕飲酒、賭博、吸鴉片煙等嗜好。持戒和守禁，本來是好的，為什麼反說不正見？這因為過患在一個「取」字。取是執著，即妄自執著其所持不正的戒禁為最殊勝。如印度，有一種外道，他們持牛戒，或持狗戒，便食草食糞，或作投岩、投水、赴火、臥荊等種種苦行，本非出世正因，而執著修此類苦行之因，是獲得最上的涅槃妙果，故又名「非因計因」。\n轉法輪\n佛說法，叫做轉法輪。輪的意義：（1）如碾米之輪，能碾破糠秕，喻佛法能破五住煩惱。（2）如舟車的輪，能通行抵達於目的地，喻佛法能通達四德涅槃（常、樂、我、淨叫做四德涅槃）。轉的意思，謂諸眾生能所改轉，即轉迷為悟，轉染為淨，轉凡成聖。故法輪依第一解，使眾生藉以破五住煩惱（五住煩惱—（1）三界見惑，（2）欲界思惑，（3）色界思惑，（4）無色界思惑，（5）無明惑。），依第二解，使眾生由聞佛法喻如舟車可運載到達四德涅槃之岸。\n五陰熾盛苦\n身心欲求如火之熾盛，故苦\n一、五蘊—即色、受、想、行、識五個蘊叫做五蘊，「蘊」是「積聚」義。又名「五陰」，「陰」是「障蔽」義，能陰覆真如法性，起諸煩惱。現把五個蘊的意義說明於下：\n１、色—即物質，變礙為義，是地、水、火、風四大種所造。\n２、受—領納為義，包括苦、樂、捨三受。\n３、想—取像構想為義，於善惡憎愛等境界，取種種相，作種種想。\n４、行—意念中遷流不息，起諸造作為義。即由行動去造作善惡業。\n５、識—了別為義，由心識了別所緣所對的境界。\n五蘊中的色蘊是屬於物質的叫做色法。受、想、行、識四蘊是屬於精神的叫做心法。\n四無量心\n菩薩須內具四無量心：就是慈、悲、喜、捨四種無量心。怎樣叫做無量呢？無量是沒有邊際，也是沒有人我的領域。當修習這慈、悲、喜、捨四無量心的時候，對每一個人，甚至整個國家，整個世界的眾生，不管是人類或其他的有情眾生，都希望他們能夠得到離苦得樂，同時得到無量的福報，所以名為無量。再分釋於下：\n一、慈無量心：慈是願人得樂的意思。世人多隻知自求快樂，忘掉別人的痛苦，但修習慈無量心的人就不同，他不但只求自己的親人得到快樂，乃至十方世界一切眾生，都要得到快樂，還要腳踏實地，去幫助他們得到快樂，即是無量的「慈」，能給無量的眾生得樂。\n二、悲無量心：悲是一種悲憐他人受苦的同情心，平常人只知自求解除痛苦，不顧別人的痛苦，但這悲無量心則相反，只知救拔別人的痛苦，卻忘記自己的痛苦，所謂「我不入地獄，誰入地獄？」這是悲心似海的宏願，為的是拔人之苦，即是無量的「悲」，能拔無量眾生之苦。\n三、喜無量心：喜是歡喜，眼看別人能夠離苦得樂，頓生一種無限喜悅的心情。世人只為自己得到快樂的事情，或其親人得到升官發財，而生歡喜心，很少見別人樂事而生起歡喜心的。且有一部份的人，還存著幸災樂禍的心理，見人快樂，心裏則悶悶不樂，見人受難，心裏則暗暗歡喜。但這種喜無量心則不同，不但見人得樂而生歡喜，就是冤家敵人，能有歡樂的事，也同樣表示無量歡喜。\n四、捨無量心：捨是捨掉一種分別執著的觀念。為善雖可樂，但恐有人我冤親的計別，故應捨棄。又無論做任何善事，也要把為善的怡然自得之心理捨掉。則心量廣闊，慈悲濟度無量眾生，心不存著，正似一澤秋水如鏡，月穿無痕，故稱捨無量心。\n序 一切眾生皆有佛性，皆可成佛，而不能成佛的原因，是無明煩惱障蔽了佛性。故佛陀的成道，是了悟緣起，斷除無明，慧光煥發，佛性顯現，內心清淨，燃起了真理光芒，照耀人間。\n","date":"2022-05-01T00:00:00Z","permalink":"https://dyhes.github.io/p/%E4%BD%9B%E5%AD%A6%E6%A6%82%E5%BF%B5/","title":"【佛学】概念"},{"content":"Introduction Git is a version control system. Git helps you keep track of code changes. Git is used to collaborate on code.\nGit and GitHub are different things. In this tutorial you will understand what Git is and how to use it on the remote repository platforms, like GitHub.\nGit allow you to:\nManage projects with Repositories Clone a project to work on a local copy Control and track changes with Staging and Committing Branch and Merge to allow for work on different parts and versions of a project Pull the latest version of the project to a local copy Push local updates to the main project work flow Initialize Git on a folder, making it a Repository Git now creates a hidden folder to keep track of changes in that folder When a file is changed, added or deleted, it is considered modified You select the modified files you want to Stage The Staged files are Committed, which prompts Git to store a permanent snapshot of the files Git allows you to see the full history of every commit. You can revert back to any previous commit. Git does not store a separate copy of every file in every commit, but keeps track of changes made in each commit! File States Files in your Git repository folder can be in one of 2 states:\nTracked - files that Git knows about and are added to the repository Untracked - files that are in your working directory, but not added to the repository When you first add files to an empty repository, they are all untracked. To get Git to track them, you need to stage them, or add them to the staging environment.\nStaging Environment One of the core functions of Git is the concepts of the Staging Environment, and the Commit.\nAs you are working, you may be adding, editing and removing files. But whenever you hit a milestone or finish a part of the work, you should add the files to a Staging Environment.\nStaged files are files that are ready to be committed to the repository you are working on.\nCommit Since we have finished our work, we are ready move from stage to commit for our repo.\nAdding commits keep track of our progress and changes as we work. Git considers each commit change point or \u0026ldquo;save point\u0026rdquo;. It is a point in the project you can go back to if you find a bug, or want to make a change.\nWhen we commit, we should always include a message.\nSometimes, when you make small changes, using the staging environment seems like a waste of time. It is possible to commit changes directly, skipping the staging environment. The -a option will automatically stage every changed, already tracked file.\nSkipping the Staging Environment is not generally recommended.\nSkipping the stage step can sometimes make you include unwanted changes.\nHelp If you are having trouble remembering commands or options for commands, you can use Git help.\nThere are a couple of different ways you can use the help command in command line:\ngit command -help - See all the available options for the specific command git help --all - See all possible commands Branch In Git, a branch is a new/separate version of the main repository.\nBranches allow you to work on different parts of a project without impacting the main branch.\nWhen the work is complete, a branch can be merged with the main project.\nYou can even switch between branches and work on different projects without them interfering with each other.\nnew branch\ngit branch branch-name list branch\ngit branch git branch -a (local and fetched remote) git branch -r (only remote) delete branch\ngit branch -d branch-name switch to branch\ngit checkout branch-name Note: Using the -b option on checkout will create a new branch, and move to it, if it does not exist.\nmerge branch\ngit merge another-branch\nmergeanother-branchto current branch.\nUndo Revert revert is the command we use when we want to take a previous commit and add it as a new commit, keeping the log intact.\nStep 1: Find the previous commit:\nStep 2: Use it to make a new commit:\nFirst thing, we need to find the point we want to return to. To do that, we need to go through the log.\nTo avoid the very long log list, we are going to use the --oneline option, which gives just one line per commit showing:\nThe first seven characters of the commit hash the commit message We revert the latest commit using git revert HEAD (revert the latest change, and then commit), adding the option --no-edit to skip the commit message editor (getting the default revert message)\ngit revert HEAD --no-edit Reset reset is the command we use when we want to move the repository back to a previous commit, discarding any changes made after that commit.\nStep 1: Find the previous commit:\nStep 2: Move the repository back to that step:\ngit reset 9a9add8 Amend commit --amend is used to modify the most recent commit.\nIt combines changes in the staging environment with the latest commit, and creates a new commit.\nThis new commit replaces the latest commit entirely.\nOne of the simplest things you can do with --amend is to change a commit message.\ngit commit --amend -m \u0026#34;Added lines to README.md\u0026#34; Ignore When sharing your code with others, there are often files or parts of your project, you do not want to share.\nExamples\nlog files temporary files hidden files personal files etc. Git can specify which files or parts of your project should be ignored by Git using a .gitignore file.\nGit will not track files and folders specified in .gitignore. However, the .gitignore file itself IS tracked by Git.\nGithub add origin\ngit remote add ori(or anything you like) URL update local\nWhen working as a team on a project, it is important that everyone stays up to date.\nAny time you start working on a project, you should get the most recent changes to your local copy.\nWith Git, you can do that with pull.\npull is a combination of 2 different commands:\nfetch merge Let\u0026rsquo;s take a closer look into how fetch, merge, and pull works.\nfetch gets all the change history of a tracked branch/repo.\nSo, on your local Git, fetch updates to see what has changed on GitHub:\ngit pull ori remote-b //create new branch in local git pull ori remote-b:local-b update remote\ngit push ori //create new branch in remote git push ori local-b:remote-b Commands init\ngit init status\ngit status On branch master No commits yet Untracked files: (use \u0026#34;git add ...\u0026#34; to include in what will be committed) index.html nothing added to commit but untracked files present (use \u0026#34;git add\u0026#34; to track) git status --short M index.html Note: Short status flags are:\n?? - Untracked files A - Files added to stage M - Modified files D - Deleted files add\ngit add filename/--all Using --all instead of individual filenames will stage all changes (new, modified, and deleted) files. The shorthand command for git add --all is git add -A\ncommit\ngit commit -m \u0026#34;First release of Hello World!\u0026#34; git commit -a -m \u0026#34;Updated index.html with a new line\u0026#34; log\ngit log commit 09f4acd3f8836b7f6fc44ad9e012f82faf861803 (HEAD -\u0026gt; master) Author: w3schools-test Date: Fri Mar 26 09:35:54 2021 +0100 Updated index.html with a new line commit 221ec6e10aeedbfd02b85264087cd9adc18e4b26 Author: w3schools-test Date: Fri Mar 26 09:13:07 2021 +0100 First release of Hello World! To view the history of commits for a repository, you can use the log command\n","date":"2022-04-13T00:00:00Z","permalink":"https://dyhes.github.io/p/gitconcepts/","title":"【Git】Concepts"},{"content":"Software testing is a process, to evaluate the functionality of a software application with an intent to find whether the developed software met the specified requirements or not and to identify the defects to ensure that the product is defect-free in order to produce a quality product.\nType By Categories White Box Testing\nIt is also called Glass Box, Clear Box, Structural Testing. White Box Testing is based on the application’s internal code structure. In white-box testing, an internal perspective of the system, as well as programming skills, are used to design test cases. This testing is usually done at the unit level.\nBlack Box Testing\nIt is also called Behavioral/Specification-Based/Input-Output Testing. Black Box Testing is a software testing method in which testers evaluate the functionality of the software under test without looking at the internal code structure.\nFunctional Testing\nIn simple words, what the system actually does is functional testing. To verify that each function of the software application behaves as specified in the requirement document. Testing all the functionalities by providing appropriate input to verify whether the actual output is matching the expected output or not. It falls within the scope of black-box testing and the testers need not concern about the source code of the application.\nNon-functional Testing\nIn simple words, how well the system performs is non-functionality testing. Non-functional testing refers to various aspects of the software such as performance, load, stress, scalability, security, compatibility, etc., The Main focus is to improve the user experience on how fast the system responds to a request.\nGrey Box Testing\nGrey box is the combination of both White Box and Black Box Testing. The tester who works on this type of testing needs to have access to design documents. This helps to create better test cases in this process.\nBy Levels Unit Testing\nUnit Testing is done to check whether the individual modules of the source code are working properly. i.e. testing each and every unit of the application separately by the developer in the developer’s environment. It is AKA Module Testing or Component Testing. It is performed by using White Box Testing method. There are two types of Unit Testing – Manual \u0026amp; Automated.\nIntegration Testing\nIntegration Testing is the process of testing the connectivity or data transfer between a couple of unit tested modules. It is AKA I\u0026amp;T Testing or String Testing. It is sub divided into Big Bang Approach, Top Down Approach, Bottom Up Approach and Sandwich or Hybrid Integration Approach (Combination of Top Down and Bottom Up).\nThis process is carried out by using dummy programs called Stubs and Drivers. Stubs and Drivers do not implement the entire programming logic of the software module but just simulate data communication with the calling module.\nBig Bang Approach\nCombining all the modules once and verifying the functionality after completion of individual module testing. In Big Bang Integration Testing, the individual modules are not integrated until all the modules are ready. Then they will run to check whether it is performing well. In this type of testing, some disadvantages might occur like, defects can be found at the later stage. It would be difficult to find out whether the defect arouse in interface or in module.\nTop-Down Approach\nIn Top Down Integration Testing, testing takes place from top to bottom. High-level modules are tested first and then low-level modules and finally integrating the low-level modules to a high level to ensure the system is working as intended.\nIn this type of testing, Stubs are used as temporary module if a module is not ready for integration testing.\nBottom-Up Approach\nIt is a reciprocate of the Top-Down Approach. In Bottom Up Integration Testing, testing takes place from bottom to up. Lowest level modules are tested first and then high-level modules and finally integrating the high-level modules to a low level to ensure the system is working as intended. Drivers are used as a temporary module for integration testing.\nHybrid Integration Approach\nSystem Testing\nIt’s a black box testing. Testing the fully integrated application is also called as an end to end scenario testing. To ensure that the software works in all intended target systems. Verify thorough testing of every input in the application to check for desired outputs. Testing of the user’s experiences with the application.\nAcceptance Testing\nTo obtain customer sign-off so that software can be delivered and payments received. Types of Acceptance Testing are Alpha, Beta \u0026amp; Gamma Testing. Acceptance tests are formal tests executed to verify if a system satisfies its business requirements. They require the entire application to be up and running and focus on replicating user behaviors. But they can also go further and measure the performance of the system and reject changes if certain goals are not met.\nAlpha Testing\nAlpha testing is mostly like performing usability testing which is done by the in-house developers who developed the software. Sometimes this alpha testing is done by the client or outsiders with the presence of developers or testers.\nBeta Testing\nBeta testing is done by a limited number of end users before delivery, the change request would be fixed if the user gives feedback or reports defect.\nGamma Testing\nGamma testing is done when the software is ready for release with specified requirements; this testing is done directly by skipping all the in-house testing activities.\nPrinciples The Principles of Testing are as follows :\nTesting shows the presence of defects Exhaustive testing is impossible Early testing Defect clustering Pesticide paradox Testing is context-dependent Absence of error – a fallacy It is equally important to test that your system doesn\u0026rsquo;t break when bad data or unexpected actions are performed.\n","date":"2022-03-25T00:00:00Z","permalink":"https://dyhes.github.io/p/software-testingconcepts/","title":"【Software Testing】Concepts"},{"content":"Domain Name A domain name is an identification string that defines a realm of administrative autonomy, authority or control within the Internet.\nDomain names are used in various networking contexts and for application-specific naming and addressing purposes. In general, a domain name identifies a network domain, or it represents an Internet Protocol (IP) resource, such as a personal computer used to access the Internet, a server computer hosting a website, or the web site itself or any other service communicated via the Internet. As of 2017, 330.6 million domain names had been registered.\nA uniform resource locator (URL), sometimes called a web address, contains the domain name of a site as well as other information, including the transfer protocol and the path.\ntypes There are two types of domain names.\ngeneric top-level domains (gTLDs) such as .com, .edu, .org, and .gov. Authority over these domains is usually delegated to private organizations.\ncountry-code top-level domains (ccTLDs). Each country in the world has its own 2-letter code. These domains are administered by authorities in each country.\nSome ccTLDs, such as .tv (for the island nation of Tuvalu) and .io (the British Indian Ocean Territory), have become popular for use outside of their home countries.\nDomain Name System The Domain Name System (DNS) is the hierarchical and decentralized naming system used to identify computers, services, and other resoures reachable through the Internet or other Internet Protocol (IP) networks.\nThe domain name system is administered by the Internet Corporation for Assigned Names and Numbers(ICANN), a non-profit organization based in California founded in 1998.\nA standard called DNSSEC seeks to beef up DNS security with encryption, but few people have adopted it.\nDNS Server Type DNS recursor ( librarian who is asked to go find a particular book somewhere in a library) The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client’s DNS query.\nRoot Nameserver ( an index in a library that points to different racks of books) The root server is the first step in translating (resolving) human readable host names into IP addresses. it serves as a reference to other more specific locations. TLD(top level domain server) Nameserver- (a specific rack of books in a library.) This nameserver is the next step in the search for a specific IP address, and it hosts the last portion of a hostname (In example.com, the TLD server is “com”). Authoritative Nameserver -( dictionary on a rack of books, in which a specific name can be translated into its definition.) The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor (the librarian) that made the initial request. Chache caching is a data persistence process that helps short-circuit the necessary requests by serving the requested resource record earlier in the DNS lookup.\nSubdomain It’s worth mentioning that in instances where the query is for a subdomain such as foo.example.com or blog.cloudflare.com an additional nameserver will be added to the sequence after the authoritative nameserver, which is responsible for storing the subdomain’s CNAME record.\n","date":"2022-03-22T00:00:00Z","permalink":"https://dyhes.github.io/p/dnsconcepts/","title":"【DNS】Concepts"},{"content":"Introduction What is Redis Redis is a fast in-memory database and cache, is’ name comes from “REmote DIctionary Server”.\nBy default Redis stores data in memory, with periodic disk persistence as a default. As Redis persists data to disk it can serve as a classical database for many use cases as well as a cache.\nIntelligent Caching By default Redis stores data in memory, with periodic disk persistence as a default. As Redis persists data to disk it can serve as a classical database for many use cases as well as a cache.\nRedis is commonly used as a cache to store frequently accessed data in memory so that applications can be responsive to users.\nWith the capacity to:\ndesignate how long you want to keep data(data structures in Redis can be marked with a Time To Live (TTL) set in seconds) which data to evict first(In some use cases a least recently used (LRU) or least frequently used (LFU) metric makes more sense for eviction. ) Redis enables a series of intelligent caching patterns.\nOther Features Publication and Subscription Messaging (Pub/Sub) Pub/Sub messaging allows for messages to be passed to channels and for all subscribers to that channel to receive that message.\nLua Scripting Redis has a scripting facility which enables custom scripts to be written and executed in the Lua language.\nGeospatial Features Redis provides a series of geospatial index data structures and commands. Latitude and longitude coordinates are stored and users can query distances between objects or query for objects within a given radius of a point.\nHyperloglog The hyperloglog data structure enables approximate set counting in a much smaller space than keeping a full unique set of items.\nBitmaps Bitmaps allow for the highly efficient storage of True and False values as 1 or 0 inside Redis strings.\nPersist to Disk Redis can be configured to write to disk in two formats, a binary format and an “append only file” (AOF) format. The binary format mirrors what is in memory and is on by default.\nThe AOF file can be turned on in the configuration and is a simple log of all commands which can be replayed to return a node to its previous state.\n","date":"2022-03-02T00:00:00Z","permalink":"https://dyhes.github.io/p/redisconcepts/","title":"【Redis】Concepts"},{"content":"Tmp In Spring’s approach to building RESTful web services, HTTP requests are handled by a controller.\nThese components are identified by the @RestController annotation ( shorthand for including both @Controller and @ResponseBody)\nThe @GetMapping annotation ensures that HTTP GET requests to /greeting are mapped to the greeting() method.\nThere are companion annotations for other HTTP verbs (e.g. @PostMapping for POST). There is also a @RequestMapping annotation that they all derive from, and can serve as a synonym (e.g. @RequestMapping(method=GET)).\n@RequestParam binds the value of the query string parameter name into the name parameter of the greeting() method. If the name parameter is absent in the request, the defaultValue of World is used.\nA key difference between a traditional MVC controller and the RESTful web service controller is the way that the HTTP response body is created. Rather than relying on a view technology to perform server-side rendering of the greeting data to HTML, this RESTful web service controller populates and returns a Greeting object. The object data will be written directly to the HTTP response as JSON.\nThe Greeting object must be converted to JSON. Thanks to Spring’s HTTP message converter support, you need not do this conversion manually. Because Jackson 2 is on the classpath, Spring’s MappingJackson2HttpMessageConverter is automatically chosen to convert the Greeting instance to JSON.\n@RestController public class GreetingController { private static final String template=\u0026#34;hello,%s!\u0026#34;; private final AtomicLong counter=new AtomicLong(); @GetMapping(\u0026#34;/greeting\u0026#34;) public Greeting greeting(@RequestParam(value = \u0026#34;name\u0026#34;,defaultValue = \u0026#34;World\u0026#34;)String name){ return new Greeting(counter.incrementAndGet(),String.format(template,name)); } } @Data public class Greeting { private final long id; private final String content; public Greeting(long id, String content){ this.id = id; this.content = content; } } @SpringBootApplication is a convenience annotation that adds all of the following:\n@Configuration: Tags the class as a source of bean definitions for the application context. @EnableAutoConfiguration: Tells Spring Boot to start adding beans based on classpath settings, other beans, and various property settings. For example, if spring-webmvc is on the classpath, this annotation flags the application as a web application and activates key behaviors, such as setting up a DispatcherServlet. @ComponentScan: Tells Spring to look for other components, configurations, and services in the com/example package, letting it find the controllers. The main() method uses Spring Boot’s SpringApplication.run() method to launch an application.\nThere was not a single line of XML? There is no web.xml file, either. This web application is 100% pure Java and you did not have to deal with configuring any plumbing or infrastructure.\n@SpringBootApplication @RestController public class LearnApplication { public static void main(String[] args) { SpringApplication.run(LearnApplication.class, args); } } Build an executable JAR You can run the application from the command line with Gradle or Maven. You can also build a single executable JAR file that contains all the necessary dependencies, classes, and resources and run that. Building an executable jar makes it easy to ship, version, and deploy the service as an application throughout the development lifecycle, across different environments, and so forth.\nIf you use Gradle, you can run the application by using ./gradlew bootRun. Alternatively, you can build the JAR file by using ./gradlew build and then run the JAR file, as follows:\njava -jar build/libs/gs-rest-service-0.1.0.jar If you use Maven, you can run the application by using ./mvnw spring-boot:run. Alternatively, you can build the JAR file with ./mvnw clean package and then run the JAR file, as follows:\njava -jar target/gs-rest-service-0.1.0.jar ","date":"2022-01-08T00:00:00Z","permalink":"https://dyhes.github.io/p/spring-bootconcepts/","title":"【Spring Boot】Concepts"},{"content":"Class类 Class类的实例提供了特定数据类型的相关信息。\n一个Class类的对象实际上表示的是一种类型。\n获取Class的三种方式\n//（1）.getClass() Employee e; Class info = e.getClass(); //（2）Class.forName() Class info = Class.forName(\u0026#34;java.util.Date\u0026#34;) //（3）T.Class Class info1 = int.class; Class info2 = Double[].class; 反射 所谓“反射（Reflection）”，就是把Java类中的各种成份映射成相应的另外一些特殊的Java类的实例，从而允许程序在运行时动态地完成一些工作：比如动态装载类型并创建实例，查询类的成员信息，动态地调用方法等。反射技术的核心类型是Class类。\n不管每个类型创建了多少个实例，也不管你用什么方法获取此类型的Class实例，每个类型都只对应一个Class实例。\n动态创建对象 使用构造方法对象或Class对象的newInstance()方法动态创建指定类型的对象\n注解 注解相当于一种标记，加了注解就等于给代码打上了某个标记。javac编译器、 IntelliJ之类IDE以及其他一些注解处理工具（Annotation Processing Tool，APT）就 可以利用它来完成一些特殊的工作。\n@Deprecated：标记类的成员已过时 @Override：要求子类必须覆盖基类的方法 @Serializable：指定某个类是可以序列化的 @FunctionalInterface：指定某个方法重写了基类的同名方法 自定义 注解，其实就是一个使用@interface定义的接口\npublic @interface MyTestAnnotation { } 元注解 定义注解的类型也可以添加“注解”，这种针对“注解”的“注解”， 称为“元注解（meta-annotation）”\n@Retention(RetentionPolicy.RUNTIME) //指明注解生存的时间 @Target({ElementType.METHOD,ElementType.TYPE}) //指明注解适用的场合 public @interface MyTestAnnotation { } ","date":"2022-01-07T00:00:00Z","permalink":"https://dyhes.github.io/p/java%E6%B3%A8%E8%A7%A3%E4%B8%8E%E5%8F%8D%E5%B0%84/","title":"【Java】注解与反射"},{"content":"从文件中读取内容，向文件中写入内容，截短文件、合并文 件、压缩文件……，诸如此类的操作，称为文件存取操作， 主要使用“流（Stream）”来完成\n与文件操作相关的类，集中于java.io包中。\nJava 1.4中加入了一个New I/O API，提供了一些类完成文件和流操作。\nJava 7中又加入了一些新类型，称为NIO.2，NIO.2中最重要的是引入了Path和 AutoCloseable接口，还有一个Files类，它的静态方法封装了文件的常用操作。\nFile类 Java使用File类来统一操作文件和文件夹。\njava.io.File代表与平台无关的文件或目录。也就是说可以通过File类在Java程序中操作文件或目录； File类只能用来操作文件或目录（包括新建、删除、重命名文件和目录等操作），但不能用来访问文件中的内容； 如果需要访问文件中的内容，则需要使用输入/输出流。 RandomAccessFile（随机文件操作）：一个独立的类，直接继承至Object.它的功能丰富，可以从文件的任意位置进行存取（输入输出）操作。RandomAccessFile类支持“随机访问”方式，这里“随机”是指可以跳转到文件的任意位置处读写数据。 在访问一个文件的时候，不必把文件从头读到尾，而是希望像访问一个数据库一样“随心所欲”地访问一个文件的某个部分，这时使用类就是最佳选择。‎ RandomAccessFile对象类有个位置指示器‎‎，指向当前读写处的位置，当前读写n个字节后，文件指示器将指向这n个字节后面的下一个字节处。 刚打开文件时，文件指示器指向文件的开头处，可以移动文件指示器到新的位置，随后的读写操作将从新的位置开始。 类在数据等长记录格式文件的随机（相对顺序而言）读取时有很大的优势，但该类仅限于操作文件，不能访问其他的I/O设备，如网络、内存映像等。\nIO Stream 分类 根据操作数据类型（能用记事本打开并能看到其中的字符内容的是文本文件，反之是二进制文件）\n字节流：二进制，以字节为单位\n字节流的两个基类 InputStream和OutputStream\n凡是以InputStream或OutputStream结尾的类型为字节流\n字符流：文本,以字符（2个字节）为单位\n字符流的两个基类 Reader和Writer\n凡是以Reader或Writer结尾的均为字符流\n区别\n字节流没有缓冲区，是直接输出的，而字符流是输出到缓冲区的。因此在输出时，字节流不调用colse()方法时，信息已经输出了，而字符流只有在调用close()方法关闭缓冲区时，信息才输出。要想字符流在未关闭时输出信息，则需要手动调用flush()方法； 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节； 处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据； 根据流向\n输入流 输出流 根据具体功能\n节点流：以从或向一个特定的地方（节点）读写数据。\n1、文件： FileInputStream 、FileOutputStream、 FileReader和 FileWriter 文件进行处理的节点流； 2、字符串： StringReader 和StringWriter 对字符串进行处理的节点流； 3、数组： ByteArrayInputStream、ByteArrayOutputStream、CharArrayReader和CharArrayWriter 对数组进行处理的节点流(对应的不再是文件，而是内存中的一个数组)； 4、管道： PipedInputStream 、PipedOutputStream 和PipedReaderPipedWriter对管道进行处理的节点流； 5、基类： InputStream、 OutputStream、 Reader和 Writer；\n处理流：是对一个已存在的流的连接和封装，通过所封装的流的功能调用实现数据读写。处理流的构造方法总是要带一个其他的流对象做参数。一个流对象经过其他流的多次包装，称为流的链接。\n1、缓冲流：BufferedInputStream、 BufferedOutputStream、 BufferedReader和BufferedWriter 增加缓冲功能，避免频繁读写硬盘。 2、转换流：InputStreamReader和 OutputStreamReader实现字节流和字符流之间的转换。 3、数据流 DataInputStream和DataOutputStream等提供将基础数据类型写入到文件中，或者读取出来。\n关闭流 流关闭的原则：先打开先关闭；如果A依赖B，则先关闭A再关闭B；对于处理流如果将节点流关闭以后再关闭处理流，会抛出IO异常，所以直接关闭处理流就行了，会自动调用关闭里面节点流的方法。\n注意：\n如果将节点流关闭以后再关闭处理流，会抛出IO异常 如果关闭了处理流，在关闭与之相关的节点流，也可能出现IO异常。 InputStream ByteArrayInputStream：字节数组输入流，它的内部缓冲区就是一个字节数组，该类的功能就是从字节数组(byte[])中进行以字节为单位的读取资源文件；\nPipedInputStream：管道字节输入流，它和PipedOutputStream一起使用，能实现**多线程间的管道通信。**多线程管道通信的主要流程是在一个线程中向PipedOutputStream写入数据，这些数据会自动传送到对应的管道输入流PipedInputStream中，其他线程通过读取PipeInputStream中缓冲的数据实现多线程间通信；\nFilterInputStream ：过滤输入流，装饰者模式中处于装饰者，具体的装饰者都要继承它，所以在该类的子类下都是用来装饰别的流的，也就是处理类。常见的子类有DataInputStream和BufferedInputStream；\nBufferedInputStream：缓冲输入流，由于基础输入流一个字节一个字节读取,频繁与磁盘进行交互,造成读取速度较低.缓冲流的存在就是先将数据读取到缓冲流(内存中),然后一次性从内存中读取多个字符.提高读取的效率；\nDataInputStream：数据输入流,以机器无关的方式读取Java的基本类型；\nFileInputSream：文件输入流，它通常用于对文件进行读取操作；\nFile：对指定目录的文件进行操作。注意，该类虽然是在IO包下，但是并不继承自四大基础类；\nObjectInputStream：对象输入流，用来提供对“基本数据或对象”的持久存储。通俗点讲，也就是能直接传输对象（反序列化中使用）。\nOutputStream ByteArrayOutputStream：字节数组输出流，它的内部缓冲区就是一个字节数组，该类的功能就是从字节数组(byte[])中进行以字节为单位的写入资源文件；\nPipedOutputStream ：管道字节输出流，它和PipedInputStream一起使用，能实现多线程间的管道通信。\nFilterOutputStream ：过滤输出流，装饰者模式中处于装饰者，具体的装饰者都要继承它，所以在该类的子类下都是用来装饰别的流的，也就是处理类。常见的子类有DatOutputStream、BufferedOutputStream和\nBufferedOutputStream：缓冲输出流，由于基础输入流一个字节一个字节写入,频繁与磁盘进行交互,造成读取速度较低.缓冲流的存在就是先将数据写入到缓冲流(内存中),然后一次性从内存中写入多个字符.提高读取的效率；\nDataOutputStream：数据输出流,以机器无关的方式读取Java的基本类型；\nPrintStream：继承了FilterOutputStream。是\u0026quot;装饰类\u0026quot;的一种,所以属于字节流体系中(与PrintStream相似的流PrintWriter继承于Writer,属于字符流体系中),为其他的输出流添加功能.使它们能够方便打印各种数据值的表示形式；\nFileOutputStream ：文件输出流，它通常用于对文件进行写入操作；\nObjectOutputStream ：对象输出流，用来提供对“基本数据或对象”的持久存储。通俗点讲，也就是能直接传输对象（反序列化中使用），和所有FilterOutputStream 的子类都是装饰流(序列化中使用)。\nReader CharArrayReader :字符数组输入流。它和ByteArrayInputStream类似，只不过ByteArrayInputStream是字节数组输入流，而CharArray是字符数组输入流\nPipedReader:管道字符流， 是从与其它线程共用的管道中读取数据。\nFilterReader：过滤输入字符流， 是所有自定义具体装饰流的父类，为所有装饰类提供一个标准、只是简单重写了父类Reader的所有方法、要求子类必须重写核心方法、和提供具有自己特色的方法、这里没有像字节流那样有很多的子类来实现不同的功能、可能是因为字符流本来就是字节流的一种装饰、所以在这里没有必要再对其进行装饰、只是提供一个扩展的接口而已；\nBufferedReader：缓冲字符流， 为了提高字符流读写的效率，引入了缓冲机制，进行字符批量的读写，提高了单个字符读写的效率；\nInputStreamReader是一个连接字节流和字符流的桥梁，它将字节流转变为字符流；\nFileReader：继承InputStreamReader，可以说是一个达到此功能、常用的工具类，在其源代码中明显使用了将FileInputStream 转变为Reader 的方法。我们可以从这个类中得到一定的技巧。Reader 中各个类的用途和使用方法基本和InputStream 中的类使用一致。后面会有Reader 与InputStream 的对应关系。\nWriter CharArrayWriter、StringWriter 是两种基本的介质流，它们分别向Char 数组、String 中写入数据。\nPipedWriter 是向与其它线程共用的管道中写入数据 BufferedWriter 是一个装饰器为Writer 提供缓冲功能。\nPrintWriter 和PrintStream 极其类似，功能和使用也非常相似。\nOutputStreamWriter 是OutputStream 到Writer 转换的桥梁，它的子类FileWriter 其实就是一个实现此功能的具体类。功能和使用和OutputStream 极其类似。\n转换流\n4.1、定义：字符和字节直接的转换，是字符流和字节流之间的桥梁，文本文件在硬盘中以字节流的形式存储时，通过InputStreamReader读取后转化为字符流给程序处理，即可对读取到的字节数据经过指定编码转换成字符；程序处理的字符流通过OutputStreamWriter转换为字节流保存，即可对读取到的字符数据经过指定编码转换成字节。\n4.2、何时使用转换流？\n①当字节和字符之间有转换动作时； ②流操作的数据需要编码或解码时。\n4.3、具体的对象体现：\nInputStreamReader:字节到字符的桥梁 OutputStreamWriter:字符到字节的桥梁 这两个流对象是字符体系中的成员，它们有转换作用，本身又是字符流，所以在构造的时候需要传入字节流对象进来，即：\nInputStreamReader(InputStream in)：将字节流以字符流输入。\nOutputStreamWriter(OutStreamout):将字节流以字符流输出。\nSystem.in and System.out System.in和System.out分别代表了系统标准的输入和输出设备 默认输入设备是：键盘，输出设备是：显示器 System.in的类型是InputStream System.out的类型是PrintStream\n对象流 ObjectInputStream和OjbectOutputSteam用于存储和读取基本数据类型数据或对象的处理流。它的强大之处就是可以把Java中的对象写入到数据源中，也能把对象从数据源中还原回来。\n序列化：用ObjectOutputStream类保存基本类型数据或对象的机制 反序列化：用ObjectInputStream类读取基本类型数据或对象的机制 ObjectOutputStream和ObjectInputStream不能序列化static和transient修饰的成员变量\n总结 InputStream类的功能不足被Scanner解决了\nOutputStream类的功能不足被PrintStream解决了\nReader类功能不足被BufferReader解决了\nWriter类的功能不足被PrintWriter解决了\n输出数据用printStream，printwriter读取数据用Scanner其次是bufferReader\nNIO java.nio全称java non-blocking IO，是指jdk1.4 及以上版本里提供的新api（New IO） ，为所有的原始类型（boolean类型除外）提供缓存支持的数据容器，使用它可以提供非阻塞式的高伸缩性网络。\n","date":"2022-01-05T00:00:00Z","permalink":"https://dyhes.github.io/p/javaio/","title":"【Java】IO"},{"content":"Intro Spring is a lightweight framework. It can be thought of as a framework of frameworks because it provides support to various frameworks such as Struts, Hibernate, Tapestry, EJB, JSF, etc.\nThe framework, in broader sense, can be defined as a structure where we find solution of the various technical problems.\nThe Spring framework comprises several modules such as IOC, AOP, DAO, Context, ORM, WEB MVC etc. We will learn these modules in next page. Let\u0026rsquo;s understand the IOC and Dependency Injection first.\nInversion Of Control (IOC) and Dependency Injection\nThese are the design patterns that are used to remove dependency from the programming code. They make the code easier to test and maintain.\n//bad class Employee{ Address address; Employee(){ address=new Address(); } } //In such case, there is dependency between the Employee and Address (tight coupling). //good class Employee{ Address address; Employee(Address address){ this.address=address; } } Thus, IOC makes the code loosely coupled. In such case, there is no need to modify the code if our logic is moved to new environment.\nIn Spring framework, IOC container is responsible to inject the dependency. We provide metadata to the IOC container either by XML file or annotation.\nadvantage:\nmakes the code loosely coupled so easy to maintain makes the code easy to test IoC Container The IoC container is responsible to instantiate, configure and assemble the objects. The IoC container gets informations from the XML file and works accordingly.\nThe main tasks performed by IoC container are:\nto instantiate the application class to configure the object to assemble the dependencies between the objects types There are two types of IoC containers. They are:\nBeanFactory ApplicationContext The ApplicationContext interface is built on top of the BeanFactory interface. It adds some extra functionality than BeanFactory such as simple integration with Spring\u0026rsquo;s AOP, message resource handling (for I18N), event propagation, application layer specific context (e.g. WebApplicationContext) for web application. So it is better to use ApplicationContext than BeanFactory.\nUsing BeanFactory The XmlBeanFactory is the implementation class for the BeanFactory interface. To use the BeanFactory, we need to create the instance of XmlBeanFactory class as given below:\nResource resource=new ClassPathResource(\u0026#34;applicationContext.xml\u0026#34;); BeanFactory factory=new XmlBeanFactory(resource); The constructor of XmlBeanFactory class receives the Resource object so we need to pass the resource object to create the object of BeanFactory.\nUsing ApplicationContext The ClassPathXmlApplicationContext class is the implementation class of ApplicationContext interface. We need to instantiate the ClassPathXmlApplicationContext class to use the ApplicationContext as given below:\nApplicationContext context = new ClassPathXmlApplicationContext(\u0026#34;applicationContext.xml\u0026#34;); The constructor of ClassPathXmlApplicationContext class receives string, so we can pass the name of the xml file to create the instance of ApplicationContext.\nDI To understand the DI better, Let\u0026rsquo;s understand the Dependency Lookup (DL) first.\nThe Dependency Lookup is an approach where we get the resource after demand. There can be various ways to get the resource.\nThere are mainly two problems of dependency lookup.\ntight coupling The dependency lookup approach makes the code tightly coupled. If resource is changed, we need to perform a lot of modification in the code. Not easy for testing This approach creates a lot of problems while testing the application especially in black box testing. The Dependency Injection is a design pattern that removes the dependency of the programs. In such case we provide the information from the external source such as XML file.\nSpring framework provides two ways to inject dependency\nBy Constructor By Setter method By Constructor The subelement of is used for constructor injection.\nResource\n\u0026lt;bean id=\u0026#34;e\u0026#34; class=\u0026#34;com.javatpoint.Employee\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;10\u0026#34; type=\u0026#34;int\u0026#34; \u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Sonoo\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; test\npublic class Test { public static void main(String[] args) { Resource r=new ClassPathResource(\u0026#34;applicationContext.xml\u0026#34;); BeanFactory factory=new XmlBeanFactory(r); Employee s=(Employee)factory.getBean(\u0026#34;e\u0026#34;); s.show(); } } With Object \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;a1\u0026#34; class=\u0026#34;com.javatpoint.Address\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;ghaziabad\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;UP\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;India\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;e\u0026#34; class=\u0026#34;com.javatpoint.Employee\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;12\u0026#34; type=\u0026#34;int\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Sonoo\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg\u0026gt; \u0026lt;ref bean=\u0026#34;a1\u0026#34;/\u0026gt; \u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; With Collection We can inject collection values by constructor in spring framework. There can be used three elements inside the constructor-arg element.\nIt can be:\nlist set map Each collection can have string based and non-string based values.\nlist \u0026lt;!--string based--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;q\u0026#34; class=\u0026#34;com.javatpoint.Question\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;111\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;What is java?\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg\u0026gt; \u0026lt;list\u0026gt; \u0026lt;value\u0026gt;Java is a programming language\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;Java is a Platform\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;Java is an Island of Indonasia\u0026lt;/value\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; \u0026lt;!--non-string based--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;ans1\u0026#34; class=\u0026#34;com.javatpoint.Answer\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Java is a programming language\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;John\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;ans2\u0026#34; class=\u0026#34;com.javatpoint.Answer\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;2\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Java is a Platform\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Ravi\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;q\u0026#34; class=\u0026#34;com.javatpoint.Question\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;111\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;What is java?\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg\u0026gt; \u0026lt;list\u0026gt; \u0026lt;ref bean=\u0026#34;ans1\u0026#34;/\u0026gt; \u0026lt;ref bean=\u0026#34;ans2\u0026#34;/\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; map \u0026lt;!--string based--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;q\u0026#34; class=\u0026#34;com.javatpoint.Question\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;11\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;What is Java?\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg\u0026gt; \u0026lt;map\u0026gt; \u0026lt;entry key=\u0026#34;Java is a Programming Language\u0026#34; value=\u0026#34;Ajay Kumar\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;entry key=\u0026#34;Java is a Platform\u0026#34; value=\u0026#34;John Smith\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;entry key=\u0026#34;Java is an Island\u0026#34; value=\u0026#34;Raj Kumar\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;/map\u0026gt; \u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; \u0026lt;!--non-string based--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;answer1\u0026#34; class=\u0026#34;com.javatpoint.Answer\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Java is a Programming Language\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;12/12/2001\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;answer2\u0026#34; class=\u0026#34;com.javatpoint.Answer\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;2\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Java is a Platform\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;12/12/2003\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;user1\u0026#34; class=\u0026#34;com.javatpoint.User\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Arun Kumar\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;arun@gmail.com\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;user2\u0026#34; class=\u0026#34;com.javatpoint.User\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;2\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Varun Kumar\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Varun@gmail.com\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;q\u0026#34; class=\u0026#34;com.javatpoint.Question\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;What is Java?\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg\u0026gt; \u0026lt;map\u0026gt; \u0026lt;entry key-ref=\u0026#34;answer1\u0026#34; value-ref=\u0026#34;user1\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;entry key-ref=\u0026#34;answer2\u0026#34; value-ref=\u0026#34;user2\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;/map\u0026gt; \u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; inheritance By using the parent attribute of bean, we can specify the inheritance relation between the beans. In such case, parent bean values will be inherited to the current bean.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;e1\u0026#34; class=\u0026#34;com.javatpoint.Employee\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;101\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Sachin\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;address1\u0026#34; class=\u0026#34;com.javatpoint.Address\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;21,Lohianagar\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Ghaziabad\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;UP\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;constructor-arg value=\u0026#34;USA\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;e2\u0026#34; class=\u0026#34;com.javatpoint.Employee\u0026#34; parent=\u0026#34;e1\u0026#34;\u0026gt; \u0026lt;constructor-arg ref=\u0026#34;address1\u0026#34;\u0026gt;\u0026lt;/constructor-arg\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; By Setter We can inject the dependency by setter method also. The subelement of is used for setter injection. Here we are going to inject\nprimitive and String-based values Dependent object (contained object) Collection values etc. \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;obj\u0026#34; class=\u0026#34;com.javatpoint.Employee\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34;\u0026gt; \u0026lt;value\u0026gt;20\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34;\u0026gt; \u0026lt;value\u0026gt;Arun\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;city\u0026#34;\u0026gt; \u0026lt;value\u0026gt;ghaziabad\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; With Object \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;address1\u0026#34; class=\u0026#34;com.javatpoint.Address\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;addressLine1\u0026#34; value=\u0026#34;51,Lohianagar\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;city\u0026#34; value=\u0026#34;Ghaziabad\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;state\u0026#34; value=\u0026#34;UP\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;country\u0026#34; value=\u0026#34;India\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;obj\u0026#34; class=\u0026#34;com.javatpoint.Employee\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;Sachin Yadav\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;address\u0026#34; ref=\u0026#34;address1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; With Collection We can inject collection values by setter method in spring framework. There can be used three elements inside the property element.\nlist set map Each collection can have string based and non-string based values.\nlist \u0026lt;!--string based--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;q\u0026#34; class=\u0026#34;com.javatpoint.Question\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;What is Java?\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;answers\u0026#34;\u0026gt; \u0026lt;list\u0026gt; \u0026lt;value\u0026gt;Java is a programming language\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;Java is a platform\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;Java is an Island\u0026lt;/value\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; \u0026lt;!--non-string based--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;answer1\u0026#34; class=\u0026#34;com.javatpoint.Answer\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;Java is a programming language\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;by\u0026#34; value=\u0026#34;Ravi Malik\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;answer2\u0026#34; class=\u0026#34;com.javatpoint.Answer\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;2\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;Java is a platform\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;by\u0026#34; value=\u0026#34;Sachin\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;q\u0026#34; class=\u0026#34;com.javatpoint.Question\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;What is Java?\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;answers\u0026#34;\u0026gt; \u0026lt;list\u0026gt; \u0026lt;ref bean=\u0026#34;answer1\u0026#34;/\u0026gt; \u0026lt;ref bean=\u0026#34;answer2\u0026#34;/\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; map \u0026lt;!--string based--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;q\u0026#34; class=\u0026#34;com.javatpoint.Question\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;What is Java?\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;answers\u0026#34;\u0026gt; \u0026lt;map\u0026gt; \u0026lt;entry key=\u0026#34;Java is a programming language\u0026#34; value=\u0026#34;Sonoo Jaiswal\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;entry key=\u0026#34;Java is a Platform\u0026#34; value=\u0026#34;Sachin Yadav\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;/map\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; \u0026lt;!--non-string based--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;answer1\u0026#34; class=\u0026#34;com.javatpoint.Answer\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;answer\u0026#34; value=\u0026#34;Java is a Programming Language\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;postedDate\u0026#34; value=\u0026#34;12/12/2001\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;answer2\u0026#34; class=\u0026#34;com.javatpoint.Answer\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;2\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;answer\u0026#34; value=\u0026#34;Java is a Platform\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;postedDate\u0026#34; value=\u0026#34;12/12/2003\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;user1\u0026#34; class=\u0026#34;com.javatpoint.User\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;Arun Kumar\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;email\u0026#34; value=\u0026#34;arun@gmail.com\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;user2\u0026#34; class=\u0026#34;com.javatpoint.User\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;2\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;Varun Kumar\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;email\u0026#34; value=\u0026#34;Varun@gmail.com\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;q\u0026#34; class=\u0026#34;com.javatpoint.Question\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;id\u0026#34; value=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;What is Java?\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;answers\u0026#34;\u0026gt; \u0026lt;map\u0026gt; \u0026lt;entry key-ref=\u0026#34;answer1\u0026#34; value-ref=\u0026#34;user1\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;entry key-ref=\u0026#34;answer2\u0026#34; value-ref=\u0026#34;user2\u0026#34;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;/map\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; Difference There are many key differences between constructor injection and setter injection.\nPartial dependency: can be injected using setter injection but it is not possible by constructor. Suppose there are 3 properties in a class, having 3 arg constructor and setters methods. In such case, if you want to pass information for only one property, it is possible by setter method only. Overriding: Setter injection overrides the constructor injection. If we use both constructor and setter injection, IOC container will use the setter injection. Changes: We can easily change the value by setter injection. It doesn\u0026rsquo;t create a new bean instance always like constructor. So setter injection is more flexible than constructor injection. Autowiring Autowiring feature of spring framework enables you to inject the object dependency implicitly. It internally uses setter or constructor injection.\nAutowiring can\u0026rsquo;t be used to inject primitive and string values. It works with reference only.\nModes No. Mode Description 1) no It is the default autowiring mode. It means no autowiring bydefault. 2) byName The byName mode injects the object dependency according to name of the bean. In such case, property name and bean name must be same. It internally calls setter method. 3) byType The byType mode injects the object dependency according to type. So property name and bean name can be different. It internally calls setter method. 4) constructor The constructor mode injects the dependency by calling the constructor of the class. It calls the constructor having largest number of parameters. 5) autodetect It is deprecated since Spring 3. \u0026lt;bean id=\u0026#34;b\u0026#34; class=\u0026#34;org.sssit.B\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;a\u0026#34; class=\u0026#34;org.sssit.A\u0026#34; autowire=\u0026#34;byName\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;b1\u0026#34; class=\u0026#34;org.sssit.B\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;a\u0026#34; class=\u0026#34;org.sssit.A\u0026#34; autowire=\u0026#34;byType\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; With Factory Method Spring framework provides facility to inject bean using factory method. To do so, we can use two attributes of bean element.\nfactory-method: represents the factory method that will be invoked to inject the bean. factory-bean: represents the reference of the bean by which factory method will be invoked. It is used if factory method is non-static. A method that returns instance of a class is called factory method.\npublic class A { public static A getA(){//factory method return new A(); } } here can be three types of factory method:\n\\1) A static factory method that returns instance of its own class. It is used in singleton design pattern.\n\u0026lt;bean id=\u0026#34;a\u0026#34; class=\u0026#34;com.javatpoint.A\u0026#34; factory-method=\u0026#34;getA\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \\2) A static factory method that returns instance of another class. It is used instance is not known and decided at runtime.\n\u0026lt;bean id=\u0026#34;b\u0026#34; class=\u0026#34;com.javatpoint.A\u0026#34; factory-method=\u0026#34;getB\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \\3) A non-static factory method that returns instance of another class. It is used instance is not known and decided at runtime.\n\u0026lt;bean id=\u0026#34;a\u0026#34; class=\u0026#34;com.javatpoint.A\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;b\u0026#34; class=\u0026#34;com.javatpoint.A\u0026#34; factory-method=\u0026#34;getB\u0026#34; factory-bean=\u0026#34;a\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; AOP Aspect Oriented Programming (AOP) compliments( 补充 ) OOPs in the sense that it also provides modularity. But the key unit of modularity is aspect than class.\nAOP breaks the program logic into distinct parts (called concerns). It is used to increase modularity by cross-cutting concerns.\nA cross-cutting concern is a concern that can affect the whole application and should be centralized in one location in code as possible, such as transaction management, authentication, logging, security etc.\nAOP is mostly used in following cases:\nto provide declarative enterprise services such as declarative transaction management. It allows users to implement custom aspects. Concepts and Terminology AOP concepts and terminologies are as follows:\nJoin point : any point in your program such as method execution, exception handling, field access etc. Spring supports only method execution join point.\nAdvice : represents an action taken by an aspect at a particular join point.\nBefore Advice: it executes before a join point. After Returning Advice: it executes after a joint point completes normally. After Throwing Advice: it executes if method exits by throwing an exception. After (finally) Advice: it executes after a join point regardless of join point exit whether normally or exceptional return. Around Advice: It executes before and after a join point. Pointcut : an expression language of AOP that matches join points.\nIntroduction : means introduction of additional method and fields for a type. It allows you to introduce new interface to any advised object.\nTarget Object : It is the object i.e. being advised by one or more aspects. It is also known as proxied object in spring because Spring AOP is implemented using runtime proxies.\nAspect : It is a class that contains advices, joinpoints etc.\nInterceptor : an aspect that contains only one advice.\nAOP Proxy : used to implement aspect contracts, created by AOP framework. It will be a JDK dynamic proxy or CGLIB proxy in spring framework.\nWeaving : the process of linking aspect with other application types or objects to create an advised object. Weaving can be done at compile time, load time or runtime. Spring AOP performs weaving at runtime.\nImplementations AOP implementations are provided by:\nAspectJ Spring AOP JBoss AOP Spring AOP Advisor class public class BeforeAdvisor implements MethodBeforeAdvice{ @Override public void before(Method method, Object[] args, Object target)throws Throwable { System.out.println(\u0026#34;additional concern before actual logic\u0026#34;); } } ProxyFactoryBean The ProxyFactoryBean class is provided by Spring Famework. It contains 2 properties target and interceptorNames. The instance of A class will be considered as target object and the instance of advisor class as interceptor. You need to pass the advisor object as the list object as in the xml file given above.\nIn xml file, create 3 beans, one for A class, second for Advisor class and third for ProxyFactoryBean class.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:p=\u0026#34;http://www.springframework.org/schema/p\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;obj\u0026#34; class=\u0026#34;com.javatpoint.A\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;ba\u0026#34; class=\u0026#34;com.javatpoint.BeforeAdvisor\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;proxy\u0026#34; class=\u0026#34;org.springframework.aop.framework.ProxyFactoryBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;target\u0026#34; ref=\u0026#34;obj\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;interceptorNames\u0026#34;\u0026gt; \u0026lt;list\u0026gt; \u0026lt;value\u0026gt;ba\u0026lt;/value\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; AspectJ The Spring Framework recommends you to use Spring AspectJ AOP implementation over the Spring 1.2 old style dtd based AOP implementation because it provides you more control and it is easy to use.\nSpring AspectJ AOP implementation provides many annotations:\n@Aspect declares the class as aspect. @Pointcut declares the pointcut expression. The annotations used to create advices are given below:\n@Before declares the before advice. It is applied before calling the actual method. @After declares the after advice. It is applied after calling the actual method and before returning result. @AfterReturning declares the after returning advice. It is applied after calling the actual method and before returning result. But you can get the result value in the advice. @Around declares the around advice. It is applied before and after calling the actual method. @AfterThrowing declares the throws advice. It is applied if actual method throws exception. @Pointcut Let\u0026rsquo;s try the understand the pointcut expressions by the examples given below:\n@Pointcut(\u0026#34;execution(public * *(..))\u0026#34;) It will be applied on all the public methods.\n@Pointcut(\u0026#34;execution(public Operation.*(..))\u0026#34;) It will be applied on all the public methods of Operation class.\n@Pointcut(\u0026#34;execution(* Operation.*(..))\u0026#34;) It will be applied on all the methods of Operation class.\n@Pointcut(\u0026#34;execution(* Operation.*(..))\u0026#34;) It will be applied on all the public setter methods of Employee class.\n@Pointcut(\u0026#34;execution(int Operation.*(..))\u0026#34;) It will be applied on all the methods of Operation class that returns int value.\nexample\n@Aspect public class TrackOperation{ @Pointcut(\u0026#34;execution(* Operation.*(..))\u0026#34;) public void k(){}//pointcut name @Before(\u0026#34;k()\u0026#34;)//applying pointcut on before advice public void myadvice(JoinPoint jp)//it is advice (before advice) { System.out.println(\u0026#34;additional concern\u0026#34;); //System.out.println(\u0026#34;Method Signature: \u0026#34; + jp.getSignature()); } } xml\n\u0026lt;bean id=\u0026#34;opBean\u0026#34; class=\u0026#34;com.javatpoint.Operation\u0026#34;\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;trackMyBean\u0026#34; class=\u0026#34;com.javatpoint.TrackOperation\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;bean class=\u0026#34;org.springframework.aop.aspectj.annotation.AnnotationAwareAspectJAutoProxyCreator\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; Spring JdbcTemplate Spring JdbcTemplate is a powerful mechanism to connect to the database and execute SQL queries. It internally uses JDBC api, but eliminates a lot of problems of JDBC API.\nThe problems of JDBC API are as follows:\nWe need to write a lot of code before and after executing the query, such as creating connection, statement, closing resultset, connection etc. We need to perform exception handling code on the database logic. We need to handle transaction. Repetition of all these codes from one to another database logic is a time consuming task. No. Method Description 1) public int update(String query) is used to insert, update and delete records. 2) public int update(String query,Object\u0026hellip; args) is used to insert, update and delete records using PreparedStatement using given arguments. 3) public void execute(String query) is used to execute DDL query. 4) public T execute(String sql, PreparedStatementCallback action) executes the query by using PreparedStatement callback. 5) public T query(String sql, ResultSetExtractor rse) is used to fetch records using ResultSetExtractor. 6) public List query(String sql, RowMapper rse) is used to fetch records using RowMapper. ORM Spring provides API to easily integrate Spring with ORM frameworks such as Hibernate, JPA(Java Persistence API), JDO(Java Data Objects), Oracle Toplink and iBATIS.\nThere are a lot of advantage of Spring framework in respect to ORM frameworks. There are as follows:\nLess coding is required: By the help of Spring framework, you don\u0026rsquo;t need to write extra codes before and after the actual database logic such as getting the connection, starting transaction, commiting transaction, closing connection etc. Easy to test: Spring\u0026rsquo;s IoC approach makes it easy to test the application. Better exception handling: Spring framework provides its own API for exception handling with ORM framework. Integrated transaction management: By the help of Spring framework, we can wrap our mapping code with an explicit template wrapper class or AOP style method interceptor. Spring MVC A Spring MVC is a Java framework which is used to build web applications. It follows the Model-View-Controller design pattern. It implements all the basic features of a core spring framework like Inversion of Control, Dependency Injection.\nA Spring MVC provides an elegant solution to use MVC in spring framework by the help of DispatcherServlet. Here, DispatcherServlet is a class that receives the incoming request and maps it to the right resource such as controllers, models, and views.\nModel - A model contains the data of the application. A data can be a single object or a collection of objects. Controller - A controller contains the business logic of an application. Here, the @Controller annotation is used to mark the class as the controller. View - A view represents the provided information in a particular format. Generally, JSP+JSTL is used to create a view page. Although spring also supports other view technologies such as Apache Velocity, Thymeleaf and FreeMarker. Front Controller - In Spring Web MVC, the DispatcherServlet class works as the front controller. It is responsible to manage the flow of the Spring MVC application. the advantages of Spring MVC Framework:-\nSeparate roles - The Spring MVC separates each role, where the model object, controller, command object, view resolver, DispatcherServlet, validator, etc. can be fulfilled by a specialized object. Light-weight - It uses light-weight servlet container to develop and deploy your application. Powerful Configuration - It provides a robust configuration for both framework and application classes that includes easy referencing across contexts, such as from web controllers to business objects and validators. Rapid development - The Spring MVC facilitates fast and parallel development. Reusable business code - Instead of creating new objects, it allows us to use the existing business objects. Easy to test - In Spring, generally we create JavaBeans classes that enable you to inject test data using the setter methods. Flexible Mapping - It provides the specific annotations that easily redirect the page. ","date":"2022-01-03T00:00:00Z","permalink":"https://dyhes.github.io/p/springconcepts/","title":"【Spring】Concepts"},{"content":"String StringBuffer 线程安全\nstringbuffer.toString() StringBuilder 线程不安全，因而性能相对更高\nArrays类 Arrays类中封装了与数组有关的静态方法\nArrays.CcopyOf() Arrays.sort() Arrays.fill() 等\n对象连接 在“+ ”运算中，当一个对象与一个String对象连接时，会 隐式地调用其toString()方法，默认情况下，此方法返回 “类名 @ + hashCode” 。 为了返回有意义的信息，子类可以重写toString()方法。\n接口 在面向对象世界中，可以使用“接口（interface）”来抽象对象的行为特性\neg:能否把“会游泳”、“能被吃”这种特性独立出来作为一种“可选项”，可以被“附加” 到具体对象上？ 这样一来，水鸟可以拥有“会游泳”这个特性，其它种类的鸟就不具备这个特性，但它 可能有其他的特性。\n如果接口不声明为public的，则自动变为package\n接口类型 接口类型的变量＝ new 实现了接口的具体类型（）；\n可以通过继承接口来扩充已有接口，并形成一个新的接口\n函数式接口 JDK中引入了一种“函数式”接口，这种接口只定义有一个公有 方法，使用@FunctionalInterface注解加以标注，这种类型的接口，主要用于“函数式编程”场景\n异常 可以有多个catch语句块，每个代码块捕获一种异常。在某个try块后有两个不同的catch 块捕获两个相同类型的异常是语法错误。 使用catch语句，只能捕获Exception类及其子类的对象。因此，一个捕获Exception对象的catch语句块可以捕获所有“可捕获”的异常。 将catch(Exception e)放在别的catch块前面会使这些catch块都不执行，因此Java不会编译这个程序 Java 7 及以后的版本，允许在一个catch块中捕获多个异常。\ncatch (SocketException | SecurityException | NullPointerException e) 泛型 不能定义泛型化数组 不能直接创建泛型类型的实例 泛型类型不能直接或间接继承自Throwable 不能定义静态泛型成员 不允许基类中有泛型参数，从泛型基类派生时，应该给其指定一个具体的类型 可以在普通类或泛型类中定义泛型方法 泛型方法支持使用“…”定义个数可变的参数 泛型约束 类型约束 public static \u0026lt;T extends Comparable\u0026gt; T min(T[] a) { //… } //T extends Comparable \u0026amp; Serializable //多个约束条件中最多只能有一个是类，且必须放在第一位 类型通配符\n在泛型约束中使用“?”，可以匹配任意类型。\nstatic void printList(List\u0026lt;?\u0026gt; lst) { for(Object item:lst) System.out.println(item); } 内部类 一个类中包容许多方法和字段，有些字段和方法从逻辑上看具有比较紧密的联系，可以把它们“放在一块”，“视为一个整体”。\n内部类可看成是外部类的成员，其地位等同于类中的其他成员。内部类编译以后，每个内部类都会产生一个.class文件，其文件名通常具有以下格式：\n外部类名$内部类名.class Comparable接口 //old public interface Comparable { int compareTo(Object other); } //new and recomended public inteface Comparable\u0026lt;T\u0026gt; { int compareTo(T other); } X.compareTo(Y); //对象X和Y相等，返回0 //对象X\u0026lt;Y,返回-1 //X\u0026gt;Y，返回1 凡是支持大小比较的类型（比如Integer），都实现了Comparable接口。\n‘==’ and equals() “ ==”施加于对象类型，是比较两个对象变量是否引用同一对象。如果需要比对对象的“内容（即各字段的值）”，通常是调用对象的 equals方法。\nequals方法由Object类所定义，其默认实现如下：\npublic boolean equals(Object obj) { return (this == obj); } 子类可根据实际情况，“重写（Override）”Object类的equals方法\n重写Object的equals方法，注意其参数类型必须是“Object”\n另外，为了让对象能放入各种容器中，通常还需要重写 hashCode方法。\n对象组合 一个对象包容另一个对象，称为“对象组合”\n方式：\nA对象完全包容B对象，容器对象管理被包容对象的生命期 B对象是独立的，A对象引用现成的B对象 Cloneable JDK中提供了一个Cloneable接口，需要实现深复制的对象应该实现这一接口。\npublic interface Cloneable { } 像这种根本就没有定义任何一个成员的接口，称为“标记接口”。\nCollection Iterator Iterator接口定义了3个方法\npublic Iterator\u0026lt;E\u0026gt; { boolean hasNext(); // 是否还有下一个元素 E next(); // 获取下一个元素 void remove(); // 移除iterator返回的上一个元素 } 所有实现了Collection接口的集合对象，都提供了一个iterator()方法， 以获取一个iterator对象\nJDK 5以后，Collection接口派生自Iterable，因此JDK中内置的所有集合都可以使 用新的foreach语句完成遍历工作\n集合对象有一个forEach()方法，它可以接收一个Lambda表达式，可用于遍历集合元素， 它与foreach循环不是一回事， foreach循环是一种语法特性， forEach则是JDK为集合类型提供的一个方法。\n如果在遍历时非要删除集合中的元素，则必须通过迭代器对象的remove方法， 而不能通过集合对象直接删除。\n尽管确实可以在迭代时移除当前访问的元素，但并不推荐这么做，特别是在多线程环境下，会带来很大的麻烦。推荐的作法是使用Stream API中的filter()方法，过滤掉不需要的元素。\nHashTable HashMap和HashTable两者功能与用法基本一样，但HashTable是线程安全的， 另外，HashMap可以使用null作为Key。\nProperties Properties派生自HashTable，可以方便地处理“属性-值”对，并且可以很方便地将其保存到文件中，在编程中很常用\nCollections Java提供了一个工具类Collections，封装了一些集合的常规操作。\n更改集合中元素的顺序：reverse、rotate、shuffle、sort和swap 用一个集合填充另一个集合：copy、fill和replaceAll、 addAll 获取集合中的最大和最小元素：max、min 查找元素： binarySearch、 indexOfSubList和lastIndexOfSubList 创建包容多个相同元素的集合：nCopies 函数式编程 Java对“函数式编程”范式的支持，是从JDK 8引入Lambda表达式之后才开始的，主要包容以下几个部分：\nLambda表达式与函数式接口特性（JDK 8起） Stream API（JDK 8起）与Flow API（JDK 9起） 对JDK原有组件进行改造，以支持函数式编程范式 函数式接口 能接收一个Lambda表达式的变量，必须是接口类型，并且这种接口，还必 须是一种“函数式接口（functional interface）”。所谓“函数式接口”，就是“只定义有一个抽象方法的接口”。Java 8中，使用“@FunctionalInterface”标识一个“函数式接口”。\nJDK8 以后接口可有默认方法和静态方法\n方法引用 ClassName::Method\n可以把方法引用看作针对仅仅涉及单一方法的Lambda的“语法 糖”\nStream API 不同于文件操作中的Stream,而是从支持数据处理操作的源生成的元素序列\nStream API采用一种“即抽取即使用即丢弃”的方式处理数据，不需要把所有数据都加载到内存中（集合就是这样的）才能工作，所以，能处理很大的数据集\nStream API中定义的数据处理函数，通常可以级联调用，构成一个数据处理链条\nStream API中定义的数据处理函数，可以分为中间操作和终端操作\nintermediate operation\npeek():similar to forEach() limit() skip() filter() sorted() map() terminal operation\nforEach() reduce() collect() 创建流对象 基于集合：所有集合对象都实现的Collection接口定义了一个Stream()或 parallelStream()方法，可以通过它来创建流对象 使用Stream.Of 使用Stream.Builder接口 针对int、long、double这三种原始数值类型，java 8提供了单独的流类型： IntStream、LongStream和DoubleStream Stream.empty()创建空流 模块化开发 JPMS（Java Platform Module System），是 JDK 9引入的最重要的新特性之一。\n模块化之后，整个程序被分解为若干个严格限定依赖关系的模块。只要不显式声明导出，模块中的所有类外界都是不可访问的。\n模块（module）是包含代码的可识别软件构件，使用了元数据（metadata）来描述模块及其与其他模块的关系。 可以把模块看成是一组用于代码重用的包（package）。模块化（ modularization）是指将系统分解成独立且相互连接的模块的过程。\n打散后的JDK模块， 以.jmod作为文件扩展名。\n模块之间存在着“单向”依赖关系，Java模块系统不允许模块之间 存在编译时的循环依赖\n每个模块都隐式依赖于一个名为“java.base”的特殊模块，它是 一种“聚合器模块（aggregator module）”，这种类型的模块主要用于对其他模块进行逻辑分组，避免在module-info.java中导入太多的模块声明。\n模块设计三个核心原则\n强封装性 定义良好的接口 显式依赖 GUI-JavaFX JavaFX在设计之初就考虑了应用MVC设计模式，整个JavaFX应用 框架围绕着MVC设计模式而展开。\nJavaBean JavaBean可以看成是一种编程约定，按照这种约定编写Java类，开发者之间就易于协作，并且代码也易于重用，并且有一些开发工具（比如 NetBeans这样的IDE）能识别JavaBean所定义的各种属性、方法和事件。\nhttp://www.blog.dyhes.cn/wp-content/uploads/2022/04/image-20220419101022585.png\n","date":"2021-12-11T00:00:00Z","permalink":"https://dyhes.github.io/p/java%E9%87%91%E6%97%AD%E4%BA%AE-java%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/","title":"【Java】金旭亮-Java程序设计"},{"content":"Application Fundamentals An Android package, which is an archive file with an .apk suffix, contains the contents of an Android app that are required at runtime and it is the file that Android-powered devices use to install the app.\nAn Android App Bundle, which is an archive file with an .aab suffix, contains the contents of an Android app project including some additional metadata that is not required at runtime. An AAB is a publishing format and is not installable on Android devices, it defers APK generation and signing to a later stage. When distributing your app through Google Play for example, Google Play\u0026rsquo;s servers generate optimized APKs that contain only the resources and code that are required by a particular device that is requesting installation of the app.\nEach Android app lives in its own security sandbox, protected by the following Android security features:\nThe Android operating system is a multi-user Linux system in which each app is a different user. By default, the system assigns each app a unique Linux user ID (the ID is used only by the system and is unknown to the app). The system sets permissions for all the files in an app so that only the user ID assigned to that app can access them. Each process has its own virtual machine (VM), so an app\u0026rsquo;s code runs in isolation from other apps. By default, every app runs in its own Linux process. The Android system starts the process when any of the app\u0026rsquo;s components need to be executed, and then shuts down the process when it\u0026rsquo;s no longer needed or when the system must recover memory for other apps. App components App components are the essential building blocks of an Android app. Each component is an entry point through which the system or a user can enter your app. Some components depend on others.\nThere are four different types of app components:\nActivities Services Broadcast receivers Content providers Each type serves a distinct purpose and has a distinct lifecycle that defines how the component is created and destroyed.\nApp Architecture An app architecture defines the boundaries between parts of the app and the responsibilities each part should have. In order to meet the needs mentioned above, you should design your app architecture to follow a few specific principles.\nPrinciples Separation of concerns The most important principle to follow is separation of concerns.\nDrive UI from data models Another important principle is that you should drive your UI from data models, preferably persistent models.\nSingle source of truth When a new data type is defined in your app, you should assign a Single Source of Truth (SSOT) to it.\nThe single source of truth principle is often used in our guides with the Unidirectional Data Flow (UDF) pattern. In UDF, state flows in only one direction. The events that modify the data flow in the opposite direction.\nRecommended app architecture Considering the common architectural principles mentioned in the previous section, each application should have at least two layers:\nThe UI layer that displays application data on the screen. The data layer that contains the business logic of your app and exposes application data. You can add an additional layer called the domain layer to simplify and reuse the interactions between the UI and data layers.\nUI layer The role of the UI layer (or presentation layer) is to display the application data on the screen. Whenever the data changes, either due to user interaction (such as pressing a button) or external input (such as a network response), the UI should update to reflect the changes.\nThe UI layer is made up of two things:\nUI elements that render the data on the screen. You build these elements using Views or Jetpack Compose functions. State holders (such as ViewModel classes) that hold data, expose it to the UI, and handle logic. Data layer The data layer of an app contains the business logic. The business logic is what gives value to your app—it\u0026rsquo;s made of rules that determine how your app creates, stores, and changes data.\nThe data layer is made of repositories that each can contain zero to many data sources. You should create a repository class for each different type of data you handle in your app.\nRepository classes are responsible for the following tasks:\nExposing data to the rest of the app. Centralizing changes to the data. Resolving conflicts between multiple data sources. Abstracting sources of data from the rest of the app. Containing business logic. Each data source class should have the responsibility of working with only one source of data, which can be a file, a network source, or a local database. Data source classes are the bridge between the application and the system for data operations.\nDomain layer The domain layer is an optional layer that sits between the UI and data layers.\nThe domain layer is responsible for encapsulating complex business logic, or simple business logic that is reused by multiple ViewModels. This layer is optional because not all apps will have these requirements. You should use it only when needed—for example, to handle complexity or favor reusability.\nClasses in this layer are commonly called use cases or interactors. Each use case should have responsibility over a single functionality. For example, your app could have a GetTimeZoneUseCase class if multiple ViewModels rely on time zones to display the proper message on the screen.\nGeneral best practices Don\u0026rsquo;t store data in app components.\nAvoid designating your app\u0026rsquo;s entry points—such as activities, services, and broadcast receivers—as sources of data.\nReduce dependencies on Android classes.\nYour app components should be the only classes that rely on Android framework SDK APIs such as Context, or Toast.\nCreate well-defined boundaries of responsibility between various modules in your app.\nFor example, don\u0026rsquo;t spread the code that loads data from the network across multiple classes or packages in your code base. Similarly, don\u0026rsquo;t define multiple unrelated responsibilities—such as data caching and data binding—in the same class.\nExpose as little as possible from each module.\nFocus on the unique core of your app so it stands out from other apps.\nDon\u0026rsquo;t reinvent the wheel by writing the same boilerplate code again and again.\nConsider how to make each part of your app testable in isolation.\nTypes are responsible for their concurrency policy.\nIf a type is performing long-running blocking work, it should be responsible for moving that computation to the right thread.\nPersist as much relevant and fresh data as possible.\nThat way, users can enjoy your app\u0026rsquo;s functionality even when their device is in offline mode.\nState There are three ways to declare a MutableState object in a composable:\nval value = remember { mutableStateOf(default) } var value by remember { mutableStateOf(default) } val (value, setValue) = remember { mutableStateOf(default) } These declarations are equivalent, and are provided as syntax sugar for different uses of state\nCaution: Using mutable objects such as ArrayList\u0026lt;T\u0026gt; or mutableListOf() as state in Compose causes your users to see incorrect or stale data in your app. Mutable objects that are not observable, such as ArrayList or a mutable data class, are not observable by Compose and don\u0026rsquo;t trigger a recomposition when they change. Instead of using non-observable mutable objects, the recommendation is to use an observable data holder such as State\u0026lt;List\u0026lt;T\u0026gt;\u0026gt; and the immutable listOf().\nWhile remember helps you retain state across recompositions, the state is not retained across configuration changes. For this, you must use rememberSaveable. rememberSaveable automatically saves any value that can be saved in a Bundle.\nCompose automatically recomposes from reading State objects. If you use another observable type such as LiveData in Compose, you should convert it to State before reading it. Make sure that type conversion happens in a composable, using a composable extension function like LiveData\u0026lt;T\u0026gt;.observeAsState().\nState hoisting State hoisting in Compose is a pattern of moving state to a composable\u0026rsquo;s caller to make a composable stateless.\nThe general pattern for state hoisting in Jetpack Compose is to replace the state variable with two parameters:\nvalue: T: the current value to display onValueChange: (T) -\u0026gt; Unit: an event that requests the value to change, where T is the proposed new value UI state UI state is the property that describes the UI. There are two types of UI state:\nScreen UI state is what you need to display on the screen. This state is usually connected with other layers of the hierarchy because it contains app data. UI element state refers to properties intrinsic to UI elements that influence how they are rendered. A UI element may be shown or hidden and may have a certain font, font size, or font color. Logic Logic in an application can be either business logic or UI logic:\nBusiness logic is the implementation of product requirements for app data. For example, bookmarking an article in a news reader app when the user taps the button. This logic to save a bookmark to a file or database is usually placed in the domain or data layers. The state holder usually delegates this logic to those layers by calling the methods they expose. UI logic is related to how to display UI state on the screen. For example, obtaining the right search bar hint when the user has selected a category, scrolling to a particular item in a list, or the navigation logic to a particular screen when the user clicks a button. UI Logic When UI logic needs to read or write state, you should scope the state to the UI, following its lifecycle. To achieve this, you should hoist the state at the correct level in a composable function. Alternatively, you can do so in a plain state holder class, also scoped to the UI lifecycle.\nKey Point: Keeping UI element state internal to composable functions is acceptable. This is a good solution if the state and logic you apply to it is simple and other parts of the UI hierarchy don\u0026rsquo;t need the state.\nBusiness logic If composables and plain state holders classes are in charge of the UI logic and UI element state, a screen level state holder is in charge of the following tasks:\nProviding access to the business logic of the application that is usually placed in other layers of the hierarchy such as the business and data layers. Preparing the application data for presentation in a particular screen, which becomes the screen UI state. The benefits of AAC ViewModels in Android development make them suitable for providing access to the business logic and preparing the application data for presentation on the screen.\nWhen you hoist UI state in the ViewModel, you move it outside of the Composition.\nFigure 6: State hoisted to the ViewModel is stored outside of the Composition.\nViewModels aren\u0026rsquo;t stored as part of the Composition. They\u0026rsquo;re provided by the framework and they\u0026rsquo;re scoped to a ViewModelStoreOwner which can be an Activity, Fragment, navigation graph, or destination of a navigation graph.\nYou should inject the ViewModel instance in your screen-level composables to provide access to business logic.\nNote: You should not pass ViewModel instances down to other composables.\nConversationScreen ConversationScreen unidirectional data flow The pattern where the state goes down, and events go up is called a unidirectional data flow.\nKey Point: When hoisting state, there are three rules to help you figure out where state should go:\nState should be hoisted to at least the lowest common parent of all composables that use the state (read). State should be hoisted to at least the highest level it may be changed (write). If two states change in response to the same events they should be hoisted together. You can hoist state higher than these rules require, but underhoisting state makes it difficult or impossible to follow unidirectional data flow.\nstate storage Parcelize\nThe simplest solution is to add the @Parcelize annotation to the object. The object becomes parcelable, and can be bundled. For example, this code makes a parcelable City data type and saves it to the state.\n@Parcelize data class City(val name: String, val country: String) : Parcelable @Composable fun CityScreen() { var selectedCity = rememberSaveable { mutableStateOf(City(\u0026#34;Madrid\u0026#34;, \u0026#34;Spain\u0026#34;)) } } MapSaver\nIf for some reason @Parcelize is not suitable, you can use mapSaver to define your own rule for converting an object into a set of values that the system can save to the Bundle.\ndata class City(val name: String, val country: String) val CitySaver = run { val nameKey = \u0026#34;Name\u0026#34; val countryKey = \u0026#34;Country\u0026#34; mapSaver( save = { mapOf(nameKey to it.name, countryKey to it.country) }, restore = { City(it[nameKey] as String, it[countryKey] as String) } ) } @Composable fun CityScreen() { var selectedCity = rememberSaveable(stateSaver = CitySaver) { mutableStateOf(City(\u0026#34;Madrid\u0026#34;, \u0026#34;Spain\u0026#34;)) } } ListSaver\nTo avoid needing to define the keys for the map, you can also use listSaver and use its indices as keys:\ndata class City(val name: String, val country: String) val CitySaver = listSaver\u0026lt;City, Any\u0026gt;( save = { listOf(it.name, it.country) }, restore = { City(it[0] as String, it[1] as String) } ) @Composable fun CityScreen() { var selectedCity = rememberSaveable(stateSaver = CitySaver) { mutableStateOf(City(\u0026#34;Madrid\u0026#34;, \u0026#34;Spain\u0026#34;)) } } Lifecycle The lifecycle of a composable is defined by the following events:\nentering the Composition getting recomposed 0 or more times leaving the Composition. Recomposition is typically triggered by a change to a State object. Compose tracks these and runs all composables in the Composition that read that particular State\u0026lt;T\u0026gt;, and any composables that they call that cannot be skipped.\nA Composable\u0026rsquo;s\u0026rsquo; lifecycle is simpler than the lifecycle of views, activities, and fragments. When a composable needs to manage or interact with external resources that do have a more complex lifecycle, you should use effects.\nAdd extra information to help smart recompositions\n@Composable fun MoviesScreen(movies: List\u0026lt;Movie\u0026gt;) { Column { for (movie in movies) { key(movie.id) { // Unique ID for this movie MovieOverview(movie) } } } } Stable If a composable is already in the Composition, it can skip recomposition if all the inputs are stable and haven\u0026rsquo;t changed.\nA stable type must comply with the following contract:\nThe result of equals for two instances will forever be the same for the same two instances. If a public property of the type changes, Composition will be notified. All public property types are also stable. There are some important common types that fall into this contract that the compose compiler will treat as stable, even though they are not explicitly marked as stable by using the @Stable annotation:\nAll primitive value types: Boolean, Int, Long, Float, Char, etc. Strings All Function types (lambdas) All of these types are able to follow the contract of stable because they are immutable. Since immutable types never change, they never have to notify Composition of the change, so it is much easier to follow this contract.\nNote: All deeply immutable types can safely be considered stable types.\nOne notable type that is stable but is mutable is Compose’s MutableState type. If a value is held in a MutableState, the state object overall is considered to be stable as Compose will be notified of any changes to the .value property of State.\nIf Compose is not able to infer that a type is stable, but you want to force Compose to treat it as stable, mark it with the @Stable annotation.\nModifiers odifiers allow you to decorate or augment a composable. Modifiers let you do these sorts of things:\nChange the composable\u0026rsquo;s size, layout, behavior, and appearance Add information, like accessibility labels Process user input Add high-level interactions, like making an element clickable, scrollable, draggable, or zoomable It\u0026rsquo;s a best practice to have all of your Composables accept a modifier parameter, and pass that modifier to its first child that emits UI.\nThe order of modifier functions is significant. Since each function makes changes to the Modifierreturned by the previous function, the sequence affects the final result.\nMultiple modifiers can be chained together to decorate or augment a composable. This chain is created via the Modifier interface which represents an ordered, immutable list of single Modifier.Elements.\nEach Modifier.Element represents an individual behavior, like layout, drawing and graphics behaviors, all gesture-related, focus and semantics behaviors, as well as device input events. Their ordering matters: modifier elements that are added first will be applied first.\nSometimes it can be beneficial to reuse the same modifier chain instances in multiple composables, by extracting them into variables and hoisting them into higher scopes. It can improve code readability or help improve your app\u0026rsquo;s performance.\nval reusableModifier = Modifier .fillMaxWidth() .background(Color.Red) .padding(12.dp) You can further chain or append your extracted modifier chains by calling the .then() function:\nval reusableModifier = Modifier .fillMaxWidth() .background(Color.Red) .padding(12.dp) // Append to your reusableModifier reusableModifier.clickable { … } // Append your reusableModifier otherModifier.then(reusableModifier) Side-effects A side-effect is a change to the state of the app that happens outside the scope of a composable function.\nComposables should be side-effect free. When you need to make changes to the state of the app, you should use the Effect APIs so that those side effects are executed in a predictable manner.\nKey Term: An effect is a composable function that doesn\u0026rsquo;t emit UI and causes side effects to run when a composition completes.\nLaunchedEffect: run suspend functions in the scope of a composable To call suspend functions safely from inside a composable, use the LaunchedEffect composable. When LaunchedEffect enters the Composition, it launches a coroutine with the block of code passed as a parameter. The coroutine will be cancelled if LaunchedEffect leaves the composition. If LaunchedEffect is recomposed with different keys, the existing coroutine will be cancelled and the new suspend function will be launched in a new coroutine.\n@Composable fun LandingScreen(onTimeout: () -\u0026gt; Unit) { // This will always refer to the latest onTimeout function that // LandingScreen was recomposed with val currentOnTimeout by rememberUpdatedState(onTimeout) // Create an effect that matches the lifecycle of LandingScreen. // If LandingScreen recomposes, the delay shouldn\u0026#39;t start again. LaunchedEffect(true) { delay(SplashWaitTimeMillis) currentOnTimeout() } /* Landing screen content */ } rememberCoroutineScope: obtain a composition-aware scope to launch a coroutine outside a composable As LaunchedEffect is a composable function, it can only be used inside other composable functions. In order to launch a coroutine outside of a composable, but scoped so that it will be automatically canceled once it leaves the composition, use rememberCoroutineScope\nrememberCoroutineScope is a composable function that returns a CoroutineScope bound to the point of the Composition where it\u0026rsquo;s called. The scope will be cancelled when the call leaves the Composition.\n@Composable fun LandingScreen(onTimeout: () -\u0026gt; Unit) { // This will always refer to the latest onTimeout function that // LandingScreen was recomposed with val currentOnTimeout by rememberUpdatedState(onTimeout) // Create an effect that matches the lifecycle of LandingScreen. // If LandingScreen recomposes, the delay shouldn\u0026#39;t start again. LaunchedEffect(true) { delay(SplashWaitTimeMillis) currentOnTimeout() } /* Landing screen content */ } rememberUpdatedState: reference a value in an effect that shouldn\u0026rsquo;t restart if the value changes LaunchedEffect restarts when one of the key parameters changes. However, in some situations you might want to capture a value in your effect that, if it changes, you do not want the effect to restart. In order to do this, it is required to use rememberUpdatedState to create a reference to this value which can be captured and updated. This approach is helpful for effects that contain long-lived operations that may be expensive or prohibitive to recreate and restart.\nFor example, suppose your app has a LandingScreen that disappears after some time. Even if LandingScreen is recomposed, the effect that waits for some time and notifies that the time passed shouldn\u0026rsquo;t be restarted:\n@Composable fun LandingScreen(onTimeout: () -\u0026gt; Unit) { // This will always refer to the latest onTimeout function that // LandingScreen was recomposed with val currentOnTimeout by rememberUpdatedState(onTimeout) // Create an effect that matches the lifecycle of LandingScreen. // If LandingScreen recomposes, the delay shouldn\u0026#39;t start again. LaunchedEffect(true) { delay(SplashWaitTimeMillis) currentOnTimeout() } /* Landing screen content */ } DisposableEffect: effects that require cleanup For side effects that need to be cleaned up after the keys change or if the composable leaves the Composition, use DisposableEffect. If the DisposableEffect keys change, the composable needs to dispose (do the cleanup for) its current effect, and reset by calling the effect again.\n@Composable fun HomeScreen( lifecycleOwner: LifecycleOwner = LocalLifecycleOwner.current, onStart: () -\u0026gt; Unit, // Send the \u0026#39;started\u0026#39; analytics event onStop: () -\u0026gt; Unit // Send the \u0026#39;stopped\u0026#39; analytics event ) { // Safely update the current lambdas when a new one is provided val currentOnStart by rememberUpdatedState(onStart) val currentOnStop by rememberUpdatedState(onStop) // If `lifecycleOwner` changes, dispose and reset the effect DisposableEffect(lifecycleOwner) { // Create an observer that triggers our remembered callbacks // for sending analytics events val observer = LifecycleEventObserver { _, event -\u0026gt; if (event == Lifecycle.Event.ON_START) { currentOnStart() } else if (event == Lifecycle.Event.ON_STOP) { currentOnStop() } } // Add the observer to the lifecycle lifecycleOwner.lifecycle.addObserver(observer) // When the effect leaves the Composition, remove the observer onDispose { lifecycleOwner.lifecycle.removeObserver(observer) } } /* Home screen content */ } SideEffect: publish Compose state to non-compose code To share Compose state with objects not managed by compose, use the SideEffect composable, as it\u0026rsquo;s invoked on every successful recomposition.\n@Composable fun rememberAnalytics(user: User): FirebaseAnalytics { val analytics: FirebaseAnalytics = remember { /* ... */ } // On every successful composition, update FirebaseAnalytics with // the userType from the current User, ensuring that future analytics // events have this metadata attached SideEffect { analytics.setUserProperty(\u0026#34;userType\u0026#34;, user.userType) } return analytics } produceState: convert non-Compose state into Compose state produceState launches a coroutine scoped to the Composition that can push values into a returned State. Use it to convert non-Compose state into Compose state, for example bringing external subscription-driven state such as Flow, LiveData, or RxJava into the Composition.\nThe producer is launched when produceState enters the Composition, and will be cancelled when it leaves the Composition.\n@Composable fun loadNetworkImage( url: String, imageRepository: ImageRepository ): State\u0026lt;Result\u0026lt;Image\u0026gt;\u0026gt; { // Creates a State\u0026lt;T\u0026gt; with Result.Loading as initial value // If either `url` or `imageRepository` changes, the running producer // will cancel and will be re-launched with the new inputs. return produceState\u0026lt;Result\u0026lt;Image\u0026gt;\u0026gt;(initialValue = Result.Loading, url, imageRepository) { // In a coroutine, can make suspend calls val image = imageRepository.load(url) // Update State with either an Error or Success result. // This will trigger a recomposition where this State is read value = if (image == null) { Result.Error } else { Result.Success(image) } } } Note: Composables with a return type should be named the way you\u0026rsquo;d name a normal Kotlin function, starting with a lowercase letter.\nderivedStateOf: convert one or multiple state objects into another state Use derivedStateOf when a certain state is calculated or derived from other state objects. Using this function guarantees that the calculation will only occur whenever one of the states used in the calculation changes.\n@Composable fun TodoList(highPriorityKeywords: List\u0026lt;String\u0026gt; = listOf(\u0026#34;Review\u0026#34;, \u0026#34;Unblock\u0026#34;, \u0026#34;Compose\u0026#34;)) { val todoTasks = remember { mutableStateListOf\u0026lt;String\u0026gt;() } // Calculate high priority tasks only when the todoTasks or highPriorityKeywords // change, not on every recomposition val highPriorityTasks by remember(highPriorityKeywords) { derivedStateOf { todoTasks.filter { it.containsWord(highPriorityKeywords) } } } Box(Modifier.fillMaxSize()) { LazyColumn { items(highPriorityTasks) { /* ... */ } items(todoTasks) { /* ... */ } } /* Rest of the UI where users can add elements to the list */ } } snapshotFlow: convert Compose\u0026rsquo;s State into Flows Use snapshotFlow to convert State objects into a cold Flow. snapshotFlow runs its block when collected and emits the result of the State objects read in it. When one of the State objects read inside the snapshotFlow block mutates, the Flow will emit the new value to its collector if the new value is not equal to the previous emitted value (this behavior is similar to that of Flow.distinctUntilChanged).\nThe following example shows a side effect that records when the user scrolls past the first item in a list to analytics:\nval listState = rememberLazyListState() LazyColumn(state = listState) { // ... } LaunchedEffect(listState) { snapshotFlow { listState.firstVisibleItemIndex } .map { index -\u0026gt; index \u0026gt; 0 } .distinctUntilChanged() .filter { it == true } .collect { MyAnalyticsService.sendScrolledPastFirstItemEvent() } } Restarting effects As a rule of thumb, mutable and immutable variables used in the effect block of code should be added as parameters to the effect composable. Apart from those, more parameters can be added to force restarting the effect. If the change of a variable shouldn\u0026rsquo;t cause the effect to restart, the variable should be wrapped in rememberUpdatedState. If the variable never changes because it\u0026rsquo;s wrapped in a remember with no keys, you don\u0026rsquo;t need to pass the variable as a key to the effect.\nYou can use a constant like true as an effect key to make it follow the lifecycle of the call site.\nPhases Like most other UI toolkits, Compose renders a frame through several distinct phases.\nCompose has three main phases:\nComposition: What UI to show. Compose runs composable functions and creates a description of your UI. Layout: Where to place UI. This phase consists of two steps: measurement and placement. Layout elements measure and place themselves and any child elements in 2D coordinates, for each node in the layout tree. Drawing: How it renders. UI elements draw into a Canvas, usually a device screen. The order of these phases is generally the same, allowing data to flow in one direction from composition to layout to drawing to produce a frame (also known as unidirectional data flow). BoxWithConstraints and LazyColumn and LazyRow are notable exceptions, where the composition of its children depends on the parent\u0026rsquo;s layout phase.\nArchitectural Layering The major layers of Jetpack Compose are:\nEach layer is built upon the lower levels, combining functionality to create higher level components.\nSemantics The Composition is a tree-structure that consists of the composables that describe your UI.\nNext to the Composition, there exists a parallel tree, called the Semantics tree. This tree describes your UI in an alternative manner that is understandable for Accessibility services and for the Testing framework.\nIf your app consists of composables and modifiers from the Compose foundation and material library, the Semantics tree is automatically filled and generated for you. However when you’re adding custom low-level composables, you will have to manually provide its semantics\nCompositionLocal CompositionLocals can be used as an implicit way to have data flow through a composition.\nCompositionLocal elements are usually provided with a value in a certain node of the UI tree. That value can be used by its composable descendants without declaring the CompositionLocal as a parameter in the composable function.\n// Define a CompositionLocal global object with a default // This instance can be accessed by all composables in the app val ActiveUser = compositionLocalOf\u0026lt;User\u0026gt; { error(\u0026#34;No active user found!\u0026#34;) } @Composable fun UserPhoto() { val user = ActiveUser.current ProfileIcon(src = user.profilePhotoUrl) } @Composable fun App(user: User) { CompositionLocalProvider(ActiveUser provides user) { SomeScreen() } } @Composable fun SomeScreen() { UserPhoto() } @Composable fun UserPhoto() { val user = ActiveUser.current ProfileIcon(src = user.profilePhotoUrl) } There are two APIs to create a CompositionLocal:\ncompositionLocalOf: Changing the value provided during recomposition invalidates only the content that reads its current value. staticCompositionLocalOf: Unlike compositionLocalOf, reads of a staticCompositionLocalOf are not tracked by Compose. Changing the value causes the entirety of the content lambda where the CompositionLocal is provided to be recomposed, instead of just the places where the current value is read in the Composition. If the value provided to the CompositionLocal is highly unlikely to change or will never change, use staticCompositionLocalOf to get performance benefits.\nNavigation The NavController is the central API for the Navigation component. It is stateful and keeps track of the back stack of composables that make up the screens in your app and the state of each screen.\nYou can create a NavController by using the rememberNavController() method in your composable:\nval navController = rememberNavController() You should create the NavController in the place in your composable hierarchy where all composables that need to reference it have access to it. This follows the principles of state hoisting and allows you to use the NavController and the state it provides via currentBackStackEntryAsState() to be used as the source of truth for updating composables outside of your screens.\nEach NavController must be associated with a single NavHost composable. The NavHost links the NavController with a navigation graph that specifies the composable destinations that you should be able to navigate between. As you navigate between composables, the content of the NavHost is automatically recomposed. Each composable destination in your navigation graph is associated with a route.\nKey Term: Route is a String that defines the path to your composable. You can think of it as an implicit deep link that leads to a specific destination. Each destination should have a unique route.\nCreating the NavHost requires the NavController previously created via rememberNavController() and the route of the starting destination of your graph. NavHost creation uses the lambda syntax from the Navigation Kotlin DSL to construct your navigation graph. You can add to your navigation structure by using the composable() method. This method requires that you provide a route and the composable that should be linked to the destination:\nNavHost(navController = navController, startDestination = \u0026#34;profile\u0026#34;) { composable(\u0026#34;profile\u0026#34;) { Profile(/*...*/) } composable(\u0026#34;friendslist\u0026#34;) { FriendsList(/*...*/) } /*...*/ } Note: the Navigation Component requires that you follow the Principles of Navigation and use a fixed starting destination. You should not use a composable value for the startDestination route.\nTo navigate to a composable destination in the navigation graph, you must use the navigate method. navigate takes a single String parameter that represents the destination’s route. To navigate from a composable within the navigation graph, call navigate:\nnavController.navigate(\u0026#34;friendslist\u0026#34;) By default, navigate adds your new destination to the back stack. You can modify the behavior of navigate by attaching additional navigation options to our navigate() call:\n// Pop everything up to the \u0026#34;home\u0026#34; destination off the back stack before // navigating to the \u0026#34;friendslist\u0026#34; destination navController.navigate(\u0026#34;friendslist\u0026#34;) { popUpTo(\u0026#34;home\u0026#34;) } // Pop everything up to and including the \u0026#34;home\u0026#34; destination off // the back stack before navigating to the \u0026#34;friendslist\u0026#34; destination navController.navigate(\u0026#34;friendslist\u0026#34;) { popUpTo(\u0026#34;home\u0026#34;) { inclusive = true } } // Navigate to the \u0026#34;search” destination only if we’re not already on // the \u0026#34;search\u0026#34; destination, avoiding multiple copies on the top of the // back stack navController.navigate(\u0026#34;search\u0026#34;) { launchSingleTop = true } Destinations can be grouped into a nested graph to modularize a particular flow in your app’s UI. An example of this could be a self-contained login flow.\nThe nested graph encapsulates its destinations. As with the root graph, a nested graph must have a destination identified as the start destination by its route. This is the destination that is navigated to when you navigate to the route associated with the nested graph.\nTo add a nested graph to your NavHost, you can use the navigation extension function:\nNavHost(navController, startDestination = \u0026#34;home\u0026#34;) { ... // Navigating to the graph via its route (\u0026#39;login\u0026#39;) automatically // navigates to the graph\u0026#39;s start destination - \u0026#39;username\u0026#39; // therefore encapsulating the graph\u0026#39;s internal routing logic navigation(startDestination = \u0026#34;username\u0026#34;, route = \u0026#34;login\u0026#34;) { composable(\u0026#34;username\u0026#34;) { ... } composable(\u0026#34;password\u0026#34;) { ... } composable(\u0026#34;registration\u0026#34;) { ... } } ... } It is strongly recommended that you split your navigation graph into multiple methods as the graph grows in size. This also allows multiple modules to contribute their own navigation graphs.\nfun NavGraphBuilder.loginGraph(navController: NavController) { navigation(startDestination = \u0026#34;username\u0026#34;, route = \u0026#34;login\u0026#34;) { composable(\u0026#34;username\u0026#34;) { ... } composable(\u0026#34;password\u0026#34;) { ... } composable(\u0026#34;registration\u0026#34;) { ... } } } By making the method an extension method on NavGraphBuilder, you can use it alongside the prebuilt navigation, composable, and dialog extension methods:\nNavHost(navController, startDestination = \u0026#34;home\u0026#34;) { ... loginGraph(navController) ... } UI Design Layout Column\nUse Column to place items vertically on the screen.\nRow\nSimilarly, use Row to place items horizontally on the screen. Both Column and Row support configuring the alignment of the elements they contain.\nBox\nUse Box to put elements on top of another. Box also supports configuring specific alignment of the elements it contains.\nOften these building blocks are all you need. You can write your own composable function to combine these layouts into a more elaborate layout that suits your app.\nTo set children\u0026rsquo;s position within a Row, set the horizontalArrangement and verticalAlignment arguments. For a Column, set the verticalArrangement and horizontalAlignment arguments.\nIn order to know the constraints coming from the parent and design the layout accordingly, you can use a BoxWithConstraints.\nContent slots Material Components that support inner content (text labels, icons, etc.) tend to offer “slots” — generic lambdas that accept composable content — as well as public constants, like size and padding, to support laying out inner content to match Material specifications.\nTesting APIs There are three main ways to interact with elements:\nFinders let you select one or multiple elements (or nodes in the Semantics tree) to make assertions or perform actions on them. Assertions are used to verify that the elements exist or have certain attributes. Actions inject simulated user events on the elements, such as clicks or other gestures. Some of these APIs accept a SemanticsMatcher to refer to one or more nodes in the semantics tree.\nFinders You can use onNode and onAllNodes to select one or multiple nodes respectively, but you can also use convenience finders for the most common searches, such as onNodeWithText , onNodeWithContentDescription, etc.\ncomposeTestRule .onNode(hasText(\u0026#34;Button\u0026#34;)) // Equivalent to onNodeWithText(\u0026#34;Button\u0026#34;) Assertions Check assertions by calling assert() on the SemanticsNodeInteraction returned by a finder with one or multiple matchers:\n// Single matcher: composeTestRule .onNode(matcher) .assert(hasText(\u0026#34;Button\u0026#34;)) // hasText is a SemanticsMatcher // Multiple matchers can use and / or composeTestRule .onNode(matcher).assert(hasText(\u0026#34;Button\u0026#34;) or hasText(\u0026#34;Button2\u0026#34;)) You can also use convenience functions for the most common assertions, such as assertExists , assertIsDisplayed , assertTextEquals , etc. You can browse the complete list in the Compose Testing cheat sheet.\nActions To inject an action on a node, call a perform…() function:\ncomposeTestRule.onNode(...).performClick() Note: You cannot chain actions inside a perform function. Instead, make multiple perform() calls.\n","date":"2021-11-18T00:00:00Z","permalink":"https://dyhes.github.io/p/androidjetpack-compose/","title":"【Android】Jetpack Compose"},{"content":"视图模式 Android Studio支持以多种方式查看Android项目，可点击左上角的下拉菜单在各视图模式之间进行切换。\n开发中最常用的是两个视图：\n（1）Android：一般开发时使用这 个视图，它帮助隐藏和折叠了一些 开发很少用到的文件和文件夹。\n（2）Project：与真实的文件管理器 中看到的文件和文件夹内容一致。\n总体结构 AndroidManifest.xml：用于设置 Android应用的各种配置参数。 Java:程序员为Activity/Fragment所写的代 码放在java文件夹下的相应包文件夹中 Android Studio使用Gradle构建项目， 这些文件中最重要的就是项目与模块所 关联的build.gradle文件。 控件 整个App界面是一个Activity， 其中包容有其他UI控件\n一个Activity代表一个单独的用户可见的界面，由各种可视化控件所构成。\nView 所有的Android控件都派生自View\nViewGroup 能够包容其他Android控件的控件，称为“容器” 控件，派生自ViewGroup\nLayout 一种特殊的“容器”控件， 本身不可见，其职责是排 列它所包容的控件。\nApp中的UI界面与功能代码是分离的\nKotlin/Java代码 （.java/.kt），完成 交互和数据处理功能。\n布局文件（.xml）确定 Activity的界面呈现\n步骤 使用xml定义UI布局文件，其中需要通过代码访问的控件需要给 定一个Id。 .在onCreate()方法中setContentView(布局文件），指定Activity使用 此特定的界面布局。 使用findViewById()方法获取特定控件的引用 调整控件的属性、给其特定事件添加响应代码 消除findViewById:\n视图绑定 （View Binding）\n数据绑定库 （Data Binding Library）\nLog类 当Android App运行时，可以使用Log类，在Logcat面板输出信息\nLog.v(String tag, String msg)；（VERBOSE） Log.d(String tag, String msg)；（DEBUG） Log.i(String tag, String msg)；（INFO） Log.w(String tag, String msg)；（WARN） Log.e(String tag, String msg)；（ERROR） compared with Timber 它会自动检测它所处的类，使用类名作为TAG，从而省去了自定 义TAG的麻烦，但在需要时你也可以人工指定TAG名称。\n当使用Release模式发布App时，它会自动地Disable掉，无需 人工处理。\n显示信息 Toast Toast.makeText(Context对象, \u0026#34;要显示的信息\u0026#34;,Toast.LENGTH_LONG).show() 有许多的Android函数都需要有一个Context对象作为参数。\n使用Context对象，你就可以获取Android操作系统的当前状态信息，访问到Android操作系统的各种组件，调用它的服务。\nAndroid应用中常见的界面组件——Activity和Fragment（后 面课程介绍），都派生自Context类，所以，它们都可以看成是一个Context对象。当你在编程中发现某个方法需要有一个 Context对象时，直接将一个Activity或Fragment对象的引用传给它就好了，也可以通过“控件变量名.context”得到一 个Context对象。\n对话框 Dialog\n图片 Android项目的drawable文件夹用于保存App中用到的图片或图标。可以从网上找到各种图片（.jpg或.png），直接将其复制到此文件夹中，即可供App所使用。\n复制到drawable文件夹下的图片，拥有一 个（资源）标识值，其格式为： 放在drawable中的图片文件，其名字中不要有汉字或空格等字 符，应该采用全英文小写字母作为文件名。\nR.drawable.图片文件名\nAndroid Studio中可以创建 两种类型的图片资源，一种 称为“Image Asset”，是 基于像素的位图，另一种是 “Vector Asset”，是一种 矢量图形。\n位图图形即常见的“数码相 片”，扩展名通常为“.jpg” 或“.png”，当放大到一定程度时，可以明显地看到其中的像素色块。\n矢量图形是使用数学方式描述的，可以随意地放大和缩小，清晰度不会改变，在 Android中，这种图像资源的扩展名为\u0026quot;.xml\u0026quot;\n菜单 在res/menu中编写菜单资源文件\n在Activity中重写onCreateOptionsMenu 和onOptionsItemSelected两个方法\ncontext menu 调用registerForContextMenu()方 法为某个view注册上下文菜单，其参 数就是那个长按弹出上下文菜单的控 件，此方法一般在onCreate里面调用\n用户输入 EditText是Android App中让用户输入信息 的主要控件之一。\n为了规范用户的输入信息， EditText提供 了InputType属性，用于过滤掉不合法的输 入字符，只有符合输入类型要求的字符，才 允许接收并显示出来。\n可以给EditText提供一个实现了TextWatcher接口的对象，在里面写代码实现特定的监控功能。\n布局 使用XML定义App界面并非唯一的方式，可以使用代码完成相同的功能：\n事实上，用XML定义的界面，最后也是要转换为代码来实现布局的\n实际开发中，应该使用XML为设计UI的主要方式，代码作为辅助手段。 但现在Google正在研发一种称为Compose的技术，完全使用kotlin取代 XML来定义界面，非常值得关注，它很可能代表了技术的发展方向\nViewGroup View有一个特殊的子类，称为ViewGroup。ViewGroup提供了对其子组件的管理功能，包括布局、动画等。ViewGroup的子组件可以是View也可以是ViewGroup。常用的布局控件（比如LinearLayout）都是ViewGroup的子类。\nBox Model • View支持padding，但是不支持margin。\n• ViewGroup支持padding和margin\nAndroid控件所占的具体位置，是由其父控件（主要就是布局控件）通过layout_width和layout_height等相关布局属性在App运行时“动态” 计算出来的，上述两个属性可以取的值有三种：\n固定值：比如100dp wrap_content：数值依控件所显示的具体内容而定 match_parent：与其父控件的数值相匹配（一致） 度量 采用设备无关的像素（dp：Density-independent Pixel），能够让图像在不同分辨率的屏幕上都显示出一样的大小。\n布局通常使用dp作为单位，而字体尺寸则使用sp（Scalable pixels） 作为单位，这两者基本上是一样的，只不过sp会保留用户的设置（比如 用户可以选择使用大字体来显示App上的文本）\n颜色 Android系统有12种已经定义好的颜色，具体的类型定义在Color类中。\ntextView.setTextColor(Color.RED) 在布局文件中设置颜色需要在色值前面加“#”，如 android:textColor=\u0026quot;#000000\u0026quot;。 也可以通过Color.rgb(int red, int green, int blue) 和Color.argb(int alpha, int red, int green, int blue)这两种方法指定颜色，后者可以同时指定“透明度”。 在代码中设置颜色可以直接填八位的十六进制数值，如 setTextColor(0xff00ff000)\ncolors.xml res/values目录下有个colors.xml文件，可用于定义颜色常量。\n如果要在布局、样式等XML文件中使用颜色常量，可以使用“@color/ 颜色常量名”的方式 如果要在代码中使用XML颜色常量，在Activity中可通过这行代码获取： getResources().getColor(R.color.颜色常量名)。 布局控件 早期的Andorid提供了很多种布局控件， 不少都被废弃了，就当前的学习者来 说，只需要掌握LinearLayout、 FrameLayout、ConstraintLayout 这三种控件就足以应付大多数Android 应用开发场景\nLinearLayout:\nLinearLayout是一种Android中最常 用的布局，它将自己包含的子元素按 照一个特定的方向进行排列。\n排列方向有两种：水平和垂直\nFrameLayout:\n所有的子控件都被放置在FrameLayout区域最左上 的区域。而且无法为这些控件指定一个精确的位置， 但可以通过layout_gravity指定放置区域。\n如果一个FrameLayout里边有多个子控件，不做任 何调整的话，那么后边的子控件的显示会重叠在前一个控件上。\nConstraintLayout:\nVery important\n扁平的控件层次结构优于多层 的控件层次结构。\n因此，为了能提升Android应用界面的绘制速度，现在推荐使用 “扁平化”的ConstraintLayout来设计那些比较复杂的原先必须 使用“多层嵌套”才能实现的用户界面。\nlayout_gravity和gravity layout_gravity设定该控件与上级 级控件的对齐方式，而gravity设定 布局内部子控件对齐与排列方式。\n使用layout_gravity，通常 要求控件的尺寸是 \u0026ldquo;wrap_content\u0026rdquo;，而要使用 gravity，控件的尺寸通常 是\u0026quot;match_parent\u0026quot;\nweightSum和layout_weight “指定所有子控件weight 之和”。 • 子控件的layout_weight值设定其在 weightSum中所占的份额。\n也可以不指定weightSum，直接按照子 控件的layout_weight值“协商”瓜分 可用的空间，值大的，占据更大的空间。\nConstraintLayout 在build.gradle中添加以下依赖：\ndependencies { implementation \u0026#39;androidx.constraintlayout:constraintlayout:2.1.3\u0026#39; } Android内置的控件，比如TextView，可以直接在XML中使用其 类名。另外，其属性名前通常有“android:”前缀，表明它是 系统内置的控件，随着用户手机操作系统而一同安装，App可以 直接使用它们。\nConstraintLayout是一个外部库（library）控件，并不是 Android的有机组成部分。它本身必须被打包到Apk中，与应用一起 安装。它的属性以app:开头\nActivity生命周期 状态：\nInitialized\nCreated\nStarted\nResumed\nDestroyed\n在Android 中，Activity 的生命周期交给操作系统统一管理。\n在android.app.Activity 类中，Android 定义了一系列与生命周期相 关的方法，这些方法将被系统在合适的时机回调，我们可以根据 需要“覆盖（或重写，override）”特定的方法。\npublic class Activity { protected void onCreate(Bundle savedInstanceState); protected void onStart(); protected void onResume(); protected void onPause(); protected void onStop(); protected void onDestroy(); } 调用Activity的finish()方法可以通知Android销毁Activity对 象并将它从Back Stack（回退堆栈）中移除，这时，onDestory() 方法会被调用，可以在此方法中“清理必要的资源”\n创建activity 为新Activity创建一个布局文件，设计好其布局。 在App的清单文件中注册这个Activity。 创建一个Intent实例，调用startActivity(intent)方法，即可启动并显示这个Activity。 创建一个Activity类，并且在Activity的onCreate()方法中将布局文件与Activity关联起来 activity信息交流 Intent提供了putExtra()系列 方法将信息放到Bundle中 可以使用getXXExtra()系列 方法取出信息 如果是离散的基础数据类型（比如Int和String）信息，直接调用putXXX系 列方法存入\n如果是一个对象，需要进行特殊的处理，让其实现Parcelable接口，并 附加@Parcelize注解\n随着技术的不断演进，特别是Android Jetpack相关组件的推出，现在更推荐 采用“单Activity多Fragment”的方式\nBack Stack 用户使用App的过程可以看成是多个Activity顺序显示 的过程，用户可以随时使用“Back”键回退回前一 个Activity。但这里有一个问题，那就是Android有可 能会销毁掉不在前台的Activity，所以，Android引入 了一个Task的机制解决这个问题。Activity可以被销 毁，但它的相关信息仍然放在Task中，依据这些信息 Android就可以重新创建并显示“前一个”Activity。\n位于Task栈顶的Activity称为“前台Activity”，用户可以看到它，并且能与它交互。 其余的Activity称为“后台Activity”，用户看不到它，只有它上面的Activity被移除之后，它成为栈顶，才能被用户看到。 由于用户影响Task栈的最常见也是最主要的方式是使用手机的“Back”按键，所以，Task栈通常被称为“回退堆栈（Back Stack）”或“返回栈”。 当系统资源紧张时，Android可以销毁掉一些“后台Activity\nTask Android操作系统允许用户打开多个App，每个打开的App都对应着一个Task。\n当前正在与用户交互的App所对应着的Task，称为“前台Task”\n其他的App对应的Task，称为“后台Task”\nactivity启动模式 \u0026lt;activity android:name=\u0026#34;.MainActivity\u0026#34; android:launchMode=\u0026#34;standard\u0026#34;/\u0026gt; standard：这是默认模式，每次激活Activity时都会创建Activity实例，并放入Back Stack中。 singleTop：如果在栈顶已有一个实例，则重用此实例，并调用此实例的onNewIntent 方法。否则，创建新的实例并放入栈顶（即使栈中已经存在该Activity的实例，只要不在栈顶，都会创建实例） singleTask: 这种模式的Activity在一个Task中只能有一个实例存在。当需要启 动一个SingleTask模式的Activity时，如果所有前台后台Task中都没 有它的实例，系统就会创建它并将它压入当前Task的堆栈中。如果另外的Task中已存在该Activity 的实例，则系 统会通过调用这一Activity的 onNewIntent() 方法将 intent 转送给它，而不是创建新实例 singleInstance：在一个新栈中创建实例，并让多个应用共享该实例。一 旦该模式的Activity的实例已经存在，任何应用在激活该Activity时，都会重 用该栈中的实例（并会自动调用其onNewIntent 方法）。 Intent Android平台采用“松散协同配合”的理念进行设计，淡化了进程的概念。 Android应用可以使用多个不同来源的可重用组件以聚合的方式构建（比如在你的应用中直接集成Android系统提供的拍照程序完成照像功能）。\n为了让这些组件能相互沟通和协作，Android引入了“Intent”这一特殊的组件当作“信使”，完成组件间相互通信的工作。\nIntent Filter 每个Activity都可以指定一个或多个Intent Filter，以便告诉系统该 Activity 可以响应什么类型的Intent\n类型 显式（Explicit）Intent\n直接启动特定的Activtity，只需要两个参数：context和要启动的Activity的类型，通常用于在同一个App内切换显示Activity\n隐式（implicit）Intent\n告诉Android“你想干什么”，由Android帮助你筛选启动特定的Activtity，至少需要两个参数：一个是Action，另一个是Data URI，还可以附加有其他的参数（比如Category，Extra等）\nFragment 将复杂的Activity进行分解，使Android App应用更为模块化。\n可使用代码方便地进行控制，因此可以构建出高度灵活的UI。 它是许多其它Android技术的基础，比如一些控件（ViewPages）就使用Fragment作为其界面元素，Jetpack架构组件（比如导航）也是以Fragment作为基本构建单元的。\nFragment由两部分组成：一个是布局文件，另一个是派生 自Fragment的自定义类，至少要重写onCreateView方法。\n规范 Fragment类名称通常采用驼峰命名法，以“Fragment”结束： 例如：SignInFragment\n相应的XML文件应该遵守以下命名规范： fragment_\u0026lt;FRAGMENT_NAME\u0026gt;.xml例如：fragment_sign_in.xml\n状态 Resumed：处于可见状态，能接收用户响应\nStarted：处于可见状态，但不能接收用户响应\nCreated：处于不可见状态\n信息交流 from Activity\n直接（不推荐）\nActivity可以通过FragmentManager的findFragmentByTag等方法获取 特定Fragment对象的引用，然后直接访问Fragment类所定义的公有属性或 方法，即可向这一Fragment对象传送特定的信息。\nArguments实现（推荐）\n为了解决Activity向Fragment传送信息比较麻烦且易出错的问题， Android为Fragment提供了一个名为arguments的Bundle对象，使用它 来传送信息，就不会出错。\n工厂方法\nto Activity\n定义一个专用接口，此接口中所定义方法的参数代表Fragment需要传给Activity的信息；\n在Fragment内部定义一个类型为这个接口的属性；\nActivity实现这个接口，并在App运行创建Fragment时，将自身引用传给Fragment；\nFragment在适合的时机，回调Activity实现的接口方法。\n与Fragment\nJetpack 构成 基础组件 架构组件 行为组件 界面组件 由于Android Jetpack提供了现成的ViewModel、DataBinding等组件， 所以，MVVM就成为了推荐的Android App UI层设计模式\nLifecycles ViewModel ViewModel是一个类，它包容那 些Activity/Fragment需要显示 的数据\n架构 ViewPage ViewPage主要用于实现界面的切换\n进程与线程 郑乐祺：注册/登录\n李赛伽：校园论坛+学生事务\n余力：校园监测\n林宏鹏：主页+教务管理\n","date":"2021-11-16T00:00:00Z","permalink":"https://dyhes.github.io/p/android%E9%87%91%E6%97%AD%E4%BA%AE-android%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/","title":"【Android】金旭亮-Android应用开发"},{"content":"In Android and iOS development, a view is the basic building block of UI: a small rectangular element on the screen which can be used to display text, images, or respond to user input. Even the smallest visual elements of an app, like a line of text or a button, are kinds of views. Some kinds of views can contain other views.\nNative Components In Android development, you write views in Kotlin or Java;\nin iOS development, you use Swift or Objective-C.\nWith React Native, you can invoke these views with JavaScript using React components.\nAt runtime, React Native creates the corresponding Android and iOS views for those components. Because React Native components are backed by the same views as Android and iOS, React Native apps look, feel, and perform like any other apps.\nWe call these platform-backed components Native Components.\nCore Components React Native comes with a set of essential, ready-to-use Native Components you can use to start building your app today. These are React Native\u0026rsquo;s Core Components.\nREACT NATIVE UI COMPONENT ANDROID VIEW IOS VIEW WEB ANALOG DESCRIPTION \u0026lt;View\u0026gt; \u0026lt;ViewGroup\u0026gt; \u0026lt;UIView\u0026gt; A non-scrollling \u0026lt;div\u0026gt; A container that supports layout with flexbox, style, some touch handling, and accessibility controls \u0026lt;Text\u0026gt; \u0026lt;TextView\u0026gt; \u0026lt;UITextView\u0026gt; \u0026lt;p\u0026gt; Displays, styles, and nests strings of text and even handles touch events \u0026lt;Image\u0026gt; \u0026lt;ImageView\u0026gt; \u0026lt;UIImageView\u0026gt; \u0026lt;img\u0026gt; Displays different types of images \u0026lt;ScrollView\u0026gt; \u0026lt;ScrollView\u0026gt; \u0026lt;UIScrollView\u0026gt; \u0026lt;div\u0026gt; A generic scrolling container that can contain multiple components and views \u0026lt;TextInput\u0026gt; \u0026lt;EditText\u0026gt; \u0026lt;UITextField\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot;\u0026gt; Allows the user to enter text Platform Specific Code When building a cross-platform app, you\u0026rsquo;ll want to re-use as much code as possible. Scenarios may arise where it makes sense for the code to be different, for example you may want to implement separate visual components for Android and iOS.\nReact Native provides two ways to organize your code and separate it by platform:\nUsing the Platform module. Using platform-specific file extensions. Certain components may have properties that work on one platform only. All of these props are annotated with @platform and have a small badge next to them on the website.\nUsing platform-specific file extensions. For example, say you have the following files in your project:\nBigButton.ios.js BigButton.android.js You can then require the component as follows:\nimport BigButton from \u0026#39;./BigButton\u0026#39;; React Native will automatically pick up the right file based on the running platform.\nYou can also use the .native.js extension when a module needs to be shared between NodeJS/Web and React Native but it has no Android/iOS differences. This is especially useful for projects that have common code shared among React Native and ReactJS.\nFor example, say you have the following files in your project:\nContainer.js ## picked up by Webpack, Rollup or any other Web bundler Container.native.js ## picked up by the React Native bundler for both Android and iOS (Metro) You can still require it without the .native extension, as follows:\nimport Container from \u0026#39;./Container\u0026#39;; Pro tip: Configure your Web bundler to ignore .native.js extensions in order to avoid having unused code in your production bundle, thus reducing the final bundle size.\nStyle With React Native, you style your application using JavaScript. All of the core components accept a prop named style. The style names and values usually match how CSS works on the web, except names are written using camel casing\nconst styles = StyleSheet.create({ container: { marginTop: 50, }, bigBlue: { color: \u0026#39;blue\u0026#39;, fontWeight: \u0026#39;bold\u0026#39;, fontSize: 30, }, red: { color: \u0026#39;red\u0026#39;, }, }); Dimensions Fixed Dimensions The general way to set the dimensions of a component is by adding a fixed width and height to style. All dimensions in React Native are unitless, and represent density-independent pixels.\nFlex Dimensions Use flex in a component\u0026rsquo;s style to have the component expand and shrink dynamically based on available space. Normally you will use flex: 1, which tells a component to fill all available space, shared evenly amongst other components with the same parent. The larger the flex given, the higher the ratio of space a component will take compared to its siblings.\nA component can only expand to fill available space if its parent has dimensions greater than 0. If a parent does not have either a fixed width and height or flex, the parent will have dimensions of 0 and the flex children will not be visible.\nPercentage Dimensions If you want to fill a certain portion of the screen, but you don\u0026rsquo;t want to use the flex layout, you can use percentage values in the component\u0026rsquo;s style. Similar to flex dimensions, percentage dimensions require parent with a defined size.\nLayout You will normally use a combination of flexDirection, alignItems, and justifyContent to achieve the right layout.\nflexDirection controls the direction in which the children of a node are laid out. This is also referred to as the main axis. The cross axis is the axis perpendicular to the main axis, or the axis which the wrapping lines are laid out in.\ncolumn(default) row column-reverse row-reverse justifyContent describes how to align children within the main axis of their container.\nflex-start(default) flex-end center space-between space-around space=evenly alignItems describes how to align children along the cross axis of their container. It is very similar to justifyContent but instead of applying to the main axis, alignItems applies to the cross axis.\nstretch (default):Stretch children of a container to match the height of the container\u0026rsquo;s cross axis. flex-start flex-end center baseline:Align children of a container along a common baseline. Individual children can be set to be the reference baseline for their parents. alignSelf has the same options and effect as alignItems but instead of affecting the children within a container, you can apply this property to a single child to change its alignment within its parent. alignSelf overrides any option set by the parent with alignItems.\nThe flexWrap property is set on containers and it controls what happens when children overflow the size of the container along the main axis. By default, children are forced into a single line (which can shrink elements). If wrapping is allowed, items are wrapped into multiple lines along the main axis if needed. When wrapping lines, alignContent can be used to specify how the lines are placed in the container\nFlex Basis, Grow, and Shrink flexBasis is an axis-independent way of providing the default size of an item along the main axis.\nflexGrow describes how any space within a container should be distributed among its children along the main axis.\nflexShrink describes how to shrink children along the main axis in the case in which the total size of the children overflows the size of the container on the main axis.\nImage Static Image Resources \u0026lt;Image source={require(\u0026#39;./img/check.png\u0026#39;)} /\u0026gt; Note that image sources required this way include size (width, height) info for the Image. If you need to scale the image dynamically (i.e. via flex), you may need to manually set { width: undefined, height: undefined } on the style attribute.\nNetwork Images \u0026lt;Image source={{uri: \u0026#39;https://reactjs.org/logo-og.png\u0026#39;}} style={{width: 400, height: 400}} /\u0026gt; Background Image return ( \u0026lt;ImageBackground source={...} style={{width: \u0026#39;100%\u0026#39;, height: \u0026#39;100%\u0026#39;}}\u0026gt; \u0026lt;Text\u0026gt;Inside\u0026lt;/Text\u0026gt; \u0026lt;/ImageBackground\u0026gt; ); Handling Touches Users interact with mobile apps mainly through touch.\nButton provides a basic button component that is rendered nicely on all platforms.\nIf the basic button doesn\u0026rsquo;t look right for your app, you can build your own button using any of the \u0026ldquo;Touchable\u0026rdquo; components provided by React Native.\nNavigation Mobile apps are rarely made up of a single screen. Managing the presentation of, and transition between, multiple screens is typically handled by what is known as a navigator.\nIf you are getting started with navigation, you will probably want to use React Navigation.\nIf you\u0026rsquo;re integrating React Native into an app that already manages navigation natively, or looking for an alternative to React Navigation, the following library provides native navigation on both platforms: react-native-navigation.\nReact Navigation Now, you need to wrap the whole app in NavigationContainer. Usually you\u0026rsquo;d do this in your entry file, such as index.js or App.js\nconst App = () =\u0026gt; { return ( \u0026lt;NavigationContainer\u0026gt; {/* Rest of your app code */} \u0026lt;/NavigationContainer\u0026gt; ); }; Animation React Native provides two complementary animation systems: Animated for granular and interactive control of specific values, and LayoutAnimation for animated global layout transactions.\nAnimated exports six animatable component types: View, Text, Image, ScrollView, FlatList and SectionList, but you can also create your own using Animated.createAnimatedComponent().\n","date":"2021-10-11T00:00:00Z","permalink":"https://dyhes.github.io/p/react-nativeconcepts/","title":"【React Native】Concepts"},{"content":"fromSomeBlog We create a dynamic URL by creating a dynamic page with the [] syntax.\nHow? We add a pages/blog/[id].js file. This file will handle all the dynamic URLs under the /blog/ route\nThe router is a library provided by Next.js.\nWe import it from next/router:\nimport { useRouter } from \u0026#39;next/router\u0026#39; and once we have useRouter, we instantiate the router object using:\nconst router = useRouter() Once we have this router object, we can extract information from it.\nIn particular we can get the dynamic part of the URL in the [id].js file by accessing router.query.id.\nThe dynamic part can also just be a portion of the URL, like post-[id].js.\nSo let\u0026rsquo;s go on and apply all those things in practice.\nCreate the file pages/blog/[id].js\nCreate the file pages/blog/[id].js:\nimport { useRouter } from \u0026#39;next/router\u0026#39; export default () =\u0026gt; { const router = useRouter() return ( \u0026lt;\u0026gt; \u0026lt;h1\u0026gt;Blog post\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Post id: {router.query.id}\u0026lt;/p\u0026gt; \u0026lt;/\u0026gt; ) } fromOfficialWebsite Link If you need to link to an external page outside the Next.js app, just use an \u0026lt;a\u0026gt; tag without Link.\nIf you need to add attributes like, for example, className, add it to the a tag, not to the Link tag\nIn Next.js, a page is a React Component exported from a .js, .jsx, .ts, or .tsx file in the pages directory. Each page is associated with a route based on its file name.\nNext.js can serve static assets, like images, under the top-level public directory. Files inside public can be referenced from the root of the application similar to pages.\nImage Using primitive html image tag means you have to manually handle:\nEnsuring your image is responsive on different screen sizes Optimizing your images with a third-party tool or library Only loading images when they enter the viewport And more. Instead, Next.js provides an Image component out of the box to handle this for you.\nImages are lazy loaded by default. That means your page speed isn\u0026rsquo;t penalized for images outside the viewport. Images load as they are scrolled into viewport.\n(the url didn\u0026rsquo;t include public)\nHead In addition to metadata, scripts that need to load and execute as soon as possible are usually added within the \u0026lt;head\u0026gt; of a page. Using a regular HTML \u0026lt;script\u0026gt; element, an external script would be added as follows:\n\u0026lt;Head\u0026gt; \u0026lt;title\u0026gt;First Post\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;https://connect.facebook.net/en_US/sdk.js\u0026#34; /\u0026gt; \u0026lt;/Head\u0026gt; Script next/script is an extension of the HTML \u0026lt;script\u0026gt; element and optimizes when additional scripts are fetched and executed.\nexport default function FirstPost() { return ( \u0026lt;\u0026gt; \u0026lt;Head\u0026gt; \u0026lt;title\u0026gt;First Post\u0026lt;/title\u0026gt; \u0026lt;/Head\u0026gt; \u0026lt;Script src=\u0026#34;https://connect.facebook.net/en_US/sdk.js\u0026#34; strategy=\u0026#34;lazyOnload\u0026#34; onLoad={() =\u0026gt; console.log(`script loaded correctly, window.FB has been populated`) } /\u0026gt; \u0026lt;h1\u0026gt;First Post\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt; \u0026lt;Link href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;a\u0026gt;Back to home\u0026lt;/a\u0026gt; \u0026lt;/Link\u0026gt; \u0026lt;/h2\u0026gt; \u0026lt;/\u0026gt; ) } Notice that a few additional properties have been defined in the Script component:\nstrategy controls when the third-party script should load. A value of lazyOnload tells Next.js to load this particular script lazily during browser idle time onLoad is used to run any JavaScript code immediately after the script has finished loading. In this example, we log a message to the console that mentions that the script has loaded correctly CSS Modules Important: To use CSS Modules, the CSS file name must end with .module.css.\nThis is what CSS Modules does: It automatically generates unique class names. As long as you use CSS Modules, you don’t have to worry about class name collisions.\nFurthermore, Next.js’s code splitting feature works on CSS Modules as well. It ensures the minimal amount of CSS is loaded for each page. This results in smaller bundle sizes.\nCSS Modules are extracted from the JavaScript bundles at build time and generate .css files that are loaded automatically by Next.js.\nGlobal Modules CSS Modules are useful for component-level styles. But if you want some CSS to be loaded by every page, Next.js has support for that as well.\nTo load global CSS files, create a file called pages/_app.js with the following content:\nexport default function App({ Component, pageProps }) { return \u0026lt;Component {...pageProps} /\u0026gt; } This App component is the top-level component which will be common across all the different pages. You can use this App component to keep state when navigating between pages\nImportant: You need to restart the development server when you add pages/_app.js.\nIn Next.js, you can add global CSS files by importing them from pages/_app.js. You cannot import global CSS anywhere else.\nThe reason that global CSS can\u0026rsquo;t be imported outside of pages/_app.js is that global CSS affects all elements on the page.\nYou can place the global CSS file anywhere and use any name. So let’s do the following:\nCreate a top-level styles directory and create global.css inside. Pre-rendering Next.js has two forms of pre-rendering: Static Generation and Server-side Rendering. The difference is in when it generates the HTML for a page.\nStatic Generation is the pre-rendering method that generates the HTML at build time. The pre-rendered HTML is then reused on each request. Server-side Rendering is the pre-rendering method that generates the HTML on each request. In development mode (when you run npm run dev or yarn dev), every page is pre-rendered on each request — even for pages that use Static Generation.\nin Next.js, when you export a page component, you can also export an async function called getStaticProps.\ngetStaticProps In development (npm run dev or yarn dev), getStaticProps runs on every request. In production, getStaticProps runs at build time. However, this behavior can be enhanced using the fallback key returned by getStaticPaths If you need to fetch data at request time instead of at build time, you can try Server-side Rendering\nTo use Server-side Rendering, you need to export getServerSideProps instead of getStaticProps from your page.\nexport async function getServerSideProps(context) { return { props: { // props for your component } } } Because getServerSideProps is called at request time, its parameter (context) contains request specific parameters.\nYou should use getServerSideProps only if you need to pre-render a page whose data must be fetched at request time. Time to first byte (TTFB) will be slower than getStaticProps because the server must compute the result on every request, and the result cannot be cached by a CDN without extra configuration.\nIf you do not need to pre-render the data, you can also use the following strategy (called Client-side Rendering):\nStatically generate (pre-render) parts of the page that do not require external data. When the page loads, fetch external data from the client using JavaScript and populate the remaining parts. Pre-rendering and Data Fetching 1\n2\n3\n4\n5\n6\n7\n8\n9\nFetching Data at Request Time If you need to fetch data at request time instead of at build time, you can try Server-side Rendering:\nTo use Server-side Rendering, you need to export getServerSideProps instead of getStaticProps from your page.\nUsing getServerSideProps Here’s the starter code for getServerSideProps. It’s not necessary for our blog example, so we won’t be implementing it.\nexport async function getServerSideProps(context) { return { props: { // props for your component } } } Because getServerSideProps is called at request time, its parameter (context) contains request specific parameters.\nYou should use getServerSideProps only if you need to pre-render a page whose data must be fetched at request time. Time to first byte (TTFB) will be slower than getStaticProps because the server must compute the result on every request, and the result cannot be cached by a CDN without extra configuration.\nClient-side Rendering If you do not need to pre-render the data, you can also use the following strategy (called Client-side Rendering):\nStatically generate (pre-render) parts of the page that do not require external data. When the page loads, fetch external data from the client using JavaScript and populate the remaining parts. This approach works well for user dashboard pages, for example. Because a dashboard is a private, user-specific page, SEO is not relevant, and the page doesn’t need to be pre-rendered. The data is frequently updated, which requires request-time data fetching.\nSWR The team behind Next.js has created a React hook for data fetching called SWR. We highly recommend it if you’re fetching data on the client side. It handles caching, revalidation, focus tracking, refetching on interval, and more. We won’t cover the details here, but here’s an example usage:\nimport useSWR from \u0026#39;swr\u0026#39; function Profile() { const { data, error } = useSWR(\u0026#39;/api/user\u0026#39;, fetch) if (error) return \u0026lt;div\u0026gt;failed to load\u0026lt;/div\u0026gt; if (!data) return \u0026lt;div\u0026gt;loading...\u0026lt;/div\u0026gt; return \u0026lt;div\u0026gt;hello {data.name}!\u0026lt;/div\u0026gt; } Catch-all Routes Dynamic routes can be extended to catch all paths by adding three dots (...) inside the brackets.\nAPI routers You should not fetch an API Route from getStaticProps or getStaticPaths. Instead, write your server-side code directly in getStaticProps or getStaticPaths (or call a helper function).\nHere’s why: getStaticProps and getStaticPaths runs only on the server-side. It will never be run on the client-side. It won’t even be included in the JS bundle for the browser. That means you can write code such as direct database queries without them being sent to browsers.\nA good use case for API Routes is handling form input. For example, you can create a form on your page and have it send a POST request to your API Route. You can then write code to directly save it to your database. The API Route code will not be part of your client bundle, so you can safely write server-side code.\nDeploy to Vercel The easiest way to deploy Next.js to production is to use the Vercel platform developed by the creators of Next.js.\n","date":"2021-05-26T00:00:00Z","permalink":"https://dyhes.github.io/p/nextknowledge/","title":"【Next】Knowledge"},{"content":"Introduction Node.js is an open-source and cross-platform JavaScript runtime environment.\nNode.js runs the V8 JavaScript engine, the core of Google Chrome, outside of the browser.\nA Node.js app runs in a single process, without creating a new thread for every request. Node.js provides a set of asynchronous I/O primitives in its standard library that prevent JavaScript code from blocking and generally, libraries in Node.js are written using non-blocking paradigms, making blocking behavior the exception rather than the norm.\nWhen Node.js performs an I/O operation, like reading from the network, accessing a database or the filesystem, instead of blocking the thread and wasting CPU cycles waiting, Node.js will resume the operations when the response comes back.\nNode.js is a low-level platform. In order to make things easy and exciting for developers, thousands of libraries were built upon Node.js by the community.\nMany of those established over time as popular options. Here is a non-comprehensive list of the ones worth learning:\nExpress: It provides one of the most simple yet powerful ways to create a web server.\nkoa: It is built by the same team behind Express, aims to be even simpler and smaller,\nEgg.js: A framework to build better enterprise frameworks and apps with Node.js \u0026amp; Koa.\nGatsby: A React-based, GraphQL-powered, static site generator with a very rich ecosystem of plugins and starters.\nNext.js: React framework that gives you the best developer experience with all the features you need for production: hybrid static \u0026amp; server rendering, TypeScript support, smart bundling, route pre-fetching, and more.\nDifference to Browsers You can use Babel to transform your code to be ES5-compatible before shipping it to the browser, but in Node.js, you won\u0026rsquo;t need that.\nAnother difference is that Node.js uses the CommonJS module system, while in the browser we are starting to see the ES Modules standard being implemented. In practice, this means that for the time being you use require() in Node.js and import in the browser.\nV8 javascript engine V8 is the name of the JavaScript engine that powers Google Chrome. V8 was chosen to be the engine that powered Node.js back in 2009. The Node.js ecosystem is huge and thanks to V8 which also powers desktop apps, with projects like Electron.\nV8 is written in C++, and it\u0026rsquo;s continuously improved. It is portable and runs on Mac, Windows, Linux and several other systems.\nJavaScript is generally considered an interpreted language, but modern JavaScript engines no longer just interpret JavaScript, they compile it. This has been happening since 2009, when the SpiderMonkey JavaScript compiler was added to Firefox 3.5, and everyone followed this idea. JavaScript is internally compiled by V8 with just-in-time (JIT) compilation to speed up the execution.\nRun and Exit RUN\nThe usual way to run a Node.js program is to run the node globally available command (once you install Node.js) and pass the name of the file you want to execute.\nto restart the application automatically, nodemon module is used.\nRun the application using nodemon followed by application file name.\nnodemon app.js EXIT\nWhen running a program in the console you can close it with ctrl-C\nThe process core module providesprocess.exit() that allows you to programmatically exit from a Node.js program. When Node.js runs this line, the process is immediately forced to terminate. This means that any callback that\u0026rsquo;s pending, any network request still being sent, any filesystem access, or processes writing to stdout or stderr - all is going to be ungracefully terminated right away.\nNote: process does not require a \u0026ldquo;require\u0026rdquo;, it\u0026rsquo;s automatically available.\nprocess.exit(1) By default the exit code is 0, which means success. Different exit codes have different meaning, which you might want to use in your own system to have the program communicate to other programs.\nYou can also set the process.exitCode property\nprocess.exitCode = 1 Many times with Node.js we start servers, like this HTTP server: const express = require(\u0026#39;express\u0026#39;) const app = express() app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.send(\u0026#39;Hi!\u0026#39;) }) app.listen(3000, () =\u0026gt; console.log(\u0026#39;Server ready\u0026#39;)) Express is a framework that uses the http module under the hood, app.listen() returns an instance of http. You would use https.createServer if you needed to serve your app using HTTPS, as app.listen only uses the http module.\nThis program is never going to end. If you call process.exit(), any currently pending or running request is going to be aborted. This is not nice.\nIn this case you need to send the command a SIGTERM signal, and handle that with the process signal handler:\nconst express = require(\u0026#39;express\u0026#39;) const app = express() app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.send(\u0026#39;Hi!\u0026#39;) }) const server = app.listen(3000, () =\u0026gt; console.log(\u0026#39;Server ready\u0026#39;)) process.on(\u0026#39;SIGTERM\u0026#39;, () =\u0026gt; { server.close(() =\u0026gt; { console.log(\u0026#39;Process terminated\u0026#39;) }) }) SIGKILL is the signal that tells a process to immediately terminate, and would ideally act like process.exit().\nSIGTERM is the signal that tells a process to gracefully terminate. It is the signal that\u0026rsquo;s sent from process managers like upstart or supervisord and many others.\nYou can send this signal from inside the program, in another function\nprocess.kill(process.pid, \u0026#39;SIGTERM\u0026#39;) Or from another Node.js running program, or any other app running in your system that knows the PID of the process you want to terminate.\nEnvironment variables The process core module of Node.js provides the env property which hosts all the environment variables that were set at the moment the process was started.\nThe below code runs app.js and set USER_ID and USER_KEY.\nUSER_ID=239482 USER_KEY=foobar node app.js and you can access them like:\nprocess.env.USER_ID // \u0026#34;239482\u0026#34; process.env.USER_KEY // \u0026#34;foobar\u0026#34; If you have multiple environment variables in your node project, you can also create an .env file in the root directory of your project, and then use the dotenv package to load them during runtime.\nrequire(\u0026#39;dotenv\u0026#39;).config(); You can also run your js file with node -r dotenv/config index.js command if you don\u0026rsquo;t want to import the package in your code.\nREPL If we run the node command without any script to execute or without any arguments, we start a REPL session:\nnode Note: REPL stands for Read Evaluate Print Loop, and it is a programming language environment (basically a console window) that takes single expression as user input and returns the result back to the console after execution. The REPL session provides a convenient way to quickly test simple JavaScript code.\nJavaScript objects\nTry entering the name of a JavaScript class, like Number, add a dot and press tab. The REPL will print all the properties and methods you can access on that class\nglobal objects\nYou can inspect the globals you have access to by typing global. and pressing tab.\nSpecial commands\nThe REPL has some special commands, all starting with a dot .:\n.help: shows the dot commands help .editor: enables editor mode, to write multiline JavaScript code with ease. Once you are in this mode, enter ctrl-D to run the code you wrote. .break: when inputting a multi-line expression, entering the .break command will abort further input. Same as pressing ctrl-C. .clear: resets the REPL context to an empty object and clears any multi-line expression currently being input. .load: loads a JavaScript file, relative to the current working directory .save: saves all you entered in the REPL session to a file (specify the filename) .exit: exits the repl (same as pressing ctrl-C two times) Accept argument The way you retrieve accepted arguments is using the process object built into Node.js.\nIt exposes an argv property, which is an array that contains all the command line invocation arguments.\nThe first element is the full path of the node command.\nThe second element is the full path of the file being executed.\nAll the additional arguments are present from the third position going forward.\nYou can get only the additional arguments by creating a new array that excludes the first 2 params:\nconst args = process.argv.slice(2) In this case:\nnode app.js name=joe args[0] is name=joe, and you need to parse it. The best way to do so is by using the minimist library, which helps dealing with arguments:\nconst args = require(\u0026#39;minimist\u0026#39;)(process.argv.slice(2)) args[\u0026#39;name\u0026#39;] //joe Input Node.js since version 7 provides the readline module to perform exactly this: get input from a readable stream such as the process.stdin stream, which during the execution of a Node.js program is the terminal input, one line at a time.\nconst readline = require(\u0026#39;readline\u0026#39;).createInterface({ input: process.stdin, output: process.stdout }) readline.question(`What\u0026#39;s your name?`, name =\u0026gt; { console.log(`Hi ${name}!`) readline.close() }) Output Node.js provides a console module which provides tons of very useful ways to interact with the command line.\nIt is basically the same as the console object you find in the browser.\nconsole.log()\nThe most basic and most used method is console.log(), which prints the string you pass to it to the console.\nIf you pass an object, it will render it as a string.\nYou can also pass multiple variables to console.log()\nconsole.clear()\n``console.clear()` clears the console (the behavior might depend on the console used)\nconsole.count()\nWhat happens is that console.count() will count the number of times a string is printed, and print the count next to it.\nconsole.countReset()\nThe console.countReset() method resets counter used with console.count().\nconsole.trace()\nThere might be cases where it\u0026rsquo;s useful to print the call stack trace of a function, maybe to answer the question how did you reach that part of the code\nconst function2 = () =\u0026gt; console.trace() const function1 = () =\u0026gt; function2() function1() output:\nTrace at function2 (repl:1:33) at function1 (repl:1:25) at repl:1:1 at ContextifyScript.Script.runInThisContext (vm.js:44:33) at REPLServer.defaultEval (repl.js:239:29) at bound (domain.js:301:14) at REPLServer.runBound [as eval] (domain.js:314:12) at REPLServer.onLine (repl.js:440:10) at emitOne (events.js:120:20) at REPLServer.emit (events.js:210:7) console.time() and console.timeEnd()\nYou can easily calculate how much time a function takes to run, using time() and timeEnd()\nconst doSomething = () =\u0026gt; console.log(\u0026#39;test\u0026#39;) const measureDoingSomething = () =\u0026gt; { console.time(\u0026#39;doSomething()\u0026#39;) //do something, and measure the time it takes doSomething() console.timeEnd(\u0026#39;doSomething()\u0026#39;) } measureDoingSomething() console.error()\nAs we saw console.log is great for printing messages in the Console. This is what\u0026rsquo;s called the standard output, or stdout.\nconsole.error prints to the stderr stream. It will not appear in the console, but it will appear in the error log.\nColorful The simplest way to go about coloring the console output is by using a library. Chalk is such a library, and in addition to coloring it also helps with other styling facilities, like making text bold, italic or underlined.\nconst chalk = require(\u0026#39;chalk\u0026#39;) console.log(chalk.yellow(\u0026#39;hi!\u0026#39;)) Progress bar Progress is an awesome package to create a progress bar in the console. Install it using npm install progress\nThis snippet creates a 10-step progress bar, and every 100ms one step is completed. When the bar completes we clear the interval:\nconst ProgressBar = require(\u0026#39;progress\u0026#39;) const bar = new ProgressBar(\u0026#39;:bar\u0026#39;, { total: 10 }) const timer = setInterval(() =\u0026gt; { bar.tick() if (bar.complete) { clearInterval(timer) } }, 100) Export Node.js has a built-in module system. A Node.js file can import functionality exposed by other Node.js files.\n//1 const car = { brand: \u0026#39;Ford\u0026#39;, model: \u0026#39;Fiesta\u0026#39; } module.exports = car const car = require(\u0026#39;./car\u0026#39;) //2 const car = { brand: \u0026#39;Ford\u0026#39;, model: \u0026#39;Fiesta\u0026#39; } exports.car = car //or exports.car = { brand: \u0026#39;Ford\u0026#39;, model: \u0026#39;Fiesta\u0026#39; } const items = require(\u0026#39;./items\u0026#39;) const car = items.car const car = require(\u0026#39;./items\u0026#39;).car const { car } = require(\u0026#39;./items\u0026#39;) The difference between module.exports and exports:\nmodule.exports : exposes the object it points to. exports: exposes the properties of the object it points to. Production Node.js assumes it\u0026rsquo;s always running in a development environment. You can signal Node.js that you are running in production by setting the NODE_ENV=production environment variable or:\nNODE_ENV=production node app.js Npm Install If a project has a package.json file, by running\nnpm install it will install everything the project needs, in the node_modules folder, creating it if it\u0026rsquo;s not existing already.\nYou can also install a specific package by running\nnpm install \u0026lt;package-name\u0026gt; Furthermore, since npm 5, this command adds \u0026lt;package-name\u0026gt; to the package.json file dependencies. Before version 5, you needed to add the flag --save.\nOften you\u0026rsquo;ll see more flags added to this command:\n--save-dev installs and adds the entry to the package.json file devDependencies --no-save installs but does not add the entry to the package.json file dependencies --save-optional installs and adds the entry to the package.json file optionalDependencies --no-optional will prevent optional dependencies from being installed Shorthands of the flags can also be used:\n-S: \u0026ndash;save -D: \u0026ndash;save-dev -O: \u0026ndash;save-optional The difference between devDependencies and dependencies is that the former contains development tools, like a testing library, while the latter is bundled with the app in production.\nAs for the optionalDependencies the difference is that build failure of the dependency will not cause installation to fail. But it is your program\u0026rsquo;s responsibility to handle the lack of the dependency.\nA global installation is performed using the -g flag:\nnpm install -g lodash You can install an old version of an npm package using the @ syntax:\nnpm install \u0026lt;package\u0026gt;@\u0026lt;version\u0026gt; You might also be interested in listing all the previous versions of a package. You can do it with npm view \u0026lt;package\u0026gt; versions\nWhen you install a package using npm install \u0026lt;packagename\u0026gt;, the latest available version of the package is downloaded and put in the node_modules folder, and a corresponding entry is added to the package.json and package-lock.json files that are present in your current folder.\nUninstall npm uninstall \u0026lt;package-name\u0026gt; Using the -S flag, or --save, this operation will also remove the reference in the package.json file.\npackage.json will be automatically updated with devDependency and dependency once you uninstall npm package.\nIf the package is installed globally, you need to add the -g / --global flag:\nUpdate Updating is also made easy, by running\nnpm update npm will check all packages for a newer version that satisfies your versioning constraints.\nYou can specify a single package to update as well:\nnpm update \u0026lt;package-name\u0026gt; Since npm version 5.0.0, npm update will update the package.json with the updated version. Use npm update --no-save to not update package.json.\nTo discover new releases of the packages, you run npm outdated.\nSome of those updates are major releases. Running npm update won\u0026rsquo;t update the version of those. Major releases are never updated in this way because they (by definition) introduce breaking changes, and npm wants to save you trouble.\nTo update all packages to a new major version, install the npm-check-updates package globally:\nthen run :\nncu -u Script The package.json file supports a format for specifying command line tasks that can be run by using\nnpm run \u0026lt;task-name\u0026gt; { \u0026#34;scripts\u0026#34;: { \u0026#34;start-dev\u0026#34;: \u0026#34;node lib/server-development\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;node lib/server-production\u0026#34; } } The npm root -g command will tell you where that global installation location is on your machine.\nPackage.json The package.json file is kind of a manifest for your project. It can do a lot of things, completely unrelated. It\u0026rsquo;s a central repository of configuration for tools, for example. It\u0026rsquo;s also where npm and yarn store the names and versions for all the installed packages.\nThere are no fixed requirements of what should be in a package.json file, for an application. The only requirement is that it respects the JSON format, otherwise it cannot be read by programs that try to access its properties programmatically.\nmain\nSets the entry point for the package. When you import this package in an application, that\u0026rsquo;s where the application will search for the module exports.\nprivate\nif set to true prevents the app/package to be accidentally published on npm\nengines\nSets which versions of Node.js and other commands this package/app work on\nbrowserslist\nIs used to tell which browsers (and their versions) you want to support. It\u0026rsquo;s referenced by Babel, Autoprefixer, and other tools, to only add the polyfills and fallbacks needed to the browsers you target.\npackage-lock.json The goal of package-lock.json file is to keep track of the exact version of every package that is installed so that a product is 100% reproducible in the same way even if packages are updated by their maintainers.\nYou don\u0026rsquo;t commit to Git your node_modules folder, which is generally huge, and when you try to replicate the project on another machine by using the npm install command, if you specified the ~ syntax and a patch release of a package has been released, that one is going to be installed. Same for ^ and minor releases.\nThe package-lock.json sets your currently installed version of each package in stone, and npm will use those exact versions when running npm ci.\nThe dependencies versions will be updated in the package-lock.json file when you run npm update.\nSemantic Version The Semantic Versioning concept is simple: all versions have 3 digits: x.y.z.\nthe first digit is the major version the second digit is the minor version the third digit is the patch version The rules use those symbols:\n^: It will only do updates that do not change the leftmost non-zero number i.e there can be changes in minor version or patch version but not in major version. If you write ^13.1.0, when running npm update, it can update to 13.2.0, 13.3.0 even 13.3.1, 13.3.2 and so on, but not to 14.0.0 or above. ~: if you write ~0.13.0 when running npm update it can update to patch releases: 0.13.1 is ok, but 0.14.0 is not. \u0026gt;: you accept any version higher than the one you specify \u0026gt;=: you accept any version equal to or higher than the one you specify \u0026lt;=: you accept any version equal or lower to the one you specify \u0026lt;: you accept any version lower than the one you specify =: you accept that exact version -: you accept a range of versions. Example: 2.1.0 - 2.6.2 ||: you combine sets. Example: \u0026lt; 2.1 || \u0026gt; 2.6 There are other rules too:\nno symbol: you accept only that specific version you specify (1.2.1) latest: you want to use the latest version available Scope In your code you can only require local packages:\nrequire(\u0026#39;package-name\u0026#39;) In general, all packages should be installed locally. This makes sure you can have dozens of applications in your computer, all running a different version of each package if needed. A package should be installed globally when it provides an executable command that you run from the shell (CLI), and it\u0026rsquo;s reused across projects.\nGreat examples of popular global packages which you might know are\nnpm vue-cli grunt-cli mocha react-native-cli gatsby-cli forever nodemon NPX npx is a very powerful command that\u0026rsquo;s been available in npm starting version 5.2, released in July 2017.\nnpx lets you run code built with Node.js and published through the npm registry.\nRunning npx commandname automatically finds the correct reference of the command inside the node_modules folder of a project, without needing to know the exact path, and without requiring the package to be installed globally and in the user\u0026rsquo;s path.\nEvent On the backend side, Node.js offers us the option to build a similar system using the events module.\nThis module, in particular, offers the EventEmitter class, which we\u0026rsquo;ll use to handle our events.\nYou initialize that using\nconst EventEmitter = require(\u0026#39;events\u0026#39;) const eventEmitter = new EventEmitter() This object exposes, among many others, the on and emit methods.\nemit is used to trigger an event on is used to add a callback function that\u0026rsquo;s going to be executed when the event is triggered eventEmitter.on(\u0026#39;start\u0026#39;, () =\u0026gt; { console.log(\u0026#39;started\u0026#39;) }) when we run\neventEmitter.emit(\u0026#39;start\u0026#39;) the event handler function is triggered, and we get the console log.\neventEmitter.on(\u0026#39;start\u0026#39;, (start, end) =\u0026gt; { console.log(`started from ${start} to ${end}`) }) eventEmitter.emit(\u0026#39;start\u0026#39;, 1, 100) The EventEmitter object also exposes several other methods to interact with events, like\nonce(): add a one-time listener removeListener() / off(): remove an event listener from an event removeAllListeners(): remove all listeners for an event Event Loop The Node.js JavaScript code runs on a single thread. There is just one thing happening at a time.\nThis is a limitation that\u0026rsquo;s actually very helpful, as it simplifies a lot how you program without worrying about concurrency issues.\nYou mainly need to be concerned that your code will run on a single event loop, and write code with this thing in mind to avoid blocking it.\nAlmost all the I/O primitives in JavaScript are non-blocking. Network requests, filesystem operations, and so on. Being blocking is the exception, and this is why JavaScript is based so much on callbacks, and more recently on promises and async/await.\nCall Stack The event loop continuously checks the call stack to see if there\u0026rsquo;s any function that needs to run.\nconst bar = () =\u0026gt; console.log(\u0026#39;bar\u0026#39;) const baz = () =\u0026gt; console.log(\u0026#39;baz\u0026#39;) const foo = () =\u0026gt; { console.log(\u0026#39;foo\u0026#39;) setTimeout(bar, 0) baz() } foo() //output foo baz bar The Message Queue When setTimeout() is called, the Browser or Node.js starts the timer. Once the timer expires, in this case immediately as we put 0 as the timeout, the callback function is put in the Message Queue.\nThe Message Queue is also where user-initiated events like click or keyboard events, or fetch responses are queued before your code has the opportunity to react to them. Or also DOM events like onload.\nThe loop gives priority to the call stack, and it first processes everything it finds in the call stack, and once there\u0026rsquo;s nothing in there, it goes to pick up things in the message queue.\nWe don\u0026rsquo;t have to wait for functions like setTimeout, fetch or other things to do their own work, because they are provided by the browser, and they live on their own threads. For example, if you set the setTimeout timeout to 2 seconds, you don\u0026rsquo;t have to wait 2 seconds - the wait happens elsewhere.\nES6 Job Queue ECMAScript 2015 introduced the concept of the Job Queue, which is used by B (also introduced in ES6/ES2015). It\u0026rsquo;s a way to execute the result of an async function as soon as possible, rather than being put at the end of the call stack.\nPromises that resolve before the current function ends will be executed right after the current function.\nprocess.nextTick() As you try to understand the Node.js event loop, one important part of it is process.nextTick(). Every time the event loop takes a full trip, we call it a tick. When we pass a function to process.nextTick(), we instruct the engine to invoke this function at the end of the current operation, before the next event loop tick starts. Calling setTimeout(() =\u0026gt; {}, 0) will execute the function at the end of next tick, much later than when using nextTick() which prioritizes the call and executes it just before the beginning of the next tick. Use nextTick() when you want to make sure that in the next event loop iteration that code is already executed.\nsetImmediate() When you want to execute some piece of code asynchronously, but as soon as possible, one option is to use the setImmediate() function. Any function passed as the setImmediate() argument is a callback that\u0026rsquo;s executed in the next iteration of the event loop.\nThe differences between setImmediate() , setTimeout(() =\u0026gt; {}, 0) , and process.nextTick():\nA function passed to process.nextTick() is going to be executed on the current iteration of the event loop, after the current operation ends. This means it will always execute before setTimeout and setImmediate.\nA setTimeout() callback with a 0ms delay is very similar to setImmediate(). The execution order will depend on various factors, but they will be both run in the next iteration of the event loop. If you specify the timeout delay to 0, the callback function will be executed as soon as possible, but after the current function execution. This is called zero delay.\nAsynchronicity Computers are asynchronous by design. Asynchronous means that things can happen independently of the main program flow. In the current consumer computers, every program runs for a specific time slot and then it stops its execution to let another program continue their execution. This thing runs in a cycle so fast that it\u0026rsquo;s impossible to notice. We think our computers run many programs simultaneously, but this is an illusion (except on multiprocessor machines). JavaScript is synchronous by default and is single threaded. This means that code cannot create new threads and run in parallel. Lines of code are executed in series, one after another.\nPromise A promise is commonly defined as a proxy for a value that will eventually become available. Promises are one way to deal with asynchronous code, without getting stuck in callback hell. Promises have been part of the language for years (standardized and introduced in ES2015), and have recently become more integrated, with async and await in ES2017.\nAsync functions use promises behind the scenes, so understanding how promises work is fundamental to understanding how async and await work.\nlet done = true const isItDoneYet = new Promise((resolve, reject) =\u0026gt; { if (done) { const workDone = \u0026#39;Here is the thing I built\u0026#39; resolve(workDone) } else { const why = \u0026#39;Still working on something else\u0026#39; reject(why) } }) Async and Await Async functions are a combination of promises and generators, and basically, they are a higher level abstraction over promises. Let me repeat: async/await is built on promises. An async function returns a promise. When you want to call this function you prepend await, and the calling code will stop until the promise is resolved or rejected. One caveat: the client function must be defined as async.\nconst doSomethingAsync = () =\u0026gt; { return new Promise(resolve =\u0026gt; { setTimeout(() =\u0026gt; resolve(\u0026#39;I did something\u0026#39;), 3000) }) } const doSomething = async () =\u0026gt; { console.log(await doSomethingAsync()) } console.log(\u0026#39;Before\u0026#39;) doSomething() console.log(\u0026#39;After\u0026#39;) File and Folder File Descriptor A file descriptor is a reference to an open file, a number (fd) returned by opening the file using the open() method offered by the fs module. This number (fd) uniquely identifies an open file in operating system\nconst fs = require(\u0026#39;fs\u0026#39;) fs.open(\u0026#39;/Users/joe/test.txt\u0026#39;, \u0026#39;r\u0026#39;, (err, fd) =\u0026gt; { //fd is our file descriptor }) Other flags you\u0026rsquo;ll commonly use are:\nr+ open the file for reading and writing, if file doesn\u0026rsquo;t exist it won\u0026rsquo;t be created. w+ open the file for reading and writing, positioning the stream at the beginning of the file. The file is created if not existing. a open the file for writing, positioning the stream at the end of the file. The file is created if not existing. a+ open the file for reading and writing, positioning the stream at the end of the file. The file is created if not existing. You can also open the file by using the fs.openSync method, which returns the file descriptor, instead of providing it in a callback:\nconst fs = require(\u0026#39;fs\u0026#39;) try { const fd = fs.openSync(\u0026#39;/Users/joe/test.txt\u0026#39;, \u0026#39;r\u0026#39;) } catch (err) { console.error(err) } Stats The file information is included in the stats variable. We can obtain information about the file by:\nif the file is a directory or a file, using stats.isFile() and stats.isDirectory() if the file is a symbolic link using stats.isSymbolicLink() the file size in bytes using stats.size. const fs = require(\u0026#39;fs\u0026#39;) fs.stat(\u0026#39;/Users/joe/test.txt\u0026#39;, (err, stats) =\u0026gt; { if (err) { console.error(err) return } //we have access to the file stats in `stats` }) //sync const fs = require(\u0026#39;fs\u0026#39;) try { const stats = fs.statSync(\u0026#39;/Users/joe/test.txt\u0026#39;) } catch (err) { console.error(err) } Path Given a path, you can extract information out of it using those methods:\ndirname: get the parent folder of a file basename: get the filename part extname: get the file extension You can get the absolute path calculation of a relative path using path.resolve(), path.normalize() is another useful function that will try and calculate the actual path, when it contains relative specifiers like . or .., or double slashes. Neither resolve nor normalize will check if the path exists.\nconst path = require(\u0026#39;path\u0026#39;) const notes = \u0026#39;/users/joe/notes.txt\u0026#39; path.dirname(notes) // /users/joe path.basename(notes) // notes.txt path.extname(notes) // .txt const name = \u0026#39;joe\u0026#39; path.join(\u0026#39;/\u0026#39;, \u0026#39;users\u0026#39;, name, \u0026#39;notes.txt\u0026#39;) //\u0026#39;/users/joe/notes.txt\u0026#39; path.resolve(\u0026#39;joe.txt\u0026#39;) //\u0026#39;/Users/joe/joe.txt\u0026#39; if run from my home folder path.normalize(\u0026#39;/users/joe/..//test.txt\u0026#39;) //\u0026#39;/users/test.txt\u0026#39; Read The simplest way to read a file in Node.js is to use the fs.readFile() method, passing it the file path, encoding and a callback function that will be called with the file data (and the error).\nAlternatively, you can use the synchronous version fs.readFileSync().\nconst fs = require(\u0026#39;fs\u0026#39;) fs.readFile(\u0026#39;/Users/joe/test.txt\u0026#39;, \u0026#39;utf8\u0026#39; , (err, data) =\u0026gt; { if (err) { console.error(err) return } console.log(data) }) //sync const fs = require(\u0026#39;fs\u0026#39;) try { const data = fs.readFileSync(\u0026#39;/Users/joe/test.txt\u0026#39;, \u0026#39;utf8\u0026#39;) console.log(data) } catch (err) { console.error(err) } Both fs.readFile() and fs.readFileSync() read the full content of the file in memory before returning the data.\nThis means that big files are going to have a major impact on your memory consumption and speed of execution of the program.\nIn this case, a better option is to read the file content using streams.\nWrite The easiest way to write to files in Node.js is to use the fs.writeFile() API.\nAlternatively, you can use the synchronous version fs.writeFileSync()。\nconst fs = require(\u0026#39;fs\u0026#39;) const content = \u0026#39;Some content!\u0026#39; fs.writeFile(\u0026#39;/Users/joe/test.txt\u0026#39;, content, err =\u0026gt; { if (err) { console.error(err) return } //file written successfully }) //sync const fs = require(\u0026#39;fs\u0026#39;) const content = \u0026#39;Some content!\u0026#39; try { fs.writeFileSync(\u0026#39;/Users/joe/test.txt\u0026#39;, content) //file written successfully } catch (err) { console.error(err) } By default, this API will replace the contents of the file if it does already exist.\nYou can modify the default by specifying a flag:\nfs.writeFile(\u0026#39;/Users/joe/test.txt\u0026#39;, content, { flag: \u0026#39;a+\u0026#39; }, err =\u0026gt; {}) The flags you\u0026rsquo;ll likely use are\nr+ open the file for reading and writing w+ open the file for reading and writing, positioning the stream at the beginning of the file. The file is created if it does not exist a open the file for writing, positioning the stream at the end of the file. The file is created if it does not exist a+ open the file for reading and writing, positioning the stream at the end of the file. The file is created if it does not exist Append The easiest way to append content to files in Node.js is to use the fs.appendFile() API.\nconst content = \u0026#39;Some content!\u0026#39; fs.appendFile(\u0026#39;file.log\u0026#39;, content, err =\u0026gt; { if (err) { console.error(err) return } //done! }) Folders Use fs.access() to check if the folder exists and Node.js can access it with its permissions.\nUse fs.mkdir() or fs.mkdirSync() to create a new folder.\nconst fs = require(\u0026#39;fs\u0026#39;) const folderName = \u0026#39;/Users/joe/test\u0026#39; try { if (!fs.existsSync(folderName)) { fs.mkdirSync(folderName) } } catch (err) { console.error(err) } Use fs.readdir() or fs.readdirSync() to read the contents of a directory.\nUse fs.rename() or fs.renameSync() to rename folder.\nUse fs.rmdir() or fs.rmdirSync() to remove a folder.\nRemoving a folder that has content can be more complicated than you need. You can pass the option { recursive: true } to recursively remove the contents\nNOTE: In Node v16.x the option recursive is deprecated for fs.rmdir of callback API, instead use fs.rm to delete folders that have content in them\nconst fs = require(\u0026#39;fs\u0026#39;) fs.rm(dir, { recursive: true, force: true }, (err) =\u0026gt; { if (err) { throw err; } console.log(`${dir} is deleted!`) }); Buffer and Stream Buffer A buffer is an area of memory. It represents a fixed-size chunk of memory (can\u0026rsquo;t be resized) allocated outside of the V8 JavaScript engine. You can think of a buffer like an array of integers, which each represent a byte of data.\nA buffer is created using the Buffer.from(), Buffer.alloc(), and Buffer.allocUnsafe() methods.\nBuffer.from(array) Buffer.from(arrayBuffer[, byteOffset[, length\\]]) Buffer.from(buffer) Buffer.from(string[, encoding\\]) const buf = Buffer.from(\u0026#39;Hey!\u0026#39;) const buf = Buffer.alloc(1024) //or const buf = Buffer.allocUnsafe(1024) Stream Streams are a way to handle reading/writing files, network communications, or any kind of end-to-end information exchange in an efficient way. The Node.js stream module provides the foundation upon which all streaming APIs are built. All streams are instances of EventEmitter.\nStreams basically provide two major advantages over using other data handling methods:\nMemory efficiency: you don\u0026rsquo;t need to load large amounts of data in memory before you are able to process it Time efficiency: it takes way less time to start processing data, since you can start processing as soon as you have it, rather than waiting till the whole data payload is available const http = require(\u0026#39;http\u0026#39;) const fs = require(\u0026#39;fs\u0026#39;) const server = http.createServer((req, res) =\u0026gt; { const stream = fs.createReadStream(__dirname + \u0026#39;/data.txt\u0026#39;) stream.pipe(res) }) server.listen(3000) the pipe() method is called on the file stream. It takes the source, and pipes it into a destination.\nThe return value of the pipe() method is the destination stream, which is a very convenient thing that lets us chain multiple pipe() calls, like this:\nsrc.pipe(dest1).pipe(dest2) Due to their advantages, many Node.js core modules provide native stream handling capabilities, most notably:\nprocess.stdin returns a stream connected to stdin process.stdout returns a stream connected to stdout process.stderr returns a stream connected to stderr fs.createReadStream() creates a readable stream to a file fs.createWriteStream() creates a writable stream to a file net.connect() initiates a stream-based connection http.request() returns an instance of the http.ClientRequest class, which is a writable stream zlib.createGzip() compress data using gzip (a compression algorithm) into a stream zlib.createGunzip() decompress a gzip stream. zlib.createDeflate() compress data using deflate (a compression algorithm) into a stream zlib.createInflate() decompress a deflate stream types There are four classes of streams:\nReadable: a stream you can pipe from, but not pipe into (you can receive data, but not send data to it). When you push data into a readable stream, it is buffered, until a consumer starts to read the data. Writable: a stream you can pipe into, but not pipe from (you can send data, but not receive from it) Duplex: a stream you can both pipe into and pipe from, basically a combination of a Readable and Writable stream Transform: a Transform stream is similar to a Duplex, but the output is a transform of its input We get the Readable stream from the stream module, and we initialize it and implement the readable._read() method.\nconst Stream = require(\u0026#39;stream\u0026#39;) const readableStream = new Stream.Readable() readableStream._read = () =\u0026gt; {} //or const readableStream = new Stream.Readable({ read() {} }) readableStream.push(\u0026#39;hi!\u0026#39;) readableStream.push(\u0026#39;ho!\u0026#39;) const writableStream = new Stream.Writable() writableStream._write = (chunk, encoding, next) =\u0026gt; { console.log(chunk.toString()) next() } You can also consume a readable stream directly, using the readable event:\nJScopy readableStream.on(\u0026#39;readable\u0026#39;, () =\u0026gt; { console.log(readableStream.read()) }) Modules fs module fs.access(): check if the file exists and Node.js can access it with its permissions fs.appendFile(): append data to a file. If the file does not exist, it\u0026rsquo;s created fs.chmod(): change the permissions of a file specified by the filename passed. Related: fs.lchmod(), fs.fchmod() fs.chown(): change the owner and group of a file specified by the filename passed. Related: fs.fchown(), fs.lchown() fs.close(): close a file descriptor fs.copyFile(): copies a file fs.createReadStream(): create a readable file stream fs.createWriteStream(): create a writable file stream fs.link(): create a new hard link to a file fs.mkdir(): create a new folder fs.mkdtemp(): create a temporary directory fs.open(): set the file mode fs.readdir(): read the contents of a directory fs.readFile(): read the content of a file. Related: fs.read() fs.readlink(): read the value of a symbolic link fs.realpath(): resolve relative file path pointers (., ..) to the full path fs.rename(): rename a file or folder fs.rmdir(): remove a folder fs.stat(): returns the status of the file identified by the filename passed. Related: fs.fstat(), fs.lstat() fs.symlink(): create a new symbolic link to a file fs.truncate(): truncate to the specified length the file identified by the filename passed. Related: fs.ftruncate() fs.unlink(): remove a file or a symbolic link fs.unwatchFile(): stop watching for changes on a file fs.utimes(): change the timestamp of the file identified by the filename passed. Related: fs.futimes() fs.watchFile(): start watching for changes on a file. Related: fs.watch() fs.writeFile(): write data to a file. Related: fs.write() One peculiar thing about the fs module is that all the methods are asynchronous by default, but they can also work synchronously by appending Sync.\nos module This module provides many functions that you can use to retrieve information from the underlying operating system and the computer the program runs on, and interact with it.\nconst os = require(\u0026#39;os\u0026#39;) There are a few useful properties that tell us some key things related to handling files:\nos.EOL gives the line delimiter sequence. It\u0026rsquo;s \\n on Linux and macOS, and \\r\\n on Windows.\nos.constants.signals tells us all the constants related to handling process signals, like SIGHUP, SIGKILL and so on.\nos.constants.errno sets the constants for error reporting, like EADDRINUSE, EOVERFLOW and more.\n","date":"2021-05-16T00:00:00Z","permalink":"https://dyhes.github.io/p/nodebasics/","title":"【Node】Basics"},{"content":"What is Internet The internet is the world’s most popular computer network. It began as an academic research project in 1969, and became a global commercial network in the 1990s.\nThe internet began as ARPANET, an academic research network that was funded by the military’s Advanced Research Projects Agency (ARPA, now DARPA) and that began operations in 1969.\nIn 1973, software engineers Vint Cerf and Bob Kahn began work on the next generation of networking standards for the ARPANET. These standards, known as TCP/IP, became the foundation of the modern internet.\nThe shared technical standards that make the internet work are managed by an organization called the Internet Engineering Task Force.\nThe Internet Corporation for Assigned Names and Numbers(ICANN) is in charge of distributing domain names and IP addresses.\nThe World Wide Web was created by Timothy Berners-Lee in 1991. It offered a more powerful and user-friendly interface than other internet applications. The web supported hyperlinks, allowing users to browse from one document to another with a single click. Over time, the web became increasingly sophisticated, supporting images, audio, video, and interactive content.\nIn 1994, Berners-Lee created the World Wide Web Consortium(W3C) to be the web’s official standards organization. He is still the W3C’s director and continues to oversee the development of web standards.\nHow we access Internet Most people access internet content using a web browser.\nthe web is just one of many internet applications. Other popular Internet applications include email and BitTorrent.\nBasic parts of the Internet The internet has three basic parts:\nThe last mile is the part of the internet that connects homes and small businesses to the internet. Data centers are rooms full of servers that store user data and host online apps and content. The backbone consists of long-distance networks — mostly on fiber optic cables —that carry data between data centers and consumers. Wireless Internet There are two basic types of wireless internet access:\nwifi Wifi networks use unlicensed spectrum: electromagnetic frequencies that are available for anyone to use without charge. To prevent neighbors’ networks from interfering with each other, there are strict limits on the power (and therefore the range) of wifi networks.\ncellular. Cellular networks are more centralized. They work by breaking up the service territory into cells. Each cell has a tower at its center providing services to devices there.\nIP address Internet Protocol addresses are numbers that computers use to identify each other on the internet.\nPacket A packet is the basic unit of information transmitted over the internet. Splitting information up into small, digestible pieces allows the network’s capacity to be used more efficiently.\nA packet consists of two parts:\nheader The header contains information that helps the packet get to its destination, including the length of the packet, its source and destination, and a checksum value that helps the recipient detect if a packet was damaged in transit.\nbody ( actual data ) Discard packet If internet routers experience congestion or other technical problems, they are allowed to deal with it by simply discarding packets.\nIt’s the sending computer’s responsibility to detect that a packet didn’t reach its destination and send another copy.\nOSI The open systems interconnection (OSI) model is a conceptual model which enables diverse communication systems to communicate using standard protocols.\nThe OSI Model can be seen as a universal language for computer networking.\nAlthough the modern Internet doesn’t strictly follow the OSI Model (it more closely follows the simpler Internet protocol suite), the OSI Model is still very useful for troubleshooting network problems.\n7. The application layer This is the only layer that directly interacts with data from the user. Software applications like web browsers and email clients rely on the application layer to initiate communications. But it should be made clear that client software applications are not part of the application layer. Application layer protocols include HTTP as well as SMTP (Simple Mail Transfer Protocol is one of the protocols that enables email communications).\n6. The presentation layer This layer is primarily responsible for preparing data so that it can be used by the application layer; in other words, layer 6 makes the data presentable for applications to consume.\nThe presentation layer is responsible for translation, encryption, and compression of data.\n5. The session layer This is the layer responsible for opening and closing communication between the two devices. The time between when the communication is opened and closed is known as the session. The session layer ensures that the session stays open long enough to transfer all the data being exchanged, and then promptly closes the session in order to avoid wasting resources.\nThe session layer also synchronizes data transfer with checkpoints. For example, if a 100 megabyte file is being transferred, the session layer could set a checkpoint every 5 megabytes. In the case of a disconnect or a crash after 52 megabytes have been transferred, the session could be resumed from the last checkpoint, meaning only 50 more megabytes of data need to be transferred. Without the checkpoints, the entire transfer would have to begin again from scratch.\n4. The transport layer Layer 4 is responsible for end-to-end communication between the two devices. This includes taking data from the session layer and breaking it up into chunks called segments before sending it to layer 3. The transport layer on the receiving device is responsible for reassembling the segments into data the session layer can consume.\nThe transport layer is also responsible for flow control and error control. Flow control determines an optimal speed of transmission to ensure that a sender with a fast connection doesn’t overwhelm a receiver with a slow connection. The transport layer performs error control on the receiving end by ensuring that the data received is complete, and requesting a retransmission if it isn’t.\n3. The network layer The network layer breaks up segments from the transport layer into smaller units, called packets, on the sender’s device, and reassembling these packets on the receiving device. The network layer also finds the best physical path for the data to reach its destination; this is known as routing.\n2. The data link layer The data link layer is very similar to the network layer, except the data link layer facilitates data transfer between two devices on the SAME network. The data link layer takes packets from the network layer and breaks them into smaller pieces called frames. Like the network layer, the data link layer is also responsible for flow control and error control in intra-network communication (The transport layer only does flow control and error control for inter-network communications).\n1. The physical layer This layer includes the physical equipment involved in the data transfer, such as the cables and switches. This is also the layer where the data gets converted into a bit stream, which is a string of 1s and 0s. The physical layer of both devices must also agree on a signal convention so that the 1s can be distinguished from the 0s on both devices.\n","date":"2021-05-15T00:00:00Z","permalink":"https://dyhes.github.io/p/internetbasics/","title":"【Internet】Basics"},{"content":"HTTP HTTP stands for Hyper Text Transfer Protocol\nHTTP is generally designed to be simple and human readable, even with the added complexity introduced in HTTP/2 by encapsulating HTTP messages into frames.\nRequest A typical HTTP request contains:\nHTTP version type URL HTTP method HTTP request header Optional HTTP body. The body of an HTTP request contains any information being submitted to the web server, such as a username and password, or any other data entered into a form.\nStatus HTTP status codes are 3-digit codes most often used to indicate whether an HTTP request has been successfully completed. Status codes are broken into the following 5 blocks:\n1xx Informational 2xx Success 3xx Redirection 4xx Client Error 5xx Server Error Keep in mind that HTTP is a “stateless” protocol, which means that each command runs independent of any other command. In the original spec, HTTP requests each created and closed a TCP connection. In newer versions of the HTTP protocol (HTTP 1.1 and above), persistent connection allows for multiple HTTP requests to pass over a persistent TCP connection, improving resource consumption.\nwhile the core of HTTP itself is stateless, HTTP cookies allow the use of stateful sessions. Using header extensibility, HTTP Cookies are added to the workflow, allowing session creation on each HTTP request to share the same context, or the same state.\nProxy HTTP is a client-server protocol: requests are sent by one entity, the user-agent (or a proxy on behalf of it).Each individual request is sent to a server, which handles it and provides an answer called the response. A server is not necessarily a single machine, but several server software instances can be hosted on the same machine. With HTTP/1.1 and the Host header, they may even share the same IP address.\nBetween the client and the server there are numerous entities, collectively called proxies, which perform different operations and act as gateways or caches. Proxies are those numerous computers and machines which operate at the application layers and relay the HTTP messages between the Web browser and the server.\nIn reality, there are more computers between a browser and the server handling the request: there are routers, modems, and more.\nThese can be transparent, forwarding on the requests they receive without altering them in any way, or non-transparent, in which case they will change the request in some way before passing it along to the server.\nProxies may perform numerous functions:\ncaching (the cache can be public or private, like the browser cache) filtering (like an antivirus scan or parental controls) load balancing (to allow multiple servers to serve different requests) authentication (to control access to different resources) logging (allowing the storage of historical information) Procedure When a client wants to communicate with a server, either the final server or an intermediate proxy, it performs the following steps:\nOpen a TCP connection: The TCP connection is used to send a request, or several, and receive an answer. The client may open a new connection, reuse an existing connection, or open several TCP connections to the servers. Send an HTTP message: HTTP messages (before HTTP/2) are human-readable. With HTTP/2, these simple messages are encapsulated in frames, making them impossible to read directly, but the principle remains the same. Read the response sent by the server Close or reuse the connection for further requests. If HTTP pipelining is activated, several requests can be sent without waiting for the first response to be fully received. HTTP pipelining has proven difficult to implement in existing networks, where old pieces of software coexist with modern versions. HTTP pipelining has been superseded in HTTP/2 with more robust multiplexing requests within a frame.\nHTTP messages, as defined in HTTP/1.1 and earlier, are human-readable. In HTTP/2, these messages are embedded into a binary structure, a frame, allowing optimizations like compression of headers and multiplexing.\nHTTP/0.9 (The One Liner) -1991 The first documented version of HTTP was HTTP/0.9\nThe server would get the request, reply with the HTML in response and as soon as the content has been transferred, the connection will be closed.\nThere were\nNo headers GET was the only allowed method Response had to be HTML Sample\nGET /index.html (response body) (connection closed)\nHTTP/1.0 - 1996 could deal with other response formats. It added more methods (i.e. POST and HEAD), HTTP headers got added to both the request and responses status codes were added to identify the response character set support was introduced multi-part types authorization caching content encoding and more Sample\nGET / HTTP/1.0 Host: kamranahmed.info User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) Accept: */* HTTP/1.0 200 OK Content-Type: text/plain Content-Length: 137582 Expires: Thu, 05 Dec 1997 16:00:00 GMT Last-Modified: Wed, 5 August 1996 15:55:28 GMT Server: Apache 0.84 (response body) (connection closed)\ndrawback One of the major drawbacks of HTTP/1.0 were you couldn’t have multiple requests per connection. That is, whenever a client will need something from the server, it will have to open a new TCP connection and after that single request has been fulfilled, connection will be closed. And for any next requirement, it will have to be on a new connection.\nlet’s assume that you visit a webpage having 10 images, 5 stylesheets and 5 javascript files, totalling to 20 items that needs to fetched when request to that webpage is made. Since the server closes the connection as soon as the request has been fulfilled, there will be a series of 20 separate connections where each of the items will be served one by one on their separate connections.\nApart from being connectionless, HTTP also is a stateless protocol. server doesn’t maintain the information about the client and so each of the requests has to have the information necessary for the server to fulfill the request on it’s own without any association with any old requests. So it also has to send some redundant data on the wire causing increased bandwidth usage.\nHTTP/1.1 - 1999 New HTTP methods were added, which introduced PUT, PATCH, OPTIONS, DELETE Hostname Identification In HTTP/1.0 Host header wasn’t required but HTTP/1.1 made it required. Persistent Connections HTTP/1.1 introduced the persistent connections i.e. connections weren’t closed by default and were kept open which allowed multiple sequential requests. To close the connections, the header Connection: close had to be available on the request. Clients usually send this header in the last request to safely close the connection. Pipelining It also introduced the support for pipelining, where the client could send multiple requests to the server without waiting for the response from server on the same connection and server had to send the response in the same sequence in which requests were received. To solve this problem that the client don\u0026rsquo;t know where is the point where first response download completes and the content for next response starts, there must be Content-Length header present which clients can use to identify where the response ends and it can start waiting for the next response. But there was still an issue with this approach. And that is, what if the data is dynamic and server cannot find the content length before hand? Well in that case, you really can’t benefit from persistent connections, could you?! In order to solve this HTTP/1.1 introduced chunked encoding. In such cases server may omit content-Length in favor of chunked encoding. However, if none of them are available, then the connection must be closed at the end of request.\nChunked Transfers In case of dynamic content, when the server cannot really find out the Content-Length when the transmission starts, it may start sending the content in pieces (chunk by chunk) and add the Content-Length for each chunk when it is sent. And when all of the chunks are sent. whole transmission has completed, it sends an empty chunk the one with Content-Length set to zero in order to identify the client that transmission has completed. In order to notify the client about the chunked transfer, server includes the header Transfer-Encoding: chunked Unlike HTTP/1.0 which had Basic authentication only, HTTP/1.1 included digest and proxy authentication Caching Byte Ranges Character sets Language negotiation Client cookies Enhanced compression support New status codes ..and more SPDY - 2009 Google went ahead and started experimenting with alternative protocols to make the web faster and improving web security while reducing the latency of web pages. In 2009, they announced SPDY.\nIn 2015, at Google, they didn’t want to have two competing standards and so they decided to merge it into HTTP while giving birth to HTTP/2 and deprecating SPDY.\nHTTP/2 - 2015 HTTP/2 was designed for low latency transport of content. The key features or differences from the old version of HTTP/1.1 include\nBinary instead of Textual Multiplexing - Multiple asynchronous HTTP requests over a single connection Header compression using HPACK Server Push - Multiple responses for single request Request Prioritization Security Frames and Streams HTTP messages are now composed of one or more frames. There is a HEADERS frame for the meta data and DATA frame for the payload and there exist several other types of frames (HEADERS, DATA, RST_STREAM, SETTINGS, PRIORITY etc)\nEvery HTTP/2 request and response is given a unique stream ID and it is divided into frames.\nFrames are nothing but binary pieces of data. A collection of frames is called a Stream.\nEach frame has a stream id that identifies the stream to which it belongs and each frame has a common header.\nAlso, apart from stream ID being unique, it is worth mentioning that, any request initiated by client uses odd numbers and the response from server has even numbers stream IDs.\nclient can use RST_STREAM and stop receiving a specific stream while the connection will still be open and the other streams will still be in play.\nSSL SSL, short for Secure Sockets Layer, is a family of encryption technologies that allows web users to protect the privacy of information they transmit over the internet introduced by Netscape in 1994.\nThat lock is supposed to signal that third parties won\u0026rsquo;t be able to read any information you send or receive.\nSSL accomplishes that by transforming your data into a coded message that only the recipient knows how to decipher.\n","date":"2021-05-14T00:00:00Z","permalink":"https://dyhes.github.io/p/httpbasics/","title":"【HTTP】Basics"},{"content":"What is a web browser A web browser is a computer program that allows users to download and view websites.\nhistory of web browser The first web browser, called WorldWideWeb, was created in 1990 by Sir Tim Berners-Lee. He then recruited Nicola Pellow to write the Line Mode Browser, which displayed web pages on dumb terminals. 1993 was a landmark year with the release of Mosaic, credited as \u0026ldquo;the world\u0026rsquo;s first popular browser\u0026rdquo;. Its innovative graphical user interface made the World Wide Web system easy to use and thus more accessible to the average person. This, in turn, sparked the Internet boom of the 1990s, when the Web grew at a very rapid rate. Marc Andreessen, the leader of the Mosaic team, soon started his own company, Netscape, which released the Mosaic-influenced Netscape Navigator in 1994. Navigator quickly became the most popular browser.\nMicrosoft debuted Internet Explorer in 1995, leading to a browser war with Netscape. Within a few years, Microsoft gained a dominant position in the browser market for two reasons: it bundled Internet Explorer with its popular Windows operating system and did so as freeware with no restrictions on usage. The market share of Internet Explorer peaked at over 95% in the early 2000s.\nIn 1998, Netscape launched what would become the Mozilla Foundation to create a new browser using the open source software model. This work evolved into the Firefox browser, first released by Mozilla in 2004. Firefox market share peaked at 32% in 2010.\nApple released its Safari browser in 2003. Safari remains the dominant browser on Apple devices, though it did not become popular elsewhere.\nGoogle debuted its Chrome browser in 2008, which steadily took market share from Internet Explorer and became the most popular browser in 2012. Chrome has remained dominant ever since.\nMicrosoft released its Edge browser in 2015 as part of the Windows 10 release. (Internet Explorer is still used on older versions of Windows.)\nComponents User Interface: This component allows end-users to interact with all visual elements available on the web page. The visual elements include the address bar, home button, next button, and all other elements that fetch and display the web page requested by the end-user. Browser Engine: It is a core component of every web browser. The browser engine functions as an intermediary or a bridge between the user interface and the rendering engine. It queries and handles the rendering engine as per the inputs received from the user interface. Rendering Engine: As the name suggests, this component is responsible for rendering a specific web page requested by the user on their screen. Networking: This component is responsible for managing network calls using standard protocols like HTTP or FTP. It also looks after security issues associated with internet communication. JavaScript Interpreter: As the name suggests, it is responsible for parsing and executing the JavaScript code embedded in a website. UI Backend: This component uses the user interface methods of the underlying operating system. It is mainly used for drawing basic widgets (windows and combo boxes). Data Storage/Persistence: It is a persistent layer. A web browser needs to store various types of data locally, for example, cookies. As a result, browsers must be compatible with data storage mechanisms such as WebSQL, IndexedDB, FileSystem, etc. Process Navigation TCP Handshake TCP (Transmission Control Protocol) uses a three-way handshake to set up a TCP/IP connection over an IP based network.\nSYNchronize The host, generally the browser, sends a TCP SYNchronize packet to the server.\nSYNchronize-ACKnowledgement The server receives the SYN and sends back a SYNchronize-ACKnowledgement.\nACKnowledge The host receives the server\u0026rsquo;s SYN-ACK and sends an ACKnowledge. The server receives ACK and the TCP socket connection is established.\nThis handshake step happens after a DNS lookup and before the TLS handshake, which creating a secure connection. The connection can be terminated independently by each side of the connection via a four-way handshake.\nTLS(Transport Layer Security) Negotiation This step determines which cipher will be used to encrypt the communication, verifies the server, and establishes that a secure connection is in place before beginning the actual transfer of data. This requires three more round trips to the server before the request for content is actually sent.\n(The DNS lookup, the TCP handshake, and 5 steps of the TLS handshake including clienthello, serverhello and certificate, clientkey and finished for both server and client)\nWhile making the connection secure adds time to the page load, a secure connection is worth the latency expense, as the data transmitted between the browser and the web server cannot be decrypted by a third party.\nAfter the 8 round trips, the browser is finally able to make the request.\nTTFB(Time to First Byte) TTFB refers to the time between the browser requesting a page and when it receives the first byte of information from the server. This time includes\nDNS lookup establishing the connection using a TCP handshake SSL handshake(if the request is made over https) TCP Slow Start / 14kb rule The first response packet will be 14Kb. This is part of TCP slow start, an algorithm which balances the speed of a network connection. Slow start gradually increases the amount of data transmitted until the network\u0026rsquo;s maximum bandwidth can be determined.\nAs the server sends data in TCP packets, the user\u0026rsquo;s client confirms delivery by returning acknowledgements, or ACKs. If the server sends too many packets too quickly, they will be dropped. Meaning, there will be no acknowledgement.\nCritical Rendering Path Parsing Once the browser receives the first chunk of data, it can begin parsing the information received. Parsing is the step the browser takes to turn the data it receives over the network into the DOM and CSSOM, which is used by the renderer to paint a page to the screen.\nIt\u0026rsquo;s important for web performance optimization to include everything the browser needs to start rendering a page, or at least a template of the page - the CSS and HTML needed for the first render \u0026ndash; in the first 14 kilobytes.\nBuilding the DOM tree\nThe first step is processing the HTML markup and building the DOM tree. HTML parsing involves tokenization and tree construction. HTML tokens include start and end tags, as well as attribute names and values. If the document is well-formed, parsing it is straightforward and faster. The parser parses tokenized input into the document, building up the document tree.\nWhen the parser finds non-blocking resources, such as an image, the browser will request those resources and continue parsing. Parsing can continue when a CSS file is encountered, but \u0026lt;script\u0026gt; tags—particularly those without an async or defer attribute—block rendering, and pause the parsing of HTML. Though the browser\u0026rsquo;s preload scanner hastens this process, excessive scripts can still be a significant bottleneck.\nPreload Scanner\nWhile the browser builds the DOM tree, this process occupies the main thread. As this happens, the preload scanner will parse through the content available and request high priority resources like CSS, JavaScript, and web fonts.\nBuilding the CSSOM\nThe second step in the critical rendering path is processing CSS and building the CSSOM tree. The CSS object model is similar to the DOM. The DOM and CSSOM are both trees. They are independent data structures. The browser converts the CSS rules into a map of styles it can understand and work with. The browser goes through each rule set in the CSS, creating a tree of nodes with parent, child, and sibling relationships based on the CSS selectors.\nAs with HTML, the browser needs to convert the received CSS rules into something it can work with. Hence, it repeats the HTML-to-object process, but for the CSS.\nThe CSSOM tree includes styles from the user agent style sheet. The browser begins with the most general rule applicable to a node and recursively refines the computed styles by applying more specific rules. In other words, it cascades the property values.\nOther Processes\nJavaScript Compilation While the CSS is being parsed and the CSSOM is created, other assets, including JavaScript files, are downloading (thanks to the preload scanner). JavaScript is interpreted, compiled, parsed and executed. The scripts are parsed into abstract syntax trees. Some browser engines take the Abstract Syntax Tree and pass it into an interpreter, outputting bytecode which is executed on the main thread. This is known as JavaScript compilation.\nBuilding the Accessibility Tree The accessibility object model (AOM) is like a semantic version of the DOM. The browser updates the accessibility tree when the DOM is updated. The accessibility tree is not modifiable by assistive technologies themselves. Until the AOM is built, the content is not accessible to screen readers.\nRender Rendering steps include style, layout, paint and, in some cases, compositing. The CSSOM and DOM trees created in the parsing step are combined into a render tree which is then used to compute the layout of every visible element, which is then painted to the screen. In some cases, content can be promoted to their own layers and composited, improving performance by painting portions of the screen on the GPU instead of the CPU, freeing up the main thread.\nStyle\nThe third step in the critical rendering path is combining the DOM and CSSOM into a render tree. The computed style tree, or render tree, construction starts with the root of the DOM tree, traversing each visible node.\ntags that aren\u0026rsquo;t going to be displayed are not included in the render tree Nodes with visibility: hidden applied are included in the render tree, as they do take up space. As we have not given any directives to override the user agent default, the script node will not be included in the render tree. Each visible node has its CSSOM rules applied to it. The render tree holds all the visible nodes with content and computed styles \u0026ndash; matching up all the relevant styles to every visible node in the DOM tree, and determining, based on the CSS cascade, what the computed styles are for each node.\nLayout\nThe fourth step in the critical rendering path is running layout on the render tree to compute the geometry of each node.\nOnce the render tree is built, layout commences. The render tree identified which nodes are displayed (even if invisible) along with their computed styles, but not the dimensions or location of each node. To determine the exact size and location of each object, the browser starts at the root of the render tree and traverses it.\nThe btime the size and position of nodes are determined is called layout. Subsequent recalculations of node size and locations are called reflows. Suppose the initial layout occurs before the image is returned. If the size of a image is not declared, there will be a reflow once the image size is known.\nPaint\nThe last step in the critical rendering path is painting the individual nodes to the screen, the first occurrence of which is called the first meaningful paint.\nIn the painting or rasterization phase, the browser converts each box calculated in the layout phase to actual pixels on the screen. Painting involves drawing every visual part of an element to the screen, including text, colors, borders, shadows, and replaced elements like buttons and images. The browser needs to do this super quickly.\nTo ensure smooth scrolling and animation, everything occupying the main thread, including calculating styles, along with reflow and paint, must take the browser less than 16.67ms to accomplish.\nTo ensure repainting can be done even faster than the initial paint, the drawing to the screen is generally broken down into several layers. If this occurs, then compositing is necessary.\nPainting can break the elements in the layout tree into layers. Promoting content into layers on the GPU (instead of the main thread on the CPU) improves paint and repaint performance. There are specific properties and elements that instantiate a layer, including and , and any element which has the CSS properties of opacity, a 3D transform, will-change, and a few others. These nodes will be painted onto their own layer, along with their descendants, unless a descendant necessitates its own layer for one (or more) of the above reasons.\nCompositing\nWhen sections of the document are drawn in different layers, overlapping each other, compositing is necessary to ensure they are drawn to the screen in the right order and the content is rendered correctly.\nAs the page continues to load assets, reflows can happen. A reflow sparks a repaint and a re-composite. Only the layer that needed to be repainted would be repainted, and composited if necessary.\nInteractivity Once the main thread is done painting the page, you would think we would be \u0026ldquo;all set.\u0026rdquo; That isn\u0026rsquo;t necessarily the case. If the load includes JavaScript, that was correctly deferred, and only executed after the onload event fires, the main thread might be busy, and not available for scrolling, touch, and other interactions.\nTTI(Time to Interactive) TTI is the measurement of how long it took from that first request which led to the DNS lookup to when the page is interactive.\n","date":"2021-05-13T00:00:00Z","permalink":"https://dyhes.github.io/p/webbrowser/","title":"【Web】Browser"},{"content":"Create an Application Every Vue application starts by creating a new application instance with the createApp function.\nThe object we are passing into createApp is in fact a component. Every app requires a \u0026ldquo;root component\u0026rdquo; that can contain other components as its children.\nAn application instance won\u0026rsquo;t render anything until its .mount() method is called. It expects a \u0026ldquo;container\u0026rdquo; argument, which can either be an actual DOM element or a selector string\nYou are not limited to a single application instance on the same page. The createApp API allows multiple Vue applications to co-exist on the same page, each with its own scope for configuration and global assets\nconst app1 = createApp({ /* ... */ }) app1.mount(\u0026#39;#container-1\u0026#39;) const app2 = createApp({ /* ... */ }) app2.mount(\u0026#39;#container-2\u0026#39;) If you are using Vue to enhance server-rendered HTML and only need Vue to control specific parts of a large page, avoid mounting a single Vue application instance on the entire page. Instead, create multiple small application instances and mount them on the elements they are responsible for.\nTemplate Syntax The double mustaches interprets the data as plain text, not HTML. In order to output real HTML, you will need to use the v-html directive\n\u0026lt;p\u0026gt;Using text interpolation: {{ rawHtml }}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Using v-html directive: \u0026lt;span v-html=\u0026#34;rawHtml\u0026#34;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; dynamically binding multiple attributes \u0026lt;script setup\u0026gt; const objectOfAttrs = { id: \u0026#39;container\u0026#39;, class: \u0026#39;wrapper\u0026#39; } \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;div v-bind=\u0026#34;objectOfAttrs\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div :=\u0026#34;objectOfAttrs\u0026#34;\u0026gt;for short\u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; Directives Directives are special attributes with the v- prefix. Vue provides a number of built-in directives\nIt is also possible to use a JavaScript expression in a directive argument by wrapping it with square brackets.\nDynamic arguments \u0026lt;a v-bind:[attributeName]=\u0026#34;url\u0026#34;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- shorthand --\u0026gt; \u0026lt;a :[attributeName]=\u0026#34;url\u0026#34;\u0026gt; ... \u0026lt;/a\u0026gt; Dynamic arguments are expected to evaluate to a string, with the exception of null. The special value null can be used to explicitly remove the binding. Any other non-string value will trigger a warning.\nModifiers Modifiers are special postfixes denoted by a dot, which indicate that a directive should be bound in some special way. For example, the .prevent modifier tells the v-on directive to call event.preventDefault() on the triggered event.\nReactivity We can create a reactive object or array with the reactive(), and non-object by the ref().\nWhen you mutate reactive state, the DOM is updated automatically. However, it should be noted that the DOM updates are not applied synchronously. Instead, Vue buffers them until the \u0026ldquo;next tick\u0026rdquo; in the update cycle to ensure that each component needs to update only once no matter how many state changes you have made.\nTo wait for the DOM update to complete after a state change, you can use the nextTick() global API\nimport { nextTick } from \u0026#39;vue\u0026#39; function increment() { count.value++ nextTick(() =\u0026gt; { // access updated DOM }) } Reactive Proxy It is important to note that the returned value from reactive() is a Proxy of the original object, which is not equal to the original object.\nOnly the proxy is reactive - mutating the original object will not trigger updates.\nTo ensure consistent access to the proxy, calling reactive() on the same object always returns the same proxy, and calling reactive() on an existing proxy also returns that same proxy\nRef The reactive() API has two limitations:\nIt only works for object types we couldn\u0026rsquo;t pass it around without losing reactivity. ref() allows us to create a \u0026ldquo;reference\u0026rdquo; to any value and pass it around without losing reactivity.\nWhen refs are accessed as top-level properties in the template, they are automatically \u0026ldquo;unwrapped\u0026rdquo; so there is no need to use .value.\nComputed Properties For complex logic that includes reactive data, it is recommended to use a computed property.\nThe computed() function expects to be passed a getter function, and the returned value is a computed ref.\nA computed property automatically tracks its reactive dependencies.\nInstead of a computed property, we can define the same function as a method. For the end result, the two approaches are indeed exactly the same. However, the difference is that computed properties are cached based on their reactive dependencies. A computed property will only re-evaluate when some of its reactive dependencies have changed. This means as long as author.books has not changed, multiple access to publishedBooksMessage will immediately return the previously computed result without having to run the getter function again. In comparison, a method invocation will always run the function whenever a re-render happens.\nThis also means the following computed property will never update, because Date.now() is not a reactive dependency.\nconst now = computed(() =\u0026gt; Date.now()) setter Computed properties are by default getter-only. If you attempt to assign a new value to a computed property, you will receive a runtime warning. In the rare cases where you need a \u0026ldquo;writable\u0026rdquo; computed property, you can create one by providing both a getter and a setter:\n\u0026lt;script setup\u0026gt; import { ref, computed } from \u0026#39;vue\u0026#39; const firstName = ref(\u0026#39;John\u0026#39;) const lastName = ref(\u0026#39;Doe\u0026#39;) const fullName = computed({ // getter get() { return firstName.value + \u0026#39; \u0026#39; + lastName.value }, // setter set(newValue) { // Note: we are using destructuring assignment syntax here. [firstName.value, lastName.value] = newValue.split(\u0026#39; \u0026#39;) } }) \u0026lt;/script\u0026gt; Now when you run fullName.value = 'John Doe', the setter will be invoked and firstName and lastName will be updated accordingly.\nClass and Style Bindings Vue provides special enhancements when v-bind is used with class and style. In addition to strings, the expressions can also evaluate to objects or arrays.\n\u0026lt;div :class=\u0026#34;{ active: isActive }\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; The above syntax means the presence of the active class will be determined by the truthiness of the data property isActive. We can also bind to a computed property that returns an object.\nWe can bind :class to an array to apply a list of classes\n\u0026lt;div :class=\u0026#34;[isActive ? activeClass : \u0026#39;\u0026#39;, errorClass]\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div :class=\u0026#34;[{ active: isActive }, errorClass]\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; With Components When you use the class attribute on a component with a single root element, those classes will be added to the component\u0026rsquo;s root element, and merged with any existing class already on it.\nIf your component has multiple root elements, you would need to define which element will receive this class. You can do this using the $attrs component property\n\u0026lt;p :class=\u0026#34;$attrs.class\u0026#34;\u0026gt;Hi!\u0026lt;/p\u0026gt; \u0026lt;span\u0026gt;This is a child component\u0026lt;/span\u0026gt; Conditional Rendering v-if,v-else-if and v-else can be used to attain conditional rendering and they also work for \u0026lt;template\u0026gt;.\nThe difference is that an element with v-show will always be rendered and remain in the DOM; v-show only toggles the display CSS property of the element.\nv-show doesn\u0026rsquo;t support the \u0026lt;template\u0026gt; element, nor does it work with v-else.\nGenerally speaking, v-if has higher toggle costs while v-show has higher initial render costs. So prefer v-show if you need to toggle something very often, and prefer v-if if the condition is unlikely to change at runtime.\nIt\u0026rsquo;s not recommended to use v-if and v-for on the same element due to implicit precedence. Refer to style guide for details.\nWhen v-if and v-for are both used on the same element, v-if will be evaluated first.\nList Rendering For nested v-for, scoping also works similar to nested functions. Each v-for scope has access to parent scopes.\nSimilar to template v-if, you can also use a \u0026lt;template\u0026gt; tag with v-for to render a block of multiple elements.\n\u0026lt;ul\u0026gt; \u0026lt;template v-for=\u0026#34;item in items\u0026#34;\u0026gt; \u0026lt;li\u0026gt;{{ item.msg }}\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;divider\u0026#34; role=\u0026#34;presentation\u0026#34;\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;/ul\u0026gt; When they exist on the same node, v-if has a higher priority than v-for. That means the v-if condition will not have access to variables from the scope of the v-for\nThis can be fixed by moving v-for to a wrapping \u0026lt;template\u0026gt; tag (which is also more explicit):\n\u0026lt;template v-for=\u0026#34;todo in todos\u0026#34;\u0026gt; \u0026lt;li v-if=\u0026#34;!todo.isComplete\u0026#34;\u0026gt; {{ todo.name }} \u0026lt;/li\u0026gt; \u0026lt;/template\u0026gt; To give Vue a hint so that it can track each node\u0026rsquo;s identity, and thus reuse and reorder existing elements, you need to provide a unique key attribute for each item\nWhen using \u0026lt;template v-for\u0026gt;, the key should be placed on the \u0026lt;template\u0026gt; container\n\u0026lt;template v-for=\u0026#34;todo in todos\u0026#34; :key=\u0026#34;todo.name\u0026#34;\u0026gt; \u0026lt;li\u0026gt;{{ todo.name }}\u0026lt;/li\u0026gt; \u0026lt;/template\u0026gt; with Components You can directly use v-for on a component, like any normal element (don\u0026rsquo;t forget to provide a key).\nHowever, this won\u0026rsquo;t automatically pass any data to the component, because components have isolated scopes of their own. In order to pass the iterated data into the component, we should also use props.\n\u0026lt;my-component v-for=\u0026#34;(item, index) in items\u0026#34; :item=\u0026#34;item\u0026#34; :index=\u0026#34;index\u0026#34; :key=\u0026#34;item.id\u0026#34; \u0026gt;\u0026lt;/my-component\u0026gt; Array Change Vue wraps an observed array\u0026rsquo;s mutation methods so they will also trigger view updates. The wrapped methods are:\npush() pop() shift() unshift() splice() sort() reverse() There are also non-mutating methods, e.g. filter(), concat() and slice(), which do not mutate the original array but always return a new array. When working with non-mutating methods, we should replace the old array with the new one.\nBe careful with reverse() and sort() in a computed property! These two methods will mutate the original array, which should be avoided in computed getters. Create a copy of the original array before calling these methods.\n- return numbers.reverse() //wrong + return [...numbers].reverse() //true Event Handling We can use the v-on directive, which we typically shorten to the @ symbol, to listen to DOM events and run some JavaScript when they\u0026rsquo;re triggered.\nThe handler value can be one of the following:\nInline handlers: Inline JavaScript to be executed when the event is triggered (similar to the native onclick attribute). Method handlers: A property name or path that points to a method defined on the component. \u0026lt;button @click=\u0026#34;count++\u0026#34;\u0026gt;Add 1\u0026lt;/button\u0026gt; \u0026lt;p\u0026gt;Count is: {{ count }}\u0026lt;/p\u0026gt; The template compiler detects method handlers by checking whether the v-on value string is a valid JavaScript identifier or property access path.\nSometimes we also need to access the original DOM event in an inline handler. You can pass it into a method using the special $event variable, or use an inline arrow function.\n\u0026lt;button @click=\u0026#34;warn(\u0026#39;Form cannot be submitted yet.\u0026#39;, $event)\u0026#34;\u0026gt; Submit \u0026lt;/button\u0026gt; \u0026lt;!-- using inline arrow function --\u0026gt; \u0026lt;button @click=\u0026#34;(event) =\u0026gt; warn(\u0026#39;Form cannot be submitted yet.\u0026#39;, event)\u0026#34;\u0026gt; Submit \u0026lt;/button\u0026gt; Event Modifiers Recall that modifiers are directive postfixes denoted by a dot.\n.stop .prevent .self .capture .once .passive \u0026lt;!-- the click event\u0026#39;s propagation will be stopped --\u0026gt; \u0026lt;a @click.stop=\u0026#34;doThis\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- the submit event will no longer reload the page --\u0026gt; \u0026lt;form @submit.prevent=\u0026#34;onSubmit\u0026#34;\u0026gt;\u0026lt;/form\u0026gt; \u0026lt;!-- modifiers can be chained, and the order matters--\u0026gt; \u0026lt;a @click.stop.prevent=\u0026#34;doThat\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- just the modifier --\u0026gt; \u0026lt;form @submit.prevent\u0026gt;\u0026lt;/form\u0026gt; \u0026lt;!-- only trigger handler if event.target is the element itself --\u0026gt; \u0026lt;!-- i.e. not from a child element --\u0026gt; \u0026lt;div @click.self=\u0026#34;doThat\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; The .capture, .once, and .passive modifiers mirror the options of the native addEventListener method.\n\u0026lt;!-- use capture mode when adding the event listener --\u0026gt; \u0026lt;!-- i.e. an event targeting an inner element is handled here before being handled by that element --\u0026gt; \u0026lt;div @click.capture=\u0026#34;doThis\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; \u0026lt;!-- the click event will be triggered at most once --\u0026gt; \u0026lt;a @click.once=\u0026#34;doThis\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- the scroll event\u0026#39;s default behavior (scrolling) will happen --\u0026gt; \u0026lt;!-- immediately, instead of waiting for `onScroll` to complete --\u0026gt; \u0026lt;!-- in case it contains `event.preventDefault()` --\u0026gt; \u0026lt;div @scroll.passive=\u0026#34;onScroll\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; Key Modifiers You can directly use any valid key names exposed via KeyboardEvent.key as modifiers by converting them to kebab-case.\nkey aliases Vue provides aliases for the most commonly used keys:\n.enter .tab .delete (captures both \u0026ldquo;Delete\u0026rdquo; and \u0026ldquo;Backspace\u0026rdquo; keys) .esc .space .up .down .left .right System Modifier Keys You can use the following modifiers to trigger mouse or keyboard event listeners only when the corresponding modifier key is pressed:\n.ctrl .alt .shift .meta Note that modifier keys are different from regular keys and when used with keyup events, they have to be pressed when the event is emitted. In other words, keyup.ctrl will only trigger if you release a key while holding down ctrl. It won\u0026rsquo;t trigger if you release the ctrl key alone.\n.exact Modifier The .exact modifier allows control of the exact combination of system modifiers needed to trigger an event.\n\u0026lt;!-- this will fire even if Alt or Shift is also pressed --\u0026gt; \u0026lt;button @click.ctrl=\u0026#34;onClick\u0026#34;\u0026gt;A\u0026lt;/button\u0026gt; \u0026lt;!-- this will only fire when Ctrl and no other keys are pressed --\u0026gt; \u0026lt;button @click.ctrl.exact=\u0026#34;onCtrlClick\u0026#34;\u0026gt;A\u0026lt;/button\u0026gt; \u0026lt;!-- this will only fire when no system modifiers are pressed --\u0026gt; \u0026lt;button @click.exact=\u0026#34;onClick\u0026#34;\u0026gt;A\u0026lt;/button\u0026gt; Mouse Button Modifiers .left .right .middle These modifiers restrict the handler to events triggered by a specific mouse button.\nForm Input Bindings v-model will ignore the initial value, checked or selected attributes found on any form elements. It will always treat the current bound JavaScript state as the source of truth. You should declare the initial value on the JavaScript side, using reactivity APIs.\nLifecycle Hooks There are also other hooks which will be called at different stages of the instance\u0026rsquo;s lifecycle, with the most commonly used being onMounted, onUpdated, and onUnmounted.\nWatchs With Composition API, we can use the watch function to trigger a callback whenever a piece of reactive state changes.\nwatch\u0026rsquo;s first argument can be different types of reactive \u0026ldquo;sources\u0026rdquo;: it can be a ref (including computed refs), a reactive object, a getter function, or an array of multiple sources.\nconst x = ref(0) const y = ref(0) // single ref watch(x, (newX) =\u0026gt; { console.log(`x is ${newX}`) }) // getter watch( () =\u0026gt; x.value + y.value, (sum) =\u0026gt; { console.log(`sum of x + y is: ${sum}`) } ) // array of multiple sources watch([x, () =\u0026gt; y.value], ([newX, newY]) =\u0026gt; { console.log(`x is ${newX} and y is ${newY}`) }) Do note that you can\u0026rsquo;t watch a property of a reactive object like this:\nconst obj = reactive({ count: 0 }) // this won\u0026#39;t work because we are passing a number to watch() watch(obj.count, (count) =\u0026gt; { console.log(`count is: ${count}`) }) Instead, use a getter:\n// instead, use a getter: watch( () =\u0026gt; obj.count, (count) =\u0026gt; { console.log(`count is: ${count}`) } ) watchEffect watchEffect() allows us to perform a side effect immediately while automatically tracking the effect\u0026rsquo;s reactive dependencies.\nwatch and watchEffect both allow us to reactively perform side effects. Their main difference is the way they track their reactive dependencies:\nwatch only tracks the explicitly watched source. It won\u0026rsquo;t track anything accessed inside the callback. In addition, the callback only triggers when the source has actually changed. watch separates dependency tracking from the side effect, giving us more precise control over when the callback should fire. watchEffect, on the other hand, combines dependency tracking and side effect into one phase. It automatically tracks every reactive property accessed during its synchronous execution. This is more convenient and typically results in terser code, but makes its reactive dependencies less explicit. By default, user-created watcher callbacks are called before Vue component updates. This means if you attempt to access the DOM inside a watcher callback, the DOM will be in the state before Vue has applied any updates.\nIf you want to access the DOM in a watcher callback after Vue has updated it, you need to specify the flush: 'post' option.\nwatch(source, callback, { flush: \u0026#39;post\u0026#39; }) watchEffect(callback, { flush: \u0026#39;post\u0026#39; }) Post-flush watchEffect() also has a convenience alias, watchPostEffect().\nWatchers declared synchronously inside setup() or \u0026lt;script setup\u0026gt; are bound to the owner component instance, and will be automatically stopped when the owner component is unmounted. In most cases, you don\u0026rsquo;t need to worry about stopping the watcher yourself.\nThe key here is that the watcher must be created synchronously: if the watcher is created in an async callback, it won\u0026rsquo;t be bound to the owner component and must be stopped manually to avoid memory leaks.\nTo manually stop a watcher, use the returned handle function. This works for both watch and watchEffect.\nNote that there should be very few cases where you need to create watchers asynchronously, and synchronous creation should be preferred whenever possible. If you need to wait for some async data, you can make your watch logic conditional instead.\nTemplate Refs There may still be cases where we need direct access to the underlying DOM elements. To achieve this, we can use the special ref attribute:\n\u0026lt;input ref=\u0026#34;input\u0026#34;\u0026gt; ref is a special attribute, similar to the key attribute discussed in the v-for chapter. It allows us to obtain a direct reference to a specific DOM element or child component instance after it\u0026rsquo;s mounted.\nTo obtain the reference with Composition API, we need to declare a ref with the same name\nComponents Registration Global Registration We can make components available globally in the current Vue application using the app.component() method.\nIf using SFCs, you will be registering the imported .vue files.\nThe app.component() method can be chained:\napp .component(\u0026#39;ComponentA\u0026#39;, ComponentA) .component(\u0026#39;ComponentB\u0026#39;, ComponentB) .component(\u0026#39;ComponentC\u0026#39;, ComponentC) Globally registered components can be used in the template of any component within this application\ndrawbacks prevents build systems from removing unused components makes dependency relationships less explicit in large applications Local Registration When using SFC with \u0026lt;script setup\u0026gt;, imported components are automatically registered locally.\nNote that locally registered components are *not* also available in descendent components.\nProps Vue components require explicit props declaration so that Vue knows what external props passed to the component should be treated as fallthrough attributes.\nIn SFCs using \u0026lt;script setup\u0026gt;, props can be declared using the defineProps() macro.\nEvents $emit() A component can emit custom events directly in template expressions (e.g. in a v-on handler) using the built-in $emit function.\n\u0026lt;button @click=\u0026#34;$emit(\u0026#39;someEvent\u0026#39;)\u0026#34;\u0026gt;click me\u0026lt;/button\u0026gt; The parent can then listen to it using v-on:\n\u0026lt;MyComponent @some-event=\u0026#34;callback\u0026#34; /\u0026gt; Like components and props, event names provide an automatic case transformation. Like components and props, event names provide an automatic case transformation.\nUnlike native DOM events, component emitted events do not bubble. You can only listen to the events emitted by a direct child component.\nIt\u0026rsquo;s sometimes useful to emit a specific value with an event.\nAll extra arguments passed to $emit() after the event name will be forwarded to the listener. For example, with $emit('foo', 1, 2, 3) the listener function will receive three arguments.\ndefineEmits Emitted events can be explicitly declared on the component via the defineEmits() macro.\nFallthrough Attributes A \u0026ldquo;fallthrough attribute\u0026rdquo; is an attribute or v-on event listener that is passed to a component, but is not explicitly declared in the receiving component\u0026rsquo;s props or emits. Common examples of this include class, style, and id attributes.\nWhen a component renders a single root element, fallthrough attributes will be automatically added to the root element\u0026rsquo;s attributes.\nUnlike components with a single root node, components with multiple root nodes do not have an automatic attribute fallthrough behavior. If $attrs are not bound explicitly, a runtime warning will be issued.\nuseAttrs If needed, you can access a component\u0026rsquo;s fallthrough attributes in \u0026lt;script setup\u0026gt; using the useAttrs() API:\n\u0026lt;script setup\u0026gt; import { useAttrs } from \u0026#39;vue\u0026#39; const attrs = useAttrs() \u0026lt;/script\u0026gt; Provider/inject Prop Drilling We can solve props drilling with provide and inject. A parent component can serve as a dependency provider for all its descendants. Any component in the descendant tree, regardless of how deep it is, can inject dependencies provided by components up in its parent chain.\nProvide provide(/* key */ \u0026#39;message\u0026#39;, /* value */ \u0026#39;hello!\u0026#39;) The provide() function accepts two arguments. The first argument is called the injection key, which can be a string or a Symbol. The injection key is used by descendent components to lookup the desired value to inject. A single component can call provide() multiple times with different injection keys to provide different values.\nThe second argument is the provided value. The value can be of any type, including reactive state such as refs.\nInject const message = inject(\u0026#39;message\u0026#39;) To inject data provided by an ancestor component, use the inject() function.\nIf the provided value is a ref, it will be injected as-is and will not be automatically unwrapped. This allows the injector component to retain the reactivity connection to the provider component.\nBy default, inject assumes that the injected key is provided somewhere in the parent chain. In the case where the key is not provided, there will be a runtime warning.\nIf we want to make an injected property work with optional providers, we need to declare a default value, similar to props\nconst value = inject(\u0026#39;message\u0026#39;, \u0026#39;default value\u0026#39;) It is recommended to keep any mutations to reactive state inside of the *provider* whenever possible.\nThere may be times when we need to update the data from a injector component. In such cases, we recommend providing a function that is responsible for mutating the state.\nprovide(\u0026#39;location\u0026#39;, { location, updateLocation }) Finally, you can wrap the provided value with readonly() if you want to ensure that the data passed through provide cannot be mutated by the injected component.\nprovide(\u0026#39;read-only-count\u0026#39;, readonly(count)) symbol\nconst myInjectionKey = Symbol() provide(myInjectionKey, { /* data to provide */ }) const injected = inject(myInjectionKey) Async Components In large applications, we may need to divide the app into smaller chunks and only load a component from the server when it\u0026rsquo;s needed. To make that possible, Vue has a defineAsyncComponent function.\nconst AsyncComp = defineAsyncComponent(() =\u0026gt; { return new Promise((resolve, reject) =\u0026gt; { // ...load component from server resolve(/* loaded component */) }) }) // ... use `AsyncComp` like a normal component ES module dynamic import also returns a Promise, so most of the time we will use it in combination with defineAsyncComponent. Bundlers like Vite and webpack also support the syntax, so we can use it to import Vue SFCs.\nconst AsyncComp = defineAsyncComponent(() =\u0026gt; import(\u0026#39;./components/MyComponent.vue\u0026#39;) ) Loading and Error States const AsyncComp = defineAsyncComponent({ // the loader function loader: () =\u0026gt; import(\u0026#39;./Foo.vue\u0026#39;), // A component to use while the async component is loading loadingComponent: LoadingComponent, // Delay before showing the loading component. Default: 200ms. delay: 200, // A component to use if the load fails errorComponent: ErrorComponent, // The error component will be displayed if a timeout is // provided and exceeded. Default: Infinity. timeout: 3000 }) Reusability Composables In the context of Vue applications, a \u0026ldquo;composable\u0026rdquo; is a function that leverages Vue Composition API to encapsulate and reuse stateful logic.\nexport function useMouse() { // state encapsulated and managed by the composable const x = ref(0) const y = ref(0) // a composable can update its managed state over time. function update(event) { x.value = event.pageX y.value = event.pageY } // a composable can also hook into its owner component\u0026#39;s // lifecycle to setup and teardown side effects. onMounted(() =\u0026gt; window.addEventListener(\u0026#39;mousemove\u0026#39;, update)) onUnmounted(() =\u0026gt; window.removeEventListener(\u0026#39;mousemove\u0026#39;, update)) // expose managed state as return value return { x, y } } extract the logic into an external file, as a composable function.\nThe cooler part about composables though, is that you can also nest them: one composable function can call one or more other composable functions. This enables us to compose complex logic using small, isolated units, similar to how we compose an entire application using components.\nexport function useFetch(url) { const data = ref(null) const error = ref(null) function doFetch() { // reset state before fetching.. data.value = null error.value = null // unref() unwraps potential refs fetch(unref(url)) .then((res) =\u0026gt; res.json()) .then((json) =\u0026gt; (data.value = json)) .catch((err) =\u0026gt; (error.value = err)) } if (isRef(url)) { // setup reactive re-fetch if input URL is a ref watchEffect(doFetch) } else { // otherwise, just fetch once // and avoid the overhead of a watcher doFetch() } return { data, error } } \u0026lt;script setup\u0026gt; is the only place where you can call composables after usage of await. The compiler automatically restores the active instance context after the async operation for you.\nTo some extent, you can think of these extracted cas component-scoped services that can talk to one another.\nCustom Directives In addition to the default set of directives shipped in core (like v-model or v-show), Vue also allows you to register your own custom directives.\nWe have introduced two forms of code reuse in Vue: components and composables. Components are the main building blocks, while composables are focused on reusing stateful logic. Custom directives, on the other hand, are mainly intended for reusing logic that involves low-level DOM access on plain elements.\nA custom directive is defined as an object containing lifecycle hooks similar to those of a component.\nIn \u0026lt;script setup\u0026gt;, any camelCase variable that starts with the v prefix can be used as a custom directive. In the example above, vFocus can be used in the template as v-focus.\nconst myDirective = { // called before bound element\u0026#39;s attributes // or event listeners are applied created(el, binding, vnode, prevVnode) { // see below for details on arguments }, // called right before the element is inserted into the DOM. beforeMount() {}, // called when the bound element\u0026#39;s parent component // and all its children are mounted. mounted() {}, // called before the parent component is updated beforeUpdate() {}, // called after the parent component and // all of its children have updated updated() {}, // called before the parent component is unmounted beforeUnmount() {}, // called when the parent component is unmounted unmounted() {} } } Directive hooks are passed these arguments:\nel: the element the directive is bound to. This can be used to directly manipulate the DOM. binding: an object containing the following properties. value: The value passed to the directive. For example in v-my-directive=\u0026quot;1 + 1\u0026quot;, the value would be 2. oldValue: The previous value, only available in beforeUpdate and updated. It is available whether or not the value has changed. arg: The argument passed to the directive, if any. For example in v-my-directive:foo, the arg would be \u0026quot;foo\u0026quot;. modifiers: An object containing modifiers, if any. For example in v-my-directive.foo.bar, the modifiers object would be { foo: true, bar: true }. instance: The instance of the component where the directive is used. dir: the directive definition object. vnode: the underlying VNode representing the bound element. prevNode: the VNode representing the bound element from the previous render. Only available in the beforeUpdate and updated hooks. It is also common to globally register custom directives at the app level.\napp.directive(\u0026#39;focus\u0026#39;, { /* ... */ }) Custom directives should only be used when the desired functionality can only be achieved via direct DOM manipulation. Prefer declarative templating using built-in directives such as v-bind when possible because they are more efficient and server-rendering friendly.\nshorthand It\u0026rsquo;s common for a custom directive to have the same behavior for mounted and updated, with no need for the other hooks. In such cases we can define the directive as a function:\napp.directive(\u0026#39;color\u0026#39;, (el, binding) =\u0026gt; { // this will be called for both `mounted` and `updated` el.style.color = binding.value }) When used on components, custom directives will always apply to a component\u0026rsquo;s root node, similar to Fallthrough Attributes.\n\u0026lt;MyComponent v-demo=\u0026#34;test\u0026#34; /\u0026gt; \u0026lt;!-- template of MyComponent --\u0026gt; \u0026lt;div\u0026gt; \u0026lt;!-- v-demo directive will be applied here --\u0026gt; \u0026lt;span\u0026gt;My component content\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; Note that components can potentially have more than one root node. When applied to a multi-root component, a directive will be ignored and a warning will be thrown. Unlike attributes, directives can\u0026rsquo;t be passed to a different element with v-bind=\u0026quot;$attrs\u0026quot;. In general, it is not recommended to use custom directives on components.\nPlugins Plugins are self-contained code that usually add app-level functionality to Vue. This is how we install a plugin:\nimport { createApp } from \u0026#39;vue\u0026#39; const app = createApp({}) app.use(myPlugin, { /* optional options */ }) A plugin is defined as either an object that exposes an install() method, or simply a function that acts as the install function itself. The install function receives the app instance along with additional options passed to app.use(), if any:\nconst myPlugin = { install(app, options) { // configure the app } } There is no strictly defined scope for a plugin, but common scenarios where plugins are useful include:\nRegister one or more global components or custom directives with app.component() and app.directive(). Make a resource injectable throughout the app by calling app.provide(). Add some global instance properties or methods by attaching them to app.config.globalProperties. A library that needs to perform some combination of the above (e.g. vue-router). Built-in Transition Vue offers two built-in components that can help work with transitions and animations in response to changing state:\n\u0026lt;Transition\u0026gt; for applying animations when an element or component is entering and leaving the DOM. \u0026lt;TransitionGroup\u0026gt; for applying animations when an element or component is inserted into, removed from, or moved within a v-for list. \u0026lt;Transition\u0026gt; only supports a single element or component as its slot content. If the content is a component, the component must also have only one single root element.\nWhen an element in a \u0026lt;Transition\u0026gt; component is inserted or removed, this is what happens:\nVue will automatically sniff whether the target element has CSS transitions or animations applied. If it does, a number of CSS transition classes will be added / removed at appropriate timings. If there are listeners for JavaScript hooks, these hooks will be called at appropriate timings. If no CSS transitions / animations are detected and no JavaScript hooks are provided, the DOM operations for insertion and/or removal will be executed on the browser\u0026rsquo;s next animation frame. v-enter-from: Starting state for enter. Added before the element is inserted, removed one frame after the element is inserted. v-enter-active: Active state for enter. Applied during the entire entering phase. Added before the element is inserted, removed when the transition/animation finishes. This class can be used to define the duration, delay and easing curve for the entering transition. v-enter-to: Ending state for enter. Added one frame after the element is inserted (at the same time v-enter-from is removed), removed when the transition/animation finishes. v-leave-from: Starting state for leave. Added immediately when a leaving transition is triggered, removed after one frame. v-leave-active: Active state for leave. Applied during the entire leaving phase. Added immediately when a leave transition is triggered, removed when the transition/animation finishes. This class can be used to define the duration, delay and easing curve for the leaving transition. v-leave-to: Ending state for leave. Added one frame after a leaving transition is triggered (at the same time v-leave-from is removed), removed when the transition/animation finishes. For a named transition, its transition classes will be prefixed with its name instead of v.\n\u0026lt;Transition name=\u0026#34;fade\u0026#34;\u0026gt; ... \u0026lt;/Transition\u0026gt; \u0026lt;style\u0026gt; .fade-enter-active, .fade-leave-active { transition: opacity 0.5s ease; } .fade-enter-from, .fade-leave-to { opacity: 0; } \u0026lt;/style\u0026gt; TransitionGroup \u0026lt;TransitionGroup\u0026gt; is a built-in component designed for animating the insertion, removal, and order change of elements or components that are rendered in a list.\nKeepAlive \u0026lt;KeepAlive\u0026gt; is a built-in component that allows us to conditionally cache component instances (to keep the state) when dynamically switching between multiple components.\n\u0026lt;!-- Inactive components will be cached! --\u0026gt; \u0026lt;KeepAlive\u0026gt; \u0026lt;component :is=\u0026#34;activeComponent\u0026#34; /\u0026gt; \u0026lt;/KeepAlive\u0026gt; Teleport \u0026lt;Teleport\u0026gt; is a built-in component that allows us to \u0026ldquo;teleport\u0026rdquo; a part of a component\u0026rsquo;s template into a DOM node that exists outside the DOM hierarchy of that component.\nSuspense ……\n","date":"2021-05-08T00:00:00Z","permalink":"https://dyhes.github.io/p/vueguide/","title":"【Vue】Guide"},{"content":"Single File Component (SFC) An SFC is a reusable self-contained block of code that encapsulates HTML, CSS and JavaScript that belong together, written inside a .vue file.\ndeclarative rendering The core feature of Vue is declarative rendering: using a template syntax that extends HTML, we can describe how the HTML should look like based on JavaScript state.\nReactive State that can trigger updates when changed are considered reactive. We can declare reactive state using Vue\u0026rsquo;s reactive() API. Objects created from reactive() are JavaScript Proxies that work just like normal objects.\nreactive() only works on objects (including arrays and built-in types like Map and Set). ref(), on the other hand, can take any value type and create an object that exposes the inner value under a .value property.\nReactive state declared in the component\u0026rsquo;s \u0026lt;script setup\u0026gt; block can be used directly in the template.\nwe did not need to use .value when accessing the message ref in templates: it is automatically unwrapped for more succinct usage.\n\u0026lt;script setup\u0026gt; const message = ref(\u0026#39;Hello World!\u0026#39;) console.log(message.value) \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;h1\u0026gt; {{message}} \u0026lt;/h1\u0026gt; \u0026lt;/template\u0026gt; Directive \u0026lt;script setup\u0026gt; import { ref } from \u0026#39;vue\u0026#39; const titleClass = ref(\u0026#39;title\u0026#39;) \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;h1 v-bind:class=\u0026#39;titleClass\u0026#39;\u0026gt;Make me red\u0026lt;/h1\u0026gt; \u0026lt;h1 :class=\u0026#39;titleClass\u0026#39;\u0026gt; for short \u0026lt;/h1\u0026gt; \u0026lt;/template\u0026gt; A directive is a special attribute that starts with the v- prefix. They are part of Vue\u0026rsquo;s template syntax. Similar to text interpolations, directive values are JavaScript expressions that have access to the component\u0026rsquo;s state.\nEvent Listener \u0026lt;script setup\u0026gt; import { ref } from \u0026#39;vue\u0026#39; const count = ref(0) function increment() { // update component state count.value++ } \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;button v-on:click=\u0026#34;increment\u0026#34;\u0026gt;{{ count }}\u0026lt;/button\u0026gt; \u0026lt;button @click=\u0026#34;increment\u0026#34;\u0026gt;for short\u0026lt;/button\u0026gt; \u0026lt;/template\u0026gt; Two-Way bindings \u0026lt;script setup\u0026gt; const text = ref(\u0026#39;\u0026#39;) function onInput(e) { // a v-on handler receives the native DOM event // as the argument. text.value = e.target.value } \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;input :value=\u0026#34;text\u0026#34; @input=\u0026#34;onInput\u0026#34;\u0026gt; \u0026lt;input v-model=\u0026#34;text\u0026#34;\u0026gt;\u0026lt;!--for short--\u0026gt; \u0026lt;/template\u0026gt; To simplify two-way bindings, Vue provides a directive, v-model, which is essentially a syntax sugar.\nv-model automatically syncs the \u0026lt;input\u0026gt;\u0026rsquo;s value with the bound state, so we no longer need to use a event handler for that.\nv-model works not only on text inputs, but also other input types such as checkboxes, radio buttons, and select dropdowns.\nConditional Rendering We can use the v-if directive to conditionally render an element, we can also use v-else and v-else-if to denote other branches of the condition.\n\u0026lt;script setup\u0026gt; import { ref } from \u0026#39;vue\u0026#39; const awesome = ref(true) function toggle() { awesome.value=!awesome.value } \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;button @click=\u0026#34;toggle\u0026#34;\u0026gt;toggle\u0026lt;/button\u0026gt; \u0026lt;h1 v-if=\u0026#39;awesome\u0026#39;\u0026gt;Vue is awesome!\u0026lt;/h1\u0026gt; \u0026lt;h1 v-else\u0026gt;Oh no 😢\u0026lt;/h1\u0026gt; \u0026lt;/template\u0026gt; List Rendering We can use the v-for directive to render a list of elements based on a source array\nwe are also giving each todo object a unique id, and binding it as the special key attribute for each \u0026lt;li\u0026gt;. The key allows Vue to accurately move each \u0026lt;li\u0026gt; to match the position of its corresponding object in the array.\nThere are two ways to update the list:\nCall mutating methods on the source array Replace the array with a new one \u0026lt;script setup\u0026gt; import { ref } from \u0026#39;vue\u0026#39; // give each todo a unique id let id = 0 const newTodo = ref(\u0026#39;\u0026#39;) const todos = ref([ { id: id++, text: \u0026#39;Learn HTML\u0026#39; }, { id: id++, text: \u0026#39;Learn JavaScript\u0026#39; }, { id: id++, text: \u0026#39;Learn Vue\u0026#39; } ]) function addTodo() { todos.value.push({ id: id++, text: newTodo.value }) newTodo.value = \u0026#39;\u0026#39; } function removeTodo(todo) { todos.value = todos.value.filter((t) =\u0026gt; t !== todo) } \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;input v-model=\u0026#34;newTodo\u0026#34; @keyup.enter=\u0026#34;addTodo\u0026#34;\u0026gt; \u0026lt;button @click=\u0026#34;addTodo\u0026#34;\u0026gt;Add Todo\u0026lt;/button\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li v-for=\u0026#34;todo in todos\u0026#34; :key=\u0026#34;todo.id\u0026#34;\u0026gt; {{ todo.text }} \u0026lt;button @click=\u0026#34;removeTodo(todo)\u0026#34;\u0026gt;X\u0026lt;/button\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/template\u0026gt; Computed property We can create a computed ref that computes its .value based on other reactive data sources by using computed()\nA computed property tracks other reactive state used in its computation as dependencies. It caches the result and automatically updates it when its dependencies change.\n\u0026lt;script setup\u0026gt; import { ref, computed } from \u0026#39;vue\u0026#39; let id = 0 const newTodo = ref(\u0026#39;\u0026#39;) const hideCompleted = ref(false) const todos = ref([ { id: id++, text: \u0026#39;Learn HTML\u0026#39;, done: true }, { id: id++, text: \u0026#39;Learn JavaScript\u0026#39;, done: true }, { id: id++, text: \u0026#39;Learn Vue\u0026#39;, done: false } ]) const filteredTodos = computed(() =\u0026gt; { return hideCompleted.value ? todos.value.filter((t) =\u0026gt; !t.done) : todos.value }) function addTodo() { todos.value.push({ id: id++, text: newTodo.value, done: false }) newTodo.value = \u0026#39;\u0026#39; } function removeTodo(todo) { todos.value = todos.value.filter((t) =\u0026gt; t !== todo) } \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;input v-model=\u0026#34;newTodo\u0026#34; @keyup.enter=\u0026#34;addTodo\u0026#34; /\u0026gt; \u0026lt;button @click=\u0026#34;addTodo\u0026#34;\u0026gt;Add Todo\u0026lt;/button\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li v-for=\u0026#34;todo in filteredTodos\u0026#34; :key=\u0026#34;todo.id\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;checkbox\u0026#34; v-model=\u0026#34;todo.done\u0026#34; /\u0026gt; \u0026lt;span :class=\u0026#34;{ done: todo.done }\u0026#34;\u0026gt;{{ todo.text }}\u0026lt;/span\u0026gt; \u0026lt;button @click=\u0026#34;removeTodo(todo)\u0026#34;\u0026gt;X\u0026lt;/button\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;button @click=\u0026#34;hideCompleted = !hideCompleted\u0026#34;\u0026gt; {{ hideCompleted ? \u0026#39;Show all\u0026#39; : \u0026#39;Hide completed\u0026#39; }} \u0026lt;/button\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;style\u0026gt; .done { text-decoration: line-through; } \u0026lt;/style\u0026gt; Lifecycle and Template Refs We can request a template ref - i.e. a reference to an element in the template - using the special ref attribute.\nTo access the ref, we need to declare a ref with matching name in \u0026lt;script setup\u0026gt; . Notice the ref is initialized with null value. This is because the element doesn\u0026rsquo;t exist yet when \u0026lt;script setup\u0026gt; is executed. The template ref is only accessible after the component is mounted.\n\u0026lt;script setup\u0026gt; import { ref, onMounted } from \u0026#39;vue\u0026#39; const p = ref(null) onMounted(() =\u0026gt; { p.value.textContent = \u0026#39;mounted!\u0026#39; }) \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;p ref=\u0026#34;p\u0026#34;\u0026gt;hello\u0026lt;/p\u0026gt; \u0026lt;/template\u0026gt; hooks such as onUpdated and onUnmounted are lifecycle hooks.\nWatchers Sometimes we may need to perform \u0026ldquo;side effects\u0026rdquo; reactively\nwatch() can directly watch a ref, and the callback gets called whenever count\u0026rsquo;s value changes.\n\u0026lt;script setup\u0026gt; import { ref, watch } from \u0026#39;vue\u0026#39; const todoId = ref(1) const todoData = ref(null) async function fetchData() { todoData.value = null const res = await fetch( `https://jsonplaceholder.typicode.com/todos/${todoId.value}` ) todoData.value = await res.json() } fetchData() watch(todoId, fetchData) \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;p\u0026gt;Todo id: {{ todoId }}\u0026lt;/p\u0026gt; \u0026lt;button @click=\u0026#34;todoId++\u0026#34;\u0026gt;Fetch next todo\u0026lt;/button\u0026gt; \u0026lt;p v-if=\u0026#34;!todoData\u0026#34;\u0026gt;Loading...\u0026lt;/p\u0026gt; \u0026lt;pre v-else\u0026gt;{{ todoData }}\u0026lt;/pre\u0026gt; \u0026lt;/template\u0026gt; Props A child component can accept input from the parent via props. First, it needs to declare the props it accepts:\n\u0026lt;!-- ChildComp.vue --\u0026gt; \u0026lt;script setup\u0026gt; const props = defineProps({ msg: String }) \u0026lt;/script\u0026gt; Note defineProps() is a compile-time macro and doesn\u0026rsquo;t need to be imported. Once declared, the msg prop can be used in the child component\u0026rsquo;s template. It can also be accessed in JavaScript via the returned object of defineProps().\nEmits In addition to receiving props, a child component can also emit events to the parent:\n\u0026lt;script setup\u0026gt; // declare emitted events const emitter = defineEmits([\u0026#39;response\u0026#39;,\u0026#39;test\u0026#39;]) // emit with argument emitter(\u0026#39;response\u0026#39;, \u0026#39;hello from child\u0026#39;) emitter(\u0026#39;test\u0026#39;, \u0026#39;testtest\u0026#39;) \u0026lt;/script\u0026gt; The first argument to emit() is the event name. Any additional arguments are passed on to the event listener.\nThe parent can listen to child-emitted events using v-on - here the handler receives the extra argument from the child emit call and assigns it to local state:\n\u0026lt;ChildComp @response=\u0026#34;(msg) =\u0026gt; childMsg = msg\u0026#34; /\u0026gt; Slots In addition to passing data via props, the parent component can also pass down template fragments to the child via slots.\nIn the child component, it can render the slot content from the parent using the \u0026lt;slot\u0026gt; element as outlet.\n//parent \u0026lt;template\u0026gt; \u0026lt;ChildComp\u0026gt; message to pass \u0026lt;/ChildComp\u0026gt; \u0026lt;/template\u0026gt; //child \u0026lt;template\u0026gt; \u0026lt;p\u0026gt; some word here \u0026lt;/p\u0026gt; \u0026lt;slot\u0026gt; message passed form parent will be present here, and this is the default words. \u0026lt;/slot\u0026gt; \u0026lt;/template\u0026gt; ","date":"2021-05-07T00:00:00Z","permalink":"https://dyhes.github.io/p/vuetutorial/","title":"【Vue】Tutorial"},{"content":"JSX JSX is a syntax extension to JavaScript. it is recomended to use with React to describe what the UI should look like. JSX may remind you of a template language, but it comes with the full power of JavaScript.\nSince JSX is closer to JavaScript than to HTML, React DOM uses camelCase property naming convention instead of HTML attribute names.\nBabel compiles JSX down to React.createElement() calls.\n//before const element = ( \u0026lt;h1 className=\u0026#34;greeting\u0026#34;\u0026gt; Hello, world! \u0026lt;/h1\u0026gt; ); //after compilation const element = React.createElement( \u0026#39;h1\u0026#39;, {className: \u0026#39;greeting\u0026#39;}, \u0026#39;Hello, world!\u0026#39; ); In-depth Choosing the Type at Runtime\nYou cannot use a general expression as the React element type. If you do want to use a general expression to indicate the type of the element, just assign it to a capitalized variable first. This often comes up when you want to render a different component based on a prop:\nimport React from \u0026#39;react\u0026#39;; import { PhotoStory, VideoStory } from \u0026#39;./stories\u0026#39;; const components = { photo: PhotoStory, video: VideoStory }; function Story(props) { // Correct! JSX type can be a capitalized variable. const SpecificStory = components[props.storyType]; return \u0026lt;SpecificStory story={props.story} /\u0026gt;; } Props default to \u0026rsquo;true'\n\u0026lt;MyTextBox autocomplete /\u0026gt; //equals to \u0026lt;MyTextBox autocomplete={true} /\u0026gt; In general, we don’t recommend not passing a value for a prop, because it can be confused with the ES6 object shorthand {foo} which is short for {foo: foo} rather than {foo: true}. This behavior is just there so that it matches the behavior of HTML.\nSpread Attributes\nIn general, we don’t recommend not passing a value for a prop, because it can be confused with the ES6 object shorthand {foo} which is short for {foo: foo} rather than {foo: true}. This behavior is just there so that it matches the behavior of HTML.\nfunction App1() { return \u0026lt;Greeting firstName=\u0026#34;Ben\u0026#34; lastName=\u0026#34;Hector\u0026#34; /\u0026gt;; } //equals to function App2() { const props = {firstName: \u0026#39;Ben\u0026#39;, lastName: \u0026#39;Hector\u0026#39;}; return \u0026lt;Greeting {...props} /\u0026gt;; } //eg2 const Button = props =\u0026gt; { const { kind, ...other } = props; const className = kind === \u0026#34;primary\u0026#34; ? \u0026#34;PrimaryButton\u0026#34; : \u0026#34;SecondaryButton\u0026#34;; return \u0026lt;button className={className} {...other} /\u0026gt;; }; Children\nIn JSX expressions that contain both an opening tag and a closing tag, the content between those tags is passed as a special prop: props.children.\nBoolean\nfalse, null, undefined, and true are valid children. They simply don’t render. This can be useful to conditionally render React elements.\n\u0026lt;div\u0026gt; {showHeader \u0026amp;\u0026amp; \u0026lt;Header /\u0026gt;} \u0026lt;Content /\u0026gt; \u0026lt;/div\u0026gt; Conversely, if you want a value like false, true, null, or undefined to appear in the output, you have to convert it to a string first:\n\u0026lt;div\u0026gt; My JavaScript variable is {String(myVariable)}. \u0026lt;/div\u0026gt; String Literals\nYou can put a string between the opening and closing tags and props.children will just be that string. HTML is unescaped, so you can generally write JSX just like you would write HTML.\n\u0026lt;MyComponent\u0026gt; \u0026lt;div\u0026gt;This is valid HTML \u0026amp;amp; JSX at the same time.\u0026lt;/div\u0026gt; \u0026lt;/MyComponent\u0026gt; JSX removes whitespace at the beginning and ending of a line. It also removes blank lines. New lines adjacent to tags are removed; new lines that occur in the middle of string literals are condensed into a single space.\nJSX\nYou can provide more JSX elements as the children. This is useful for displaying nested components.\n\u0026lt;MyContainer\u0026gt; \u0026lt;MyFirstComponent /\u0026gt; \u0026lt;MySecondComponent /\u0026gt; \u0026lt;/MyContainer\u0026gt; JS Expression\nYou can pass any JavaScript expression as children, by enclosing it within {}.\nComponents Components let you split the UI into independent, reusable pieces, and think about each piece in isolation.\nConceptually, components are like JavaScript functions. They accept arbitrary inputs (called “props”) and return React elements describing what should appear on the screen.\nNote: Always start component names with a capital letter.\nFunction Component\nfunction Welcome(props) { return \u0026lt;h1\u0026gt;Hello, {props.name}\u0026lt;/h1\u0026gt;; } const element = \u0026lt;Welcome name=\u0026#34;Sara\u0026#34; /\u0026gt;; ReactDOM.render( element, document.getElementById(\u0026#39;root\u0026#39;) ); Class Component\nclass Welcome extends React.Component { render() { return \u0026lt;h1\u0026gt;Hello, {this.props.name}\u0026lt;/h1\u0026gt;; } } const element = \u0026lt;Welcome/\u0026gt;; ReactDOM.render( element, document.getElementById(\u0026#39;root\u0026#39;) ) //with state class Clock extends React.Component { constructor(props) { super(props); this.state = {date: new Date()}; } render() { return ( \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;It is {this.state.date.toLocaleTimeString()}.\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; ); } } ReactDOM.render( \u0026lt;Clock /\u0026gt;, document.getElementById(\u0026#39;root\u0026#39;) ); //with method class Toggle extends React.Component { constructor(props) { super(props); this.state = {isToggleOn: true}; // This binding is necessary to make `this` work in the callback this.handleClick = this.handleClick.bind(this); } handleClick() { this.setState(prevState =\u0026gt; ({ isToggleOn: !prevState.isToggleOn })); } render() { return ( \u0026lt;button onClick={this.handleClick}\u0026gt; {this.state.isToggleOn ? \u0026#39;ON\u0026#39; : \u0026#39;OFF\u0026#39;} \u0026lt;/button\u0026gt; ); } } ReactDOM.render( \u0026lt;Toggle /\u0026gt;, document.getElementById(\u0026#39;root\u0026#39;) ); Lifecycle Methods include less used method\n[more detail\u0026hellip;](React lifecycle methods diagram (wojtekmaj.pl))\nHooks Hooks are a new addition in React 16.8. They let you use state and other React features without writing a class.\nuseState const [state, setState] = useState(initialState); Returns a stateful value, and a function to update it.\nDuring the initial render, the returned state (state) is the same as the value passed as the first argument (initialState).\nThe setState function is used to update the state. It accepts a new state value and enqueues a re-render of the component.\nIf the new state is computed using the previous state, you can pass a function to setState. The function will receive the previous value, and return an updated value.\n\u0026lt;button onClick={() =\u0026gt; setCount(prevCount =\u0026gt; prevCount - 1)}\u0026gt;-\u0026lt;/button\u0026gt; Unlike the setState method found in class components, useState does not automatically merge update objects. You can replicate this behavior by combining the function updater form with object spread syntax:\nconst [state, setState] = useState({}); setState(prevState =\u0026gt; { // Object.assign would also work return {...prevState, ...updatedValues}; }); Another option is useReducer, which is more suited for managing state objects that contain multiple sub-values.\nThe initialState argument is the state used during the initial render. In subsequent renders, it is disregarded. If the initial state is the result of an expensive computation, you may provide a function instead, which will be executed only on the initial render:\nconst [state, setState] = useState(() =\u0026gt; { const initialState = someExpensiveComputation(props); return initialState; }); If you update a State Hook to the same value as the current state, React will bail out without rendering the children or firing effects.\nNote that React may still need to render that specific component again before bailing out. That shouldn’t be a concern because React won’t unnecessarily go “deeper” into the tree. If you’re doing expensive calculations while rendering, you can optimize them with useMemo.\nuseEffect useEffect(didUpdate); Accepts a function that contains imperative, possibly effectful code.\nMutations, subscriptions, timers, logging, and other side effects are not allowed inside the main body of a function component (referred to as React’s render phase). Doing so will lead to confusing bugs and inconsistencies in the UI.\nInstead, use useEffect. The function passed to useEffect will run after the render is committed to the screen. Think of effects as an escape hatch from React’s purely functional world into the imperative world.\nBy default, effects run after every completed render, but you can choose to fire them only when certain values have changed.\nCleaning up an effect Often, effects create resources that need to be cleaned up before the component leaves the screen, such as a subscription or timer ID. To do this, the function passed to useEffect may return a clean-up function. For example, to create a subscription:\nuseEffect(() =\u0026gt; { const subscription = props.source.subscribe(); return () =\u0026gt; { // Clean up the subscription subscription.unsubscribe(); }; }); The clean-up function runs before the component is removed from the UI to prevent memory leaks. Additionally, if a component renders multiple times (as they typically do), the previous effect is cleaned up before executing the next effect. In our example, this means a new subscription is created on every update. To avoid firing an effect on every update, refer to the next section.\nTiming of effects Unlike componentDidMount and componentDidUpdate, the function passed to useEffect fires after layout and paint, during a deferred event. This makes it suitable for the many common side effects, like setting up subscriptions and event handlers, because most types of work shouldn’t block the browser from updating the screen.\nHowever, not all effects can be deferred. For example, a DOM mutation that is visible to the user must fire synchronously before the next paint so that the user does not perceive a visual inconsistency. (The distinction is conceptually similar to passive versus active event listeners.) For these types of effects, React provides one additional Hook called useLayoutEffect. It has the same signature as useEffect, and only differs in when it is fired.\nAdditionally, starting in React 18, the function passed to useEffect will fire synchronously before layout and paint when it’s the result of a discrete user input such as a click, or when it’s the result of an update wrapped in flushSync. This behavior allows the result of the effect to be observed by the event system, or by the caller of flushSync.\nNote : This only affects the timing of when the function passed to useEffect is called - updates scheduled inside these effects are still deferred. This is different than useLayoutEffect, which fires the function and processes the updates inside of it immediately.\nEven in cases where useEffect is deferred until after the browser has painted, it’s guaranteed to fire before any new renders. React will always flush a previous render’s effects before starting a new update.\nConditionally firing an effect The default behavior for effects is to fire the effect after every completed render. That way an effect is always recreated if one of its dependencies changes.\nHowever, this may be overkill in some cases, like the subscription example from the previous section. We don’t need to create a new subscription on every update, only if the source prop has changed.\nTo implement this, pass a second argument to useEffect that is the array of values that the effect depends on. Our updated example now looks like this:\nuseEffect( () =\u0026gt; { const subscription = props.source.subscribe(); return () =\u0026gt; { subscription.unsubscribe(); }; }, [props.source], ); Now the subscription will only be recreated when props.source changes.\nNote\nIf you use this optimization, make sure the array includes all values from the component scope (such as props and state) that change over time and that are used by the effect. Otherwise, your code will reference stale values from previous renders. Learn more about how to deal with functions and what to do when the array values change too often.\nIf you want to run an effect and clean it up only once (on mount and unmount), you can pass an empty array ([]) as a second argument. This tells React that your effect doesn’t depend on any values from props or state, so it never needs to re-run. This isn’t handled as a special case — it follows directly from how the dependencies array always works.\nIf you pass an empty array ([]), the props and state inside the effect will always have their initial values. While passing [] as the second argument is closer to the familiar componentDidMount and componentWillUnmount mental model, there are usually better solutions to avoid re-running effects too often. Also, don’t forget that React defers running useEffect until after the browser has painted, so doing extra work is less of a problem.\nWe recommend using the exhaustive-deps rule as part of our eslint-plugin-react-hooks package. It warns when dependencies are specified incorrectly and suggests a fix.\nThe array of dependencies is not passed as arguments to the effect function. Conceptually, though, that’s what they represent: every value referenced inside the effect function should also appear in the dependencies array. In the future, a sufficiently advanced compiler could create this array automatically.\nuseContext const value = useContext(MyContext); Accepts a context object (the value returned from React.createContext) and returns the current context value for that context. The current context value is determined by the value prop of the nearest \u0026lt;MyContext.Provider\u0026gt; above the calling component in the tree.\nDon’t forget that the argument to useContext must be the context object itself:\nCorrect: useContext(MyContext) Incorrect: useContext(MyContext.Consumer) Incorrect: useContext(MyContext.Provider) Rules of Hooks Hooks are JavaScript functions, but you need to follow two rules when using them.\nOnly Call Hooks at the Top Level\nDon’t call Hooks inside loops, conditions, or nested functions. Instead, always use Hooks at the top level of your React function, before any early returns. By following this rule, you ensure that Hooks are called in the same order each time a component renders. That’s what allows React to correctly preserve the state of Hooks between multiple useState and useEffect calls.\nOnly Call Hooks from React Functions\nDon’t call Hooks from regular JavaScript functions. Instead, you can:\n✅ Call Hooks from React function components. ✅ Call Hooks from custom Hooks By following this rule, you ensure that all stateful logic in a component is clearly visible from its source code.\nCustom Hooks Building your own Hooks lets you extract component logic into reusable functions.\nTraditionally in React, we’ve had two popular ways to share stateful logic between components: render props and higher-order components. We will now look at how Hooks solve many of the same problems without forcing you to add more components to the tree.\nA custom Hook is a JavaScript function whose name starts with ”use” and that may call other Hooks.\nimport { useState, useEffect } from \u0026#39;react\u0026#39;; function useFriendStatus(friendID) { const [isOnline, setIsOnline] = useState(null); useEffect(() =\u0026gt; { function handleStatusChange(status) { setIsOnline(status.isOnline); } ChatAPI.subscribeToFriendStatus(friendID, handleStatusChange); return () =\u0026gt; { ChatAPI.unsubscribeFromFriendStatus(friendID, handleStatusChange); }; }); return isOnline; } function FriendStatus(props) { const isOnline = useFriendStatus(props.friend.id); if (isOnline === null) { return \u0026#39;Loading...\u0026#39;; } return isOnline ? \u0026#39;Online\u0026#39; : \u0026#39;Offline\u0026#39;; } function FriendListItem(props) { const isOnline = useFriendStatus(props.friend.id); return ( \u0026lt;li style={{ color: isOnline ? \u0026#39;green\u0026#39; : \u0026#39;black\u0026#39; }}\u0026gt; {props.friend.name} \u0026lt;/li\u0026gt; ); } Unlike a React component, a custom Hook doesn’t need to have a specific signature. We can decide what it takes as arguments, and what, if anything, it should return. In other words, it’s just like a normal function.\nLists function NumberList(props) { const numbers = props.numbers; const listItems = numbers.map((number) =\u0026gt; \u0026lt;li key={number.toString()}\u0026gt; {number} \u0026lt;/li\u0026gt; ); return ( \u0026lt;ul\u0026gt;{listItems}\u0026lt;/ul\u0026gt; ); } const numbers = [1, 2, 3, 4, 5]; ReactDOM.render( \u0026lt;NumberList numbers={numbers} /\u0026gt;, document.getElementById(\u0026#39;root\u0026#39;) ); //using index const todoItems = todos.map((todo, index) =\u0026gt; // Only do this if items have no stable IDs \u0026lt;li key={index}\u0026gt; {todo.text} \u0026lt;/li\u0026gt; ); //We don’t recommend using indexes for keys if the order of items may change. This can negatively impact performance and may cause issues with component state. Keys help React identify which items have changed, are added, or are removed.\nForms Controlled Components In HTML, form elements such as \u0026lt;input\u0026gt;, \u0026lt;textarea\u0026gt;, and \u0026lt;select\u0026gt; typically maintain their own state and update it based on user input. In React, mutable state is typically kept in the state property of components, and only updated with setState().\nWe can combine the two by making the React state be the “single source of truth”. Then the React component that renders a form also controls what happens in that form on subsequent user input. An input form element whose value is controlled by React in this way is called a “controlled component”.\nclass NameForm extends React.Component { constructor(props) { super(props); this.state = {value: \u0026#39;\u0026#39;}; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); } handleChange(event) { this.setState({value: event.target.value}); } handleSubmit(event) { alert(\u0026#39;A name was submitted: \u0026#39; + this.state.value); event.preventDefault(); } render() { return ( \u0026lt;form onSubmit={this.handleSubmit}\u0026gt; \u0026lt;label\u0026gt; Name: \u0026lt;input type=\u0026#34;text\u0026#34; value={this.state.value} onChange={this.handleChange} /\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Submit\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; ); } } Since the value attribute is set on our form element, the displayed value will always be this.state.value, making the React state the source of truth. Since handleChange runs on every keystroke to update the React state, the displayed value will update as the user types.\nIt can sometimes be tedious to use controlled components, because you need to write an event handler for every way your data can change and pipe all of the input state through a React component. This can become particularly annoying when you are converting a preexisting codebase to React, or integrating a React application with a non-React library. In these situations, we can use uncontrolled components, an alternative technique for implementing input forms.\nUncontrolled Components In most cases, we recommend using controlled components to implement forms. In a controlled component, form data is handled by a React component. The alternative is uncontrolled components, where form data is handled by the DOM itself.\nTo write an uncontrolled component, instead of writing an event handler for every state update, you can use a ref to get form values from the DOM.\nIn the React rendering lifecycle, the value attribute on form elements will override the value in the DOM. With an uncontrolled component, you often want React to specify the initial value, but leave subsequent updates uncontrolled. To handle this case, you can specify a defaultValue attribute instead of value. Changing the value of defaultValue attribute after a component has mounted will not cause any update of the value in the DOM.\nclass NameForm extends React.Component { constructor(props) { super(props); this.handleSubmit = this.handleSubmit.bind(this); this.input = React.createRef(); } handleSubmit(event) { alert(\u0026#39;A name was submitted: \u0026#39; + this.input.current.value); event.preventDefault(); } render() { return ( \u0026lt;form onSubmit={this.handleSubmit}\u0026gt; \u0026lt;label\u0026gt; Name: \u0026lt;input defaultValue=\u0026#34;Bob\u0026#34; type=\u0026#34;text\u0026#34; ref={this.input} /\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Submit\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; ); } } Lifting state up In React, sharing state is accomplished by moving it up to the closest common ancestor of the components that need it. This is called “lifting state up”.\nContext Context provides a way to pass data through the component tree without having to pass props down manually at every level.\nContext is designed to share data that can be considered “global” for a tree of React components, such as the current authenticated user, theme, or preferred language.\nContext is primarily used when some data needs to be accessible by many components at different nesting levels. Apply it sparingly because it makes component reuse more difficult. If you only want to avoid passing some props through many levels, component composition is often a simpler solution than context.\nUsing context, we can avoid passing props through intermediate elements:\n// Context lets us pass a value deep into the component tree without explicitly threading it through every component. // Create a context for the current theme (with \u0026#34;light\u0026#34; as the default). const ThemeContext = React.createContext(\u0026#39;light\u0026#39;); class App extends React.Component { render() { // Use a Provider to pass the current theme to the tree below. // Any component can read it, no matter how deep it is. // In this example, we\u0026#39;re passing \u0026#34;dark\u0026#34; as the current value. return ( \u0026lt;ThemeContext.Provider value=\u0026#34;dark\u0026#34;\u0026gt; \u0026lt;Toolbar /\u0026gt; \u0026lt;/ThemeContext.Provider\u0026gt; ); } } // A component in the middle doesn\u0026#39;t have to // pass the theme down explicitly anymore.function Toolbar() { return ( \u0026lt;div\u0026gt; \u0026lt;ThemedButton /\u0026gt; \u0026lt;/div\u0026gt; ); } class ThemedButton extends React.Component { // Assign a contextType to read the current theme context. // React will find the closest theme Provider above and use its value. // In this example, the current theme is \u0026#34;dark\u0026#34;. static contextType = ThemeContext; render() { return \u0026lt;Button theme={this.context} /\u0026gt;; } } API React.createContext const MyContext = React.createContext(defaultValue); Creates a Context object. When React renders a component that subscribes to this Context object it will read the current context value from the closest matching Provider above it in the tree.\nThe defaultValue argument is only used when a component does not have a matching Provider above it in the tree. This default value can be helpful for testing components in isolation without wrapping them. Note: passing undefined as a Provider value does not cause consuming components to use defaultValue.\nContext.Provider \u0026lt;MyContext.Provider value={/* some value */}\u0026gt; Every Context object comes with a Provider React component that allows consuming components to subscribe to context changes.\nThe Provider component accepts a value prop to be passed to consuming components that are descendants of this Provider. One Provider can be connected to many consumers. Providers can be nested to override values deeper within the tree.\nAll consumers that are descendants of a Provider will re-render whenever the Provider’s value prop changes. The propagation from Provider to its descendant consumers (including .contextType and useContext) is not subject to the shouldComponentUpdate method, so the consumer is updated even when an ancestor component skips an update.\nClass.contextType const MyContext = React.createContext(defaultValue); class MyClass extends React.Component { static contextType = MyContext; render() { let value = this.context; /* render something based on the value */ } } The contextType property on a class can be assigned a Context object created by React.createContext(). Using this property lets you consume the nearest current value of that Context type using this.context. You can reference this in any of the lifecycle methods including the render function.\nYou can only subscribe to a single context using this API. If you need to read more than one see Consuming Multiple Contexts.\nContext.Consumer \u0026lt;MyContext.Consumer\u0026gt; {value =\u0026gt; /* render something based on the context value */} \u0026lt;/MyContext.Consumer\u0026gt; A React component that subscribes to context changes. Using this component lets you subscribe to a context within a function component.\nRequires a function as a child. The function receives the current context value and returns a React node. The value argument passed to the function will be equal to the value prop of the closest Provider for this context above in the tree. If there is no Provider for this context above, the value argument will be equal to the defaultValue that was passed to createContext().\nContext.displayName Context object accepts a displayName string property. React DevTools uses this string to determine what to display for the context.\nFor example, the following component will appear as MyDisplayName in the DevTools:\nconst MyContext = React.createContext(/* some value */); MyContext.displayName = \u0026#39;MyDisplayName\u0026#39;; \u0026lt;MyContext.Provider\u0026gt; // \u0026#34;MyDisplayName.Provider\u0026#34; in DevTools \u0026lt;MyContext.Consumer\u0026gt; // \u0026#34;MyDisplayName.Consumer\u0026#34; in DevTools Error Boundaries A JavaScript error in a part of the UI shouldn’t break the whole app. To solve this problem for React users, React 16 introduces a new concept of an “error boundary”.\nError boundaries are React components that catch JavaScript errors anywhere in their child component tree, log those errors, and display a fallback UI instead of the component tree that crashed. Error boundaries catch errors during rendering, in lifecycle methods, and in constructors of the whole tree below them.\nError boundaries do not catch errors for:\nEvent handlers React doesn’t need error boundaries to recover from errors in event handlers. Unlike the render method and lifecycle methods, the event handlers don’t happen during rendering. So if they throw, React still knows what to display on the screen. If you need to catch an error inside an event handler, use the regular JavaScript try / catch statement\nAsynchronous code (e.g. setTimeout or requestAnimationFrame callbacks)\nServer side rendering\nErrors thrown in the error boundary itself (rather than its children)\nA class component becomes an error boundary if it defines either (or both) of the lifecycle methods static getDerivedStateFromError() or componentDidCatch(). Use static getDerivedStateFromError() to render a fallback UI after an error has been thrown. Use componentDidCatch() to log error information.\nclass ErrorBoundary extends React.Component { constructor(props) { super(props); this.state = { hasError: false }; } static getDerivedStateFromError(error) { // Update state so the next render will show the fallback UI. return { hasError: true }; } componentDidCatch(error, errorInfo) { // You can also log the error to an error reporting service logErrorToMyService(error, errorInfo); } render() { if (this.state.hasError) { // You can render any custom fallback UI return \u0026lt;h1\u0026gt;Something went wrong.\u0026lt;/h1\u0026gt;; } return this.props.children; } } \u0026lt;ErrorBoundary\u0026gt; \u0026lt;MyWidget /\u0026gt; \u0026lt;/ErrorBoundary\u0026gt; Error boundaries work like a JavaScript catch {} block, but for components. Only class components can be error boundaries. In practice, most of the time you’ll want to declare an error boundary component once and use it throughout your application.\nNote that error boundaries only catch errors in the components below them in the tree. An error boundary can’t catch an error within itself. If an error boundary fails trying to render the error message, the error will propagate to the closest error boundary above it.\nAs of React 16, errors that were not caught by any error boundary will result in unmounting of the whole React component tree.\nRef Refs provide a way to access DOM nodes or React elements created in the render method.\nIn the typical React dataflow, props are the only way that parent components interact with their children. To modify a child, you re-render it with new props. However, there are a few cases where you need to imperatively modify a child outside of the typical dataflow.\nThere are a few good use cases for refs:\nManaging focus, text selection, or media playback. Triggering imperative animations. Integrating with third-party DOM libraries. Avoid using refs for anything that can be done declaratively！\nCreate Refs are created using React.createRef() and attached to React elements via the ref attribute. Refs are commonly assigned to an instance property when a component is constructed so they can be referenced throughout the component.\nclass MyComponent extends React.Component { constructor(props) { super(props); this.myRef = React.createRef(); } render() { return \u0026lt;div ref={this.myRef} /\u0026gt;; } By default, you may not use the ref attribute on function components because they don’t have instances:\nIf you want to allow people to take a ref to your function component, you can\nuse forwardRef (possibly in conjunction with useImperativeHandle), or convert the component to a class. You can, however, use the ref attribute inside a function component as long as you refer to a DOM element or a class component\nfunction CustomTextInput(props) { // textInput must be declared here so the ref can refer to it const textInput = useRef(null); function handleClick() { textInput.current.focus(); } return ( \u0026lt;div\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; ref={textInput} /\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;Focus the text input\u0026#34; onClick={handleClick} /\u0026gt; \u0026lt;/div\u0026gt; ); } Access The value of the ref differs depending on the type of the node:\nWhen the ref attribute is used on an HTML element, the ref created in the constructor with React.createRef() receives the underlying DOM element as its current property. When the ref attribute is used on a custom class component, the ref object receives the mounted instance of the component as its current. You may not use the ref attribute on function components because they don’t have instances. Ref forwarding Ref forwarding is a technique for automatically passing a ref through a component to one of its children. Ref forwarding lets components opt into exposing any child component’s ref as their own.\nRef forwarding is an opt-in feature that lets some components take a ref they receive, and pass it further down (in other words, “forward” it) to a child.\nIn the example below, FancyButton uses React.forwardRef to obtain the ref passed to it, and then forward it to the DOM button that it renders:\nconst FancyButton = React.forwardRef((props, ref) =\u0026gt; ( \u0026lt;button ref={ref} className=\u0026#34;FancyButton\u0026#34;\u0026gt; {props.children} \u0026lt;/button\u0026gt; )); // You can now get a ref directly to the DOM button: const ref = React.createRef(); \u0026lt;FancyButton ref={ref}\u0026gt;Click me!\u0026lt;/FancyButton\u0026gt;; This way, components using FancyButton can get a ref to the underlying button DOM node and access it if necessary—just like if they used a DOM button directly.\nNote: The second ref argument only exists when you define a component with React.forwardRef call. Regular function or class components don’t receive the ref argument, and ref is not available in props either.\nRef forwarding is not limited to DOM components. You can forward refs to class component instances, too.\nCallback Refs React also supports another way to set refs called “callback refs”, which gives more fine-grain control over when refs are set and unset.\nInstead of passing a ref attribute created by createRef(), you pass a function. The function receives the React component instance or HTML DOM element as its argument, which can be stored and accessed elsewhere.\nclass CustomTextInput extends React.Component { constructor(props) { super(props); this.textInput = null; this.setTextInputRef = element =\u0026gt; { this.textInput = element; }; this.focusTextInput = () =\u0026gt; { // Focus the text input using the raw DOM API if (this.textInput) this.textInput.focus(); }; } componentDidMount() { // autofocus the input on mount this.focusTextInput(); } render() { // Use the `ref` callback to store a reference to the text input DOM // element in an instance field (for example, this.textInput). return ( \u0026lt;div\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; ref={this.setTextInputRef} /\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;Focus the text input\u0026#34; onClick={this.focusTextInput} /\u0026gt; \u0026lt;/div\u0026gt; ); } } React will call the ref callback with the DOM element when the component mounts, and call it with null when it unmounts. Refs are guaranteed to be up-to-date before componentDidMount or componentDidUpdate fires.\nNOTE: If the ref callback is defined as an inline function, it will get called twice during updates, first with null and then again with the DOM element. This is because a new instance of the function is created with each render, so React needs to clear the old ref and set up the new one. You can avoid this by defining the ref callback as a bound method on the class, but note that it shouldn’t matter in most cases.\nYou can pass callback refs between components like you can with object refs that were created with React.createRef().\nfunction CustomTextInput(props) { return ( \u0026lt;div\u0026gt; \u0026lt;input ref={props.inputRef} /\u0026gt; \u0026lt;/div\u0026gt; ); } class Parent extends React.Component { render() { return ( \u0026lt;CustomTextInput inputRef={el =\u0026gt; this.inputElement = el} /\u0026gt; ); } } In the example above, Parent passes its ref callback as an inputRef prop to the CustomTextInput, and the CustomTextInput passes the same function as a special ref attribute to the \u0026lt;input\u0026gt;. As a result, this.inputElement in Parent will be set to the DOM node corresponding to the \u0026lt;input\u0026gt; element in the CustomTextInput.\nLegacy API: String Refs If you worked with React before, you might be familiar with an older API where the ref attribute is a string, like \u0026quot;textInput\u0026quot;, and the DOM node is accessed as this.refs.textInput. We advise against it because string refs have some issues, are considered legacy, and are likely to be removed in one of the future releases.\nFragment Fragment is a common pattern in React is for a component to return multiple elements. Fragments let you group a list of children without adding extra nodes to the DOM.\nrender() { return ( \u0026lt;React.Fragment\u0026gt; \u0026lt;ChildA /\u0026gt; \u0026lt;ChildB /\u0026gt; \u0026lt;ChildC /\u0026gt; \u0026lt;/React.Fragment\u0026gt; ); } short syntax\nclass Columns extends React.Component { render() { return ( \u0026lt;\u0026gt; \u0026lt;td\u0026gt;Hello\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;World\u0026lt;/td\u0026gt; \u0026lt;/\u0026gt; ); } } You can use \u0026lt;\u0026gt;\u0026lt;/\u0026gt; the same way you’d use any other element except that it doesn’t support keys or attributes.\nKeyed Fragments Fragments declared with the explicit \u0026lt;React.Fragment\u0026gt; syntax may have keys. A use case for this is mapping a collection to an array of fragments — for example, to create a description list:\nfunction Glossary(props) { return ( \u0026lt;dl\u0026gt; {props.items.map(item =\u0026gt; ( // Without the `key`, React will fire a key warning \u0026lt;React.Fragment key={item.id}\u0026gt; \u0026lt;dt\u0026gt;{item.term}\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt;{item.description}\u0026lt;/dd\u0026gt; \u0026lt;/React.Fragment\u0026gt; ))} \u0026lt;/dl\u0026gt; ); } key is the only attribute that can be passed to Fragment.\nHigh-Order Components A higher-order component (HOC) is an advanced technique in React for reusing component logic. HOCs are not part of the React API, per se. They are a pattern that emerges from React’s compositional nature.\nConcretely, a higher-order component is a function that takes a component and returns a new component.\nconst EnhancedComponent = higherOrderComponent(WrappedComponent); HOCs are common in third-party React libraries, such as Redux’s connect and Relay’s createFragmentContainer.\nFor example we can imagine that in a large app, the same pattern of subscribing to DataSource and calling setState will occur over and over again. We want an abstraction that allows us to define this logic in a single place and share it across many components. This is where higher-order components excel.\n// This function takes a component... function withSubscription(WrappedComponent, selectData) { // ...and returns another component... return class extends React.Component { constructor(props) { super(props); this.handleChange = this.handleChange.bind(this); this.state = { data: selectData(DataSource, props) }; } componentDidMount() { // ... that takes care of the subscription... DataSource.addChangeListener(this.handleChange); } componentWillUnmount() { DataSource.removeChangeListener(this.handleChange); } handleChange() { this.setState({ data: selectData(DataSource, this.props) }); } render() { // ... and renders the wrapped component with the fresh data! // Notice that we pass through any additional props return \u0026lt;WrappedComponent data={this.state.data} {...this.props} /\u0026gt;; } }; } const CommentListWithSubscription = withSubscription( CommentList, (DataSource) =\u0026gt; DataSource.getComments() ); const BlogPostWithSubscription = withSubscription( BlogPost, (DataSource, props) =\u0026gt; DataSource.getBlogPost(props.id) ); The first parameter is the wrapped component. The second parameter retrieves the data we’re interested in, given a DataSource and the current props.\nWhen CommentListWithSubscription and BlogPostWithSubscription are rendered, CommentList and BlogPost will be passed a data prop with the most current data retrieved from DataSource:\nNote that a HOC doesn’t modify the input component, nor does it use inheritance to copy its behavior. Rather, a HOC composes the original component by wrapping it in a container component. A HOC is a pure function with zero side-effects.\nAnd that’s it! The wrapped component receives all the props of the container, along with a new prop, data, which it uses to render its output. The HOC isn’t concerned with how or why the data is used, and the wrapped component isn’t concerned with where the data came from.\nCaveat Don’t Use HOCs Inside the render Method\nInstead, apply HOCs outside the component definition so that the resulting component is created only once. Then, its identity will be consistent across renders. This is usually what you want, anyway.\nStatic Methods Must Be Copied Over\nfunction enhance(WrappedComponent) { class Enhance extends React.Component {/*...*/} // Must know exactly which method(s) to copy :( Enhance.staticMethod = WrappedComponent.staticMethod; return Enhance; } //or import hoistNonReactStatic from \u0026#39;hoist-non-react-statics\u0026#39;; function enhance(WrappedComponent) { class Enhance extends React.Component {/*...*/} hoistNonReactStatic(Enhance, WrappedComponent); return Enhance; } Refs Aren’t Passed Through\nWhile the convention for higher-order components is to pass through all props to the wrapped component, this does not work for refs. That’s because ref is not really a prop — like key, it’s handled specially by React. If you add a ref to an element whose component is the result of a HOC, the ref refers to an instance of the outermost container component, not the wrapped component.\nFortunately, we can explicitly forward refs to the inner FancyButton component using the React.forwardRef API.\nfunction logProps(Component) { class LogProps extends React.Component { componentDidUpdate(prevProps) { console.log(\u0026#39;old props:\u0026#39;, prevProps); console.log(\u0026#39;new props:\u0026#39;, this.props); } render() { const {forwardedRef, ...rest} = this.props; // Assign the custom prop \u0026#34;forwardedRef\u0026#34; as a ref return \u0026lt;Component ref={forwardedRef} {...rest} /\u0026gt;; } } // Note the second param \u0026#34;ref\u0026#34; provided by React.forwardRef. // We can pass it along to LogProps as a regular prop, e.g. \u0026#34;forwardedRef\u0026#34; // And it can then be attached to the Component. return React.forwardRef((props, ref) =\u0026gt; { return \u0026lt;LogProps {...props} forwardedRef={ref} /\u0026gt;; }); } Render Props A render prop is a function prop that a component uses to know what to render.\nThis technique makes the behavior that we need to share extremely portable. To get that behavior, render a \u0026lt;Mouse\u0026gt; with a render prop that tells it what to render with the current (x, y) of the cursor.\nOne interesting thing to note about render props is that you can implement most higher-order components (HOC) using a regular component with a render prop.\nclass Cat extends React.Component { render() { const mouse = this.props.mouse; return ( \u0026lt;img src=\u0026#34;/cat.jpg\u0026#34; style={{ position: \u0026#39;absolute\u0026#39;, left: mouse.x, top: mouse.y }} /\u0026gt; ); } } class Mouse extends React.Component { constructor(props) { super(props); this.handleMouseMove = this.handleMouseMove.bind(this); this.state = { x: 0, y: 0 }; } handleMouseMove(event) { this.setState({ x: event.clientX, y: event.clientY }); } render() { return ( \u0026lt;div style={{ height: \u0026#39;100vh\u0026#39; }} onMouseMove={this.handleMouseMove}\u0026gt; {/* Instead of providing a static representation of what \u0026lt;Mouse\u0026gt; renders, use the `render` prop to dynamically determine what to render. */} {this.props.render(this.state)} \u0026lt;/div\u0026gt; ); } } class MouseTracker extends React.Component { render() { return ( \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;Move the mouse around!\u0026lt;/h1\u0026gt; \u0026lt;Mouse render={mouse =\u0026gt; ( \u0026lt;Cat mouse={mouse} /\u0026gt; )}/\u0026gt; \u0026lt;/div\u0026gt; ); } } It’s important to remember that just because the pattern is called “render props” you don’t have to use a prop named render to use this pattern. In fact, any prop that is a function that a component uses to know what to render is technically a “render prop”.\nStrict Mode StrictMode is a tool for highlighting potential problems in an application. Like Fragment, StrictMode does not render any visible UI. It activates additional checks and warnings for its descendants.\nNote: Strict mode checks are run in development mode only; they do not impact the production build.\nYou can enable strict mode for any part of your application.\nimport React from \u0026#39;react\u0026#39;; function ExampleApplication() { return ( \u0026lt;div\u0026gt; \u0026lt;Header /\u0026gt; \u0026lt;React.StrictMode\u0026gt; \u0026lt;div\u0026gt; \u0026lt;ComponentOne /\u0026gt; \u0026lt;ComponentTwo /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/React.StrictMode\u0026gt; \u0026lt;Footer /\u0026gt; \u0026lt;/div\u0026gt; ); } StrictMode currently helps with:\nIdentifying components with unsafe lifecycles Warning about legacy string ref API usage Warning about deprecated findDOMNode usage Detecting unexpected side effects Detecting legacy context API Detecting unsafe effects PropTypes As your app grows, you can catch a lot of bugs with typechecking. For some applications, you can use JavaScript extensions like Flow or TypeScript to typecheck your whole application. But even if you don’t use those, React has some built-in typechecking abilities. To run typechecking on the props for a component, you can assign the special propTypes property:\nimport PropTypes from \u0026#39;prop-types\u0026#39;; class Greeting extends React.Component { render() { return ( \u0026lt;h1\u0026gt;Hello, {this.props.name}\u0026lt;/h1\u0026gt; ); } } Greeting.propTypes = { name: PropTypes.string }; Note : React.PropTypes has moved into a different package since React v15.5. Please use the prop-types library instead.\nWhen an invalid value is provided for a prop, a warning will be shown in the JavaScript console. For performance reasons, propTypes is only checked in development mode.\nimport PropTypes from \u0026#39;prop-types\u0026#39;; MyComponent.propTypes = { // You can declare that a prop is a specific JS type. By default, these // are all optional. optionalArray: PropTypes.array, optionalBool: PropTypes.bool, optionalFunc: PropTypes.func, optionalNumber: PropTypes.number, optionalObject: PropTypes.object, optionalString: PropTypes.string, optionalSymbol: PropTypes.symbol, // Anything that can be rendered: numbers, strings, elements or an array // (or fragment) containing these types. optionalNode: PropTypes.node, // A React element. optionalElement: PropTypes.element, // A React element type (ie. MyComponent). optionalElementType: PropTypes.elementType, // You can also declare that a prop is an instance of a class. This uses // JS\u0026#39;s instanceof operator. optionalMessage: PropTypes.instanceOf(Message), // You can ensure that your prop is limited to specific values by treating // it as an enum. optionalEnum: PropTypes.oneOf([\u0026#39;News\u0026#39;, \u0026#39;Photos\u0026#39;]), // An object that could be one of many types optionalUnion: PropTypes.oneOfType([ PropTypes.string, PropTypes.number, PropTypes.instanceOf(Message) ]), // An array of a certain type optionalArrayOf: PropTypes.arrayOf(PropTypes.number), // An object with property values of a certain type optionalObjectOf: PropTypes.objectOf(PropTypes.number), // An object taking on a particular shape optionalObjectWithShape: PropTypes.shape({ color: PropTypes.string, fontSize: PropTypes.number }), // An object with warnings on extra properties optionalObjectWithStrictShape: PropTypes.exact({ name: PropTypes.string, quantity: PropTypes.number }), // You can chain any of the above with `isRequired` to make sure a warning // is shown if the prop isn\u0026#39;t provided. requiredFunc: PropTypes.func.isRequired, // A required value of any data type requiredAny: PropTypes.any.isRequired, // You can also specify a custom validator. It should return an Error // object if the validation fails. Don\u0026#39;t `console.warn` or throw, as this // won\u0026#39;t work inside `oneOfType`. customProp: function(props, propName, componentName) { if (!/matchme/.test(props[propName])) { return new Error( \u0026#39;Invalid prop `\u0026#39; + propName + \u0026#39;` supplied to\u0026#39; + \u0026#39; `\u0026#39; + componentName + \u0026#39;`. Validation failed.\u0026#39; ); } }, // You can also supply a custom validator to `arrayOf` and `objectOf`. // It should return an Error object if the validation fails. The validator // will be called for each key in the array or object. The first two // arguments of the validator are the array or object itself, and the // current item\u0026#39;s key. customArrayProp: PropTypes.arrayOf(function(propValue, key, componentName, location, propFullName) { if (!/matchme/.test(propValue[key])) { return new Error( \u0026#39;Invalid prop `\u0026#39; + propFullName + \u0026#39;` supplied to\u0026#39; + \u0026#39; `\u0026#39; + componentName + \u0026#39;`. Validation failed.\u0026#39; ); } }) }; Single Child With PropTypes.element you can specify that only a single child can be passed to a component as children.\nimport PropTypes from \u0026#39;prop-types\u0026#39;; class MyComponent extends React.Component { render() { // This must be exactly one element or it will warn. const children = this.props.children; return ( \u0026lt;div\u0026gt; {children} \u0026lt;/div\u0026gt; ); } } MyComponent.propTypes = { children: PropTypes.element.isRequired }; Default Prop Values You can define default values for your props by assigning to the special defaultProps property:\nclass Greeting extends React.Component { render() { return ( \u0026lt;h1\u0026gt;Hello, {this.props.name}\u0026lt;/h1\u0026gt; ); } } // Specifies the default values for props: Greeting.defaultProps = { name: \u0026#39;Stranger\u0026#39; }; // Renders \u0026#34;Hello, Stranger\u0026#34;: const root = ReactDOM.createRoot(document.getElementById(\u0026#39;example\u0026#39;)); root.render(\u0026lt;Greeting /\u0026gt;); //or class Greeting extends React.Component { static defaultProps = { name: \u0026#39;stranger\u0026#39; } render() { return ( \u0026lt;div\u0026gt;Hello, {this.props.name}\u0026lt;/div\u0026gt; ) } } The defaultProps will be used to ensure that this.props.name will have a value if it was not specified by the parent component. The propTypes typechecking happens after defaultProps are resolved, so typechecking will also apply to the defaultProps.\nFunction Components If you are using function components in your regular development, you may want to make some small changes to allow PropTypes to be properly applied.\nimport PropTypes from \u0026#39;prop-types\u0026#39; function HelloWorldComponent({ name }) { return ( \u0026lt;div\u0026gt;Hello, {name}\u0026lt;/div\u0026gt; ) } HelloWorldComponent.propTypes = { name: PropTypes.string } export default HelloWorldComponent Portals Portals provide a first-class way to render children into a DOM node that exists outside the DOM hierarchy of the parent component.\nReactDOM.createPortal(child, container) The first argument (child) is any renderable React child, such as an element, string, or fragment. The second argument (container) is a DOM element.\nNormally, when you return an element from a component’s render method, it’s mounted into the DOM as a child of the nearest parent node. However, sometimes it’s useful to insert a child into a different location in the DOM. A typical use case for portals is when a parent component has an overflow: hidden or z-index style, but you need the child to visually “break out” of its container.\nEven though a portal can be anywhere in the DOM tree, it behaves like a normal React child in every other way. Features like context work exactly the same regardless of whether the child is a portal, as the portal still exists in the React tree regardless of position in the DOM tree.\nThis includes event bubbling. An event fired from inside a portal will propagate to ancestors in the containing React tree, even if those elements are not ancestors in the DOM tree.\nCatching an event bubbling up from a portal in a parent component allows the development of more flexible abstractions that are not inherently reliant on portals.\nProfiler The Profiler measures how often a React application renders and what the “cost” of rendering is. Its purpose is to help identify parts of an application that are slow and may benefit from optimizations such as memoization.\nNote: Profiling adds some additional overhead, so it is disabled in the production build.\nA Profiler can be added anywhere in a React tree to measure the cost of rendering that part of the tree. It requires two props: an id (string) and an onRender callback (function) which React calls any time a component within the tree “commits” an update. It receives parameters describing what was rendered and how long it took.\nunction onRenderCallback( id, // the \u0026#34;id\u0026#34; prop of the Profiler tree that has just committed phase, // either \u0026#34;mount\u0026#34; (if the tree just mounted) or \u0026#34;update\u0026#34; (if it re-rendered) actualDuration, // time spent rendering the committed update baseDuration, // estimated time to render the entire subtree without memoization startTime, // when React began rendering this update commitTime, // when React committed this update interactions // the Set of interactions belonging to this update ) { // Aggregate or log render timings... } id: string - The id prop of the Profiler tree that has just committed. This can be used to identify which part of the tree was committed if you are using multiple profilers. phase: \u0026quot;mount\u0026quot; | \u0026quot;update\u0026quot; - Identifies whether the tree has just been mounted for the first time or re-rendered due to a change in props, state, or hooks. actualDuration: number - Time spent rendering the Profiler and its descendants for the current update. This indicates how well the subtree makes use of memoization (e.g. React.memo, useMemo, shouldComponentUpdate). Ideally this value should decrease significantly after the initial mount as many of the descendants will only need to re-render if their specific props change. baseDuration: number - Duration of the most recent render time for each individual component within the Profiler tree. This value estimates a worst-case cost of rendering (e.g. the initial mount or a tree with no memoization). startTime: number - Timestamp when React began rendering the current update. commitTime: number - Timestamp when React committed the current update. This value is shared between all profilers in a commit, enabling them to be grouped if desirable. interactions: Set - Set of “interactions” that were being traced when the update was scheduled (e.g. when render or setState were called). Web Components to be continued\nOptimizing Performance to be continued\n","date":"2021-05-01T00:00:00Z","permalink":"https://dyhes.github.io/p/reactbasics/","title":"【React】Basics"},{"content":"Introduction TypeScript is Typed JavaScript. TypeScript adds types to JavaScript to help you speed up the development by catching errors before you even run the JavaScript code.\nTypeScript is a super set of JavaScript.\nThere are two main reasons to use TypeScript:\nTypeScript adds a type system to help you avoid many problems with dynamic types in JavaScript. TypeScript implements the future features of JavaScript a.k.a ES Next so that you can use them today. Type In TypeScript, a type is a convenient way to refer to the different properties and functions that a value has.\nA value is anything that you can assign to a variable e.g., a number, a string, an array, an object, and a function.\nIn TypeScript:\na type is a label that describes the different properties and method that a value has every value has a type. TypeScript inherits the built-in types from JavaScript. TypeScript types is categorized into:\nPrimitive types The following illustrates the primitive types in TypeScript:\nName Description string represents text data number represents numeric values boolean has true and false values null has one value: null undefined has one value: undefined. It is a default value of an uninitialized variable symbol represents a unique constant value Object types Objec types are functions, arrays, classes, etc. Later, you’ll learn how to create custom object types.\ndetail number\nAll numbers in TypeScript are either floating-point values that get the number type or big integers that get the bigint type.\nlet hexadecimal: number = 0XA; let big: bigint = 9007199254740991n; string\nIn TypeScript, all strings get the string type. Like JavaScript, TypeScript uses double quotes (\u0026quot;), single quotes ('), and backtick (`) to surround string literals.\nObject and object\nThe object type represents all non-primitive values while the Object type describes the functionality of all objects.\nempty type {}\nTypeScript has another type called empty type denoted by {} , which is quite similar to the object type.\nThe empty type {} describes an object that has no property on its own. If you try to access a property on such object, TypeScript will issue a compile-time error\nTuple\nA tuple works like an array with some additional considerations:\nThe number of elements in the tuple is fixed. The types of elements are known, and need not be the same. Since TypeScript 3.0, a tuple can have optional elements specified using the question mark (?) postfix.\nlet bgColor, headerColor: [number, number, number, number?]; bgColor = [0, 255, 255, 0.5]; headerColor = [0, 255, 255]; Enum\nAn enum is a group of named constant values. Enum stands for enumerated type.\nTo define an enum, you follow these steps:\nFirst, use the enum keyword followed by the name of the enum. Then, define constant values for the enum. enum name {constant1, constant2, ...}; An enum member is both a number and a defined constant.\nTypeScript defines the numeric value of an enum’s member based on the order of that member that appears in the enum definition.\nIt’s possible to explicitly specify numbers for the members of an enum:\nenum Month { Jan = 1, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec }; You should use an enum when you:\nHave a small set of fixed values that are closely related And these values are known at compile time. any\nThe any type allows you to assign a value of any type to a variable.\nThe any type provides you with a way to work with existing JavaScript codebase. It allows you to gradually opt-in and opt-out of type checking during compilation. Therefore, you can use the any type for migrating a JavaScript project over to TypeScript.\nIf you declare a variable without specifying a type, TypeScript assumes that you use the any type. This feature is called type inference. Basically, TypeScript guesses the type of the variable.\nNote that to disable implicit typing to the any type, you change the noImplicitAny option in the tsconfig.json file to b.\nIf you declare a variable with the object type, you can also assign it any value.\nHowever, you cannot call a method on it even the method actually exists.\nvoid\nThe void type denotes the absence of having any type at all. It is a little like the opposite of the any type.\nTypically, you use the void type as the return type of functions that do not return a value.\nfunction log(message): void { console.log(messsage); } It is a good practice to add the void type as the return type of a function or a method that doesn’t return any value. By doing this, you can gain the following benefits:\nImprove clarity of the code: you do not have to read the whole function body to see if it returns anything. Ensure type-safe: you will never assign the function with the void return type to a variable. Notice that if you use the void type for a variable, you can only assign undefined to that variable. In this case, the void type is not useful.\nnever\nThe never type is a type that contains no values. Because of this, you cannot assign any value to a variable with a never type.\nTypically, you use the never type to represent the return type of a function that always throws an error.\nfunction raiseError(message: string): never { throw new Error(message); } Variables can also acquire the never type when you narrow its type by a type guard that can never be true.\nFor example, without the never type, the following function causes an error because not all code paths return a value.\nfunction fn(a: string | number): boolean { if (typeof a === \u0026#34;string\u0026#34;) { return true; } else if (typeof a === \u0026#34;number\u0026#34;) { return false; } } To make the code valid, you can return a function whose return type is the never type.\nfunction fn(a: string | number): boolean { if (typeof a === \u0026#34;string\u0026#34;) { return true; } else if (typeof a === \u0026#34;number\u0026#34;) { return false; } // make the function valid return neverOccur(); } let neverOccur = () =\u0026gt; { throw new Error(\u0026#39;Never!\u0026#39;); } union\nThe union type allows you to combine multiple types into one type.\nlet result: number | string; result = 10; // OK result = \u0026#39;Hi\u0026#39;; // also OK result = false; // a boolean value, not OK function\nA function type has two parts: parameters and return type. When declaring a function type, you need to specify both parts with the following syntax:\n(parameter: type, parameter:type,...) =\u0026gt; type Once annotating a variable with a function type, you can assign the function with the same type to the variable.\nIf a function has different branches that return different types, the TypeScript compiler may infer the union type or any type.\nTherefore, it is important to add type annotations to a function as much as possible.\nTypeScript compiler can figure out the function type when you have the type on one side of the equation. This form of type inference is called contextual typing.\nString Literal Type\nThe string literal types allow you to define a type that accepts only one specified string literal.\nlet click: \u0026#39;click\u0026#39;; The string literal type is useful to limit a possible string value in a variable.\nThe string literal types can combine nicely with the union types to define a finite set of string literal values for a variable:\nlet mouseEvent: \u0026#39;click\u0026#39; | \u0026#39;dblclick\u0026#39; | \u0026#39;mouseup\u0026#39; | \u0026#39;mousedown\u0026#39;; mouseEvent = \u0026#39;click\u0026#39;; // valid mouseEvent = \u0026#39;dblclick\u0026#39;; // valid mouseEvent = \u0026#39;mouseup\u0026#39;; // valid mouseEvent = \u0026#39;mousedown\u0026#39;; // valid mouseEvent = \u0026#39;mouseover\u0026#39;; // compiler error If you use the string literal types in multiple places, they will be very verbose.\nTo avoid this, you can use the type aliases.\ntype MouseEvent: \u0026#39;click\u0026#39; | \u0026#39;dblclick\u0026#39; | \u0026#39;mouseup\u0026#39; | \u0026#39;mousedown\u0026#39;; let mouseEvent: MouseEvent; mouseEvent = \u0026#39;click\u0026#39;; // valid mouseEvent = \u0026#39;dblclick\u0026#39;; // valid mouseEvent = \u0026#39;mouseup\u0026#39;; // valid mouseEvent = \u0026#39;mousedown\u0026#39;; // valid mouseEvent = \u0026#39;mouseover\u0026#39;; // compiler error let anotherEvent: MouseEvent; Type Annotation TypeScript uses type annotations to explicitly specify types for identifiers such variables, functions, objects, etc.\nTypeScript uses the syntax : type after an identifier as the type annotation, where type can be any valid type.\nlet variableName: type; let variableName: type = value; const constantName: type = value; //array let arrayName: type[]; //object let person: { name: string; age: number }; let employee: object; //functions let greeting : (name: string) =\u0026gt; string; Type Inference Type inference describes where and how TypeScript infers types when you don’t explicitly annotate them. Type inference occurs when you initialize variables, set parameter default values, and determine function return types. TypeScript uses the best common type algorithm to select the best candidate types that are compatible with all variables. TypeScript also uses contextual typing to infer types of variables based on the locations of the variables.\nIn practice, you should always use the type inference as much as possible. And you use the type annotation in the folowing cases:\nWhen you declare a variable and assign it a value later. When you want a variable that can’t be inferred. When a function returns the any type and you need to clarify the value. Type Aliases Type aliases allow you to create a new name for an existing type. The following shows the syntax of the type alias:\ntype alias = existingType; type chars= string; The existing type can be any valid TypeScript type.\nIt’s useful to create type aliases for union types.\nType Gaurd Type Guards allow you to narrow down the type of a variable within a conditional block.\ntypeof type alphanumeric = string | number; function add(a: alphanumeric, b: alphanumeric) { if (typeof a === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;number\u0026#39;) { return a + b; } if (typeof a === \u0026#39;string\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;string\u0026#39;) { return a.concat(b); } throw new Error(\u0026#39;Invalid arguments. Both arguments must be either numbers or strings.\u0026#39;); } instanceof\nclass Customer { isCreditAllowed(): boolean { // ... return true; } } class Supplier { isInShortList(): boolean { // ... return true; } } type BusinessPartner = Customer | Supplier; function signContract(partner: BusinessPartner) : string { let message: string; if (partner instanceof Customer) { message = partner.isCreditAllowed() ? \u0026#39;Sign a new contract with the customer\u0026#39; : \u0026#39;Credit issue\u0026#39;; } if (partner instanceof Supplier) { message = partner.isInShortList() ? \u0026#39;Sign a new contract the supplier\u0026#39; : \u0026#39;Need to evaluate further\u0026#39;; } return message; } in\nThe in operator carries a safe check for the existence of a property on an object. You can also use it as a type guard.\nfunction signContract(partner: BusinessPartner) : string { let message: string; if (\u0026#39;isCreditAllowed\u0026#39; in partner) { message = partner.isCreditAllowed() ? \u0026#39;Sign a new contract with the customer\u0026#39; : \u0026#39;Credit issue\u0026#39;; } else { // must be Supplier message = partner.isInShortList() ? \u0026#39;Sign a new contract the supplier \u0026#39; : \u0026#39;Need to evaluate further\u0026#39;; } return message; } User-Defined Type Guard\nUser-defined type guards allow you to define a type guard or help TypeScript infer a type when you use a function.\nA user-defined type guard function is a function that simply returns arg is aType. For example:\nfunction isCustomer(partner: any): partner is Customer { return partner instanceof Customer; } In this example, the isCustomer() is a user-defined type guard function. Now you can use it in as follows:\nfunction signContract(partner: BusinessPartner): string { let message: string; if (isCustomer(partner)) { message = partner.isCreditAllowed() ? \u0026#39;Sign a new contract with the customer\u0026#39; : \u0026#39;Credit issue\u0026#39;; } else { message = partner.isInShortList() ? \u0026#39;Sign a new contract with the supplier\u0026#39; : \u0026#39;Need to evaluate further\u0026#39;; } return message; } Type Casting Type casting allow you to convert a variable from one type to another type.\nIn TypeScript, you can use the as keyword or \u0026lt;\u0026gt; operator for type castings.\nType Casting using the as keyword The following selects the first input element by using the querySelector() method:\nlet input = document.querySelector(\u0026#39;input[\u0026#34;type=\u0026#34;text\u0026#34;]\u0026#39;); Since the returned type of the document.querySelector() method is the Element type, the following code causes a compiler error:\nconsole.log(input.value);JS The reason is that the value property doesn’t exist in the Element type. It only exists on the HTMLInputElement type.\nTo resolve this, you can use type casting that cast the Element to HTMLInputElement by using the as keyword like this:\nlet input = document.querySelector(\u0026#39;input[type=\u0026#34;text\u0026#34;]\u0026#39;) as HTMLInputElement; Now, the input variable has the type HTMLInputElement.\nAnother way to cast the Element to HTMLInputElement is when you access the property as follows:\nlet enteredText = (input as HTMLInputElement).value; Note that the HTMLInputElement type extends the HTMLElement type that extends to the Element type. When you cast the HTMLElement to HTMLInputElement, this type casting is also known as a down casting.\nThe syntax for converting a variable from typeA to typeB is as follows:\nlet a: typeA; let b = a as typeB; Code language: TypeScript (typescript) Type Casting using the \u0026lt;\u0026gt; operator Besides the as keyword, you can use the \u0026lt;\u0026gt; operator to carry a type casting. For example:\nlet input = \u0026lt;HTMLInputElement\u0026gt;document.querySelector(\u0026#39;input[type=\u0026#34;text\u0026#34;]\u0026#39;); console.log(input.value); The syntax for type casting using the \u0026lt;\u0026gt; is:\nlet a: typeA; let b = \u0026lt;typeB\u0026gt;a; Type Assertion Type assertions instruct the TypeScript compiler to treat a value as a specified type. It uses the as keyword to do so:\nexpression as targetType A type assertion is also known as type narrowing. It allows you to narrow a type from a union type.\nNote that a type assertion does not carry any type casting. It only tells the compiler which type it should apply to a value for the type checking purposes.\nYou can also uses the angle bracket syntax \u0026lt;\u0026gt; to assert a type, like this:\n\u0026lt;targetType\u0026gt; value For example:\nlet netPrice = \u0026lt;number\u0026gt;getNetPrice(100, 0.05, false); Note that you cannot use angle bracket syntax \u0026lt;\u0026gt; with some libraries such as React.\nIntersection Type An intersection type creates a new type by combining multiple existing types. The new type has all features of the existing types.\nTo combine types, you use the \u0026amp; operator as follows:\ntype typeAB = typeA \u0026amp; typeB; The typeAB will have all properties from both typeA and typeB.\ninterface BusinessPartner { name: string; credit: number; } interface Identity { id: number; name: string; } interface Contact { email: string; phone: string; } type Employee = Identity \u0026amp; Contact; type Customer = BusinessPartner \u0026amp; Contact; type Employee = Identity \u0026amp; Contact; let e: Employee = { id: 100, name: \u0026#39;John Doe\u0026#39;, email: \u0026#39;john.doe@example.com\u0026#39;, phone: \u0026#39;(408)-897-5684\u0026#39; }; When you intersect types, the order of the types doesn’t matter.\nFunction Optional Parameters\nBecause the compiler thoroughly checks the passing arguments, you need to annotate optional parameters to instruct the compiler not to issue an error when you omit the arguments.\nTo make a function parameter optional, you use the ? after the parameter name.\nfunction multiply(a: number, b: number, c?: number): number { if (typeof c !== \u0026#39;undefined\u0026#39;) { return a * b * c; } return a * b; } The optional parameters must appear after the required parameters in the parameter list.\nDefault Parameters\nfunction name(parameter1:type=defaultvalue1, parameter2:type=defaultvalue2,...) { // } Notice that you cannot include default parameters in function type definitions. The following code will result in an error:\nlet promotion: (price: number, discount: number = 0.05) =\u0026gt; number; //error TS2371: A parameter initializer is only allowed in a function or constructor implementation. both the default parameters and trailing default parameters share the same type.\nfunction applyDiscount(price: number, discount: number = 0.05): number { // ... } function applyDiscount(price: number, discount?: number): number { // ... } //both are (price: number, discount?: number) =\u0026gt; number However, default parameters don’t need to appear after the required parameters.\nWhen a default parameter appears before a required parameter, you need to explicitly pass undefined to get the default initialized value.\nRest Parameters\nTo declare a rest parameter, you prefix the parameter name with three dots and use the array type as the type annotation:\nfunction fn(...rest: type[]) { //... } function overloadings\nfunction add(a: number | string, b: number | string): number | string { if (typeof a === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;number\u0026#39;) return a + b; if (typeof a === \u0026#39;string\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;string\u0026#39;) return a + b; } The union type doesn’t express the relationship between the parameter types and results accurately.\nTo better describe the relationships between the types used by a function, TypeScript supports function overloadings.\nfunction add(a: number, b: number): number; function add(a: string, b: string): string; function add(a: any, b: any): any { return a + b; } //with optional parameter function sum(a: number, b: number): number; function sum(a: number, b: number, c: number): number; function sum(a: number, b: number, c?: number): number { if (c) return a + b + c; return a + b; } Class JavaScript does not have a concept of class like other programming languages such as Java and C#. In ES5, you can use a constructor function and prototype inheritance to create a “class”.\nfunction Person(ssn, firstName, lastName) { this.ssn = ssn; this.firstName = firstName; this.lastName = lastName; } Person.prototype.getFullName = function () { return `${this.firstName} ${this.lastName}`; } let person = new Person(\u0026#39;171-28-0926\u0026#39;,\u0026#39;John\u0026#39;,\u0026#39;Doe\u0026#39;); console.log(person.getFullName()); ES6 allowed you to define a class which is simply syntactic sugar for creating constructor function and prototypal inheritance\nclass Person { ssn; firstName; lastName; constructor(ssn, firstName, lastName) { this.ssn = ssn; this.firstName = firstName; this.lastName = lastName; } getFullName() { return `${this.firstName} ${this.lastName}`; } } TypeScript class adds type annotations to the properties and methods of the class. The following shows the Person class in TypeScript:\nclass Person { ssn: string; firstName: string; lastName: string; constructor(ssn: string, firstName: string, lastName: string) { this.ssn = ssn; this.firstName = firstName; this.lastName = lastName; } getFullName(): string { return `${this.firstName} ${this.lastName}`; } } Access Modifier Access modifiers change the visibility of the properties and methods of a class. TypeScript provides three access modifiers:\nprivate\nThe private modifier limits the visibility to the same-class only. When you add the private modifier to a property or method, you can access that property or method within the same class. Any attempt to access private properties or methods outside the class will result in an error at compile time.\nclass Person { private ssn: string; private firstName: string; private lastName: string; // ... } protected\nThe protected modifier allows properties and methods of a class to be accessible within same class and within subclasses.\nTo make the code shorter, TypeScript allows you to both declare properties and initialize them in the constructor like this:\nclass Person { constructor(protected ssn: string, private firstName: string, private lastName: string) { this.ssn = ssn; this.firstName = firstName; this.lastName = lastName; } getFullName(): string { return `${this.firstName} ${this.lastName}`; } } public\nThe public modifier allows class properties and methods to be accessible from all locations. If you don’t specify any access modifier for properties and methods, they will take the public modifier by default.\nNote that TypeScript controls the access logically during compilation time, not at runtime.\nreadonly TypeScript provides the readonly modifier that allows you to mark the properties of a class immutable. The assignment to a readonly property can only occur in one of two places:\nIn the property declaration. In the constructor of the same class. class Person { readonly birthDate: Date; constructor(birthDate: Date) { this.birthDate = birthDate; } } Like other access modifiers, you can consolidate the declaration and initialization of a readonly property in the constructor like this:\nclass Person { constructor(readonly birthDate: Date) { this.birthDate = birthDate; } } Readonly vs. const The following shows the differences between readonly and const:\nreadonly const Use for Class properties Variables Initialization In the declaration or in the constructor of the same class In the declaration Interface TypeScript interfaces define the contracts within your code. They also provide explicit names for type checking.\nBy convention, the interface names are in the camel case. They use a single capitalized letter to separate words in there names.\ninterface Person { firstName: string; lastName: string; } function getFullName(person: Person) { return `${person.firstName} ${person.lastName}`; } let john = { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39; }; console.log(getFullName(john)); Optional Properties An interface may have optional properties. To declare an optional property, you use the question mark (?) at the end of the property name in the declaration\ninterface Person { firstName: string; middleName?: string; lastName: string; } Function Types In addition to describing an object with properties, interfaces also allow you to describe function types.\nTo describe a function type, you assign the interface to the function signature that contains the parameter list with types and returned types.\ninterface StringFormat { (str: string, isUpper: boolean): string } Class Types The interface can also be used to define a contract between unrelated classes.\nFor example, the following Json interface can be implemented by any unrelated classes:\ninterface Json { toJSON(): string } The following declares a class that implements the Json interface:\nclass Person implements Json { constructor(private firstName: string, private lastName: string) { } toJson(): string { return JSON.stringify(this); } } extending interface Mailable { send(email: string): boolean queue(email: string): boolean } interface FutureMailable extends Mailable { later(email: string, after: number): boolean } Like classes, the FutureMailable interface inherits the send() and queue() methods from the Mailable interface.\nAn interface can extend multiple interfaces, creating a combination of all the interfaces.\ninterface A { a(): void } interface B extends A { b(): void } interface C { c(): void } interface D extends B, C { d(): void } TypeScript allows an interface to extend a class. In this case, the interface inherits the properties and methods of the class. Also, the interface can inherit the private and protected members of the class, not just the public members.\nIt means that when an interface extends a class with private or protected members, the interface can only be implemented by that class or subclasses of that class from which the interface extends.\nBy doing this, you restrict the usage of the interface to only class or subclasses of the class from which the interface extends. If you attempt to implement the interface from a class that is not a subclass of the class that the interface inherited, you’ll get an error. For example:\nclass Control { private state: boolean; } interface StatefulControl extends Control { enable(): void } class Button extends Control implements StatefulControl { enable() { } } class TextBox extends Control implements StatefulControl { enable() { } } class Label extends Control { } // Error: cannot implement class Chart implements StatefulControl { enable() { } } Generics TypeScript generics allow you to write the reusable and generalized form of functions, classes, and interfaces.\nIn order to avoid code duplication. We can:\nfunction getRandomAnyElement(items: any[]): any { let randomIndex = Math.floor(Math.random() * items.length); return items[randomIndex]; } This solution works fine. However, it has a drawback.\nIt doesn’t allow you to enforce the type of the returned element. In other words, it isn’t type-safe.\nA better solution to avoid code duplication while preserving the type is to use generics.\nThe following shows a generic function that returns the random element from an array of type T:\nfunction getRandomElement\u0026lt;T\u0026gt;(items: T[]): T { let randomIndex = Math.floor(Math.random() * items.length); return items[randomIndex]; } function merge\u0026lt;U, V\u0026gt;(obj1: U, obj2: V) { return { ...obj1, ...obj2 }; } This function uses type variable T. The T allows you to capture the type that is provided at the time of calling the function.\nThis getRandomElement() function is generic because it can work with any data type including string, number, objects,…\nBy convention, we use the letter T as the type variable. However, you can freely use other letters such as A, B C, …\nThe following shows how to use the getRandomElement() with an array of numbers:\nlet numbers = [1, 5, 7, 4, 2, 9]; let randomEle = getRandomElement\u0026lt;number\u0026gt;(numbers); console.log(randomEle); This example explicitly passes number as the T type into the getRandomElement() function.\nIn practice, you’ll use type inference for the argument. It means that you let the TypeScript compiler set the value of T automatically based on the type of argument that you pass into, like this:\nlet numbers = [1, 5, 7, 4, 2, 9]; let randomEle = getRandomElement(numbers); console.log(randomEle); In this example, we didn’t pass the number type to the getRandomElement() explicitly. The compiler just looks at the argument and sets T to its type.\nGenerics Constraint In order to denote the constraint, you use the extends keyword. For example:\nfunction merge\u0026lt;U extends object, V extends object\u0026gt;(obj1: U, obj2: V) { return { ...obj1, ...obj2 }; } Because the merge() function is now constrained, it will no longer work with all types. Instead, it works with the object type only.\nfunction prop\u0026lt;T, K\u0026gt;(obj: T, key: K) { return obj[key]; } //Type \u0026#39;K\u0026#39; cannot be used to index type \u0026#39;T\u0026#39;. function prop\u0026lt;T, K extends keyof T\u0026gt;(obj: T, key: K) { return obj[key]; } Use extends keyword to constrain the type parameter to a specific type. Use extends keyof to constrain a type that is the property of another object. Generics Class A generic class has a generic type parameter list in an angle brackets \u0026lt;\u0026gt; that follows the name of the class:\nclass className\u0026lt;T\u0026gt;{ //... } class className\u0026lt;K,T\u0026gt;{ //... } class className\u0026lt;T extends TypeA\u0026gt;{ //... } class Stack\u0026lt;T\u0026gt; { private elements: T[] = []; constructor(private size: number) { } isEmpty(): boolean { return this.elements.length === 0; } isFull(): boolean { return this.elements.length === this.size; } push(element: T): void { if (this.elements.length === this.size) { throw new Error(\u0026#39;The stack is overflow!\u0026#39;); } this.elements.push(element); } pop(): T { if (this.elements.length == 0) { throw new Error(\u0026#39;The stack is empty!\u0026#39;); } return this.elements.pop(); } } Generics Interface Like classes, interfaces also can be generic. A generic interface has generic type parameter list in an angle brackets \u0026lt;\u0026gt; following the name of the interface:\ninterface interfaceName\u0026lt;T\u0026gt; { // ... } interface interfaceName\u0026lt;U,V\u0026gt; { // ... } interface Pair\u0026lt;K, V\u0026gt; { key: K; value: V; } interface Options\u0026lt;T\u0026gt; { [name: string]: T } let inputOptions: Options\u0026lt;boolean\u0026gt; = { \u0026#39;disabled\u0026#39;: false, \u0026#39;visible\u0026#39;: true }; ","date":"2021-05-01T00:00:00Z","permalink":"https://dyhes.github.io/p/tsbasics/","title":"【TS】Basics"},{"content":"Parameter Default Parameter function say(message=\u0026#39;Hi\u0026#39;) { console.log(message); } say(); // \u0026#39;Hi\u0026#39; say(\u0026#39;Hello\u0026#39;) // \u0026#39;Hello\u0026#39; If you want to use the default values for the parameters, pass undefined values to the parameters.\nThe parameter can take a default value which is a result of a function. You can assign a parameter a default value that references to other default parameters as shown in the following example:\nfunction add(x = 1, y = x, z = x + y) { return x + y + z; } console.log(add()); ；‘’、》The value of the arguments object inside the function is the number of actual arguments that you pass to the function. For example:\nfunction add(x, y = 1, z = 2) { console.log( arguments.length ); return x + y + z; } add(10); // 1 add(10, 20); // 2 add(10, 20, 30); // 3 Rest Parameter ES6 provides a new kind of parameter so-called rest parameter that has a prefix of three dots (...). A rest parameter **allows you to represent an indefinite number of arguments as an array. See the following syntax:\nfunction fn(a,b,...args) { //... } Notice that the rest parameters must appear at the end of the argument list.\nSee the following example:\nfunction sum(...args) { let total = 0; for (const a of args) { total += a; } return total; } sum(1, 2, 3); Spread Operator ES6 provides a new operator called spread operator that consists of three dots (...). The spread operator allows you to spread out elements of an iterable object such as an array,a map, or a set. For example:\nconst odd = [1,3,5]; const combined = [2,4,6, ...odd]; Output:\n[ 2, 4, 6, 1, 3, 5 ] So the three dots ( ...) represent both the spread operator and the rest parameter.\nHere are the main differences:\nThe spread operator unpacks elements. The rest parameter packs elements into an array. The rest parameters must be the last arguments of a function. However, the spread operator can be anywhere:\nvar rivers = [\u0026#39;Nile\u0026#39;, \u0026#39;Ganges\u0026#39;, \u0026#39;Yangte\u0026#39;]; var moreRivers = [\u0026#39;Danube\u0026#39;, \u0026#39;Amazon\u0026#39;]; //ES5 Array.prototype.push.apply(rivers, moreRivers); console.log(rivers); //ES6 rivers.push(...moreRivers); Note that ES2018 expands the spread operator to objects. It is known as the object spread.\nconst circle = { radius: 10 }; const coloredCircle = { ...circle, color: \u0026#39;black\u0026#39; }; Object literal ES6 allows you to eliminate the duplication when a property of an object is the same as the local variable name by including the name without a colon and value.\nfunction createMachine(name, status) { return { name, status }; } You could use the square brackets( []) to enable the computed property names for the properties on objects.\nThe square brackets allow you to use the string literals and variables as the property names.\nWhen a property name is placed inside the square brackets, the JavaScript engine evaluates it as a string. It means that you can use an expression as a property name.\nlet prefix = \u0026#39;machine\u0026#39;; let machine = { [prefix + \u0026#39; name\u0026#39;]: \u0026#39;server\u0026#39;, [prefix + \u0026#39; hours\u0026#39;]: 10000 }; console.log(machine[\u0026#39;machine name\u0026#39;]); // server console.log(machine[\u0026#39;machine hours\u0026#39;]); // 10000 Concise Method Syntax //ES5 let server = { name: \u0026#34;Server\u0026#34;, restart: function () { console.log(\u0026#34;The\u0026#34; + this.name + \u0026#34; is restarting...\u0026#34;); } }; //ES6 let server = { name: \u0026#39;Server\u0026#39;, restart() { console.log(\u0026#34;The\u0026#34; + this.name + \u0026#34; is restarting...\u0026#34;); } }; This shorthand syntax is also known as the concise method syntax. It’s valid to have spaces in the property name.\nlet server = { name: \u0026#39;Server\u0026#39;, restart() { console.log(\u0026#34;The \u0026#34; + this.name + \u0026#34; is restarting...\u0026#34;); }, \u0026#39;starting up\u0026#39;() { console.log(\u0026#34;The \u0026#34; + this.name + \u0026#34; is starting up!\u0026#34;); } }; server[\u0026#39;starting up\u0026#39;]();Code language: JavaScript (javascript) In this example, the method 'starting up' has spaces in its name. To call the method, you use the following syntax:\nobject_name[\u0026#39;property name\u0026#39;](); New Way to Loop ES6 introduced a new statement for...of that iterates over an iterable object such as:\nBuilt-in Array, String, Map, Set, … Array-like objects such as arguments or NodeList User-defined objects that implement the iterator protocol. The following illustrates the syntax of the for...of:\nYou can use var, let, or const to declare the variable.\nfor (variable of iterable) { // statements } // Array destructing let colors = [\u0026#39;Red\u0026#39;, \u0026#39;Green\u0026#39;, \u0026#39;Blue\u0026#39;]; for (const [index, color] of colors.entries()) { console.log(`${color} is at index ${index}`); } // object destructing const ratings = [ {user: \u0026#39;John\u0026#39;,score: 3}, {user: \u0026#39;Jane\u0026#39;,score: 4}, {user: \u0026#39;David\u0026#39;,score: 5}, {user: \u0026#39;Peter\u0026#39;,score: 2}, ]; let sum = 0; for (const {score} of ratings) { sum += score; } Destructure Array Destructuring let [x, y ,...args] = someFuncReturnArray() [a, b] = [b, a]; If the value taken from the array is undefined, you can assign the variable a default value, like this:\nlet a, b; [a = 1, b = 2] = [10]; //a=10,b=2; Object Destructuring ES6 introduces the object destructuring syntax that provides an alternative way to assign properties of an object to variables:\nlet { firstName: fname, lastName: lname } = person In this example, the firstName and lastName properties are assigned to the fName and lName variables respectively.\nThe identifier before the colon (:) is the property of the object and the identifier after the colon is the variable.\nIf the variables have the same names as the properties of the object, you can make the code more concise as follows:\nlet { firstName, lastName } = person; It’s possible to separate the declaration and assignment. However, you must surround the variables in parentheses:\n({firstName, lastName} = person); If you don’t use the parentheses, the JavaScript engine will interpret the left-hand side as a block and throw a syntax error.\nWhen you assign a property that does not exist to a variable using the object destructuring, the variable is set to undefined.\nlet { firstName, lastName, middleName = \u0026#39;\u0026#39;, currentAge: age = 18 } = person; To avoid typeError caused by null returned, you can use || to fallback the null object to an empty object:\nlet { firstName, lastName } = getPerson() || {}; Module An ES6 module is a JavaScript file that executes in strict mode only. It means that any variables or functions declared in the module won’t be added automatically to the global scope.\nFirst, create a new file called message.js and add the following code:\nexport let message = \u0026#39;ES6 Modules\u0026#39;; The message.js is a module in ES6 that contains the message variable. The export statement exposes the message variable to other modules.\nSecond, create another new file named app.js that uses the message.js module. The app.js module creates a new heading 1 (h1) element and attaches it to an HTML page. The import statement imports the message variable from the message.js module.\nimport { message } from \u0026#39;./message.js\u0026#39; const h1 = document.createElement(\u0026#39;h1\u0026#39;); h1.textContent = message document.body.appendChild(h1) Third, create a new HTML page that uses the app.js module:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;ES6 Modules\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;./app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Note that we used the type=\u0026quot;module\u0026quot; in the script tag to load the app.js module.\nTo export a variable, a function, or a class, you place the export keyword in front of it.\nNote that the export keyword requires the function or class to have a name to be exported. You can’t export an anonymous function or class using this syntax.\nOnce you define a module with exports, you can access the exported variables, functions, and classes in another module by using the import keyword.\nIn this syntax:\nFirst, specify what to import inside the curly braces, which are called bindings. Then, specify the module from which you import the given bindings. Note that when you import a binding from a module, the binding behaves like it was defined using const. It means you can’t have another identifier with the same name or change the value of the binding.\nBehind the scenes, when you called the setMessage() function. JavaScript went back to the greeting.js module and executed code in there and changed the message variable. The change was then automatically reflected on the imported message binding.\nTo import everything from a module as a single object, you use the asterisk (*) pattern\nall the bindings become properties of the object, so you can access them.\nJavaScript must statically determine what will be exported and imported.\nJavaScript allows you to create aliases for variables, functions, or classes when you export and import.\nexport { add as sum }; import {sum as total} from \u0026#39;./math.js\u0026#39;; import { sum } from \u0026#39;./math.js\u0026#39;; export { sum }; It’s possible to export bindings that you have imported. This is called re-exporting.\nA module can have one and only one default export. The default export is easier to import. The default for a module can be a variable, a function, or a class.\n// sort.js export default function(arr) { // sorting here } import sort from sort.js; Note that you don’t need to specify the name for the function because the module represents the function name.\nNotice that we didn’t use the curly brace {} surrounding the sort identifier.\nTo import both default and non-default bindings, you use the specify a list of bindings after the import keyword with the following rules:\nThe default binding must come first. The non-default binding must be surrounded by curly braces. Arrow Function Use the (...args) =\u0026gt; expression; to define an arrow function. Use the (...args) =\u0026gt; { statements } to define an arrow function that has multiple statements. An arrow function doesn’t have its binding to this or super. An arrow function doesn’t have arguments object, new.target keyword, and prototype property. the arrow function doesn’t have its own this value. It uses the this value of the enclosing lexical scope.\nArrow functions don’t have the arguments object. Therefore, if you have a function that use arguments object, you cannot use the arrow function.\nSymbol To create a new symbol, you use the global Symbol() function as shown in this example:\nlet s = Symbol(\u0026#39;foo\u0026#39;); The Symbol() function creates a new unique value each time you call it\nThe Symbol() function accepts a description as an optional argument. The description argument will make your symbol more descriptive.\nYou can access the symbol’s description property using the toString() method. The console.log() method calls the toString() method of the symbol implicitly\nSince a symbol is a primitive value, if you attempt to create a symbol using the new operator, you will get an error\nES6 provides you with the global symbol registry that allows you to share symbols globally. If you want to create a symbol that will be shared, you use the Symbol.for() method instead of calling the Symbol() function.\nTo get the key associated with a symbol, you use the Symbol.keyFor() method To get all the enumerable properties of an object, you use the Object.keys() method.\nTo get all properties of an object whether the properties are enumerable or not, you use the Object.getOwnPropertyNames() method.\nTo get all property symbols of an object, you use the Object.getOwnPropertySymbols() method, which has been added in ES6.\nconsole.log(Object.keys(task)); // [\u0026#34;description\u0026#34;] console.log(Object.getOwnPropertyNames(task)); // [\u0026#34;description\u0026#34;] console.log(Object.getOwnPropertySymbols(task)); //[Symbol(status)] ES6 provides predefined symbols which are called well-known symbols. The well-known symbols represent the common behaviors in JavaScript. Each well-known symbol is a static property of the Symbol object.\nIterator and Generator There are two iteration protocols: iterable protocol and iterator protocol.\nIterator protocol\nAn object is an iterator when it implements an interface (or API) that answers two questions:\nIs there any element left? If there is, what is the element? Technically speaking, an object is qualified as an iterator when it has a next() method that returns an object with two properties:\ndone: a boolean value indicating whether or not there are any more elements that could be iterated upon. value: the current element. Iterable protocol\nAn object is iterable when it contains a method called [Symbol.iterator] that takes no argument and returns an object which conforms to the iterator protocol.\nThe [Symbol.iterator] is one of the built-in well-known symbols in ES6.\nSince ES6 provides built-in iterators for the collection types Array, Set, and Map, you don’t have to create iterators for these objects.\nIf you have a custom type and want to make it iterable so that you can use the for...of loop construct, you need to implement the iteration protocols.\nThe following code creates a Sequence object that returns a list of numbers in the range of ( start, end) with an interval between subsequent numbers.\nclass Sequence { constructor( start = 0, end = Infinity, interval = 1 ) { this.start = start; this.end = end; this.interval = interval; } [Symbol.iterator]() { let counter = 0; let nextIndex = this.start; return { next: () =\u0026gt; { if ( nextIndex \u0026lt;= this.end ) { let result = { value: nextIndex, done: false } nextIndex += this.interval; counter++; return result; } return { value: counter, done: true }; }, return: () =\u0026gt; { console.log(\u0026#39;cleaning up...\u0026#39;); return { value: undefined, done: true }; } } } } The following code uses the Sequence iterator in a for...of loop:\nlet evenNumbers = new Sequence(2, 10, 2); for (const num of evenNumbers) { console.log(num); } In addition to the next() method, the [Symbol.iterator]() may optionally return a method called return().\nThe return() method is invoked automatically when the iteration is stopped prematurely. It is where you can place the code to clean up the resources.\nGenerator ES6 introduces a new kind of function that is different from a regular function: function generator or generator.\nA generator can pause midway and then continues from where it paused.\nfunction* generate() { console.log(\u0026#39;invoked 1st time\u0026#39;); yield 1; console.log(\u0026#39;invoked 2nd time\u0026#39;); yield 2; } Let’s examine the generate() function in detail.\nFirst, you see the asterisk (*) after the function keyword. The asterisk denotes that the generate() is a generator, not a normal function. Second, the yield statement returns a value and pauses the execution of the function. A generator returns a Generator object without executing its body when it is invoked.\nThe Generator object returns another object with two properties: done and value. In other words, a Generator object is iterable.\nSince a generator is iterable, you can use the for...of loop:\nfor (const g of gen) { console.log(g); } Here is the output:\ninvoked 1st time 1 invoked 2nd time 2 And here is the new Sequence iterator that uses a generator:\nclass Sequence { constructor( start = 0, end = Infinity, interval = 1 ) { this.start = start; this.end = end; this.interval = interval; } * [Symbol.iterator]() { for( let index = this.start; index \u0026lt;= this.end; index += this.interval ) { yield index; } } } The following shows the syntax of the yield keyword:\n[variable_name] = yield [expression]; In this syntax:\nThe expression specifies the value to return from a generator function via the iteration protocol. If you omit the expression, the yield returns undefined. The variable_name stores the optional value passed to the next() method of the iterator object. Map and Set Map Prior to ES6, when you need to map keys to values, you often use an object, because an object allows you to map a key to a value of any type.\nHowever, using an object as a map has some side effects:\nAn object always has a default key like the prototype. A key of an object must be a string or a symbol, you cannot use an object as a key. An object does not have a property that represents the size of the map. ES6 provides a new collection type called Map that addresses these deficiencies.\nBy definition, a Map object holds key-value pairs where values of any type can be used as either keys or values. In addition, a Map object remembers the original insertion order of the keys.\nlet map = new Map([iterable]); The Map() accepts an optional iterable object whose elements are key-value pairs.\nUseful Methods clear() – removes all elements from the map object. delete(key) – removes an element specified by the key. It returns if the element is in the map, or false if it does not. entries() – returns a new Iterator object that contains an array of [key, value] for each element in the map object. The order of objects in the map is the same as the insertion order. forEach(callback[, thisArg]) – invokes a callback for each key-value pair in the map in the insertion order. The optional thisArg parameter sets the this value for each callback. get(key) – returns the value associated with the key. If the key does not exist, it returns undefined. has(key) – returns true if a value associated with the key exists, otherwise, return false. keys() – returns a new Iterator that contains the keys for elements in insertion order. set(key, value) – sets the value for the key in the map object. It returns the map object itself therefore you can chain this method with other methods. values() returns a new iterator object that contains values for each element in insertion order. WeakMap A WeakMap is similar to a Map except the keys of a WeakMap must be objects. It means that when a reference to a key (an object) is out of scope, the corresponding value is automatically released from the memory.\nA WeakMap only has subset methods of a Map object:\nget(key) set(key, value) has(key) delete(key) Here are the main difference between a Map and a WeekMap:\nElements of a WeakMap cannot be iterated. Cannot clear all elements at once. Cannot check the size of a WeakMap. Set The Set constructor also accepts an optional iterable object. If you pass an iterable object to the Set constructor, all the elements of the iterable object will be added to the new set:\nlet setObject = new Set([iterableObject]); Useful Methods The Set object provides the following useful methods:\nadd(value) – appends a new element with a specified value to the set. It returns the Set object, therefore, you can chain this method with another Set method. clear() – removes all elements from the Set object. delete(value) – deletes an element specified by the value. entries()– returns a new Iterator that contains an array of [value, value] . forEach(callback [, thisArg]) – invokes a callback on each element of the Set with the this value sets to thisArg in each call. has(value) – returns true if an element with a given value is in the set, or false if it is not. keys() – is the same as values() function. [@@iterator] – returns a new Iterator object that contains values of all elements stored in the insertion order. To get the number of elements that the set holds, you use the size property of the Set object\nWeakSet A WeakSet is similar to a Set except that it contains only objects. Since objects in a WeakSet may be automatically garbage-collected, a WeakSet does not have size property. Like a WeakMap, you cannot iterate elements of a WeakSet, therefore, you will find that WeakSet is rarely used in practice. In fact, you only use a WeakSet to check if a specified value is in the set.\nProxy A JavaScript Proxy is an object that wraps another object (target) and intercepts the fundamental operations of the target object.\nThe fundamental operations can be the property lookup, assignment, enumeration, and function invocations, etc.\nTo create a new proxy object, you use the following syntax:\nlet proxy = new Proxy(target, handler); In this syntax:\ntarget – is an object to wrap. handler – is an object that contains methods to control the behaviors of the target. The methods inside the handler object are called traps. A simple proxy example\n//First, define an object called user const user = { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, email: \u0026#39;john.doe@example.com\u0026#39;, } //Second, define a `handler` object: const handler = { get(target, property) { console.log(`Property ${property} has been read.`); return target[property]; } } //Third, create a `proxy` object: const proxyUser = new Proxy(user, handler); The following are more traps:\nconstruct – traps usage of the new operator getPrototypeOf – traps an internal call to [[GetPrototypeOf]] setPrototypeOf – traps a call to Object.setPrototypeOf isExtensible – traps a call to Object.isExtensible preventExtensions – traps a call to Object.preventExtensions getOwnPropertyDescriptor – traps a call to Object.getOwnPropertyDescriptor Reflection In computer programming, reflection is the ability of a program to manipulate variables, properties, and methods of objects at runtime.\nES6 introduces a new global object called Reflect that allows you to call methods, construct objects, get and set properties, manipulate and extend properties.\nThe Reflect API is important because it allows you to develop programs and frameworks that are able to handle dynamic code.\nUnlike the most global objects, the Reflect is not a constructor. It means that you cannot use Reflect with the new operator or invoke the Reflect as a function. It is similar to the Math and JSON objects. All the methods of the Reflect object are static.\nReflect.apply() – call a function with specified arguments. Reflect.construct() – act like the new operator, but as a function. It is equivalent to calling new target(...args). Reflect.defineProperty() – is similar to Object.defineProperty(), but return a Boolean value indicating whether or not the property was successfully defined on the object. Reflect.deleteProperty() – behave like the delete operator, but as a function. It’s equivalent to calling the delete objectName[propertyName]. Reflect.get() – return the value of a property. Reflect.getOwnPropertyDescriptor() – is similar to Object.getOwnPropertyDescriptor(). It returns a property descriptor of a property if the property exists on the object, or undefined otherwise. Reflect.getPrototypeOf() – is the same as Object.getPrototypeOf(). Reflect.has() – work like the in operator, but as a function. It returns a boolean indicating whether an property (either owned or inherited) exists. Reflect.isExtensible() – is the same as Object.isExtensible(). Reflect.ownKeys() – return an array of the owned property keys (not inherited) of an object. Reflect.preventExtensions() – is similar to Object.preventExtensions(). It returns a Boolean. Reflect.set() – assign a value to a property and return a Boolean value which is true if the property is set successfully. Reflect.setPrototypeOf() – set the prototype of an object. New Functions Array.of()\nIn ES5, when you pass a number to the Array constructor, JavaScript creates an array whose length equals the number.\nHowever, when you pass to the Array constructor a value that is not a number, JavaScript creates an array that contains one element with that value.\nES6 introduces the Array.of() method to solve this problem.\nThe Array.of() method is similar to the Array constructor except the Array.of() method does not treat a single numeric value special.\nIn other words, the Array.of() method always creates an array that contains the values that you pass to it regardless of the types or the number of arguments.\nArray.from()\nES6 introduces the Array.from() method that creates a new instance of the Array from an array-like or iterable object. The following illustrates the syntax of the Array.from() method:\nArray.from(target [, mapFn[, thisArg]]) In this syntax:\ntarget is an array-like or iterable object to convert to an array. mapFn is the map function to call on every element of the array thisArg is the this value when executing the mapFn function. The Array.from() returns a new instance of Array that contains all elements of the target object.\nfind()\nES6 introduced a new method called find()added to the Array.prototype object.\nThe find() method returns the first element in an array that satisfies a provided function.\nThe following shows the syntax of the find() method:\nfind(callback(element[, index[, array]])[, thisArg]) The find() accepts two arguments: a callback function and an optional value to use for the this inside the callback function\nThe callback is a function that executes on each element of the array. It takes three arguments:\nelement is the current element. index the index of the current element. array the array that the find() was called upon The thisArg is the object used as this inside the callback.\nThe find() executes the callback function for each element in the array until the callback returns a truthy value.\nIf the callback returns a truthy value, the find() immediately returns the element and stop searching. Otherwise, it returns undefined.\nObject.assign()\nThe following shows the syntax of the Object.assign() method:\nObject.assign(target, ...sources) The Object.assign() copies all enumerable and own propeties from the source objects to the target object. It returns the target object. It invokes the getters on the source objects and setters on the target. It assigns properties only, not copying or defining new properties. Object.assign() can be used to clone or merge objects.\n//clone let widget = { color: \u0026#39;red\u0026#39; }; let clonedWidget = Object.assign({}, widget); console.log(clonedWidget); //merge let box = { height: 10, width: 20 }; let style = { color: \u0026#39;Red\u0026#39;, borderStyle: \u0026#39;solid\u0026#39; }; let styleBox = Object.assign({}, box, style); console.log(styleBox); If the source objects have the same property, the property of the later object overwrites the earlier one\nObject.is()\nThe Object.is() behaves like the === operator with two differences:\n-0 and +0\nThe === operator treats -0 and +0 are the same value:\nHowever, the Object.is() treats +0 and -0 as different values.\nNaN\nThe === operator considers NaN and NaN are different values. The NaN is the only number that does not equal itself.\nHowever, Object.is() treats NaN as the same value\nThe startsWith() returns true if a string begins with the characters of a specified string; otherwise false.\nstartsWith()\nThe following shows the syntax of the startsWith() method:\nString.startsWith(searchString(pattern) [,position]) searchString is the characters to be searched for at the start of this string. position is an optional parameter that determines the start position to search for the searchString. It defaults to 0. endsWith()\nString.startsWith(searchString(pattern) [,position]) The endsWith() returns true if a string ends with the characters of a specified string; otherwise false.Arguments\nsearchString is the characters to be searched for at the end of the string. length is an optional parameter that determines the length of the string to search. It defaults to the length of the string. includes()\nThe includes() method determines whether a string contains another string:\nstring.includes(searchString [,position]) The includes() method returns true if the searchString found in the string; otherwise false.\nThe optional position parameter specifies the position within the string at which to begin searching for the searchString. The position defaults to 0.\nThe includes() matches string case-sensitively.\nClass A JavaScript class is a blueprint for creating objects. A class encapsulates data and functions that manipulate data.\nUnlike other programming languages such as Java and C#, JavaScript classes are syntactic sugar over the prototypal inheritance . In other words, ES6 classes are just special functions.\nA class declaration is syntactic sugar over prototypal inheritance with additional enhancements.\nES6 introduced a new syntax for declaring a class as shown in this example:\nclass Person { constructor(name) { this.name = name; } ... } Despite the similarities between a class and a custom type defined via a constructor function, there are some important differences.\nclass declarations are not hoisted like function declarations. all the code inside a class automatically executes in the strict mode. And you cannot change this behavior. class methods are non-emunerable.If you use a constructor/prototype pattern, you have to use the Object.defineProperty() method to make a property non-enumerable. calling the class constructor without the new operator will result in an error ES6 provides a specific syntax for defining the getter and setter using the get and set keywords. For example:\nclass Person { constructor(name) { this.name = name; } get name() { return this._name; } set name(newName) { newName = newName.trim(); if (newName === \u0026#39;\u0026#39;) { throw \u0026#39;The name cannot be empty\u0026#39;; } this._name = newName; } Getter\nTo call the getter, you use the following syntax:\nlet name = person.name; When JavaScript sees the access to name property of the Person class, it checks if the Person class has any name property. If not, JavaScript checks if the Person class has any method that binds to the name property. In this example, the name() method binds to the name property via the get keyword. Once JavaScript finds the getter method, it executes the getter method and returns a value.\nSetter\nThe setter uses the set keyword followed by the method name:\nset name(newName) { newName = newName.trim(); if (newName === \u0026#39;\u0026#39;) { throw \u0026#39;The name cannot be empty\u0026#39;; } this._name = newName; } JavaScript will call the name() setter when you assign a value to the name property like this:\nperson.name = \u0026#39;Jane Smith\u0026#39;; If a class has only getter but not setter and you attempt to use the setter, the change won’t take any effect.\nStatic Method\nBy definition, static methods are bound to a class, not the instances of that class. Therefore, static methods are useful for defining helper or utility methods.\nThe following adds a static method called createAnonymous() to the Person type:\nPerson.createAnonymous = function (gender) { let name = gender == \u0026#34;male\u0026#34; ? \u0026#34;John Doe\u0026#34; : \u0026#34;Jane Doe\u0026#34;; return new Person(name); } The createAnonymous() method is considered a static method because it doesn’t depend on any instance of the Person type for its property values.\nTo call the createAnonymous() method, you use the Person type instead of its instances:\nvar anonymous = Person.createAnonymous(); In ES6, you define static methods using the static keyword.\nclass Person { constructor(name) { this.name = name; } getName() { return this.name; } static createAnonymous(gender) { let name = gender == \u0026#34;male\u0026#34; ? \u0026#34;John Doe\u0026#34; : \u0026#34;Jane Doe\u0026#34;; return new Person(name); } } If you attempt to call the static method from an instance of the class, you’ll get an error.\nTo call a static method from a class constructor or an instance method, you use the class name, followed by the . and the static method:\nclassName.staticMethodName(); //or this.constructor.staticMethodName(); Static Property\nLike a static method, a static property is shared by all instances of a class. To define static property, you use the static keyword followed by the property name like this:\nclass Item { static count = 0; } Code language: JavaScript (javascript) To access a static property, you use the class name followed by the . operator and the static property name\nTo access a static property in a class constructor or instance methods, you use the following syntax:\nclassName.staticPropertyName; //or this.constructor.staticPropertyName; class expression\nA class expression doesn’t require an identifier after the class keyword. And you can use a class expression in a variable declaration and pass it into a function as an argument.\nFor example, the following defines a class expression:\nlet Person = class { constructor(name) { this.name = name; } getName() { return this.name; } } Similar to function expressions, class expressions are not hoisted. It means that you cannot create an instance of the class before defining the class expression.\nJavaScript classes are first-class citizens. It means that you can pass a class into a function, return it from a function, and assign it to a variable.\nextends and super keyword\nPrior to ES6, implementing a proper inheritance required multiple steps. One of the most commonly used strategies is the prototypal inheritance.\nES6 simplified these steps by using the extends and super keywords.\nFirst, use the extends keyword to make the Bird class inheriting from the Animal class:\nclass Bird extends Animal { // ... } The Animal class is called a base class or parent class while the Bird class is known as a derived class or child class. By doing this, the Bird class inherits all methods and properties of the Animal class.\nSecond, in the Bird‘s constructor, call super() to invoke the Animal‘s constructor with the legs argument.\nJavaScript requires the child class to call super() if it has a constructor.\nBecause the super() initializes the this object, you need to call the super() before accessing the this object. Trying to access this before calling super() also results in an error.\nES6 allows the child class and parent class to have methods with the same name. In this case, when you call the method of an object of the child class, the method in the child class will shadow the method in the parent class.\nTo call the method of the parent class in the child class, you use super.method(arguments),\nnew.target\nES6 provides a metaproperty named new.target that allows you to detect whether a function or constructor was called using the new operator.\nThe new.target consists of the new keyword, a dot, and target property. The new.target is available in all functions.\nHowever, in arrow functions, the new.target is the one that belongs to the surrounding function.\nThe new.target is very useful to inspect at runtime whether a function is being executed as a function or as a constructor. It is also handy to determine a specific derived class that was called by using the new operator from within a parent class.\nIn a regular function call, the new.target returns undefined. If the function was called with the new operator, the new.target returns a reference to the function.\nfunction Person(name) { if (!new.target) { throw \u0026#34;must use new operator with Person\u0026#34;; } this.name = name; } Promise A Promise is a proxy for a value not necessarily known when the promise is created. It allows you to associate handlers with an asynchronous action\u0026rsquo;s eventual success value or failure reason. This lets asynchronous methods return values like synchronous methods: instead of immediately returning the final value, the asynchronous method returns a promise to supply the value at some point in the future.\nCreate To create a promise in JavaScript, you use the Promise constructor:\nlet completed = true; let learnJS = new Promise(function (resolve, reject) { if (completed) { resolve(\u0026#34;I have completed learning JS.\u0026#34;); } else { reject(\u0026#34;I haven\u0026#39;t completed learning JS yet.\u0026#34;); } }); The Promise constructor accepts a function as an argument. This function is called the executor.\nThe executor function accepts two callback functions with the names resolve() and reject().\nBy convention, the callback functions passed into the executor are resolve and reject. However, you can use any names you want.\nThe new Promise(executor) will automatically call the executor function.\nInside the executor function, you call the resolve() callback if the executor in the success case and the reject() callback in the failed case.\nStates Suppose you promise to complete learning JavaScript by next month.\nAnd you don’t know if you will spend your time and effort learning JavaScript until next month. You can either be completing learning JavaScript or not.\nA promise has three states:\nPending: you don’t know if you will complete learning JavaScript by the next month. Fulfilled: you complete learning JavaScript by the next month. Rejected: you don’t learn JavaScript at all. A promise starts in the pending state which indicates that the promise hasn’t been completed. It ends with either fulfilled (successful) or rejected (failed) state.\nOnce the promise reaches either fulfilled state or rejected state, it stays in that state and can’t switch.\nIn other words, a promise cannot go from the fulfilled state to the rejected state and vice versa. Also, it cannot go back from the fulfilled state or rejected state to the pending state.\nIf the promise reaches fulfilled or rejected state, the promise is resolved. Once a new Promise object is created, it is in the pending state until resolved.\nConsuming a promise then()\nTo get the value of a promise when it’s fulfilled, you call the then() method of the promise object. The following shows the syntax of the then() method:\npromise.then(onFulfilled,onRejected); The then() method accepts two callback functions: onFulfilled and onRejected.\nThe then() method calls the onFulfilled() with a value, if the promise is fulfilled or the onRejected() with an error if the promise is rejected.\nThe then() method returns a new Promise with a value resolved to a value.\nWhen you return a value in the then() method, the then() method returns a new Promise that immediately resolves to the return value.\nAlso, you can return a new promise in the then() method.\nNote that both onFulfilled and onRejected arguments are optional. We usually use then() method to schedule a callback to be executed when the promise is fulfilled\ncatch()\ncatch() method schedule a callback to be invoked when the promise is rejected. If the catch() method to handle the error inside the promise is not provided. It will cause a runtime error and terminate the program.\nfinally()\nPlace the code that you want to execute in the finally() method whether the promise is fulfilled or rejected.\nPromise Chaining Sometimes, you have multiple asynchronous tasks that you want to execute in sequence. In addition, you need to pass the result of the previous step to the next one. In this case, you can use the following syntax:\nstep1() .then(result =\u0026gt; step2(result)) .then(result =\u0026gt; step3(result)) ... If you need to pass the result from the previous task to the next one without passing the result, you use this syntax:\nstep1() .then(step2) .then(step3) ... Processing continues to the next link of the chain even when a .then() lacks a callback function that returns a Promise object. Therefore, a chain can safely omit every rejection callback function until the final .catch().\nSettled Promise An action can be assigned to an already \u0026ldquo;settled\u0026rdquo; promise. In that case, the action (if appropriate) will be performed at the first asynchronous opportunity. Note that promises are guaranteed to be asynchronous. Therefore, an action for an already \u0026ldquo;settled\u0026rdquo; promise will occur only after the stack has cleared and a clock-tick has passed. The effect is much like that of setTimeout(action,10).\nconst promiseA = new Promise( (resolutionFunc,rejectionFunc) =\u0026gt; { resolutionFunc(777); }); // At this point, \u0026#34;promiseA\u0026#34; is already settled. promiseA.then( (val) =\u0026gt; console.log(\u0026#34;asynchronous logging has val:\u0026#34;,val) ); console.log(\u0026#34;immediate logging\u0026#34;); // produces output in this order: // immediate logging // asynchronous logging has val: 777 Static methods Promise.all(iterable)\nWait for all promises to be resolved, or for any to be rejected.If the returned promise resolves, it is resolved with an aggregating array of the values from the resolved promises, in the same order as defined in the iterable of multiple promises. If it rejects, it is rejected with the reason from the first promise in the iterable that was rejected.\nPromise.allSettled(iterable)\nWait until all promises have settled (each may resolve or reject).Returns a Promise that resolves after all of the given promises is either fulfilled or rejected, with an array of objects that each describe the outcome of each promise.\nPromise.any(iterable)\nTakes an iterable of Promise objects and, as soon as one of the promises in the iterable fulfills, returns a single promise that resolves with the value from that promise.\nPromise.race(iterable)\nWait until any of the promises is fulfilled or rejected.If the returned promise resolves, it is resolved with the value of the first promise in the iterable that resolved.If it rejects, it is rejected with the reason from the first promise that was rejected.\nPromise.reject(reason)\nReturns a new Promise object that is rejected with the given reason.\nPromise.resolve(value)\nReturns a new Promise object that is resolved with the given value. If the value is a thenable (i.e. has a then method), the returned promise will \u0026ldquo;follow\u0026rdquo; that thenable, adopting its eventual state; otherwise, the returned promise will be fulfilled with the value. Generally, if you don\u0026rsquo;t know if a value is a promise or not, Promise.resolve(value) it instead and work with the return value as a promise.\nasync/await ES2017 introduced the async/await keywords that build on top of promises, allowing you to write asynchronous code that looks more like synchronous code and more readable. Technically speaking, the async / await is syntactic sugar for promises.\nIf a function returns a Promise, you can place the await keyword in front of the function call. The await keyword to wait for a Promise to settle either in resolved or rejected state.\nThe await will wait for the Promise returned from the f() to settle.\nThe await keyword can be used only inside the async functions.\nThe following defines an async function that calls the three asynchronous operations in sequence:\nasync function showServiceCost() { let user = await getUser(100); let services = await getServices(user); let cost = await getServiceCost(services); console.log(`The service cost is ${cost}`); } showServiceCost(); ","date":"2021-04-19T00:00:00Z","permalink":"https://dyhes.github.io/p/jses6/","title":"【JS】ES6"},{"content":"CSS stands for Cascading Style Sheets\nComment /* comment */ How to use External \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;mystyle.css\u0026#34;\u0026gt; Internal \u0026lt;style\u0026gt; ... \u0026lt;/style\u0026gt; Inline \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;mystyle.css\u0026#34;\u0026gt; If some properties have been defined for the same selector (element) in different style sheets, the value from the last read style sheet will be used.\nAll the styles in a page will \u0026ldquo;cascade\u0026rdquo; into a new \u0026ldquo;virtual\u0026rdquo; style sheet by the following rules, where number one has the highest priority:\nInline style (inside an HTML element) External and internal style sheets (in the head section) Browser default So, an inline style has the highest priority, and will override external and internal styles and browser defaults.\nSelector Simple selectors (select elements based on name, id, class) Combinator selectors (select elements based on a specific relationship between them) Pseudo-class selectors (select elements based on a certain state) Pseudo-elements selectors (select and style a part of an element) Attribute selectors (select elements based on an attribute or attribute value) Simple Selector element selector id selector class selector Combinator Selector descendant selector (space) ：matches all elements that are descendants of a specified element div p { background-color: yellow; } child selector (\u0026gt;) ：selects all elements that are the children of a specified element. div \u0026gt; p { background-color: yellow; } adjacent sibling selector (+) : select an element that is directly after another specific element.: div + p { background-color: yellow; } general sibling selector (~) : selects all elements that are next siblings of a specified element. div ~ p { background-color: yellow; } Pseudo-classes Selector A pseudo-class is used to define a special state of an element.\nsyntax selector:pseudo-class { property: value; } Anchor pseudo-classes order :link (unvisited) :visited :hover :active (clicked) :first-child The :first-child pseudo-class matches a specified element that is the first child of another element.\np:first-child { color: blue; } //first i in p p i:first-child { color: blue; } //all i in first p p:first-child i{ } :lang The :lang pseudo-class allows you to define special rules for different languages.\n\u0026lt;p\u0026gt;Some text \u0026lt;q lang=\u0026#34;no\u0026#34;\u0026gt;A quote in a paragraph\u0026lt;/q\u0026gt; Some text.\u0026lt;/p\u0026gt; \u0026lt;style\u0026gt; q:lang(no) { quotes: \u0026#34;~\u0026#34; \u0026#34;~\u0026#34;; } \u0026lt;/style\u0026gt; all pseudo classes Selector Example Example description :active a:active Selects the active link :checked input:checked Selects every checked element :disabled input:disabled Selects every disabled element :empty p:empty Selects every element that has no children :enabled input:enabled Selects every enabled element :first-child p:first-child Selects every elements that is the first child of its parent :first-of-type p:first-of-type Selects every element that is the first element of its parent :focus input:focus Selects the element that has focus :hover a:hover Selects links on mouse over :in-range input:in-range Selects elements with a value within a specified range :invalid input:invalid Selects all elements with an invalid value :lang(language) p:lang(it) Selects every element with a lang attribute value starting with \u0026ldquo;it\u0026rdquo; :last-child p:last-child Selects every elements that is the last child of its parent :last-of-type p:last-of-type Selects every element that is the last element of its parent :link a:link Selects all unvisited links :not(selector) :not(p) Selects every element that is not a element :nth-child(n) p:nth-child(2) Selects every element that is the second child of its parent :nth-last-child(n) p:nth-last-child(2) Selects every element that is the second child of its parent, counting from the last child :nth-last-of-type(n) p:nth-last-of-type(2) Selects every element that is the second element of its parent, counting from the last child :nth-of-type(n) p:nth-of-type(2) Selects every element that is the second element of its parent :only-of-type p:only-of-type Selects every element that is the only element of its parent :only-child p:only-child Selects every element that is the only child of its parent :optional input:optional Selects elements with no \u0026ldquo;required\u0026rdquo; attribute :out-of-range input:out-of-range Selects elements with a value outside a specified range :read-only input:read-only Selects elements with a \u0026ldquo;readonly\u0026rdquo; attribute specified :read-write input:read-write Selects elements with no \u0026ldquo;readonly\u0026rdquo; attribute :required input:required Selects elements with a \u0026ldquo;required\u0026rdquo; attribute specified :root root Selects the document\u0026rsquo;s root element :target #news:target Selects the current active #news element (clicked on a URL containing that anchor name) :valid input:valid Selects all elements with a valid value :visited a:visited Selects all visited links Pseudo-element Selector A CSS pseudo-element is used to style specified parts of an element.\nsyntax selector::pseudo-element { property: value; } ::first-line Note: The ::first-line pseudo-element can only be applied to block-level elements.\nThe following properties apply to the ::first-line pseudo-element:\nfont properties color properties background properties word-spacing letter-spacing text-decoration vertical-align text-transform line-height clear ::first-letter Note: The ::first-letter pseudo-element can only be applied to block-level elements.\nThe following properties apply to the ::first-letter pseudo- element:\nfont properties color properties background properties margin properties padding properties border properties text-decoration vertical-align (only if \u0026ldquo;float\u0026rdquo; is \u0026ldquo;none\u0026rdquo;) text-transform line-height float clear ::before and ::after The ::before and ::after pseudo-element can be used to insert some content before the content of an element.\nh1::before { content: url(smiley.gif); } all pseudo elements Selector Example Example description ::after p::after Insert something after the content of each element ::before p::before Insert something before the content of each element ::first-letter p::first-letter Selects the first letter of each element ::first-line p::first-line Selects the first line of each element ::marker ::marker Selects the markers of list items ::selection p::selection Selects the portion of an element that is selected by a user Attribute Selector [attribute] Selector The [attribute] selector is used to select elements with a specified attribute.\na[target] { background-color: yellow; } [attribute=\u0026ldquo;value\u0026rdquo;] Selector （等于） The [attribute=\u0026quot;value\u0026quot;] selector is used to select elements with a specified attribute and value.\na[target=\u0026#34;_blank\u0026#34;] { background-color: yellow; } [attribute~=\u0026ldquo;value\u0026rdquo;] Selector（限制包含） The [attribute~=\u0026quot;value\u0026quot;] selector is used to select elements with an attribute value containing a specified word.\n[title~=\u0026#34;flower\u0026#34;] { border: 5px solid yellow; } The example above will match elements with title=\u0026ldquo;flower\u0026rdquo;, title=\u0026ldquo;summer flower\u0026rdquo;, and title=\u0026ldquo;flower new\u0026rdquo;, but not title=\u0026ldquo;my-flower\u0026rdquo; or title=\u0026ldquo;flowers\u0026rdquo;.\n[attribute*=\u0026ldquo;value\u0026rdquo;] Selector（包含） The [attribute*=\u0026quot;value\u0026quot;] selector is used to select elements whose attribute value contains a specified value.\nThe following example selects all elements with a class attribute value that contains \u0026ldquo;te\u0026rdquo;:\nNote: The value does not have to be a whole word!\n[class*=\u0026#34;te\u0026#34;] { background: yellow; } [attribute|=\u0026ldquo;value\u0026rdquo;] Selector（限制开始） The [attribute|=\u0026quot;value\u0026quot;] selector is used to select elements with the specified attribute, whose value can be exactly the specified value, or the specified value followed by a hyphen (-).\nNote: The value has to be a whole word, either alone, like class=\u0026ldquo;top\u0026rdquo;, or followed by a hyphen( - ), like class=\u0026ldquo;top-text\u0026rdquo;.\n[class|=\u0026#34;top\u0026#34;] { background: yellow; } [attribute^=\u0026ldquo;value\u0026rdquo;] Selector（开始） The [attribute^=\u0026quot;value\u0026quot;] selector is used to select elements with the specified attribute, whose value starts with the specified value.\nThe following example selects all elements with a class attribute value that starts with \u0026ldquo;top\u0026rdquo;:\nNote: The value does not have to be a whole word!\n[class^=\u0026#34;test\u0026#34;] { background: yellow; } [attribute$=\u0026ldquo;value\u0026rdquo;] Selector（结束） The [attribute$=\u0026quot;value\u0026quot;] selector is used to select elements whose attribute value ends with a specified value.\nThe following example selects all elements with a class attribute value that ends with \u0026ldquo;test\u0026rdquo;:\nNote: The value does not have to be a whole word!\n[class$=\u0026#34;test\u0026#34;] { background: yellow; } Special Selector Universal Selector The universal selector (*) selects all HTML elements on the page.\nGrouping Selector The universal selector (*) selects all HTML elements on the page.\nh1, h2, p { text-align: center; color: red; } Specificity If there are two or more CSS rules that point to the same element, the selector with the highest specificity value will \u0026ldquo;win\u0026rdquo;, and its style declaration will be applied to that HTML element.\nEvery CSS selector has its place in the specificity hierarchy.\nThere are four categories which define the specificity level of a selector:\nInline styles (1000)- Example: IDs (100) - Example: #navbar Classes, pseudo-classes, attribute selectors (10) - Example: .test, :hover, [href] Elements and pseudo-elements (1 ) - Example: h1, ::before Equal specificity: the latest rule wins\nThe universal selector (*) and inherited values have a specificity of 0 - The universal selector (*) and inherited values are ignored!\n！important if you use the !important rule, it will override ALL previous styling rules for that specific property on that element!\n#myid { background-color: blue; } .myclass { background-color: gray; } p { background-color: red !important; } The only way to override an !important rule is to include another !important rule on a declaration with the same (or higher) specificity in the source code - and here the problem starts! This makes the CSS code confusing and the debugging will be hard, especially if you have a large style sheet!\ndo not use it unless you absolutely have to.\nColors type background color\nbackground-color background text color\ncolor border color\nborder value RGB rgb(red,green, blue)\nEach parameter (red, green, and blue) defines the intensity of the color between 0 and 255.\nRGBA rgba(red,green,blue, alpha)\nThe alpha parameter is a number between 0.0 (fully transparent) and 1.0 (not transparent at all)\nHEX A hexadecimal color is specified with: #RRGGBB, where the RR (red), GG (green) and BB (blue) hexadecimal integers specify the components of the color.\nIn CSS, a color can be specified using a hexadecimal value in the form:\n#rrggbb\nWhere rr (red), gg (green) and bb (blue) are hexadecimal values between 00 and ff (same as decimal 0-255).\nHSL In CSS, a color can be specified using hue, saturation, and lightness (HSL) in the form:\nhsl(hue, saturation, lightness)\nHue is a degree on the color wheel from 0 to 360. 0 is red, 120 is green, and 240 is blue.\nSaturation is a percentage value, 0% means a shade of gray, and 100% is the full color.\nLightness is also a percentage, 0% is black, 50% is neither light or dark, 100% is white\nHSLA hsla(hue, saturation, lightness, alpha)\nThe alpha parameter is a number between 0.0 (fully transparent) and 1.0 (not transparent at all)\nKeyword transparent equal to rgba(0,0,0,0)\ncurrentcolor is like a variable that holds the current value of the color property of an element.\nOpacity The alpha parameter is a number between 0.0 (fully transparent) and 1.0 (not transparent at all)\nGradients CSS defines three types of gradients:\nLinear Gradients (goes down/up/left/right/diagonally) background-image: (repeating)-linear-gradient(direction/angle, color-stop1, color-stop2, \u0026hellip;);\nRadial Gradients (defined by their center) background-image: (repeating)-radial-gradient(shape size at position, start-color, \u0026hellip;, last-color);\nConic Gradients (rotated around a center point) background-image: (repeating)-radial-gradient(shape size at position, start-color, \u0026hellip;, last-color);\nBackground background-color explained before\nbackground-image By default, the image is repeated so it covers the entire element.\nbody { background-image: url(\u0026#34;paper.gif\u0026#34;); } background-repeat By default, the background-image property repeats an image both horizontally and vertically.\nbody { background-image: url(\u0026#34;gradient_bg.png\u0026#34;); background-repeat: repeat(default) | repeat-x | repeat-y | no-repeat | space | round | initial | inherit; } background-position The background-position property is used to specify the position of the background image.\nbackground-attachment The background-attachment property specifies whether the background image should scroll or be fixed (will not scroll with the rest of the page)\nbackground shorthand property for above properties\nBox Model Explanation of the different parts:\nContent - The content of the box, where text and images appear Padding - Clears an area around the content. The padding is transparent Border - A border that goes around the padding and content Margin - Clears an area outside the border. The margin is transparent Padding Padding is used to create space around an element\u0026rsquo;s content, inside of any defined borders.\nCSS has properties for specifying the padding for each side of an element:\npadding-top padding-right padding-bottom padding-left padding(shorthand) All the padding properties can have the following values:\nlength - specifies a padding in px, pt, cm, etc. % - specifies a padding in % of the width of the containing element inherit - specifies that the padding should be inherited from the parent element Note: Negative values are not allowed.\nIf the padding property has three values:\npadding: 25px 50px 75px; top padding is 25px right and left paddings are 50px bottom padding is 75px Border The CSS border properties allow you to specify the style, width, and color of an element\u0026rsquo;s border.\nborder-style border-width The width can be set as a specific size (in px, pt, cm, em, etc) or by using one of the three pre-defined values: thin, medium, or thick\nThe border-width property can have from one to four values (for the top border, right border, bottom border, and the left border)\nborder-color border(shorthand) the border-style is required\nborder-image you can set an image to be used as the border around an element.\nTip: The border-image property is actually a shorthand property for the border-image-source, border-image-slice, border-image-width, border-image-outset and border-image-repeat properties.\nSide-Specific border-(side)-(value)\nRounded With the CSS border-radius property, you can give any element \u0026ldquo;rounded corners\u0026rdquo;.\nTip: The border-radius property is actually a shorthand property for the border-top-left-radius, border-top-right-radius, border-bottom-right-radius and border-bottom-left-radius properties\nMargins Margins are used to create space around elements, outside of any defined borders.\nmargin-top margin-right margin-bottom margin-left margin (shorthand) All the margin properties can have the following values:\nauto - the browser calculates the margin length - specifies a margin in px, pt, cm, etc. % - specifies a margin in % of the width of the containing element inherit - specifies that the margin should be inherited from the parent element Tip: Negative values are allowed.\nIf the margin property has three values:\nmargin: 25px 50px 75px; top margin is 25px right and left margins are 50px bottom margin is 75px Margin Collapse Top and bottom margins of elements are sometimes collapsed into a single margin that is equal to the largest of the two margins.\nThis does not happen on left and right margins! Only top and bottom margins!\nWidth/Height The height and width properties are used to set the height and width of an element.\nThe height and width properties do not include padding, borders, or margins. It sets the height/width of the area inside the padding, border, and margin of the element.\nThe height and width properties may have the following values:\nauto - This is default. The browser calculates the height and width length - Defines the height/width in px, cm etc. % - Defines the height/width in percent of the containing block initial - Sets the height/width to its default value inherit - The height/width will be inherited from its parent value we can also use min/max-width/height to achieve our goal.\nBox Sizing The CSS box-sizing property allows us to include the padding and border in an element\u0026rsquo;s total width and height.\nBy default, the width and height of an element is calculated like this:\nwidth + padding + border = actual width of an element height + padding + border = actual height of an element\nThis means: When you set the width/height of an element, the element often appears bigger than you have set\nIf you set box-sizing: border-box; on an element, padding and border are included in the width and height\nOutline An outline is a line drawn outside the element\u0026rsquo;s border.\nCSS has the following outline properties:\noutline-style outline-color outline-width outline-offset outline Note: Outline differs from borders! Unlike border, the outline is drawn outside the element\u0026rsquo;s border, and may overlap other content. Also, the outline is NOT a part of the element\u0026rsquo;s dimensions; the element\u0026rsquo;s total width and height is not affected by the width of the outline.\nText Text Color The color property is used to set the color of the text.\nText Alignment text-align set the horizontal alignment of a text.\nWhen the text-align property is set to \u0026ldquo;justify\u0026rdquo;, each line is stretched so that every line has equal width, and the left and right margins are straight (like in magazines and newspapers)\ntext-align-last The text-align-last property specifies how to align the last line of a text.\ndirection unicode-bidi The direction and unicode-bidi properties can be used to change the text direction of an element\nvertical-align vertical-align property sets the vertical alignment of an element.\nText Decoration text-decoration-line text-decoration-color text-decoration-style text-decoration-thickness text-decoration Text Transformation The text-transform property is used to specify uppercase and lowercase letters in a text.\ntext-transform: none|capitalize|uppercase|lowercase|initial|inherit;\nText Spacing text-indent specify the indentation of the first line of a text\nletter-spacing line-height word-spacing white-space Text Shadow horizontal shadow ;vertical shadow ;blur effect;colors\nh1 { color: white; text-shadow: 2px 2px 4px #000000; } Text Effect text-overflow specifies how overflowed content that is not displayed should be signaled to the user.\nword-wrap allows long words to be able to be broken and wrap onto the next line.\nword-break specifies line breaking rules.\nwriting-mode specifies whether lines of text are laid out horizontally or vertically.\nFont Font Family In CSS there are five generic font families:\nSerif fonts have a small stroke at the edges of each letter. They create a sense of formality and elegance. Sans-serif fonts have clean lines (no small strokes attached). They create a modern and minimalistic look. Monospace fonts - here all the letters have the same fixed width. They create a mechanical look. Cursive fonts imitate human handwriting. Fantasy fonts are decorative/playful fonts. Note: If the font name is more than one word, it must be in quotation marks, like: \u0026ldquo;Times New Roman\u0026rdquo;.\nTip: The font-family property should hold several font names as a \u0026ldquo;fallback\u0026rdquo; system, to ensure maximum compatibility between browsers/operating systems. Start with the font you want, and end with a generic family (to let the browser pick a similar font in the generic family, if no other fonts are available). The font names should be separated with comma.\nThe following list are the best web safe fonts for HTML and CSS:\nArial (sans-serif) Verdana (sans-serif) Helvetica (sans-serif) Tahoma (sans-serif) Trebuchet MS (sans-serif) Times New Roman (serif) Georgia (serif) Garamond (serif) Courier New (monospace) Brush Script MT (cursive) Font Style font-style property is mostly used to specify italic text.\nThis property has three values:\nnormal - The text is shown normally italic - The text is shown in italics oblique - The text is \u0026ldquo;leaning\u0026rdquo; (oblique is very similar to italic, but less supported) font-weight specifies the weight of a font ( normal bold)\nfont-variant property specifies whether or not a text should be displayed in a small-caps font.( In a small-caps font, all lowercase letters are converted to uppercase letters. However, the converted uppercase letters appears in a smaller font size than the original uppercase letters in the text. )\nFont Size The font-size value can be an absolute, or relative size.\nAbsolute size:\nSets the text to a specified size Does not allow a user to change the text size in all browsers (bad for accessibility reasons) Absolute size is useful when the physical size of the output is known Relative size:\nSets the size relative to surrounding elements Allows a user to change the text size in browsers Note: If you do not specify a font size, the default size for normal text, like paragraphs, is 16px (16px=1em).\n1em is equal to the current font size. The default text size in browsers is 16px. So, the default size of 1em is 16px.\nwith the em size, it is possible to adjust the text size in all browsers.\nThe solution that works in all browsers, is to set a default font-size in percent for the element:\nbody { font-size: 100%; } h1 { font-size: 2.5em; } h2 { font-size: 1.875em; } p { font-size: 0.875em; } font The font property is a shorthand property for:\nfont-style font-variant font-weight font-size/line-height font-family Note: The font-size and font-family values are required. If one of the other values is missing, their default value are used.\nGoogle Fonts If you do not want to use any of the standard fonts in HTML, you can use Google Fonts.\nGoogle Fonts are free to use, and have more than 1000 fonts to choose from.\nJust add a special style sheet link in the section and then refer to the font in the CSS.\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://fonts.googleapis.com/css?family=Sofia|Audiowide|Sofia|Trirong\u0026#34;\u0026gt; Google have also enabled different font effects that you can use.\nFirst add effect=effectname to the Google API, then add a special class name to the element that is going to use the special effect. The class name always starts with font-effect- and ends with the effectname.\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://fonts.googleapis.com/css?family=Sofia\u0026amp;effect=fire\u0026#34;\u0026gt; \u0026lt;h1 class=\u0026#34;font-effect-fire\u0026#34;\u0026gt;Sofia on Fire\u0026lt;/h1\u0026gt; Web Fonts Web fonts allow Web designers to use fonts that are not installed on the user\u0026rsquo;s computer.\nWhen you have found/bought the font you wish to use, just include the font file on your web server, and it will be automatically downloaded to the user when needed.\nYour \u0026ldquo;own\u0026rdquo; fonts are defined within the CSS @font-face rule.\n@font-face { font-family: myFirstFont; src: url(sansation_light.woff); } div { font-family: myFirstFont; } Lists list-style-type `list-style-image`` ``list-style-position` list-style ( shorthand ) Layout Display The display property is the most important CSS property for controlling layout.\nEvery HTML element has a default display value depending on what type of element it is. The default display value for most elements is block or inline.\ndisplay: none; is commonly used with JavaScript to hide and show elements without deleting and recreating them.\nvisibility:hidden; also hides an element.\nHowever, the element will still take up the same space as before. The element will be hidden, but still affect the layout\nValue Description Play it inline Displays an element as an inline element (like ). Any height and width properties will have no effect Demo ❯ block Displays an element as a block element (like ). It starts on a new line, and takes up the whole width Demo ❯ contents Makes the container disappear, making the child elements children of the element the next level up in the DOM flex Displays an element as a block-level flex container grid Displays an element as a block-level grid container inline-block Displays an element as an inline-level block container. The element itself is formatted as an inline element, but you can apply height and width values inline-flex Displays an element as an inline-level flex container inline-grid Displays an element as an inline-level grid container inline-table The element is displayed as an inline-level table list-item Let the element behave like a element Demo ❯ run-in Displays an element as either block or inline, depending on context table Let the element behave like a element table-caption Let the element behave like a element table-column-group Let the element behave like a element table-header-group Let the element behave like a element table-footer-group Let the element behave like a element table-row-group Let the element behave like a element table-cell Let the element behave like a element table-column Let the element behave like a element table-row Let the element behave like a element none The element is completely removed initial Sets this property to its default value. Read about initial inherit Inherits this property from its parent element. Read about inherit display:inline-block Compared to display: inline, the major difference is that display: inline-block allows to set a width and height on the element.\nAlso, with display: inline-block, the top and bottom margins/paddings are respected, but with display: inline they are not.\nCompared to display: block, the major difference is that display: inline-block does not add a line-break after the element, so the element can sit next to other elements.\nPosition The position property specifies the type of positioning method used for an element.\nThere are five different position values:\nstatic HTML elements are positioned static by default.\nAn element with position: static; is not positioned in any special way; it is always positioned according to the normal flow of the page\nrelative An element with position: relative; is positioned relative to its normal position.\nSetting the top, right, bottom, and left properties of a relatively-positioned element will cause it to be adjusted away from its normal position. Other content will not be adjusted to fit into any gap left by the element.\nfixed An element with position: fixed; is positioned relative to the viewport, which means it always stays in the same place even if the page is scrolled. The top, right, bottom, and left properties are used to position the element.\nabsolute An element with position: absolute; is positioned relative to the nearest positioned ancestor (instead of positioned relative to the viewport, like fixed).\nHowever; if an absolute positioned element has no positioned ancestors, it uses the document body, and moves along with page scrolling.\nsticky A sticky element toggles between relative and fixed, depending on the scroll position. It is positioned relative until a given offset position is met in the viewport - then it \u0026ldquo;sticks\u0026rdquo; in place (like position:fixed).\nZ-index specifies the stack order of an element.\nz-index only works on positioned elements (position: absolute, position: relative, position: fixed, or position: sticky) and flex items (elements that are direct children of display: flex elements).\nOverflow controls what happens to content that is too big to fit into an area, specifies whether to clip the content or to add scrollbars when the content of an element is too big to fit in the specified area.\nvisible - Default. The overflow is not clipped. The content renders outside the element\u0026rsquo;s box hidden - The overflow is clipped, and the rest of the content will be invisible scroll - The overflow is clipped, and a scrollbar is added to see the rest of the content auto - Similar to scroll, but it adds scrollbars only when necessary The overflow-x and overflow-y properties specifies whether to change the overflow of content just horizontally or vertically (or both):\nFloat and Clear float The CSS float property specifies how an element should float.\nThe CSS clear property specifies what elements can float beside the cleared element and on which side.\nThe float property can have one of the following values:\nleft - The element floats to the left of its container right - The element floats to the right of its container none - The element does not float (will be displayed just where it occurs in the text). This is default inherit - The element inherits the float value of its parent clear When we use the float property, and we want the next element below (not on right or left), we will have to use the clear property.\nThe clear property specifies what should happen with the element that is next to a floating element.\nThe clear property can have one of the following values:\nnone - The element is not pushed below left or right floated elements. This is default left - The element is pushed below left floated elements right - The element is pushed below right floated elements both - The element is pushed below both left and right floated elements inherit - The element inherits the clear value from its parent When clearing floats, you should match the clear to the float: If an element is floated to the left, then you should clear to the left. Your floated element will continue to float, but the cleared element will appear below it on the web page.\nIf a floated element is taller than the containing element, it will \u0026ldquo;overflow\u0026rdquo; outside of its container. We can then add a clearfix hack to solve this problem\nAlign Horizontally To horizontally center a block element (like ), use margin: auto;\nNote: Center aligning has no effect if the width property is not set (or set to 100%).\nTo just center the text inside an element, use text-align: center;\nVertically There are many ways to center an element vertically in CSS. A simple solution is to use top and bottom padding\nAnother trick is to use the line-height property with a value that is equal to the height property\nanother solution is to use positioning and the transform property\n.center { height: 200px; position: relative; border: 3px solid green; } .center p { margin: 0; position: absolute; top: 50%; left: 50%; -ms-transform: translate(-50%, -50%); transform: translate(-50%, -50%); } You can also use flexbox to center things. Just note that flexbox is not supported in IE10 and earlier versions Flex The Flexible Box Layout Module, makes it easier to design flexible responsive layout structure without using float or positioning.\ncontainer The flex container becomes flexible by setting the display property to flex.\nThe flex container properties are:\nflex-direction defines in which direction the container wants to stack the flex items.\ncolumn | column-reverse | row | row-reverse flex-wrap specifies whether the flex items should wrap or not.\nnowrap(default) | wrap |wrap-reverse\nflex-flow shorthand for flex-direction and flex-wrap\njustify-content\nflex-start center flex-end space-around :displays the flex items with space before, between, and after the lines space-between：displays the flex items with space between the lines space-evenly:equal space around them align-items :used to align the flex items.\nflex-start flex-end center stretch : Items are stretched to fit the container baseline: Items are positioned at the baseline of the container align-content: but instead of aligning flex items, it aligns flex lines\nitem The flex item properties are:\norder specifies the appearance order of the flex items.\nThe first flex item in the code does not have to appear as the first item in the layout.\nThe order value must be a number, default value is 0.\nflex-grow The flex-grow property specifies how much a flex item will grow relative to the rest of the flex items.\nflex-shrink The flex-grow property specifies how much a flex item will shrink relative to the rest of the flex items.\nflex-basis specifies the initial length of a flex item.\nflex shorthand property for the flex-grow, flex-shrink, and flex-basis properties.\nalign-self specifies the alignment for the selected item inside the flexible container.\nGrid container grid-template-columns grid-template-rows justify-content align-content item By default, a container has one grid item for each column, in each row, but you can style the grid items so that they will span multiple columns and/or rows.\ngrid-column shorthand property for the grid-column-start and the grid-column-end properties., defines on which column(s) to place an item.\n.item1 { grid-column: 1 / 5; } .item1 { grid-column: 1 / span 3; } grid-row\ngird-area : shorthand for the above two properties\n.item8 { grid-area: 1 / 2 / 5 / 6; } .item8 { grid-area: 1 / 2 / 5 / 6; } Units CSS has several different units for expressing a length.\nNote: A whitespace cannot appear between the number and the unit. However, if the value is 0, the unit can be omitted.\nThere are two types of length units: absolute and relative.\nAbsolute Absolute length units are not recommended for use on screen, because screen sizes vary so much. However, they can be used if the output medium is known, such as for print layout.\nAbsolute length units are not recommended for use on screen, because screen sizes vary so much. However, they can be used if the output medium is known, such as for print layout.\nUnit Description cm centimetersTry it mm millimetersTry it in inches (1in = 96px = 2.54cm)Try it px * pixels (1px = 1/96th of 1in)Try it pt points (1pt = 1/72 of 1in)Try it pc picas (1pc = 12 pt) * Pixels (px) are relative to the viewing device. For low-dpi devices, 1px is one device pixel (dot) of the display. For printers and high resolution screens 1px implies multiple device pixels.\nRelative Relative length units specify a length relative to another length property. Relative length units scales better between different rendering mediums.\nUnit Description em Relative to the font-size of the element (2em means 2 times the size of the current font) Try it ex Relative to the x-height of the current font (rarely used) Try it ch Relative to width of the \u0026ldquo;0\u0026rdquo; (zero) Try it rem Relative to font-size of the root element Try it vw Relative to 1% of the width of the viewport* Try it vh Relative to 1% of the height of the viewport* Try it vmin Relative to 1% of viewport\u0026rsquo;s* smaller dimension Try it vmax Relative to 1% of viewport\u0026rsquo;s* larger dimension Try it % Relative to the parent element Math Functions calc() calc(expression)\n{ width: calc(100% - 100px); } max() max(value1, value2, \u0026hellip;)\n{ width: max(50%, 300px); } min() max(value1, value2, \u0026hellip;)\n{ width: min(50%, 300px); } Transforms 2D With the CSS transform property you can use the following 2D transformation methods:\ntranslate() moves an element from its current position (according to the parameters given for the X-axis and the Y-axis).\nrotate() rotates an element clockwise(positive deg) or counter-clockwise (negative deg) according to a given degree.\nscaleX() increases or decreases the width of an element.\nscaleY() increases or decreases the height of an element.\nscale() increases or decreases the size of an element (according to the parameters given for the width and height).\nskewX() skews an element along the X-axis by the given angle.\nskewY() skews an element along the Y-axis by the given angle.\nskew() skews an element along the X and Y-axis by the given angles.\nmatrix() combines all the 2D transform methods into one.\nmatrix(scaleX(),skewY(),skewX(),scaleY(),translateX(),translateY())\n3D CSS also supports 3D transformations.\nFunction Description matrix3d (n,n,n,n,n,n,n,n,n,n,n,n,n,n,n,n) Defines a 3D transformation, using a 4x4 matrix of 16 values translate3d(x,y,z) Defines a 3D translation translateX(x) Defines a 3D translation, using only the value for the X-axis translateY(y) Defines a 3D translation, using only the value for the Y-axis translateZ(z) Defines a 3D translation, using only the value for the Z-axis scale3d(x,y,z) Defines a 3D scale transformation scaleX(x) Defines a 3D scale transformation by giving a value for the X-axis scaleY(y) Defines a 3D scale transformation by giving a value for the Y-axis scaleZ(z) Defines a 3D scale transformation by giving a value for the Z-axis rotate3d(x,y,z,angle) Defines a 3D rotation rotateX(angle) Defines a 3D rotation along the X-axis rotateY(angle) Defines a 3D rotation along the Y-axis rotateZ(angle) Defines a 3D rotation along the Z-axis perspective(n) Defines a perspective view for a 3D transformed element Transitions transition transition-delay specifies a delay (in seconds) for the transition effect.\ntransition-duration transition-property transition-timing-function To create a transition effect, you must specify two things:\nthe CSS property you want to add an effect to the duration of the effect Note: If the duration part is not specified, the transition will have no effect, because the default value is 0.\ndiv { transition: width 2s, height 4s; } div:hover { width: 300px; } /*Several Property Values*/ div:hover { width: 300px; } Timing Function The transition-timing-function property specifies the speed curve of the transition effect.\nThe transition-timing-function property can have the following values:\nease - specifies a transition effect with a slow start, then fast, then end slowly (this is default) linear - specifies a transition effect with the same speed from start to end ease-in - specifies a transition effect with a slow start ease-out - specifies a transition effect with a slow end ease-in-out - specifies a transition effect with a slow start and end cubic-bezier(n,n,n,n) - lets you define your own values in a cubic-bezier function Animations @keyframes To use CSS animation, you must first specify some keyframes for the animation.\nKeyframes hold what styles the element will have at certain times.\nanimation-name animation-duration The animation-duration property defines how long an animation should take to complete. If the animation-duration property is not specified, no animation will occur, because the default value is 0s (0 seconds).\nanimation-delay Negative values are also allowed. If using negative values, the animation will start as if it had already been playing for N seconds.\nanimation-iteration-count\nanimation-direction\nnormal - The animation is played as normal (forwards). This is default reverse - The animation is played in reverse direction (backwards) alternate - The animation is played forwards first, then backwards alternate-reverse - The animation is played backwards first, then forwards animation-timing-function\nanimation-fill-mode\nnone - Default value. Animation will not apply any styles to the element before or after it is executing forwards - The element will retain the style values that is set by the last keyframe (depends on animation-direction and animation-iteration-count) backwards - The element will get the style values that is set by the first keyframe (depends on animation-direction), and retain this during the animation-delay period both - The animation will follow the rules for both forwards and backwards, extending the animation properties in both directions CSS animations do not affect an element before the first keyframe is played or after the last keyframe is played. The animation-fill-mode property can override this behavior.\nanimation Media Queries The @media rule, introduced in CSS2, made it possible to define different style rules for different media types.\nUnfortunately these media types never got a lot of support by devices, other than the print media type.\nMedia queries in CSS3 extended the CSS2 media types idea: Instead of looking for a type of device, they look at the capability of the device.\nwidth and height of the viewport width and height of the device orientation (is the tablet/phone in landscape or portrait mode?) resolution A media query consists of a media type and can contain one or more expressions, which resolve to either true or false.\n@media not|only mediatype and (mediafeature and|or|not mediafeature) { CSS-Code; } Media Types Value Description all Default. Used for all media type devices print Used for printers screen Used for computer screens, tablets, smart-phones etc. speech Used for screenreaders that \u0026ldquo;reads\u0026rdquo; the page out loud Media Features Value Description any-hover Does any available input mechanism allow the user to hover over elements? (added in Media Queries Level 4) any-pointer Is any available input mechanism a pointing device, and if so, how accurate is it? (added in Media Queries Level 4) aspect-ratio The ratio between the width and the height of the viewport color The number of bits per color component for the output device color-gamut The approximate range of colors that are supported by the user agent and output device (added in Media Queries Level 4) color-index The number of colors the device can display grid Whether the device is a grid or bitmap height The viewport height hover Does the primary input mechanism allow the user to hover over elements? (added in Media Queries Level 4) inverted-colors Is the browser or underlying OS inverting colors? (added in Media Queries Level 4) light-level Current ambient light level (added in Media Queries Level 4) max-aspect-ratio The maximum ratio between the width and the height of the display area max-color The maximum number of bits per color component for the output device max-color-index The maximum number of colors the device can display max-height The maximum height of the display area, such as a browser window max-monochrome The maximum number of bits per \u0026ldquo;color\u0026rdquo; on a monochrome (greyscale) device max-resolution The maximum resolution of the device, using dpi or dpcm max-width The maximum width of the display area, such as a browser window min-aspect-ratio The minimum ratio between the width and the height of the display area min-color The minimum number of bits per color component for the output device min-color-index The minimum number of colors the device can display min-height The minimum height of the display area, such as a browser window min-monochrome The minimum number of bits per \u0026ldquo;color\u0026rdquo; on a monochrome (greyscale) device min-resolution The minimum resolution of the device, using dpi or dpcm min-width The minimum width of the display area, such as a browser window monochrome The number of bits per \u0026ldquo;color\u0026rdquo; on a monochrome (greyscale) device orientation The orientation of the viewport (landscape or portrait mode) overflow-block How does the output device handle content that overflows the viewport along the block axis (added in Media Queries Level 4) overflow-inline Can content that overflows the viewport along the inline axis be scrolled (added in Media Queries Level 4) pointer Is the primary input mechanism a pointing device, and if so, how accurate is it? (added in Media Queries Level 4) resolution The resolution of the output device, using dpi or dpcm scan The scanning process of the output device scripting Is scripting (e.g. JavaScript) available? (added in Media Queries Level 4) update How quickly can the output device modify the appearance of the content (added in Media Queries Level 4) width The viewport width e.g.\n/* changes the background-color to lightgreen if the viewport is 480 pixels wide or wider */ @media screen and (min-width: 480px) { body { background-color: lightgreen; } } Variables The var() function is used to insert the value of a CSS variable.\nCSS variables have access to the DOM, which means that you can create variables with local or global scope, change the variables with JavaScript, and change the variables based on media queries.\nA good way to use CSS variables is when it comes to the colors of your design. Instead of copy and paste the same colors over and over again, you can place them in variables.\nvar(\u0026ndash;name, value)\nname Required. The variable name (must start with two dashes) value Optional. The fallback value (used if the variable is not found) global and local CSS variables can have a global or local scope.\nGlobal variables can be accessed/used through the entire document, while local variables can be used only inside the selector where it is declared.\nTo create a variable with global scope, declare it inside the :root selector. The :root selector matches the document\u0026rsquo;s root element.\nTo create a variable with local scope, declare it inside the selector that is going to use it.\n:root { --blue: #1e90ff; --white: #ffffff; } body { background-color: var(--blue); } h2 { border-bottom: 2px solid var(--blue); } .container { color: var(--blue); background-color: var(--white); padding: 15px; } button { background-color: var(--white); color: var(--blue); border: 1px solid var(--blue); padding: 5px; } override we can re-declare the \u0026ndash;blue variable inside a specific selector. When we use var(\u0026ndash;blue) inside this selector, it will use the local variable redeclared within it.\nwith javascript CSS variables have access to the DOM, which means that you can change them with JavaScript.\nResponsive Viewport HTML5 introduced a method to let web designers take control over the viewport, through the \u0026lt;meta\u0026gt; tag.\nYou should include the following \u0026lt;meta\u0026gt; viewport element in all your web pages:\n\u0026lt;meta name=\u0026ldquo;viewport\u0026rdquo; content=\u0026ldquo;width=device-width, initial-scale=1.0\u0026rdquo;\u0026gt;\nThis gives the browser instructions on how to control the page\u0026rsquo;s dimensions and scaling.\nThe width=device-width part sets the width of the page to follow the screen-width of the device (which will vary depending on the device).\nThe initial-scale=1.0 part sets the initial zoom level when the page is first loaded by the browser.\nMedia Queries Media Queries are about defining different style rules for different devices (screens, tablets, mobile phones, etc.)\nOthers object-fit The CSS object-fit property is used to specify how an or should be resized to fit its container.\nfill - This is default. The image is resized to fill the given dimension. If necessary, the image will be stretched or squished to fit contain - The image keeps its aspect ratio, but is resized to fit within the given dimension cover - The image keeps its aspect ratio and fills the given dimension. The image will be clipped to fit none - The image is not resized scale-down - the image is scaled down to the smallest version of none or contain object-position Masking With CSS masking you create a mask layer to place over an element to partially or fully hide portions of the element.\nNote: Most browsers only have partial support for CSS masking. You will need to use the -webkit- prefix in addition to the standard property in most browsers.\n.mask1 { -webkit-mask-image: url(w3logo.png); mask-image: url(w3logo.png); -webkit-mask-repeat: no-repeat; mask-repeat: no-repeat; } User Interface resize The resize property specifies if (and how) an element should be resizable by the user.\noutline-offset The outline-offset property adds space between an outline and the edge or border of an element.\n","date":"2021-04-17T00:00:00Z","permalink":"https://dyhes.github.io/p/cssbasics/","title":"【CSS】Basics"},{"content":"\u0026lt;iframe\u0026gt; An iframe can be used as the target frame for a link.\nThe target attribute of the link must refer to the name attribute of the iframe:\n\u0026lt;meta\u0026gt; The \u0026lt;meta\u0026gt; element is typically used to specify the character set, page description, keywords, author of the document, and viewport settings. The metadata will not be displayed on the page, but are used by browsers (how to display content or reload page), by search engines (keywords), and other web services.\nDefine the character set used: \u0026lt;meta charset=\u0026ldquo;UTF-8\u0026rdquo;\u0026gt;\nDefine keywords for search engines: \u0026lt;meta name=\u0026ldquo;keywords\u0026rdquo; content=\u0026ldquo;HTML, CSS, JavaScript\u0026rdquo;\u0026gt;\nDefine a description of your web page: \u0026lt;meta name=\u0026ldquo;description\u0026rdquo; content=\u0026ldquo;Free Web tutorials\u0026rdquo;\u0026gt;\nDefine the author of a page: \u0026lt;meta name=\u0026ldquo;author\u0026rdquo; content=\u0026ldquo;John Doe\u0026rdquo;\u0026gt;\nRefresh document every 30 seconds: \u0026lt;meta http-equiv=\u0026ldquo;refresh\u0026rdquo; content=\u0026ldquo;30\u0026rdquo;\u0026gt;\nSetting the viewport to make your website look good on all devices: \u0026lt;meta name=\u0026ldquo;viewport\u0026rdquo; content=\u0026ldquo;width=device-width, initial-scale=1.0\u0026rdquo;\u0026gt;\nbase he \u0026lt;base\u0026gt; element specifies the base URL and/or target for all relative URLs in a page.\nThe \u0026lt;base\u0026gt; tag must have either an href or a target attribute present, or both.\nThere can only be one single \u0026lt;base\u0026gt; element in a document!\n\u0026lt;head\u0026gt; \u0026lt;base href=\u0026#34;https://www.w3schools.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; Layout There are four different techniques to create multicolumn layouts. Each technique has its pros and cons:\nCSS framework CSS float property CSS flexbox CSS grid Segmentic In HTML there are some semantic elements that can be used to define different parts of a web page:\nForm Attribute Action The action attribute defines the action to be performed when the form is submitted.\nTip: If the action attribute is omitted, the action is set to the current page.\nTarget The target attribute can have one of the following values:\nValue Description _blank The response is displayed in a new window or tab _self The response is displayed in the current window _parent The response is displayed in the parent frame _top The response is displayed in the full body of the window framename The response is displayed in a named iframe The default value is _self which means that the response will open in the current window.\nMethod The method attribute specifies the HTTP method to be used when submitting the form data.\nThe form-data can be sent as URL variables (with method=\u0026quot;get\u0026quot;) or as HTTP post transaction (with method=\u0026quot;post\u0026quot;).\nThe default HTTP method when submitting form data is GET.\nNotes on GET:\nAppends the form data to the URL, in name/value pairs NEVER use GET to send sensitive data! (the submitted form data is visible in the URL!) The length of a URL is limited (2048 characters) Useful for form submissions where a user wants to bookmark the result GET is good for non-secure data, like query strings in Google Notes on POST:\nAppends the form data inside the body of the HTTP request (the submitted form data is not shown in the URL) POST has no size limitations, and can be used to send large amounts of data. Form submissions with POST cannot be bookmarked Tip: Always use POST if the form data contains sensitive or personal information!\nAutocomplete The autocomplete attribute specifies whether a form should have autocomplete on or off.\nWhen autocomplete is on, the browser automatically complete values based on values that the user has entered before.\nNovalidate The novalidate attribute is a boolean attribute.\nWhen present, it specifies that the form-data (input) should not be validated when submitted.\nother Attribute Description accept-charset Specifies the character encodings used for form submission action Specifies where to send the form-data when a form is submitted autocomplete Specifies whether a form should have autocomplete on or off enctype Specifies how the form-data should be encoded when submitting it to the server (only for method=\u0026ldquo;post\u0026rdquo;) method Specifies the HTTP method to use when sending form-data name Specifies the name of the form novalidate Specifies that the form should not be validated when submitted rel Specifies the relationship between a linked resource and the current document target Specifies where to display the response that is received after submitting the form Element The HTML \u0026lt;form\u0026gt; element can contain one or more of the following form elements:\n\u0026lt;input\u0026gt; \u0026lt;label\u0026gt; \u0026lt;select\u0026gt; \u0026lt;textarea\u0026gt; \u0026lt;button\u0026gt; \u0026lt;fieldset\u0026gt; \u0026lt;legend\u0026gt; \u0026lt;datalist\u0026gt; \u0026lt;output\u0026gt; \u0026lt;option\u0026gt; \u0026lt;optgroup\u0026gt; input The HTML \u0026lt;input\u0026gt; element is the most used form element.\nAn \u0026lt;input\u0026gt; element can be displayed in many ways, depending on the type attribute.\nHere are some examples:\nType Description Displays a single-line text input field, If the name attribute is omitted, the value of the input field will not be sent at all. Displays a radio button (for selecting one of many choices) Displays a checkbox (for selecting zero or more of many choices) Displays a submit button (for submitting the form),The form-handler is specified in the form\u0026rsquo;s action attribute. Displays a clickable button label The \u0026lt;label\u0026gt; element is useful for screen-reader users, because the screen-reader will read out loud the label when the user focus on the input element.\nThe \u0026lt;label\u0026gt; element also help users who have difficulty clicking on very small regions (such as radio buttons or checkboxes) - because when the user clicks the text within the \u0026lt;label\u0026gt; element, it toggles the radio button/checkbox.\nThe for attribute of the \u0026lt;label\u0026gt; tag should be equal to the id attribute of the \u0026lt;input\u0026gt; element to bind them together.\nselect,option \u0026lt;label for=\u0026#34;cars\u0026#34;\u0026gt;Choose a car:\u0026lt;/label\u0026gt; \u0026lt;select id=\u0026#34;cars\u0026#34; name=\u0026#34;cars\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;volvo\u0026#34;\u0026gt;Volvo\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;saab\u0026#34;\u0026gt;Saab\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;fiat\u0026#34;\u0026gt;Fiat\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;audi\u0026#34;\u0026gt;Audi\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; The \u0026lt;option\u0026gt; elements defines an option that can be selected.\nBy default, the first item in the drop-down list is selected.\nTo define a pre-selected option, add the selected attribute to the option\nUse the multiple attribute to allow the user to select more than one value\ntextarea the \u0026lt;textarea\u0026gt; element defines a multi-line input field (a text area)\n\u0026lt;textarea name=\u0026#34;message\u0026#34; rows=\u0026#34;10\u0026#34; cols=\u0026#34;30\u0026#34;\u0026gt; The cat was playing in the garden. \u0026lt;/textarea\u0026gt; fieldset,legend The \u0026lt;fieldset\u0026gt; element is used to group related data in a form.\nThe \u0026lt;legend\u0026gt; element defines a caption for the \u0026lt;fieldset\u0026gt; element.\n\u0026lt;form action=\u0026#34;/action_page.php\u0026#34;\u0026gt; \u0026lt;fieldset\u0026gt; \u0026lt;legend\u0026gt;Personalia:\u0026lt;/legend\u0026gt; \u0026lt;label for=\u0026#34;fname\u0026#34;\u0026gt;First name:\u0026lt;/label\u0026gt;\u0026lt;br\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;fname\u0026#34; name=\u0026#34;fname\u0026#34; value=\u0026#34;John\u0026#34;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026#34;lname\u0026#34;\u0026gt;Last name:\u0026lt;/label\u0026gt;\u0026lt;br\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;lname\u0026#34; name=\u0026#34;lname\u0026#34; value=\u0026#34;Doe\u0026#34;\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Submit\u0026#34;\u0026gt; \u0026lt;/fieldset\u0026gt; \u0026lt;/form\u0026gt; datalist The \u0026lt;datalist\u0026gt; element specifies a list of pre-defined options for an \u0026lt;input\u0026gt; element.\nUsers will see a drop-down list of the pre-defined options as they input data.\nThe list attribute of the \u0026lt;input\u0026gt; element, must refer to the id attribute of the \u0026lt;datalist\u0026gt; element.\n\u0026lt;form action=\u0026#34;/action_page.php\u0026#34;\u0026gt; \u0026lt;input list=\u0026#34;browsers\u0026#34;\u0026gt; \u0026lt;datalist id=\u0026#34;browsers\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Internet Explorer\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Firefox\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Chrome\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Opera\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Safari\u0026#34;\u0026gt; \u0026lt;/datalist\u0026gt; \u0026lt;/form\u0026gt; Graphics Canvas The HTML \u0026lt;canvas\u0026gt; element is used to draw graphics, on the fly, via JavaScript.\nThe \u0026lt;canvas\u0026gt; element is only a container for graphics. You must use JavaScript to actually draw the graphics.\nCanvas has several methods for drawing paths, boxes, circles, text, and adding images.\nNote: Always specify an id attribute (to be referred to in a script), and a width and height attribute to define the size of the canvas. To add a border, use the style attribute.\nSVG SVG stands for Scalable Vector Graphics SVG is used to define graphics for the Web SVG is a W3C recommendation SVG is a language for describing 2D graphics in XML.\nCanvas draws 2D graphics, on the fly (with a JavaScript).\nSVG is XML based, which means that every element is available within the SVG DOM. You can attach JavaScript event handlers for an element.\nIn SVG, each drawn shape is remembered as an object. If attributes of an SVG object are changed, the browser can automatically re-render the shape.\nCanvas is rendered pixel by pixel. In canvas, once the graphic is drawn, it is forgotten by the browser. If its position should be changed, the entire scene needs to be redrawn, including any objects that might have been covered by the graphic.\n","date":"2021-04-16T00:00:00Z","permalink":"https://dyhes.github.io/p/html-basics/","title":"【HTML】 Basics"},{"content":"Matplotlib is a low level graph plotting library in python that serves as a visualization utility.\nMatplotlib was created by John D. Hunter.\nMost of the Matplotlib utilities lies under the pyplot submodule, and are usually imported under the plt alias.\nPlotting The plot() function is used to draw points (markers) in a diagram.\nTo plot only the markers, you can use shortcut string notation parameter \u0026lsquo;o\u0026rsquo;, which means \u0026lsquo;rings\u0026rsquo;.\nxpoints = np.array([1, 8]) ypoints = np.array([3, 10]) plt.plot(xpoints, ypoints, \u0026#39;o\u0026#39;) plt.show() If we do not specify the points in the x-axis, they will get the default values 0, 1, 2, 3,\nMaker You can use the keyword argument marker to emphasize each point with a specified marker\nplt.plot(ypoints, \u0026#39;o:r\u0026#39;) ## marker|line|color plt.show() Marker Description \u0026lsquo;o\u0026rsquo; Circle Try it » \u0026lsquo;*\u0026rsquo; Star Try it » \u0026lsquo;.\u0026rsquo; Point Try it » \u0026lsquo;,\u0026rsquo; Pixel Try it » \u0026lsquo;x\u0026rsquo; X Try it » \u0026lsquo;X\u0026rsquo; X (filled) Try it » \u0026lsquo;+\u0026rsquo; Plus Try it » \u0026lsquo;P\u0026rsquo; Plus (filled) Try it » \u0026rsquo;s\u0026rsquo; Square Try it » \u0026lsquo;D\u0026rsquo; Diamond Try it » \u0026rsquo;d' Diamond (thin) Try it » \u0026lsquo;p\u0026rsquo; Pentagon Try it » \u0026lsquo;H\u0026rsquo; Hexagon Try it » \u0026lsquo;h\u0026rsquo; Hexagon Try it » \u0026lsquo;v\u0026rsquo; Triangle Down Try it » \u0026lsquo;^\u0026rsquo; Triangle Up Try it » \u0026lsquo;\u0026lt;\u0026rsquo; Triangle Left Try it » \u0026lsquo;\u0026gt;\u0026rsquo; Triangle Right Try it » \u0026lsquo;1\u0026rsquo; Tri Down Try it » \u0026lsquo;2\u0026rsquo; Tri Up Try it » \u0026lsquo;3\u0026rsquo; Tri Left Try it » \u0026lsquo;4\u0026rsquo; Tri Right Try it » \u0026lsquo;|\u0026rsquo; Vline Try it » \u0026lsquo;_\u0026rsquo; Hline Line Syntax Description \u0026lsquo;-\u0026rsquo; Solid line Try it » \u0026lsquo;:\u0026rsquo; Dotted line Try it » \u0026lsquo;\u0026ndash;\u0026rsquo; Dashed line Try it » \u0026lsquo;-.\u0026rsquo; Dashed/dotted line Color Syntax Description \u0026lsquo;r\u0026rsquo; Red Try it » \u0026lsquo;g\u0026rsquo; Green Try it » \u0026lsquo;b\u0026rsquo; Blue Try it » \u0026lsquo;c\u0026rsquo; Cyan Try it » \u0026rsquo;m' Magenta Try it » \u0026lsquo;y\u0026rsquo; Yellow Try it » \u0026lsquo;k\u0026rsquo; Black Try it » \u0026lsquo;w\u0026rsquo; White You can use the keyword argument markersize or the shorter version, ms to set the size of the markers.\nplt.plot(ypoints, marker = \u0026#39;o\u0026#39;, ms = 20) ","date":"2021-01-14T00:00:00Z","permalink":"https://dyhes.github.io/p/pythonmatplotlib/","title":"【Python】Matplotlib"},{"content":"Introduction NumPy short for \u0026ldquo;Numerical Python\u0026rdquo; is a Python library, which is used for working with arrays.\nIn Python we have lists that serve the purpose of arrays, but they are slow to process.\nNumPy aims to provide an array object that is up to 50x faster than traditional Python lists. ( NumPy arrays are stored at one continuous place in memory unlike lists, so processes can access and manipulate them very efficiently. )\nThe array object in NumPy is called ndarray, it provides a lot of supporting functions that make working with ndarray very easy.\nCreacte ndarray Object To create an ndarray, we can pass a list, tuple or any array-like object into the array() method, and it will be converted into an ndarray\narr = np.array([1, 2, 3, 4, 5]) arr = np.array((1, 2, 3, 4, 5)) A dimension in arrays is one level of array depth (nested arrays).\nNumPy has a whole sub module dedicated towards matrix operations called numpy.mat\nNumPy Arrays provides the ndim attribute whicht is an integer that tells us how many dimensions the array have.\nAccess arr[d1,d2,d3...] Slicing [start:end]. [start:end:step] If we don\u0026rsquo;t pass start its considered 0\nIf we don\u0026rsquo;t pass end its considered length of array in that dimension\nIf we don\u0026rsquo;t pass step its considered 1\nThe result includes the start index, but excludes the end index.\n#slicing 2-D Arrays print(arr[1, 1:4]) Data Types NumPy has some extra data types, and refer to data types with one character, like i for integers, u for unsigned integers etc.\nBelow is a list of all data types in NumPy and the characters used to represent them.\ni - integer b - boolean u - unsigned integer f - float c - complex float m - timedelta M - datetime O - object S - string U - unicode string V - fixed chunk of memory for other type ( void ) The NumPy array object has a property called dtype that returns the data type of the array.\narr = np.array([1, 2, 3, 4]) print(arr.dtype) ## int64 arr = np.array([1, 2, 3, 4], dtype=\u0026#39;S\u0026#39;) If a type is given in which elements can\u0026rsquo;t be casted then NumPy will raise a ValueError.\narr = np.array([\u0026#39;a\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;], dtype=\u0026#39;i\u0026#39;) ## ValueError Converting Data Type The best way to change the data type of an existing array, is to make a copy of the array with the astype() method.\nThe astype() function creates a copy of the array, and allows you to specify the data type as a parameter.\narr = np.array([1.1, 2.1, 3.1]) newarr = arr.astype(\u0026#39;i\u0026#39;) Copy and View The main difference between a copy and a view of an array is that the copy is a new array, and the view is just a view of the original array.\nThe copy owns the data and any changes made to the copy will not affect original array, and any changes made to the original array will not affect the copy.\nThe view does not own the data and any changes made to the view will affect the original array, and any changes made to the original array will affect the view.\nEvery NumPy array has the attribute base that returns None if the array owns the data.\nOtherwise, the base attribute refers to the original object.\nx = arr.copy() y = arr.view() print(x.base) ## none print(y.base) ## arr Shape The shape of an array is the number of elements in each dimension.\nNumPy arrays have an attribute called shape that returns a tuple with each index having the number of corresponding elements.\nReshape By reshaping we can add or remove dimensions or change number of elements in each dimension.\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) newarr = arr.reshape(4, 3) newarr = arr.reshape(2, 3, 2) We can reshape an 8 elements 1D array into 4 elements in 2 rows 2D array but we cannot reshape it into a 3 elements 3 rows 2D array as that would require 3x3 = 9 elements.\nthe returned array is a view\nYou are allowed to have one \u0026ldquo;unknown\u0026rdquo; dimension.\nMeaning that you do not have to specify an exact number for one of the dimensions in the reshape method.\nPass -1 as the value, and NumPy will calculate this number for you.\nnewarr = arr.reshape(2, 2, -1) Note: We can not pass -1 to more than one dimension.\nFlattening Flattening array means converting a multidimensional array into a 1D array.\nWe can use reshape(-1) to do this.\nNote: There are a lot of functions for changing the shapes of arrays in numpy flatten, ravel and also for rearranging the elements rot90, flip, fliplr, flipud etc.\nIterating The function nditer() is a helping function that can be used from very basic to very advanced iterations. It solves some basic issues which we face in iteration.\nfor x in np.nditer(arr): print(x) With Different Data Types We can use op_dtypes argument and pass it the expected datatype to change the datatype of elements while iterating.\nNumPy does not change the data type of the element in-place (where the element is in array) so it needs some other space to perform this action, that extra space is called buffer, and in order to enable it in nditer() we pass flags=['buffered']\nfor x in np.nditer(arr, flags=[\u0026#39;buffered\u0026#39;], op_dtypes=[\u0026#39;S\u0026#39;]): print(x) With Different Step Size for x in np.nditer(arr[:, ::2]): print(x) Join In SQL we join tables based on a key, whereas in NumPy we join arrays by axes.\nWe pass a sequence of arrays that we want to join to the concatenate() function, along with the axis. If axis is not explicitly passed, it is taken as 0.\narr1 = np.array([1, 2, 3]) arr2 = np.array([4, 5, 6]) arr = np.concatenate((arr1, arr2)) #along the row arr1 = np.array([[1, 2], [3, 4]]) arr2 = np.array([[5, 6], [7, 8]]) arr = np.concatenate((arr1, arr2), axis=1) #[[1 2 5 6] ## [3 4 7 8]] Split Splitting is reverse operation of Joining.\nJoining merges multiple arrays into one and Splitting breaks one array into multiple.\nWe use array_split() for splitting arrays, we pass it the array we want to split and the number of splits.\narr = np.array([1, 2, 3, 4, 5, 6]) newarr = np.array_split(arr, 3) The return value is an array containing three arrays.\nIf the array has less elements than required, it will adjust from the end accordingly.\nWe also have the method split() available but it will not adjust the elements when elements are less in source array for splitting like in example above, array_split() worked properly but split() would fail.\nSearch You can search an array for a certain value, and return the indexes that get a match.\nTo search an array, use the where() method.\nFilter Getting some elements out of an existing array and creating a new array out of them is called filtering.\nIn NumPy, you filter an array using a boolean index list.\narr = np.array([41, 42, 43, 44]) x = [True, False, True, False] newarr = arr[x] We can directly substitute the array instead of the iterable variable in our condition and it will work just as we expect it to.\narr = np.array([41, 42, 43, 44]) filter_arr = arr \u0026gt; 42 newarr = arr[filter_arr] Random Random number does NOT mean a different number every time. Random means something that can not be predicted logically.\nPseudo Random and True Random Random numbers generated through a generation algorithm are called pseudo random.\nIn order to generate a truly random number on our computers we need to get the random data from some outside source. This outside source is generally our keystrokes, mouse movements, data on network etc.\nWe do not need truly random numbers, unless its related to security (e.g. encryption keys) or the basis of application is the randomness (e.g. Digital roulette wheels).\nNumPy offers the random module to work with random numbers.\nThe random module\u0026rsquo;s rand() method returns a random float between 0 and 1.\nfrom numpy import random ## Generate a random integer from 0 to 100 x = random.randint(100) The randint() method takes a size parameter where you can specify the shape of an array.\nx=random.randint(100, size=(5,2)) The rand() method also allows you to specify the shape of the array.\nThe choice() method allows you to generate a random value based on an array of values.\nThe choice() method takes an array as a parameter and randomly returns one of the values.\nSeaborn Seaborn is a library that uses Matplotlib underneath to plot graphs. It will be used to visualize random distributions.\nData Distribution Data Distribution is a list of all possible values, and how often each value occurs.\nSuch lists are important when working with statistics and data science.\nThe random module offer methods that returns randomly generated data distributions.\nWe can generate random numbers based on defined probabilities using the choice() method of the random module.\nThe choice() method allows us to specify the probability for each value.\nx = random.choice([3, 5, 7, 9], p=[0.1, 0.3, 0.6, 0.0], size=(100)) Permutations A permutation refers to an arrangement of elements. e.g. [3, 2, 1] is a permutation of [1, 2, 3] and vice-versa.\nThe NumPy Random module provides two methods for this: shuffle() and permutation().\nThe shuffle() method makes changes to the original array. The permutation() method returns a re-arranged array (and leaves the original array un-changed). arr = np.array([1, 2, 3, 4, 5]) random.shuffle(arr) newarr=random.permutation(arr) Numpy ufuncs ufuncs stands for \u0026ldquo;Universal Functions\u0026rdquo; and they are NumPy functions that operates on the ndarray object.\nufuncs are used to implement vectorization (Converting iterative statements into a vector based operation ) in NumPy which is way faster than iterating over elements.\nufuncs also take additional arguments, like:\nwhere boolean array or condition defining where the operations should take place.\ndtype defining the return type of elements.\nout output array where the return value should be copied.\ncreate To create you own ufunc, you have to define a normal function, like you do with normal functions in Python, then you add it to your NumPy ufunc library with the frompyfunc() method.\nThe frompyfunc() method takes the following arguments:\nfunction - the name of the function. inputs - the number of input arguments (arrays). outputs - the number of output arrays. def myadd(x, y): return x+y myadd = np.frompyfunc(myadd, 2, 1) A ufunc should return \u0026lt;class 'numpy.ufunc'\u0026gt;.\nIf it is not a ufunc, it will return another type, like this built-in NumPy function for joining two or more arrays \u0026lt;class'builtin_function_or_method'\u0026gt;\nTo test if the function is a ufunc in an if statement, use the numpy.ufunc value\nif type(np.add) == np.ufunc ","date":"2021-01-13T00:00:00Z","permalink":"https://dyhes.github.io/p/pythonnumpy/","title":"【Python】Numpy"},{"content":"Intro Pandas is a Python library, which is used to analyze data.\nIt has functions for analyzing, cleaning, exploring, and manipulating data.\nThe name \u0026ldquo;Pandas\u0026rdquo; has a reference to both \u0026ldquo;Panel Data\u0026rdquo;, and \u0026ldquo;Python Data Analysis\u0026rdquo; and was created by Wes McKinney in 2008.\nPandas allows us to analyze big data and make conclusions based on statistical theories.\nPandas gives you answers about the data. Like:\nIs there a correlation between two or more columns? What is average value? Max value? Min value? Pandas are also able to delete rows that are not relevant, or contains wrong values, like empty or NULL values. This is called cleaning the data.\nSeries A Pandas Series is like a column in a table.\nIt is a one-dimensional array holding data of any type.\nLabels If nothing else is specified, the values are labeled with their index number. First value has index 0, second value has index 1 etc.\nThis label can be used to access a specified value.\nWith the index argument, you can name your own labels.\na = [1, 7, 2] myvar = pd.Series(a, index = [\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;]) You can also use a key/value object, like a dictionary, when creating a Series.\ncalories = {\u0026#34;day1\u0026#34;: 420, \u0026#34;day2\u0026#34;: 380, \u0026#34;day3\u0026#34;: 390} myvar = pd.Series(calories) myvar = pd.Series(calories, index = [\u0026#34;day1\u0026#34;, \u0026#34;day2\u0026#34;]) ## only using data from \u0026#39;day1\u0026#39; and \u0026#39;day2\u0026#39; Note: The keys of the dictionary become the labels.\nDataFrames A Pandas DataFrame is a 2 dimensional data structure, like a 2 dimensional array, or a table with rows and columns.\nSeries is like a column, a DataFrame is the whole table.\ndata = { \u0026#34;calories\u0026#34;: [420, 380, 390], \u0026#34;duration\u0026#34;: [50, 40, 45] } myvar = pd.DataFrame(data) Pandas use the loc attribute to return one or more specified row(s)\n#use a list of indexes: print(df.loc[[0, 1]]) Note: When using [], the result is a Pandas DataFrame.\nWith the index argument, you can name your own indexes.\ndata = { \u0026#34;calories\u0026#34;: [420, 380, 390], \u0026#34;duration\u0026#34;: [50, 40, 45] } df = pd.DataFrame(data, index = [\u0026#34;day1\u0026#34;, \u0026#34;day2\u0026#34;, \u0026#34;day3\u0026#34;]) Read CSV A simple way to store big data sets is to use CSV files (comma separated files).\nCSV files contains plain text and is a well know format that can be read by everyone including Pandas.\ndf = pd.read_csv(\u0026#39;data.csv\u0026#39;) print(df.to_string()) #use to_string() to print the entire DataFrame. If you have a large DataFrame with many rows, Pandas will only return the first 5 rows, and the last 5 rows:\nRead JSON Big data sets are often stored, or extracted as JSON.\nJSON is plain text, but has the format of an object, and is well known in the world of programming, including Pandas.\ndf = pd.read_json(\u0026#39;data.json\u0026#39;) If your JSON code is not in a file, but in a Python Dictionary, you can load it into a DataFrame directly.\ndf = pd.DataFrame(data) Viewing Data head() One of the most used method for getting a quick overview of the DataFrame, is the head() method.\nThe head() method returns the headers and a specified number of rows, starting from the top.\nprint(df.head(10)) Note: if the number of rows is not specified, the head() method will return the top 5 rows.\ntail() There is also a tail() method for viewing the last rows of the DataFrame.\nThe tail() method returns the headers(names of columns) and a specified number of rows, starting from the bottom.\ninfo() The DataFrames object has a method called info(), that gives you more information about the data set.\nprint(df.info()) /* \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 169 entries, 0 to 168 Data columns (total 4 columns): ## Column Non-Null Count Dtype --- ------ -------------- ----- 0 Duration 169 non-null int64 1 Pulse 169 non-null int64 2 Maxpulse 169 non-null int64 3 Calories 164 non-null float64 dtypes: float64(1), int64(3) memory usage: 5.4 KB None */ Cleaning Data Data cleaning means fixing bad data in your data set.\nBad data could be:\nEmpty cells Data in wrong format Wrong data Duplicates Empty Cell Remove Rows Replace Empty Values Pandas uses the mean() median() and mode() methods to calculate the respective values for a specified column.\n#1 new_df = df.dropna() #If you want to change the original DataFrame, use the inplace = True argument df.dropna(inplace = True) #2 x = df[\u0026#34;Calories\u0026#34;].mean() x = df[\u0026#34;Calories\u0026#34;].median() x = df[\u0026#34;Calories\u0026#34;].mode()[0] df[\u0026#34;Calories\u0026#34;].fillna(x, inplace = True) Note: By default, the dropna() method returns a new DataFrame, and will not change the original.\nWrong Format Remove Rows Convert Format #1 df.dropna(subset=[\u0026#39;Date\u0026#39;], inplace = True) #2 df.dropna(subset=[\u0026#39;Date\u0026#39;], inplace = True) Wrong Data for x in df.index: if df.loc[x, \u0026#34;Duration\u0026#34;] \u0026gt; 120: df.loc[x, \u0026#34;Duration\u0026#34;] = 120 #or for x in df.index: if df.loc[x, \u0026#34;Duration\u0026#34;] \u0026gt; 120: df.loc[x, \u0026#34;Duration\u0026#34;] = 120 Duplicates df.drop_duplicates(inplace = True) Data Correlations A great aspect of the Pandas module is the corr() method.\nThe corr() method calculates the relationship between each column in your data set.\nNote: The corr() method ignores \u0026ldquo;not numeric\u0026rdquo; columns.\nThe Result of the corr() method is a table with a lot of numbers that represents how well the relationship is between two columns.\n","date":"2021-01-13T00:00:00Z","permalink":"https://dyhes.github.io/p/pythonpandas/","title":"【Python】Pandas"},{"content":"Variables Variables are containers for storing data values.\nA variable is created the moment you first assign a value to it.\nIf you want to specify the data type of a variable, this can be done with casting.\nx = str(3) ## x will be \u0026#39;3\u0026#39; y = int(3) ## y will be 3 z = float(3) ## z will be 3.0 You can get the data type of a variable with the type() function.\nVariable Names A variable name must start with a letter or the underscore character A variable name cannot start with a number A variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ ) Variable names are case-sensitive (age, Age and AGE are three different variables) more readable:\nCamel Case myVariableName = \u0026#34;John\u0026#34; Pascal Case MyVariableName = \u0026#34;John\u0026#34; Snake Case my_variable_name = \u0026#34;John\u0026#34; Variable Scope Variables that are created outside of a function (as in all of the examples above) are known as global variables.\nIf you create a variable with the same name inside a function, this variable will be local, and can only be used inside the function. The global variable with the same name will remain as it was, global and with the original value.\nTo create a global variable inside a function, you can use the global keyword.\nAlso, use the global keyword if you want to change a global variable inside a function.\nData Type Python has the following data types built-in by default, in these categories:\nText Type: str Numeric Types: int, float, complex Sequence Types: list, tuple, range Mapping Type: dict Set Types: set, frozenset Boolean Type: bool Binary Types: bytes, bytearray, memoryview Casting There may be times when you want to specify a type on to a variable. This can be done with casting. Python is an object-orientated language, and as such it uses classes to define data types, including its primitive types.\nExample Data Type Try it x = str(\u0026ldquo;Hello World\u0026rdquo;) str Try it » x = int(20) int Try it » x = float(20.5) float Try it » x = complex(1j) complex Try it » x = list((\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;)) list Try it » x = tuple((\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;)) tuple Try it » x = range(6) range Try it » x = dict(name=\u0026ldquo;John\u0026rdquo;, age=36) dict Try it » x = set((\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;)) set Try it » x = frozenset((\u0026ldquo;apple\u0026rdquo;, \u0026ldquo;banana\u0026rdquo;, \u0026ldquo;cherry\u0026rdquo;)) frozenset Try it » x = bool(5) bool Try it » x = bytes(5) bytes Try it » x = bytearray(5) bytearray Try it » x = memoryview(bytes(5)) memoryview Try it » Booleans Almost any value is evaluated to True if it has some sort of content.\nAny string is True, except empty strings.\nAny number is True, except 0.\nAny list, tuple, set, and dictionary are True, except empty ones.\nCollections There are four collection data types in the Python programming language:\nList is a collection which is ordered and changeable. Allows duplicate members. Tuple is a collection which is ordered and unchangeable. Allows duplicate members. Set is a collection which is unordered, unchangeable, and unindexed. No duplicate members. Dictionary is a collection which is ordered and changeable. No duplicate members. List Lists are used to store multiple items in a single variable.\nLists are one of 4 built-in data types in Python used to store collections of data, the other 3 are Tuple, Set, and Dictionary, all with different qualities and usage.\nA list can contain different data types\nadd\nTo add an item to the end of the list, use the append() method To insert a list item at a specified index, use the insert() method. To append elements from another list ( or any iterable object ) to the current list, use the extend() method. remove\nThe remove() method removes the specified item. The pop() method removes the specified index. If you do not specify the index, the pop() method removes the last item. The del keyword also removes the specified index and can also delete the list completely. The clear() method empties the list. The list still remains, but it has no content. thislist.remove(\u0026#34;banana\u0026#34;) thislist.pop(1) del thislist[0] del thislist thislist.clear() List Comprehension List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.\nnewlist = [expression for item in iterable if condition == True] Tuple A tuple is a collection which is ordered and unchangeable.\nSince tuples are indexed, they can have items with the same value\nthistuple = (\u0026#34;apple\u0026#34;,) \u0026lt;class \u0026#39;tuple\u0026#39;\u0026gt; #NOT a tuple thistuple = (\u0026#34;apple\u0026#34;) \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; You can convert the tuple into a list, change the list, and convert the list back into a tuple.\nUnpack we are allowed to extract the values back into variables. This is called \u0026ldquo;unpacking\u0026rdquo;:\nfruits = (\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;) (green, yellow, red) = fruits Asterisk* If the number of variables is less than the number of values, you can add an * to the variable name and the values will be assigned to the variable as a list:\nfruits = (\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;strawberry\u0026#34;, \u0026#34;raspberry\u0026#34;) (green, yellow, *red) = fruits (green, *tropic, red) = fruits If the asterisk is added to another variable name than the last, Python will assign values to the variable until the number of values left matches the number of variables left.\nSet A set is a collection which is unordered, unchangeable*, and unindexed.\nNote: Set items are unchangeable, but you can remove items and add new items.\nadd\nTo add one item to a set use the add() method.\nTo add items from another set into the current set, use the update() method.\nremove\nremove(): if the item to remove does not exist, remove() will raise an error. discard() : if the item to remove does not exist, discard() will NOT raise an error. pop() :remove and return the last item, you will not know what item that gets removed. clear() : you will not know what item that gets removed. del :delete the set completely join\nunion() : returns a new set containing all items from both sets update() : inserts all the items from one set into another. Set Operation The intersection_update() method will keep only the items that are present in both sets.\nThe intersection() method will return a new set, that only contains the items that are present in both sets.\nThe symmetric_difference_update() method will keep only the elements that are NOT present in both sets.\nThe symmetric_difference() method will return a new set, that contains only the elements that are NOT present in both sets.\nDictionary A dictionary is a collection which is ordered*, changeable and do not allow duplicates.\nAs of Python version 3.7, dictionaries are ordered. In Python 3.6 and earlier, dictionaries are unordered.\nAccess referring to its key name inside square brackets get() keys(): return a list of all the keys in the dictionary. values() : return a list of all the values in the dictionary. items() : return each item in a dictionary, as tuples in a list. thisdict[\u0026#34;model\u0026#34;] thisdict.get(\u0026#34;model\u0026#34;) Change / Add referring to its key name update() Remove pop() : removes the item with the specified key name popitem(): removes the last inserted item (in versions before 3.7, a random item is removed instead) del : removes the item with the specified key name or the entire dictionary clear() :empties the dictionary thisdict.pop(\u0026#34;model\u0026#34;) thisdict.popitem() del thisdict[\u0026#34;model\u0026#34;] del thisdict thisdict.clear() copy: copy() or dict() Looping for in keys() values() items() Arrays Python does not have built-in support for Arrays, but Python Lists can be used instead.\nTo work with arrays in Python you can import a library, like the NumPy library.\nFunction A function is a block of code which only runs when it is called.\nIn Python a function is defined using the def keyword\nLambda A lambda function is a small anonymous function.\nA lambda function can take any number of arguments, but can only have one expression.\nlambda arguments : expression\nThe power of lambda is better shown when you use them as an anonymous function inside another function.\nClasses Python is an object oriented programming language.\nAlmost everything in Python is an object, with its properties and methods.\nA Class is like an object constructor, or a \u0026ldquo;blueprint\u0026rdquo; for creating objects.\nTo create a class, use the keyword class\ninit() Function All classes have a function called init(), which is always executed when the class is being initiated. Use the init() function to assign values to object properties, or other operations that are necessary to do when the object is being created\nclass Person: def __init__(self, name, age): self.name = name self.age = age p1 = Person(\u0026#34;John\u0026#34;, 36) print(p1.name) print(p1.age) Note:\nThe self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\nIt does not have to be named self , you can call it whatever you like, but it has to be the first parameter of any function in the class\ndel You can delete properties on objects or objects by using the del keyword\ndel p1.age del p1 Inheritance class childClass(parentClass): def __init__(self,...): parentClass.__init__(self,...) super().__init__(self,...) where super() represent parentClass\nIterators An iterator is an object that contains a countable number of values.\nAn iterator is an object that can be iterated upon, meaning that you can traverse through all the values.\nTechnically, in Python, an iterator is an object which implements the iterator protocol, which consist of the methods __iter__() and __next__().\nIterator vs Iterable Lists, tuples, dictionaries, and sets are all iterable objects. They are iterable containers which you can get an iterator from.\nAll these objects have a iter() method which is used to get an iterator\nmytuple = (\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;) myit = iter(mytuple) print(next(myit)) print(next(myit)) print(next(myit)) The for loop actually creates an iterator object and executes the next() method for each loop.\nTo create an object/class as an iterator you have to implement the methods __iter__() and __next__() to your object.\nTo prevent the iteration to go on forever, we can use the StopIteration statement.\nIn the __next__() method, we can add a terminating condition to raise an error if the iteration is done a specified number of times\nclass MyNumbers: def __iter__(self): self.a = 1 return self def __next__(self): if self.a \u0026lt;= 20: x = self.a self.a += 1 return x else: raise StopIteration Exception The try block lets you test a block of code for errors.\nThe except block lets you handle the error.\nThe else block lets you execute code when there is no error.\nThe finally block lets you execute code, regardless of the result of the try- and except blocks.\nTo throw (or raise) an exception, use the raise keyword.\ntry: raise SomeException except: else: finally: User Input Python allows for user input.\nThat means we are able to ask the user for input.\nThe method is a bit different in Python 3.6 than Python 2.7.\nPython 3.6 uses the input() method.\nPython 2.7 uses the raw_input() method.\nString Formatting by format() The format() method allows you to format selected parts of a string.\nTo control such values, add placeholders (curly brackets {}) in the text, and run the values through the format() method\nprint(\u0026#34;The price is {} dollars\u0026#34;.format(price)) Index Numbers You can use index numbers (a number inside the curly brackets {0}) to be sure the values are placed in the correct placeholders\n\u0026#34;I want {0} pieces of item number {1} for {2:.2f} dollars.\u0026#34;.format(quantity, itemno, price) if you want to refer to the same value more than once, use the index number.\n\u0026#34;His name is {1}. {1} is {0} years old.\u0026#34;.format(age, name) Named Indexes You can also use named indexes by entering a name inside the curly brackets {carname}, but then you must use names when you pass the parameter values txt.format(carname = \u0026quot;Ford\u0026quot;)\n\u0026#34;I have a {carname}, it is a {model}.\u0026#34;.format(carname = \u0026#34;Ford\u0026#34;, model = \u0026#34;Mustang\u0026#34;) File Open The key function for working with files in Python is the open() function.\nThe open() function takes two parameters; filename, and mode.\nThere are four different methods (modes) for opening a file:\n\u0026quot;r\u0026quot; - Read - Default value. Opens a file for reading, error if the file does not exist\n\u0026quot;a\u0026quot; - Append - Opens a file for appending, creates the file if it does not exist\n\u0026quot;w\u0026quot; - Write - Opens a file for writing, creates the file if it does not exist\n\u0026quot;x\u0026quot; - Create - Creates the specified file, returns an error if the file exists\nIn addition you can specify if the file should be handled as binary or text mode\n\u0026quot;t\u0026quot; - Text - Default value. Text mode\n\u0026quot;b\u0026quot; - Binary - Binary mode (e.g. images)\nRead The open() function returns a file object, which has a read() method for reading the content of the file.\nBy default the read() method returns the whole text, but you can also specify how many characters you want to return.\nYou can return one line by using the readline() method.\nf = open(\u0026#34;demofile.txt\u0026#34;, \u0026#34;r\u0026#34;) print(f.read()) print(f.read(5)) print(f.readline()) It is a good practice to always close the file when you are done with it.\nf.close() Write/Create To write to an existing file, you must add a parameter to the open() function:\n\u0026quot;a\u0026quot; - Append - will append to the end of the file\n\u0026quot;w\u0026quot; - Write - will overwrite any existing content\nTo create a new file in Python, use the open() method, with one of the following parameters:\n\u0026quot;x\u0026quot; - Create - will create a file, returns an error if the file exist\n\u0026quot;a\u0026quot; - Append - will create a file if the specified file does not exist\n\u0026quot;w\u0026quot; - Write - will create a file if the specified file does not exist\nf = open(\u0026#34;demofile3.txt\u0026#34;, \u0026#34;w\u0026#34;) f.write(\u0026#34;Woops! I have deleted the content!\u0026#34;) Delete To delete a file, you must import the OS module, and run its os.remove() function.\nimport os if os.path.exists(\u0026#34;demofile.txt\u0026#34;): os.remove(\u0026#34;demofile.txt\u0026#34;) else: print(\u0026#34;The file does not exist\u0026#34;) To delete an entire folder, use the os.rmdir() method:\nimport os os.rmdir(\u0026#34;myfolder\u0026#34;) Note: You can only remove empty folders.\nModule Consider a module to be the same as a code library.\nA file containing a set of functions you want to include in your application.\nwe can use a module by using the import statement\nYou can name the module file whatever you like, but it must have the file extension .py\nYou can create an alias when you import a module, by using the as keyword\nimport mymodule as mx Note: When importing using the from keyword, do not use the module name when referring to elements in the module. Example: person1[\u0026quot;age\u0026quot;], not mymodule.person1[\u0026quot;age\u0026quot;]\ndir() The dir() function is a built-in function to list all the function names (or variable names) in a module.\nNote: The dir() function can be used on all modules, also the ones you create yourself.\n","date":"2021-01-12T00:00:00Z","permalink":"https://dyhes.github.io/p/python%E8%AF%AD%E6%B3%95/","title":"【Python】语法"},{"content":"Introduction It is an object-oriented programming language created by Microsoft that runs on the .NET Framework.\nC# has roots from the C family, and the language is close to other popular languages like C++ and Java.\noutput Console.WriteLine(\u0026#34;Hello World!\u0026#34;); Console.Write(\u0026#34;Hello World! \u0026#34;); Variable type variableName = value; int myNum = 15; you can add the const keyword if you don\u0026rsquo;t want others (or yourself) to overwrite existing values\nA const field requires a value to be provided\nData Type Data Type Size Description int 4 bytes Stores whole numbers from -2,147,483,648 to 2,147,483,647 long 8 bytes Stores whole numbers from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 float 4 bytes Stores fractional numbers. Sufficient for storing 6 to 7 decimal digits double 8 bytes Stores fractional numbers. Sufficient for storing 15 decimal digits bool 1 bit Stores true or false values char 2 bytes Stores a single character/letter, surrounded by single quotes string 2 bytes per character Stores a sequence of characters, surrounded by double quotes Arrays string[] cars = {\u0026#34;Volvo\u0026#34;, \u0026#34;BMW\u0026#34;, \u0026#34;Ford\u0026#34;, \u0026#34;Mazda\u0026#34;}; Console.WriteLine(cars.Length); //if you declare an array and initialize it later, you have to use the new keyword string[] cars; // Add values, using new cars = new string[] {\u0026#34;Volvo\u0026#34;, \u0026#34;BMW\u0026#34;, \u0026#34;Ford\u0026#34;}; Type Casting //explicit int myInt = (int) myDouble; It is also possible to convert data types explicitly by using built-in methods, such asConvert.ToBoolean, Convert.ToDouble, Convert.ToString, Convert.ToInt32 (int) and Convert.ToInt64 (long)\nint myInt = 10; double myDouble = 5.25; bool myBool = true; Console.WriteLine(Convert.ToString(myInt)); // convert int to string Console.WriteLine(Convert.ToDouble(myInt)); // convert int to double Console.WriteLine(Convert.ToInt32(myDouble)); // convert double to int Console.WriteLine(Convert.ToString(myBool)); // convert bool to string User Input we use Console.ReadLine() to get user input.\nThe Console.ReadLine() method returns a string.\nstring userName = Console.ReadLine(); int age = Convert.ToInt32(Console.ReadLine()); foreach Loop foreach (type variableName in arrayName) { // code block to be executed } OOP Access Modifier C# has the following access modifiers:\nModifier Description public The code is accessible for all classes private The code is only accessible within the same class protected The code is accessible within the same class, or in a class that is inherited from that class. You will learn more about inheritance in a later chapter internal The code is only accessible within its own assembly, but not from another assembly. You will learn more about this in a later chapter There\u0026rsquo;s also two combinations: protected internal and private protected.\nclass Person { private string Name // property { get; set; } } Sealed If you don\u0026rsquo;t want other classes to inherit from a class, use the sealed keyword\nsealed class Vehicle { ... } Polymorphism class Animal // Base class (parent) { public virtual void animalSound() { Console.WriteLine(\u0026#34;The animal makes a sound\u0026#34;); } } class Pig : Animal // Derived class (child) { public override void animalSound() { Console.WriteLine(\u0026#34;The pig says: wee wee\u0026#34;); } } class Dog : Animal // Derived class (child) { public override void animalSound() { Console.WriteLine(\u0026#34;The dog says: bow wow\u0026#34;); } } class Program { static void Main(string[] args) { Animal myAnimal = new Animal(); // Create a Animal object Animal myPig = new Pig(); // Create a Pig object Animal myDog = new Dog(); // Create a Dog object myAnimal.animalSound(); myPig.animalSound(); myDog.animalSound(); } } Abstract The abstract keyword is used for classes and methods:\nAbstract class: is a restricted class that cannot be used to create objects (to access it, it must be inherited from another class). Abstract method: can only be used in an abstract class, and it does not have a body. The body is provided by the derived class (inherited from). Interface An interface is a completely \u0026ldquo;abstract class\u0026rdquo;, which can only contain abstract methods and properties (with empty bodies):\ninterface Animal { void animalSound(); // interface method (does not have a body) void run(); // interface method (does not have a body) } Enums An enum is a special \u0026ldquo;class\u0026rdquo; that represents a group of constants (unchangeable/read-only variables).\nTo create an enum, use the enum keyword (instead of class or interface), and separate the enum items with a comma\nenum Level { Low, Medium, High } Level myVar = Level.Medium; Console.WriteLine(myVar); ","date":"2020-12-28T00:00:00Z","permalink":"https://dyhes.github.io/p/c%23basics/","title":"【C#】Basics"},{"content":"Introduction Every line of code that runs in Java must be inside a class. A class should always start with an uppercase first letter.\nThe name of the java file must match the class name. When saving the file, save it using the class name and add \u0026ldquo;.java\u0026rdquo; to the end of the filename.\nEvery program must contain the main() method.\npublic static void main(String[] args) Variables type variablename=value; type[] arrayname Data Types primitive types Data Type Size Description byte 1 byte Stores whole numbers from -128 to 127 short 2 bytes Stores whole numbers from -32,768 to 32,767 int 4 bytes Stores whole numbers from -2,147,483,648 to 2,147,483,647 long 8 bytes Stores whole numbers from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 float 4 bytes Stores fractional numbers. Sufficient for storing 6 to 7 decimal digits double 8 bytes Stores fractional numbers. Sufficient for storing 15 decimal digits boolean 1 bit Stores true or false values char 2 bytes Stores a single character/letter or ASCII values Non-primitive Non-primitive data types are called reference types because they refer to objects.\nString Arrays Classes Interfaces main difference Primitive types are predefined in Java. Non-primitive types are created by the programmer and is not defined by Java (except for String). A primitive type has always a value, while non-primitive types can be null. A primitive type starts with a lowercase letter, while non-primitive types starts with an uppercase letter. The size of a primitive type depends on the data type, while non-primitive types have all the same size. final final int myNum=15; //you couldn\u0026#39;t change it anymore //just like const in js name rules The general rules for naming variables are:\nNames can contain letters, digits, underscores, and dollar signs Names must begin with a letter Names should start with a lowercase letter and it cannot contain whitespace Names can also begin with $ and _ (but we will not use it in this tutorial) Names are case sensitive (\u0026ldquo;myVar\u0026rdquo; and \u0026ldquo;myvar\u0026rdquo; are different variables) Reserved words (like Java keywords, such as int or boolean) cannot be used as names Casting Widening Casting Widening casting is done automatically when passing a smaller size type to a larger size type\nint myInt = 9; double myDouble = myInt; Narrowing Casting Narrowing casting must be done manually by placing the type in parentheses in front of the value\ndouble myDouble = 9.78d; int myInt = (int) myDouble; Overload int myMethod(int x) int myMethod(int x,int y) float myMethod(float x) double myMethod(double x, double y) Classes a class is a template for objects, and an object is an instance of a class.\nConstructor A constructor in Java is a special method that is used to initialize objects. The constructor is called when an object of a class is created. It can be used to set initial values for object attributes\nNote that the constructor name must match the class name, and it cannot have a return type (like void).\nModifiers Access Modifiers **public: **accessible for all classes protected: accessible in the same package and subclasses **default: **accessible in the same package private: only accessible within the declared class Non-Access Modifiers for classes final: cannot be inherited abstract: cannot be used to create objects for attributes and methods final: cannot be overridden/modified static :belongs to the class, rather than an object abstract: Can only be used in an abstract class, and can only be used on methods. Main Concepts of OOP Encapsulation declare class variables/attributes as private\nprovide public get and set methods to access and update the value of a private variable\nTo inherit from a class, use the extends keyword.\nInheritance subclass (child) - the class that inherits from another class superclass (parent) - the class being inherited from Polymorphism subclasses can override methods inherited from superclasses\nAbstract Abstract class: is a restricted class that cannot be used to create objects (to access it, it must be inherited from another class). Abstract method: can only be used in an abstract class, and it does not have a body. The body is provided by the subclass (inherited from). Interface An interface is a completely \u0026ldquo;abstract class\u0026rdquo; that is used to group related methods with empty bodies\nTo access the interface methods, the interface must be \u0026ldquo;implemented\u0026rdquo; (kinda like inherited) by another class with the implements keyword (instead of extends). The body of the interface method is provided by the \u0026ldquo;implement\u0026rdquo; class.\nOn implementation of an interface, you must override all of its methods Interface methods are by default abstract and public Interface attributes are by default public, static and final An interface cannot contain a constructor (as it cannot be used to create objects) why To achieve security - hide certain details and only show the important details of an object (interface).\nJava does not support \u0026ldquo;multiple inheritance\u0026rdquo; (a class can only inherit from one superclass). However, it can be achieved with interfaces, because the class can implement multiple interfaces. Note: To implement multiple interfaces, separate them with a comma (see example below).\nInner Classes In Java, it is also possible to nest classes (a class within a class). The purpose of nested classes is to group classes that belong together, which makes your code more readable and maintainable.\nclass OuterClass { int x = 10; class InnerClass { int y = 5; } } public class Main { public static void main(String[] args) { OuterClass myOuter = new OuterClass(); OuterClass.InnerClass myInner = myOuter.new InnerClass(); System.out.println(myInner.y + myOuter.x); } } Wrapper Classes Wrapper classes provide a way to use primitive data types (int, boolean, etc..) as objects.\nThe table below shows the primitive type and the equivalent wrapper class:\nPrimitive Data Type Wrapper Class byte Byte short Short int Integer long Long float Float double Double boolean Boolean char Character Enums An enum is a special \u0026ldquo;class\u0026rdquo; that represents a group of constants (unchangeable variables, like final variables).\nTo create an enum, use the enum keyword (instead of class or interface), and separate the constants with a comma. Note that they should be in uppercase letters\nenum Level { LOW, MEDIUM, HIGH } //looping for (Level myVar : Level.values()) { System.out.println(myVar); } An enum can, just like a class, have attributes and methods. The only difference is that enum constants are public, static and final (unchangeable - cannot be overridden).\nAn enum cannot be used to create objects, and it cannot extend other classes (but it can implement interfaces).\nUser Input The Scanner class is used to get user input, and it is found in the java.util package.\nimport java.util.Scanner; // Import the Scanner class class Main { public static void main(String[] args) { Scanner myObj = new Scanner(System.in); // Create a Scanner object System.out.println(\u0026#34;Enter username\u0026#34;); String userName = myObj.nextLine(); // Read user input System.out.println(\u0026#34;Username is: \u0026#34; + userName); // Output user input } } Method Description nextBoolean() Reads a boolean value from the user nextByte() Reads a byte value from the user nextDouble() Reads a double value from the user nextFloat() Reads a float value from the user nextInt() Reads a int value from the user nextLine() Reads a String value from the user nextLong() Reads a long value from the user nextShort() Reads a short value from the user Collections ArrayList The ArrayList class is a resizable array\nArrayList\u0026lt;String\u0026gt; cars = new ArrayList\u0026lt;String\u0026gt;(); LinkedList The LinkedList class has all of the same methods as the ArrayList class because they both implement the List interface. But they work in different way.\nHashMap A HashMap however, store items in \u0026ldquo;key/value\u0026rdquo; pairs, and you can access them by an index of another type.(e.g. a String)\nHashMap\u0026lt;String, String\u0026gt; capitalCities = new HashMap\u0026lt;String, String\u0026gt;(); HashSet A HashSet is a collection of items where every item is unique.\nHashSet\u0026lt;String\u0026gt; cars = new HashSet\u0026lt;String\u0026gt;(); Iterator An Iterator is an object that can be used to loop through collections.\n// Import the ArrayList class and the Iterator class import java.util.ArrayList; import java.util.Iterator; public class Main { public static void main(String[] args) { // Make a collection ArrayList\u0026lt;String\u0026gt; cars = new ArrayList\u0026lt;String\u0026gt;(); cars.add(\u0026#34;Volvo\u0026#34;); cars.add(\u0026#34;BMW\u0026#34;); cars.add(\u0026#34;Ford\u0026#34;); cars.add(\u0026#34;Mazda\u0026#34;); // Get the iterator Iterator\u0026lt;String\u0026gt; it = cars.iterator(); // Print the first item System.out.println(it.next()); } } To loop through a collection, use the hasNext() and next() methods of the Iterator\nwhile(it.hasNext()) { System.out.println(it.next()); } Removing items while(it.hasNext()) { Integer i = it.next(); if(i \u0026lt; 10) { it.remove(); } } Note: Trying to remove items using a for loop or a for-each loop would not work correctly because the collection is changing size at the same time that the code is trying to loop.\nLambda parameter -\u0026gt; expression (parameter1, parameter2) -\u0026gt; expression (parameter1, parameter2) -\u0026gt; { code block } Thread Threads allows a program to operate more efficiently by doing multiple things at the same time.\nThreads can be used to perform complicated tasks in the background without interrupting the main program.\nCreating //1 public class Main extends Thread { public void run() { System.out.println(\u0026#34;This code is running in a thread\u0026#34;); } } //2 public class Main implements Runnable { public void run() { System.out.println(\u0026#34;This code is running in a thread\u0026#34;); } } Running //1 public class Main extends Thread { public static void main(String[] args) { Main thread = new Main(); thread.start(); System.out.println(\u0026#34;This code is outside of the thread\u0026#34;); } public void run() { System.out.println(\u0026#34;This code is running in a thread\u0026#34;); } } //2 public class Main implements Runnable { public static void main(String[] args) { Main obj = new Main(); Thread thread = new Thread(obj); thread.start(); System.out.println(\u0026#34;This code is outside of the thread\u0026#34;); } public void run() { System.out.println(\u0026#34;This code is running in a thread\u0026#34;); } } Exceptions try{ throw ... }catch(Exception e){ }finally{ } ","date":"2020-12-20T00:00:00Z","permalink":"https://dyhes.github.io/p/javabasics/","title":"【Java】Basics"},{"content":"History 符合C++11及之后标准的C++称之为“Modern C++”，即“现代C++”。之前的C++称为“Classic C++”，即“经典C++”\nYear C++ Standard Informal name 2020 ISO/IEC 14882:2020 C++20 2017 ISO/IEC 14882:2017 C++17, C++1z 2014 ISO/IEC 14882:2014 C++14, C++1y 2011 ISO/IEC 14882:2011 C++11, C++0x 2003 ISO/IEC 14882:2003 C++03 1998 ISO/IEC 14882:1998 C++98 Glossary 编辑器（Editor）\n程序开发中的“编辑器”一般是指“代码编辑器”。代码编辑器主要用于用来编写和查看程序源代码。通常这种编辑器有语法加亮（Syntax-Highlighting）功能。\n编译器（Compiler）\n编译器（compiler），是一种计算机程序，它会将用某种编程语言写成的源代码（原始语言），转换成另一种编程语言（目标语言）。\nA compiler is a computer program that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.\n链接器（Linker）\n是一个程序，将一个或多个由编译器或汇编器生成的目标文件外加库链接为一个可执行文件\n调试器（Debugger）\n调试器是指一种用于调试其它程序的计算机程序及工具。能够让代码在指令组模拟器（ISS）中可以检查运行状况以及选择性地运行，以便排错、调试。当开发的进度遇到瓶颈或找不出哪里有问题时，这技术将是非常有用的。但是将程序运行在调试器之下，这将比直接在运作的平台以及处理器上运行还要来得慢。\n典型的调试器通常能够在程序运行时拥有以下这些功能，例如单步运行（single-stepping）、利用中断点（breakpoint）使程序遇到各种种类的事件（event）时停止（breaking）（一般用于使程序停止在想要检查的状态）、以及追踪某些变量的变化。有些调试器也有能力在想要调试的程序在运行状态时，去改变它的状态，而不仅仅只是用来观察而己。\n解释器（interpreter）\n解释器是一种计算机程序，能够把高级编程语言一行一行解释运行。解释器像是一位“中间人”，每次运行程序时都要先转成另一种语言再作运行，因此解释器的程序运行速度比较缓慢。它不会一次把整个程序翻译出来，而是每翻译一行程序就立刻运行，然后再翻译下一行，再运行，如此不停地进行下去。\n集成开发环境（Integrated Development Environment，简称IDE）\n集成开发环境是一种辅助程序开发人员开发软件的应用软件，在开发工具内部就可以辅助编写源代码文本、并编译打包成为可用的程序，有些甚至可以设计图形接口。\nIDE通常包括编程语言编辑器、自动构建工具、通常还包括调试器。有些IDE包含编译器／解释器，如微软的Microsoft Visual Studio，有些则不包含，如Eclipse、SharpDevelop等，这些IDE是通过调用第三方编译器来实现代码的编译工作的。有时IDE还会包含版本控制系统和一些可以设计图形用户界面的工具。许多支持面向对象的现代化IDE还包括了类别浏览器、对象查看器、对象结构图。虽然当前有一些IDE支持多种编程语言（例如Eclipse、NetBeans、Microsoft Visual Studio），但是一般而言，IDE主要还是针对特定的编程语言而量身打造（例如Visual Basic）。\nVisual Studio中包含的C++集成开发环境叫做 Visual C++。\nVisial Studio 工程/项目与解决方案\nVisual C++中，将一个C++项目所需的所有源代码文件、资源文件等组织在一起，形成一个“Project”，我们俗称“C++工程”或者“C++项目”。有时简称“项目”。\nVisual C++ 将一个或者多个C++项目组织在一起，形成一个“Solution”，也就是“解决方案”。\n解决方案中的项目可能有互相的依赖关系。解决方案中的项目可以一键全部编译。\n解决方案文件夹\nC++解决方案中，可以建立虚拟的“解决方案文件夹”，将多个项目分类管理。\nC++解决方案中的项目，可能在硬盘的不同目录下，甚至在不同的硬盘上。\n命名空间 using namespace std;//not recomended using std::cout; cout\u0026lt;\u0026lt;\u0026#34;ha\u0026#34;\u0026lt;\u0026lt;std::endl; std::cout\u0026lt;\u0026lt;\u0026#34;ha\u0026#34;\u0026lt;\u0026lt;std::endl;//recomended 编译 输入输出流 引用（Reference） 大部分时候可视为指针的语法糖\nC++的引用主要是为了支持运算符重载；指针的存在主要是为了兼容C语言。\n用户自定义类型最好用引用传参，这样可以避免不必要的构造函数和析构函数调用；对于内置(C-like)类型，按值传参会比按引用传参更高效。\nint x; int\u0026amp; rx = x; int x, \u0026amp;rx = x; Any changes made through the reference variable are actually performed on the original variable (通过引用所做的读写操作实际上是作用于原变量上).\nA reference must be initialized in declaration 引用必须在声明的时候初始化。\nOnce initialized, the name of the reference cannot be assigned to other variables (引用一旦初始化，引用名字就不能再指定给其它变量)\nYou can use a reference variable as a parameter in a function and pass a regular variable to invoke the function. (引用可做函数参数，但调用时只需传普通变量即可)\nWhen you change the value through the reference variable, the original value is actually changed. (在被调函数中改变引用变量的值，则改变的是实参的值)\n动态内存管理 new \u0026lt;type\u0026gt; (initialValue); //申请一个变量的空间 new \u0026lt;type\u0026gt; [literalExpression]; //申请数组 delete \u0026lt;pointerName\u0026gt;; //删除一个变量/对象 delete [] \u0026lt;pointerName\u0026gt;; //删除数组空间 数据类型 bool 整数0和布尔false互相转化\n布尔true转化为整数1\n任意非0整数转化为布尔true\n列表初始化（C++11） List initialization is a new feature for C++11 (列表初始化是C++11的一个新特性)\nList: braced-init-list (“列表”是用花括号括起来的一(些)值)\n列表初始化的两个分类\nDirect list initialization (直接列表初始化)\n/* Variable initialization */ int x{}; // x is 0; int y{ 1 }; // y is 1; /* Array initialization */ int array1[]{ 1,2,3 }; char s1[ 3 ] { \u0026#39;o\u0026#39;, \u0026#39;k\u0026#39; }; char s3[]{ \u0026#34;Hello\u0026#34; }; Copy list initialization (拷贝列表初始化)\n/* Variable initialization */ int z = { 2 }; /* Array initialization */ int array2[] = { 4,5,6 }; char s2[] = { \u0026#39;y\u0026#39;,\u0026#39;e\u0026#39;,\u0026#39;s\u0026#39; }; char s4[] = { \u0026#34;World\u0026#34; }; char s5[] = \u0026#34;Aloha\u0026#34;; // Omit curly braces (省略花括号) List initialization is also called \u0026ldquo;unified initialization\u0026rdquo; (列表初始化也被称为“统一初始化方法”) Variables and arrays are initialized in the same form (变量和数组用同样的形式初始化)\nwhen to use Prefer {} initialization over alternatives unless you have a strong reason not to（尽量使用列表初始化，除非你有个很好的不用它的理由）\nWhy: List initialization does not allow narrowing（原因：列表初始化不允许“窄化”，即不允许丢失数据精度的隐式类型转换）\n类型转换 隐式类型转换 由编译器按照数据类型的转换规则自动转换，无需程序员干预。 可能导致数据精度损失，或者转换失败。应尽量避免使用隐式类型转换\n显式类型转换（即：强制类型转换） 由程序员用明确的类型转换语法写出类型转换代码。好处是，程序员知道自己要做什么并且把这个想法明确表达出来\n//c style (type) value; printf(\u0026#34;%d\u0026#34;, (int) 2.5); //cpp style static_cast\u0026lt;type\u0026gt; value; cout \u0026lt;\u0026lt; static_cast\u0026lt;double\u0026gt;(1 / 2); auto (C++11) C++03及之前的标准种，auto放在变量声明之前，声明变量的存储策略。但是这个关键字常省略不写。 C++11中，auto关键字放在变量之前，作用是在声明变量的时候根据变量初始值的类型自动为此变量选择匹配的类型\nauto 变量必须在定义时初始化，这类似于const关键字\n定义在一个auto序列的变量必须始终推导成同一类型\n如果初始化表达式是引用或const，则去除引用或const语义。\nint a{10}; int \u0026amp;b = a; auto c = b; //c的类型为int而非int\u0026amp;（去除引用） const int a1{10}; auto b1 = a1; //b1的类型为int而非const int（去除const） 如果auto关键字带上\u0026amp;号，则不去除引用或const语意\nint a = 10; int\u0026amp; b = a; auto\u0026amp; d = b;//此时d的类型才为int\u0026amp; const int a2 = 10; auto\u0026amp; b2 = a2;//因为auto带上\u0026amp;，故不去除const，b2类型为const in 初始化表达式为数组时，auto关键字推导类型为指针。\nint a3[3] = { 1, 2, 3 }; auto b3 = a3; cout \u0026lt;\u0026lt; typeid(b3).name() \u0026lt;\u0026lt; endl; //输出int * （输出与编译器有关） 若表达式为数组且auto带上\u0026amp;，则推导类型为数组类型。\nint a7[3] = { 1, 2, 3 }; auto\u0026amp; b7 = a7; cout \u0026lt;\u0026lt; typeid(b7).name() \u0026lt;\u0026lt; endl; //输出int [3] （输出与编译器有关） C++14中，auto可以作为函数的返回值类型和参数类型\nWhy Almost Always Auto Using auto are for correctness, performance, maintainability, robustness—and convenience (使用auto是为了代码的正确性、性能、可维护性、健壮性，以及方便)\ndecltype decltype利用已知类型声明新变量。\ndecltype是在编译期推导一个表达式的类型，它只做静态分析，因此它不会导致已知类型表达式执行。 decltype 主要用于泛型编程（模板）\n#include\u0026lt;iostream\u0026gt; using namespace std; int fun1() { return 10; } //修改fun1()时不必改动其他 auto fun2() { return \u0026#39;g\u0026#39;; } // C++14 int main(){ // Data type of x is same as return type of fun1() // and type of y is same as return type of fun2() decltype(fun1()) x; // 不会执行fun1()函数 decltype(fun2()) y = fun2(); cout \u0026lt;\u0026lt; typeid(x).name() \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; typeid(y).name() \u0026lt;\u0026lt; endl; return 0; } 内存模型 Stack\n编译器自动分配\nHeap\n由程序员分配\nGlobal/Static\n存储全局变量和静态变量\nConstant\n内容不可修改\n常量与指针 Pointer to Constant(常量指针、常指针)\n指针所指向的内容不可以通过指针的间接引用(*p)来改变。\nconst int x = 1; const int* p1; p1 = \u0026amp;x; //指针 p1的类型是 (const int*) *p1 = 10; // Error! char* s1 = \u0026#34;Hello\u0026#34;; // Error! const char* s2 = \u0026#34;Hello\u0026#34;; // Correct! Pointer Constant(指针常量)\n指针本身为常量，不可改变\nint x = 1, y = 1; int* const p2 = \u0026amp;x; //常量 p2的类型是 (int*) *p2 = 10; // Okay! à x=10 p2 = \u0026amp;y; // Error! p2 is a constant using, typedef, and #define typedef\n定义类型别名\ntypedef const unsigned long int * MyPointer; using (C11)\nusing ConstPointer = const unsigned long int *; using identifier=const using的写法比typedef的写法更加直观，所以，我们应尽量使用using声明新类型名。而且当涉及到模版类型名时，只能使用using。\n#define\n定义宏，编译器对宏进行替换\n作用域 全局作用域 局部作用域 文件作用域 函数作用域 函数中的块级作用域 一元作用域解析运算符（Unary Scope Resolution） If a local variable name is the same as a global variable name, you can access the global variable using ::globalVariable. (局部变量名与全局变量名相同时，可使用 :: 访问全局变量)\nThe :: operator is known as the unary scope resolution.（:: 这个运算符被称为一元作用域解析运算符）\n#include \u0026lt;iostream\u0026gt; int v1 = 10; int main() { int v1 = 5; std::cout \u0026lt;\u0026lt; \u0026#34;local variable v1 is \u0026#34; \u0026lt;\u0026lt; v1 \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;global variable v1 is \u0026#34; \u0026lt;\u0026lt; ::v1 \u0026lt;\u0026lt; std::endl; return 0; } 特殊函数 重载函数 根据参数（个数，类型）判断，应避免二义性\nint func(int a,double b); int func(double a,int b); func(1.1,2.1); //出现二义性 默认参数 默认值参数应后置\n内联函数（Inline Function） 普通函数性能有额外开销\n内联函数可减小函数调用开销，将代码插入到调用处，会导致程序变大\n是一种请求而非命令\n面向对象 构造函数 A class may be declared without ctors (类可不声明构造函数)\n(1) A no-arg constructor with an empty body is implicitly declared in the class. (编译器会提供一个带有空函数体的无参构造函数)\n(2) This constructor, called a default constructor is provided automatically only if no constructors are explicitly declared in the class. (只有当未明确声明构造函数时，编译器才会提供这个构造函数，并称之为“默认构造函数”)\n拷贝构造函数 Copy Constructor\n拷贝构造：用一个对象初始化另一个同类对象\n拷贝构造函数可以简写为 copy ctor，或者 cp ctor\n如何声明拷贝构造函数(copy ctor)\nCircle (Circle\u0026amp;);\nCircle (const Circle\u0026amp;);\nclass X { //来自C++11标准: 12.8节 // ... public: X(const X\u0026amp;, int = 1); }; X b(a, 0); // calls X(const X\u0026amp;, int); X c = b; // calls X(const X\u0026amp;, int); In general, if the programmer does not provide a copy ctor, the compiler will generate one.(一般情况下，如果程序员不编写拷贝构造函数，那么编译器会自动生成一个)\nThe generated copy ctor is called \u0026quot; implicitly-declared/defined copy ctor \u0026quot; (自动生成的拷贝构造函数叫做“隐式声明/定义的拷贝构造函数”\nIn general, the implicitly-declared/defined copy ctor simply copies each data field in one object to its counterpart in the other object. (一般情况下，隐式声明的copy ctor简单地将作为参数的对象中的每个数据域复制到新对象中)\n析构函数 Destructors are the opposite of constructors. (dtor vs ctor) 析构函数与构造函数正好相反\n下表中展示了ctor和dtor的对比\nDestructor Constructor When to invoke(何时调用) when the object is destroyed(对象销毁时) when an object is created(对象创建时) Prototype(原型) C::~C( ) C::C(arguments) Default prototype(默认函数的原型) C::~C( ) C::C( ) 或参数带有默认值 What if no explicit decl? (没有显式声明怎么办) Compiler will create a default one (编译器会生成默认函数) Overloadable(可否重载) No, only 1 Yes 友元函数 Private members: cannot be accessed from outside of the class. (私有成员无法从类外访问)\nOccasionally, it is convenient to allow some trusted functions and classes to access a class’s private members. (但有时又需要授权某些可信的函数和类访问这些私有成员)\nC++ enables you to use the friend keyword to declare friend functions and friend classes for a class (用friend关键字声明友元函数或者友元类)\nDisadvantage of \u0026ldquo;friend\u0026rdquo;: break the encapsulation (友元的缺点：打破了封装性)\nclass Date { private: int year{ 2019 } , month{ 1 }; int day{ 1 }; public: friend class Kid; friend void print(const Date\u0026amp; d); }; void print(const Date\u0026amp; d) { cout \u0026lt;\u0026lt; d.year \u0026lt;\u0026lt; \u0026#34;/\u0026#34; \u0026lt;\u0026lt; d.month \u0026lt;\u0026lt; \u0026#34;/\u0026#34; \u0026lt;\u0026lt; d.day \u0026lt;\u0026lt; endl; } class Kid { private: Date birthday; public: Kid() { cout \u0026lt;\u0026lt; \u0026#34;I was born in \u0026#34; \u0026lt;\u0026lt; birthday.year \u0026lt;\u0026lt; endl; } }; int main() { print(Date()); Kid k; cin.get(); } 创建对象 Circle circle1; // 正确，但不推荐这样写 Circle circle2(); // 错误！C++编译器认为这是一个函数声明 Circle circle3{}; // 正确，推荐写法。这里面明确显示用空初始化列表初始化circle3对象（调用Circle默认构造函数） 对象拷贝 How to copy the contents from one object to the other?(如何将一个对象的内容拷贝给另外一个对象)\n(1) use the assignment operator( 使用赋值运算符) ： =\n(2) By default, each data field of one object is copied to its counterpart in the other object. ( 默认情况下，对象中的每个数据域都被拷贝到另一对象的对应部分)\ncircle2 = circle1; (1) 将circle1 的radius 拷贝到circle2 中 (2) 拷贝后：circle1 和 circle2 是两个不同的对象，但是半径的值是相同的。( 但是各自有一个radius 成员变量) 浅拷贝与深拷贝 前提条件是，类A中有个指针p，指向一个外挂对象b（b是B类型的对象）；如果类A里面没有指针成员p，那也就不要谈深浅拷贝了。\n现在有一个类A的对象a1（a1的指针p指向外挂对象b1）。以拷贝构造的方式，创建a1的一个拷贝a2。\n(1) 如果仅仅将a1.p的值（这个值是个地址）拷贝给 a2.p，这就是浅拷贝。浅拷贝之后，a1.p和a2.p都指向外挂对象 b1\n(2) 如果创建一个外挂对象b2，将 a2.p指向b2；并且将b1的值拷贝给b2，这就是深拷贝\nShallow copy: if the field is a pointer to some object, the address of the pointer is copied rather than its contents. (浅拷贝：数据域是一个指针，只拷指针的地址，而非指针指向的内容)\n在两种情况下会出现浅拷贝\n(1) Implicit/default copy ctor (创建新对象时，调用类的隐式/默认构造函数)\n(2) default assignment operator for copying = (为已有对象赋值时，使用默认赋值运算符)\nEmployee e1{\u0026#34;Jack\u0026#34;, Date(1999, 5, 3), Gender::male}; Employee e2{\u0026#34;Anna\u0026#34;, Date(2000, 11, 8), Gender:female}; Employee e3{ e1 }; //cp ctor，执行一对一成员拷贝 上面的代码执行之后，e3.birthday指针指向了 e1.birthday所指向的那个Date对象\nDeep copy: Copy the contents that pointed by the pointer (深拷贝：拷贝指针指向的内容)\n如何深拷贝\n(1) 自行编写拷贝构造函数，不使用编译器隐式生成的（默认）拷贝构造函数\n(2) 重载赋值运算符，不使用编译器隐式生成的（默认）赋值运算符函数\nclass Employee { public: // Employee(const Employee \u0026amp;e) = default; //浅拷贝ctor Employee(const Employee\u0026amp; e){ //深拷贝ctor birthdate = new Date{ e.birthdate }; } // ... } Employee e1{\u0026#34;Jack\u0026#34;, Date(1999, 5, 3), Gender::male}; Employee e2{\u0026#34;Anna\u0026#34;, Date(2000, 11, 8),, Gender:female}; Employee e3{ e1 }; //cp ctor 深拷贝 Anonymous Object 匿名对象\nOccasionally, you may create an object and use it only once. (有时需要创建一个只用一次的对象)\nAn object without name is called anonymous objects. (这种不命名的对象叫做匿名对象)\nint main() { Circle c1 = Circle{1.1}; auto c2 = Circle{2.2}; // 用匿名对象做拷贝列表初始化 Circle c3{}; // 直接列表初始化,调默认Ctor c3 = Circle{3.3}; // 用匿名对象赋值 cout \u0026lt;\u0026lt; \u0026#34;Area is \u0026#34; \u0026lt;\u0026lt; Circle{4.2}.getArea() \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;Area is \u0026#34; \u0026lt;\u0026lt; Circle().getArea() \u0026lt;\u0026lt; endl; // 不推荐 cout \u0026lt;\u0026lt; \u0026#34;Area is \u0026#34; \u0026lt;\u0026lt; Circle(5).getArea() \u0026lt;\u0026lt; endl; // 不推荐 return 0; } Local class \u0026amp; Nested class 局部类和嵌套类\nLocal class : a class declared inside a function (局部类是在一个函数中声明的类)\nvoid f(){ class C { // C及其对象只在f()中可用 void g() { // 成员函数必须在C中实现 /* 访问f()的成员受限 ……. */ } }; C c1, c2; } Nested class: a class declared in another enclosing class (嵌套类是在另一个类中声明的类)\nclass E{ class N { // N及其对象可访问E的成员 /* 声明N的成员 ……. */ } }; C c1, c2; } Object Pointer\u0026amp;Dynamic Object 对象指针与动态对象\nObject pointers can be assigned new object names(对象指针可以指向新的对象名)\nArrow operator -\u0026gt; : Using pointer to access object members (箭头运算符 -\u0026gt; ：用指针访问对象成员)\nObject declared in a function is created in the stack.(在函数中声明的对象都在栈上创建)； When the function returns, the object is destroyed (函数返回，则对象被销毁).\nTo retain the object, you may create it dynamically on the heap using the new operator. (为保留对象，你可以用new运算符在堆上创建它)\nCircle *pCircle1 = new Circle{}; //用无参构造函数创建对象 ClassName *variable=new ClassName{}; Circle *pCircle2 = new Circle{5.9}; //用有参构造函数创建对象 //程序结束时，动态对象会被销毁，或者 delete pObject; //用delete显式销毁 Array of objects 对象数组\n(1) 声明方式1\nCircle ca1[10]; (2) 声明方式2\n用匿名对象构成的列表初始化数组\nCircle ca2[3] = { // 注意：不可以写成： auto ca2[3]= 因为声明数组时不能用auto Circle{3}, Circle{ }, Circle{5} }; (3) 声明方式3\n用C++11列表初始化，列表成员为隐式构造的匿名对象\nCircle ca3[3] { 3.1, {}, 5 }; Circle ca4[3] = { 3.1, {}, 5 }; (4) 声明方式4\n用new在堆区生成对象数组\nauto* p1 = new Circle[3]; auto p2 = new Circle[3]{ 3.1, {}, 5 }; delete [] p1; delete [] p2; p1 = p2 = nullptr; Objects \u0026amp; Function Objects as Function Arguments 值传递：无法改变成员值 引用传递 指针传递 一般来说，能用引用尽量不用指针。引用更加直观，更少出现意外的疏忽导致的错误。\n指针可以有二重、三重之分，比引用更加灵活。有些情况下，例如使用 new 运算符，只能用指针。\nObjects as Function Return Value 指针作为返回类型\nevil way\n// class Object { ... }; Object* f ( /*函数形参*/ ){ Object* o = new Object(args) // 这是“邪恶”的用法，不要这样做 // Do something return o; } // main() { Object* o = f ( /*实参*/ ); f( /*实参*/ )-\u0026gt;memberFunction(); // 记得要delete o 可行\n// class Object { ... }; Object* f ( Object* p, /*其它形参*/ ){ // Do something return p; } // main() { Object* o = f ( /*实参*/ ); // 不应该delete o 引用作为返回类型（提高效率）\nevil way\n// class Object { ... }; Object\u0026amp; f ( /*函数形参*/ ){ Object o {args}; // Do something return o; //这是邪恶的用法 } 可行1\n// class Object { ... }; class X { Object o; Object f( /*实参*/ ){ // Do something return o; } } 可行2\n// class Object { ... }; Object\u0026amp; f ( Object\u0026amp; p, /*其它形参*/ ){ // Do something return p; } // main() { auto\u0026amp; o = f ( /*实参*/ ); f( /*实参*/ ).memberFunction(); 成员作用域和this指针 The data members are accessible to all constructors and functions in the class. (数据成员可被类内所有函数访问)\nData fields and functions can be declared in any order in a class. (数据域与函数可按任意顺序声明)\nIf a local variable has the same name as a data field: (若成员函数中的局部变量与某数据域同名)\n(1) the local variable takes precedence ( 局部变量优先级高：就近原则)\n(2) the data field with the same name is hidden. ( 同名数据域在函数中被屏蔽)\nHow do you reference a class’s hidden data field in a function? (如何在函数内访问类中被屏蔽的数据域)？ 可以使用 this 关键字\nThis 关键字的特性\n(1) a special built-in pointer ( 特殊的内建指针)\n(2) references to the calling object. ( 引用当前函数的调用对象)\nDefault Member Initializers 就地初始化\nIn C++03, only static const members of integral types could be initialized in-class (在C++03标准中，只有静态常量整型成员才能在类中就地初始化)\nC++11 was to allow a non-static data member to be initialized where it is declared in its class (C++11标准中，非静态成员可以在它声明的时候初始化)\n数组必须声明长度\nConstructor Initializer Lists 构造函数初始化列表\n在构造函数中用初始化列表初始化数据域 ClassName (parameterList) : dataField1{value1}, dataField2{value2} { // Something to do } A data field is an object type (Object in Object / Embedded Object) (类的数据域是一个对象类型，被称为对象中的对象，或者内嵌对象)\nThe embedded object must be constructed before the body of ctor is executed (内嵌对象必须在被嵌对象的构造函数体执行前就构造完成)\nclass Time { /* Code omitted */ } class Action { public: Action(int hour, int minute, int second) { time = Time(hour, minute, second); //time对象应该在构造函数体之前构造完成 } private: Time time; }; Action a(11, 59, 30); If object type members/embedded objects are not initialized explicitly (若对象类型成员/内嵌对象成员没有被显式初始化)\n(1) the default constructor of the embedded object is automatically invoked. ( 该内嵌对象的无参构造函数会被自动调用)\n(2) If a default constructor of the embedded object does not exist, a compilation error will be reported. ( 若内嵌对象没有无参构造函数，则编译器报错)\nOrder of Member Initialization Default Member Initialization (就地初始化)\nConstructor Initialization List (构造函数初始化列表)\nAssign Values to the members in Ctor Body (在构造函数体中为成员赋值)。注意，这个不是初始化，而是赋值。\n执行次序： 就地初始化 \u0026gt; Ctor 初始化列表 \u0026gt; 在Ctor 函数体中为成员赋值\n哪个起作用（初始化/赋值优先级）： 在Ctor 函数体中为成员赋值 \u0026gt; Ctor 初始化列表 \u0026gt; 就地初始化\nDelegation Constructor 代理构造：One ctor can call another ctor (一个构造函数可以调用另外的构造函数)\nclass A{ public: A(): A(0){} A(int i): A(i, 0){} A(int i, int j) { num1=i; num2=j; average=(num1+num2)/2; } private: int num1; int num2; int average; }; 上面例子中，构造函数的调用次序:\nA() \u0026gt; A(int) \u0026gt; A(int, int)\nStatic Members 声明：Inside a class definition, \u0026ldquo;static\u0026rdquo; declares members that are not bound to class instances 在类定义中，关键字 static 声明不绑定到类实例的成员( 该成员无需创建对象即可访问)\n定义：\n(1) 声明为“constexpr”类型的静态数据成员必须 在类中声明 并初始化。自C++17 起，可不在类外定义\n(2) 声明为“inline”(C++17 起) 或者 “const int” 类型的静态数据成员可以 在类中声明 并初始化；\n(3) 其它须在类外定义并初始化，且不带static 关键字\n静态数据成员的定义规则复杂，在类外定义，大部分情况下不会出错\n静态数据成员具有静态存储期(static storage duration)或者C++11线程存储期特性\n(1) Only one instance of the object exists ( 只存在对象的一个实例)\n(2) 静态存储器对象未明确初始化时会被自动“零初始化(Zero-Initialization)”\nAccessibility private\nPrivate members can only be accessed from the inside of the class (私有成员只能在类内的函数访问)\nprotected: A protected data field or a protected function in a base class can be accessed by name in its derived classes (保护属性的数据或函数可被派生类成员访问)\npublic: Public members can be accessed from any other classes. (公有成员可被任何其他类访问)\n与继承结合 公有继承\n(1) 基类成员 在派生类中的访问属性不变。\n(2) 派生类的成员函数 可以访问基类的公有成员和保护成员，不能访问基类的私有成员;\n(3) 派生类以外的其它函数 可以通过派生类的对象，访问从基类继承的公有成员, 但不能访问从基类继承的保护成员和私有成员。\n保护继承\n) 基类成员 公有成员和保护成员变成protected，私有成员不变。\n(2) 派生类的成员函数 可以访问基类的公有成员和保护成员，不能访问基类的私有成员;\n(3) 派生类以外的其它函数 不能通过派生类的对象，访问从基类继承的任何成员。\n私有继承\n(1) 基类成员 在派生类中都变成 private。\n(2) 派生类的成员函数 可以访问基类的公有成员和保护成员，不能访问基类的私有成员;\n(3) 派生类以外的其它函数 不能通过派生类的对象，访问从基类继承的任何成员。\nAbstract Class Sometimes a base class is so abstract that it cannot have any specific instances. Such a class is referred to as an abstract class (类太抽象以至于无法实例化就叫做抽象类)\nthe class which contains abstract functions (包含抽象函数的类被称为抽象类)\nAbstract Functions / Pure Virtual Function (抽象函数/纯虚函数)\n抽象函数(abstract functions)要求子类实现它\nvirtual double getArea() = 0;\nDynamic Cast 动态类型转换\ndynamic_cast 运算符\n(1) 沿继承层级向上、向下及侧向转换到类的指针和引用\n(2) 转指针：失败返回nullptr\n(3) 转引用：失败抛异常\ntypeid typeid operator (typeid运算符)\ntypeid is used to obtain the information about the class of the object (typeid用于获取对象所属的类的信息)\n(1) typeid returns a reference to an object of class type_info. (typeid运算符返回一个type_info对象的引用)\n(2) typeid(AType).name() 返回实现定义的，含有类型名称的C风格字符串(char *)\n#include \u0026lt;typeinfo\u0026gt; //使用typeid，需要包含此头文件 class A {}; A a{}; // …… auto\u0026amp; t1 = typeid(a); if (typeid(A) == t1) { std::cout \u0026lt;\u0026lt; \u0026#34;a has type \u0026#34; \u0026lt;\u0026lt; t1.name() \u0026lt;\u0026lt; std::endl; } 继承 C++11引入final特殊标识符，可以使得类不能被继承\nC++11:派生类不继承的特殊函数\n(1) 析构函数\n(2) 友元函数\n继承基类构造函数\n(1) using A::A; 继承所有基类ctor\n(2) 不能仅继承指定的某个基类ctor\nstruct A { // 等价于 class A { public: A(int i) {} A(double d, int i) {} // ... }; struct B : A { // C++11 using A::A; // 继承基类所有构造函数 int d{0}; // 就地初始化 }; int main() { B b(1); // 调A(int i) } constructor chaining (构造函数链) Constructing an instance of a class invokes all the base class along the inheritance chain. (构造类实例会沿着继承链调用所有的基类ctor)\n调用次序: base first, derive next (父先子后)\ndestructor chaining (析构函数链) Conversely, the destructors are automatically invoked in reverse order(dtor与ctor正好相反)\n调用次序: derive first, base next (子先父后)\nName Hiding in Inheritance 继承中的名字隐藏\nNames in inner scopes hide names in outer scopes. (内部作用域的名字隐藏外部作用域的(同名)名字)\n(1) The derived class acts as an inner scope (派生类视作内部作用域)\n(2) The base class as an outer scope(基类视作外部作用域)\nwhy?\n(1) To avoid certain potentially dangerous behavior (避免某些潜在的危险行为)\n(2) Each class starts with a \u0026ldquo;clean sheet\u0026rdquo; with respect to each method name it declares (每个类在创建时，它的函数名都是写在一张干净的白纸上面，不会被基类函数名干扰)\nusing-declaration : introduce base class members into derived class definitions (using 声明语句可以将基类成员引入到派生类定义中)\nclass P { public: void f() {} }; class C :public P { public: using P::f; //此处不带小括号 void f(int x) {} }; int main() { C c; c.f(); } 多态 截止目前：多态性有两种表现的方式\n重载多态 子类型多态：不同的对象调用同名重定义函数，表现出不同的行为 联编(Binding): 确定具有多态性的语句调用哪个函数的过程\nStatic Binding (静态联编)\n在程序编译时(Compile-time)确定调用哪个函数\nDynamic Binding (动态联编)\n在程序运行时(Run-time)，才能够确定调用哪个函数\nRun-time Polymorphism 用动态联编实现的多态，也称为运行时多态(Run-time Polymorphism)。\n实现运行时多态有两个要素：\n(1) virtual function (虚函数)\n(2) Override (覆写) : redefining a virtual function in a derived class. (在派生类中重定义一个虚函数)\n同名虚函数的调用\n(1) 不由指针类型决定；\n(2) 而由指针所指的【实际对象】的类型决定\n(3) 运行时，检查指针所指对象类型\n用途：可以用父类指针访问子类对象成员\nIf a function is defined virtual in a base class, it is automatically virtual in all its derived classes. (基类定义了虚同名函数，那么派生类中的同名函数自动变为虚函数)\n类中保存着一个Virtual function table (虚函数表)\nRun-time binding (运行时联编/动态联编)\nMore overhead in run-time than non-virtual function (比非虚函数开销大)\n基类与派生类中有同名函数\n(1) 通过派生类对象访问同名函数，是静态联编\n(2) 通过基类对象的指针访问同名函数，是静态联编\n(3) 通过基类对象的指针或引用访问同名虚函数，是动态联编\noverride and final C++11引入override标识符，指定一个虚函数覆写另一个虚函数。\nclass A { public: virtual void foo() {} void bar() {} }; class B : public A { public: void foo() const override { // 错误： B::foo 不覆写 A::foo } // （签名不匹配） void foo() override; // OK ： B::foo 覆写 A::foo void bar() override {} // 错误： A::bar 非虚 }; void B::foo() override {// 错误： override只能放到类内使用 } C++11引入final特殊标识符，指定派生类不能覆写虚函数\nstruct Base { virtual void foo(); }; struct A : Base { void foo() final; // A::foo 被覆写且是最终覆写 void bar() final; // 错误：非虚函数不能被覆写或是 final }; struct B final : A // struct B 为 final，不能被继承 { void foo() override; // 错误： foo 不能被覆写，因为它在 A 中是 final }; struct可与class互换；差别在于struct的默认访问属性是public\nStructured Binding Declaration（c++17） 结构化绑定声明\nfor Array cv-auto \u0026amp;/\u0026amp;\u0026amp;(可选) [标识符列表] = 表达式;\ncv-auto \u0026amp;/\u0026amp;\u0026amp;(可选) [标识符列表] { 表达式 };\ncv-auto \u0026amp;/\u0026amp;\u0026amp;(可选) [标识符列表] ( 表达式 );\ncv-auto: 可能由const/volatile修饰的auto关键字\n\u0026amp;/\u0026amp;\u0026amp; 左值引用或者右值引用\n标识符列表：逗号分隔的标识符\nint main() { int priArr [] {42, 21, 7}; // ai/bi/ci 的基本类型都是int，只是cv标识或引用标识不同 auto [a1, a2, a3] = priArr; // a1 是 priArr[0] 的拷贝，a2, a3类推 const auto [b1, b2, b3] (priArr); // b1 是 priArr[0] 的只读拷贝，b2, b3类推 auto \u0026amp;[c1, c2, c3] {priArr}; // c1 是 priArr[0] 的引用，c2, c3类推 c3 = 14; // priArr[2]的值变为14 return 0; } int main() { std::array stdArr = {\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;}; auto [d1, d2, d3] {stdArr}; return 0; } for Object member 若初始化表达式为类/结构体类型，则标识符列表中的名字绑定到类/结构体的非静态数据成员上\n数据成员必须为公有成员 标识符数量必须等于数据成员的数量 标识符类型与数据成员类型一致 String Literals C++11 Raw String Literals R \u0026ldquo;delimiter( raw_characters )delimiter\u0026rdquo;\nconst char* s3 = R\u0026#34;NoUse(Hello World)NoUse\u0026#34;; const char* s1 = R\u0026#34;(Hello World)\u0026#34;; C++14: String Literals C++14将运算符 \u0026ldquo;\u0026ldquo;s 进行了重载，赋予了它新的含义，使得用这种运算符括起来的字符串字面量，自动变成了一个 std::string 类型的对象。\nauto hello = \u0026#34;Hello!\u0026#34;s; // hello is of std::string type auto hello = std::string{\u0026#34;Hello!\u0026#34;}; // equals to the above auto hello = \u0026#34;Hello!\u0026#34;; // hello is of const char* type array类 C Style Array (C++ raw array，也叫做C++原生数组) int arr[ ] = { 1, 2, 3 }; arr 可能会退化为指针：void f(int a[]) { std::cout \u0026laquo; sizeof(a)/sizeof(a[0]); } arr 不知道自己的大小： sizeof(arr)/sizeof(arr[0])\n两个数组之间无法直接赋值: array1 = array2;\n不能自动推导类型：auto a1[] = {1,2,3};\nC++ Style Array 是一个容器类，所以有迭代器（可以认为是一种用于访问成员的高级指针）\n可直接赋值\n知道自己大小：size()\n能和另一个数组交换内容：swap()\n能以指定值填充自己: fill()\n取某个位置的元素( 做越界检查) ：at()\nC++数组类是一个模板类，可以容纳任何类型的数据\n#include \u0026lt;array\u0026gt; std::array\u0026lt; 数组 类型, 数组大小\u0026gt; 数组名字; std::array\u0026lt; 数组 类型, 数组大小\u0026gt; 数组 名字 { 值1, 值2, …}; 限制与C风格数组相同\nstd::array\u0026lt;int , 10\u0026gt; x; std::array\u0026lt;char , 5\u0026gt; c{ \u0026#39;H\u0026#39;,\u0026#39;e\u0026#39;,\u0026#39;l\u0026#39;,\u0026#39;l\u0026#39;,\u0026#39;o\u0026#39; }; C++17 Type Deduction for std::array (std::array的类型推导)\nC++17引入了一种新特性，对类模板的参数进行推导\n示例：\nstd::array a1 {1, 3, 5}; // 推导出 std::array\u0026lt;int, 3\u0026gt; std::array a2 {\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}; // 推导出 std::array\u0026lt;char, 4\u0026gt; Constant Expressions Constant expression is an expression that can be evaluated at compile time. (常量表达式是编译期可以计算值的一个表达式)\n// 例如：C++ 数组的大小要求是编译期的一个常量（原生数组以及std::array）\nint n = 1; n ++; std::array\u0026lt;int, n\u0026gt; a1; // error: n is not a constant expression const int cn = 2; std::array\u0026lt;int, cn\u0026gt; a2; // OK: cn is a constant expression const 修饰的对象未必是编译期常量 const int rcn = n; // rcn is runtime constant, compiler does // NOT know its value at compile-time rcn = ++n; // error: rcn is read-only std::array\u0026lt;int , rcn\u0026gt; a3; // error: rcn is NOT known at compile-time C++11 constexpr: 编译期常量表达式说明符\nconstexpr specifier declares that it is possible to evaluate the value of the function or variable at compile time. (constexpr说明符声明可在编译时计算函数或变量的值)\nconstexpr int max(int a , int b) { // c++11 引入 constexpr if (a \u0026gt; b) return a; // c++14才允许constexpr函数中有分支循环等 else return b; } int main() { int m = 1; const int rcm = m++; // rcm是运行期常量 const int cm = 4; // 编译期常量，等价于: constexpr int cm = 4; int a1[ max(m , rcm)]; // 错误：m \u0026amp; rcm 不是编译期常量 std::array\u0026lt;char , max(cm , 5)\u0026gt; a2; // OK: cm 和 5 是编译期常量 } const vs constexpr\nconst ： 告知程序员，const 修饰的内容是不会被修改的。主要目的是帮程序员避免bug 。\nchar* s1 = \u0026#34;Hello\u0026#34;; // C语言允许，但C++编译出错 *s1 = \u0026#39;h’; // C语言中，语法正确，但运行时会出错 const char* s2 = \u0026#34;World\u0026#34;; // C++ 要求加const *s2 = \u0026#39;w\u0026#39;; // C++编译器报错 constexpr ：用在所有被要求使用“constant expression”的地方（就是constexpr 修饰的东西可以在编译期计算得到值），主要目的是让编译器能够优化代码提升性能 。\nassert assert:为c语言的宏（Macro）\n用法：\n包含头文件 以调试模式编译程序\nassert( bool_expr ); // bool_expr 为假则中断程序\nstd::array a{ 1, 2, 3 }; //C++17 类型参数推导 for (size_t i = 0; i \u0026lt;= a.size(); i++) { assert(i \u0026lt; 3); //断言：i必须小于3，否则失败 std::cout \u0026lt;\u0026lt; a[ i ]; std::cout \u0026lt;\u0026lt; (i == a.size() ? \u0026#34;\u0026#34; : \u0026#34; \u0026#34;); assert()依赖于NDEBUG 宏 NDEBUG这个宏是C/C++标准规定的，所有编译器都有对它的支持。\n(1) 调试(Debug)模式编译时，编译器不会定义NDEBUG，所以assert()宏起作用。\n(2) 发行(Release)模式编译时，编译器自动定义宏NDEBUG，使assert不起作用\n如果要强制使得assert()生效或者使得assert()不生效，只要手动 #define NDEBUG 或者 #undef NDEBUG即可。\n#undef NDEBUG // 强制以debug模式使用\u0026lt;cassert\u0026gt; int main() { int i; std::cout \u0026lt;\u0026lt; \u0026#34;Enter an int: \u0026#34;; std::cin \u0026gt;\u0026gt; i; assert((i \u0026gt; 0) \u0026amp;\u0026amp; \u0026#34;i must be positive\u0026#34;); return 0; } static_assert(C++11静态断言) static_assert ( bool_constexpr, message)\n(1) bool_constexpr: 编译期常量表达式，可转换为bool 类型，不可出现变量表达式\n(2) message: 字符串字面量 ，是断言失败时显示的警告信息。自C++17起，message是可选的\n声明与定义 A declaration introduces an identifier and describes its type, be it a type, object, or function. A declaration is what the compiler needs to accept references to that identifier. (“声明”是引入标识符并描述其类型，无论是类型，对象还是函数。编译器需要该“声明”，以便识别在它处使用该标识符。)\nextern int bar; extern int g(int, int); double f(int, double); // extern can be omitted for function declarations class foo; // no extern allowed for type declarations A definition actually instantiates/implements this identifier. It\u0026rsquo;s what the linker needs in order to link references to those entities (“定义”实例化/实现这个标识符。链接器需要“定义”，以便将对标识符的引用链接到标识符所表示的实体)\nint bar; int g(int lhs, int rhs) {return lhs*rhs;} double f(int i, double d) {return i+d;} class foo {}; 区别\n（1） A definition can be used in the place of a declaration ( 定义有时可取代声明，反之则不行)\n（2） An identifier can be declared more than once, but can be defined only once ( 标识符可被声明多次，但只能定义一次\n（3） 定义通常伴随着编译器为标识符分配内存\n总结\n(1) Declaration: \u0026ldquo;Somewhere, there exists a foo.\u0026rdquo; ( 声明：某个地方有个foo)\n(2) Definition: \u0026ldquo;\u0026hellip;and here it is!\u0026rdquo; ( 定义：它在这儿，长成这样)\n实现与声明分离（Seperating Declaration from Implementation） C++ allows you to separate class declaration from implementation. (C++中，类声明与实现可以分离)\n(1) .h: 类声明，描述类的结构\n(2) .cpp: 类实现，描述类方法的实现\nFunctionType ClassName :: FunctionName (Arguments) { //… } 其中，:: 这个运算符被称为binary scope resolution operator（二元作用域解析运算符），简称“域分隔符”\nWhen a function is implemented inside a class declaration, it automatically becomes an inline function. (当函数在类声明中实现，它自动成为内联函数)\nclass A { public: A() = default; //C++11 double f1() { // f1自动称为内联函数 // do something } double f2(); }; double A::f2() { // f2不是内联函数 //do something } class A { public: A() = default; //C++11 double f1(); double f2(); }; double A::f2() { //do something } inline double A::f1() { // f1是内联函数 //do something } 避免头文件被多次包含 #ifndef MY_HEADER_FILE_H #define MY_HEADER_FILE_H //CONTENT #endif #pragma once //c++03,c90 _Pragma(\u0026#34;once\u0026#34;)//c++11,c99 实际上为运算符\n文件系统 About std::filesystem(std::filesystem简介)\nC++17 std::filesystem provides facilities for performing operations on file systems and their components, such as paths, regular files, and directories。（标准库的filesystem提供在文件系统与其组件，例如路径、常规文件与目录上进行操作的方法）\nFile(文件)：持有数据的文件系统对象，能被写入或读取。文件有名称和属性，属性之一是文件类型\nPath(路径)：标识文件所处位置的一系列元素，可能包含文件名\nnamespace fs = std::filesystem; fs::path p{ \u0026#34;CheckPath.cpp\u0026#34; }; Absolute Path (platform dependent) (绝对路径)：An absolute path contains a file name with its complete path and drive letter.(包含完整的路径和驱动器符号)\nRelative Path (相对路径)\nContains NO drive letter or leading \u0026ldquo;/\u0026rdquo; (不包含驱动器及开头的/符号)\nThe file stores in the path Relative to \u0026ldquo;Current Path\u0026rdquo; (文件存在相对于“当前路径”的位置)\nOS Type Absolute path Directory path Windows(case insensitive) c:\\example\\scores.txt c:\\example Unix/Linux(case sensitive) /home/cyd/scores.txt /home/cyd Differences between Windows and Linux(两种操作系统的不同) Windows Linux C++ java 行结束字符 \\r\\n \\n - System.getProperty(\u0026ldquo;line.separator\u0026rdquo;); 路径名分隔符 \u0026lsquo;\\\u0026rsquo; \u0026lsquo;/\u0026rsquo; std::filesystem::path::preferred_separator java.io.File.separator 路径名 a:\\b\\c 或\\host\\b\\c /a/b/c std::filesystem::path - // The directory separator for Windows is a backslash (\\), which needs special treat namespace fs = std::filesystem; fs::path p1(\u0026#34;d:\\\\cpp\\\\hi.txt\u0026#34;); // 字符串中的反斜杠要被转义 fs::path p2(\u0026#34;d:/cpp/hi.txt\u0026#34;); // Windows也支持正斜杠 fs::path p3(R\u0026#34;(d:\\cpp\\hi.txt)\u0026#34;);// 使用原始字符串字面量 Path类 Members functions of path class(path类的成员函数)\n构造\npath(string)\n构造函数\nassign(string): path\u0026amp;\n为路径对象赋值\n连接\nappend(type p): path\u0026amp;\n将p追加到路径后。type是string、path或const char*。等价于 /= 运算符；自动添加目录分隔符\nconcat(type p): path\u0026amp;\n将p追加到路径后。type是string、path或const char*。等价于+=运算符；不自动添加目录分隔符\n修改器\nclear(): void\n清空存储的路径名\nremove_filename(): path\u0026amp;\n从给定的路径中移除文件名\nreplace_filename(const path\u0026amp; replacement): path\u0026amp;\n以 replacement 替换文件名\n分解\nroot_name(): path\n返回通用格式路径的根名\nroot_directory(): path\n返回通用格式路径的根目录\nroot_path(): path\n返回路径的根路径，等价于 root_name() / root_directory()，即“路径的根名 / 路径的根目录”\nrelative_path(): path\n返回相对于 root-path 的路径\nparent_path(): path\n返回到父目录的路径\nfilename(): path\n返回路径中包含的文件名\nstem(): path\n返回路径中包含的文件名，不包括文件的扩展名\nextension(): path\n返回路径中包含的文件名的扩展名\n查询\nempty(): bool\n检查路径是否为空\nhas_xxx(): bool\n其中“xxx”是上面“分解”类别中的函数名。这些函数检查路径是否含有相应路径元素\nNon-member functions (非成员函数)\noperator/( const path\u0026amp; lhs, const path\u0026amp; rhs )\n以偏好目录分隔符连接二个路径成分 lhs 和 rhs。比如 path p{\u0026ldquo;C:\u0026rdquo;}; p = p / \u0026ldquo;Users\u0026rdquo; / \u0026ldquo;batman\u0026rdquo;;\noperator \u0026laquo;, \u0026raquo; (path p)\n进行路径 p 上的流输入或输出\n文件类型\ns_regular_file( const path\u0026amp; p ): bool\n检查路径是否是常规文件\nis_directory( const path\u0026amp; p ): bool\n检查路径是否是目录\nis_empty( const path\u0026amp; p ): bool\n检查给定路径是否指代一个空文件或目录\n查询\ncurrent_path(): path\n返回当前工作目录的绝对路径（类似linux指令 pwd）\ncurrent_path( const path\u0026amp; p ): void\n更改当前路径为p （类似linux指令 cd）\nfile_size( const path\u0026amp; p ): uintmax_t\n对于常规文件 p ，返回其大小；尝试确定目录(以及其他非常规文件)的大小的结果是由编译器决定的\nspace(const path\u0026amp; p): space_info\n返回路径名 p 定位于其上的文件系统信息。space_info中有三个成员：capacity ——文件系统的总大小(字节)，free ——文件系统的空闲空间(字节)，available ——普通进程可用的空闲空间（小于或等于 free ）\nstatus(const path\u0026amp; p): file_status\n返回 p 所标识的文件系统对象的类型与属性。返回的file_status是一个类，其中包含文件的类型(type)和权限(permissions)\n修改\nremove(const path\u0026amp; p): bool\n删除路径 p 所标识的文件或空目录\nremove_all(const path\u0026amp; p): uintmax_t\n递归删除 p 的内容（若它是目录）及其子目录的内容，然后删除 p 自身，返回被删文件及目录数量\nrename(const path\u0026amp; old_p,const path\u0026amp; new_p): void\n移动或重命名 old_p 所标识的文件系统对象到 new_p(类似linux指令mv)\ncopy( const path\u0026amp; from, const path\u0026amp; to ): void\n复制文件与目录。另外一个函数 bool copy_file(from, to) 拷贝单个文件\ncreate_directory( const path\u0026amp; p ): bool\n创建目录 p （父目录必须已经存在）,若 p 已经存在，则函数无操作\ncreate_directories( const path\u0026amp; p ): bool\n创建目录 p （父目录不一定存在）,若 p 已经存在，则函数无操作\nComparision of File Manipulation between C and C++ (文件操作对比) C++ C file input ifstream (i: input; f:file) Header File (头文件) file output ofstream (o: ouput; f:file) stdio.h file input \u0026amp; output fstream read from file (读文件) \u0026raquo;;get(); get(char); get(char*);getline();read(char*,streamsize); fscanf();fgets(char*, size_t , FILE*);fread(void *ptr, size, nitems, FILE *stream); Read/Write (读写操作) write to file (写文件) \u0026laquo;;put(char), put(int);write (const char*, streamsize);flush() fprintf();fwrite(const void *ptr, size, nitems, FILE stream);fputs(const char, FILE *); Status test (状态测试) eof(); bad(); good(); fail() feof(); ferror(); Hierarchy of C++ I/O Stream Classes(C++ I/O流类层次) C++的流类主要有五类：\n流基类（ios_base和ios） 标准输入输出流类（istream/ostream/iostream） 字符串流类（istringstream/ostringstream） 文件流类（ifstream/ofstream/fstream） 缓冲区类（streambuf/stringbuf/filebuf） 标准输入输出流对象 cin 和 cout 分别是类 istream 和 ostream 的实例\n输出至文件 自文件读入 To read data correctly, you need to know exactly how data is stored.(若想正确读出数据，必须确切了解数据的存储格式)\nTesting if a file is successfully opened (检测文件是否成功打开)\nErrors may occur (可能出现错误): the file does not exist when reading a file (读文件时文件不存在) the media is ReadOnly when writing a file (e.g. write to a CD) (写文件时介质只读) To detect if a file is successfully opened: (检测文件是否正确打开的方法) invoke fail() immediately after open(). (open()之后马上调用fail()函数), if fail() returns true, the file is not opened (does not exist). (fail()返回true, 文件未打开)\nofstream output(\u0026#34;scores.txt\u0026#34;); if (output.fail()) { cout \u0026lt;\u0026lt; R\u0026#34;(Can\u0026#39;t open file \u0026#34;scores.txt\u0026#34;!)\u0026#34;; } Testing End of File (检测是否已到文件末尾)\nUse eof() function to detect the end of file (用eof()函数检查是否是文件末尾)\nifstream in(\u0026#34;scores.txt\u0026#34;); while (in.eof() == false) { cout \u0026lt;\u0026lt; static_cast\u0026lt;char\u0026gt;(in.get()); } Functions for I/O stream getline()\nmember function getline(char* buf, int size, char delimiter)\nnon-member function std::getline(istream\u0026amp; is, string\u0026amp; str, char delimiter)\nWhen using (\u0026raquo;), data are delimited by whitespace. (\u0026raquo;运算符用空格分隔数据)\nconstexpr int SIZE{ 40 }; std::array\u0026lt;char , SIZE\u0026gt; name{}; while (!input.eof()) {// not end of file input.getline(\u0026amp;name[ 0 ] , SIZE , \u0026#39;#\u0026#39;); std::cout \u0026lt;\u0026lt; \u0026amp;name[ 0 ] \u0026lt;\u0026lt; std::endl; } std::string name2{}; while (!input.eof()) { std::getline(input, name2, \u0026#39;#\u0026#39;); std::cout \u0026lt;\u0026lt; n \u0026lt;\u0026lt; std::endl; } get() and put()\nTwo other useful functions are get and put.\nget: read a character\nint istream::get(); istream\u0026amp; get (char\u0026amp; c); is.get(c); put:write a character.\nostream\u0026amp; put (char c) flush()\nFlush output stream buffer (将输出流缓存中的数据写入目标文件)\nostream\u0026amp; flush();\ncout.flush(); // 其它输出流对象也可以调用 flush() cout \u0026lt;\u0026lt; \u0026#34;Hello\u0026#34; \u0026lt;\u0026lt; std::flush; // 与endl类似作为manipulator的调用方式 格式化输出 setw manipulator(“设置域宽”控制符)\n要包含头文件 setw(n) 设置域宽，即数据所占的总字符数\nstd::cout \u0026lt;\u0026lt; std::setw(3) \u0026lt;\u0026lt; \u0026#39;a\u0026#39; \u0026lt;\u0026lt; std::endl; //_ _a setw()控制符只对其后输出的第一个数据有效，其他控制符则对其后的所有输入输出产生影响。\nstd::cout \u0026lt;\u0026lt; std::setw(5) \u0026lt;\u0026lt; \u0026#39;a\u0026#39; \u0026lt;\u0026lt; \u0026#39;b\u0026#39; \u0026lt;\u0026lt; std::endl; //_ _ _ _ab setw()的默认为setw(0)，按实际输出\n如果输出的数值占用的宽度超过setw(int n)设置的宽度，则按实际宽度输出。\nsetprecision manipulator(“设置浮点精度”控制符)\nsetprecision(int n)\n(1) 控制显示浮点数的有效位\n(2) n代表数字总位数\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;iomanip\u0026gt; using namespace std; int main() { float f = 17 / 7.0; cout \u0026lt;\u0026lt; f \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; setprecision(0) \u0026lt;\u0026lt; f \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; setprecision(1) \u0026lt;\u0026lt; f \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; setprecision(2) \u0026lt;\u0026lt; f \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; setprecision(3) \u0026lt;\u0026lt; f \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; setprecision(6) \u0026lt;\u0026lt; f \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; setprecision(8) \u0026lt;\u0026lt; f \u0026lt;\u0026lt; endl; return 0; } setfill manipulator(“设置填充字符”控制符)\nsetfill(c)\n设置填充字符，即“\u0026laquo;\u0026ldquo;符号后面的数据长度小于域宽时，使用什么字符进行填充\nstd::cout \u0026lt;\u0026lt; std::setfill(\u0026#39;*\u0026#39;) \u0026lt;\u0026lt; std::setw(5) \u0026lt;\u0026lt; \u0026#39;a\u0026#39; \u0026lt;\u0026lt; std::endl; //****a 控制符 用途 setw(width) 设置输出字段的宽度(仅对其后第一个输出有效) setprecision(n) 设置浮点数的输/入出精度(总有效数字个数等于n) fixed 将浮点数以定点数形式输入/出(小数点后有效数字个数等于setprecision指定的n) showpoint 将浮点数以带小数点和结尾0的形式输入/出，即便该浮点数没有小数部分 left 输出内容左对齐 right 输出内容右对齐 hexfloat/defaultfloat C++11新增；前者以定点科学记数法的形式输出十六进制浮点数，后者还原默认浮点格式 get_money(money)put_money(money) C++11新增；从流中读取货币值，或者将货币值输出到流。支持不同语言和地区的货币格式https://en.cppreference.com/w/cpp/io/manip/get_moneyhttps://en.cppreference.com/w/cpp/io/manip/put_money get_time(tm, format)put_time(tm,format) C++11新增；从流中读取日期时间值，或者将日期时间值输出到流。https://en.cppreference.com/w/cpp/io/manip/get_timehttps://en.cppreference.com/w/cpp/io/manip/put_time The stream manipulator also works to format output to a file(流控制符同样可以用于文件输入/输出)\nFile Open Mode When opening an fstream object, a \u0026ldquo;file open mode\u0026rdquo; should be specified(创建fstream对象时，应指定文件打开模式)。\nMode(模式) Description(描述) ios::in 打开文件读数据 ios::out 打开文件写数据 ios::app 把输出追加到文件末尾。app = append ios::ate 打开文件，把文件光标移到末尾。ate = at end ios::trunc 若文件存在则舍弃其内容。这是ios::out的默认行为。trunc = truncate ios::binary 打开文件以二进制模式读写 std::fstream{p,ios::app|ios::binary} Open Mode Combine several modes (几种模式可以组合在一起)\nusing the | operator (bitwise inclusive OR) (用“位或”运算符)\n// std::ios_base::openmode 被ios继承 typedef /*implementation defined*/ openmode; static constexpr openmode app = /*implementation defined*/ Binary IO TEXT file vs BINARY file (not technically precise) (文本文件与二进制文件)\n(1) Both stores as a sequence of bits (in binary format) (都按二进制格式存储比特序列)\n(2) text file : interpreted as a sequence of characters (解释为一系列字符)\n(3) binary file : interpreted as a sequence of bits. (解释为一系列比特)\nText I/O is built upon binary I/O to provide a level of abstraction for character encoding and decoding. (文本模式的读写是建立在二进制模式读写的基础上的，只不过是将二进制信息进行了字符编解码)\nBy default, a file is opened in text mode.(文件默认以文本模式打开)\nopen a file using the binary mode ios::binary.(用ios::binary以二进制模式打开文件)\nText I/O (文本模式) Binary I/O function:(二进制模式) 读 operator \u0026raquo;; get(); getline(); read(); 写 operator \u0026laquo;; put(); write(); The write Function (write函数) ostream\u0026amp; write( const char* s, std::streamsize count )\nfstream fs(\u0026#34;GreatWall.dat\u0026#34;, ios::binary|ios::trunc); char s[] = \u0026#34;ShanHaiGuan\\nJuYongGuan\u0026#34;; fs.write(s, sizeof(s)); 将非字符数据写入文件\n(1) Convert any data into a sequence of bytes (byte stream) (先将数据转换为字节序列，即字节流)\n(2) Write the sequence of bytes to file with write() (再用write函数将字节序列写入文件)\nHow to convert any data into byte stream? (如何将信息转换为字节流)\n2.1. reinterpret_cast\n该运算符有两种用途：\n(1) cast the address of a type to another type (将一种类型的地址转为另一种类型的地址)\n(2) cast the address to a number, i.e. integer (将地址转换为数值，比如转换为整数)\n语法：\nreinterpret_cast(address)\naddress is the starting address of the data (address是待转换的数据的起始地址)\ndataType is the data type you are converting to. (dataType是要转至的目标类型)\nFor binary I/O, dataType is char . (对于二进制I/O来说，dataType是 char)\nlong int x {0}; int a[3] {21,42,63}; std::string str{\u0026#34;Hello\u0026#34;}; char* p1 = reinterpret_cast\u0026lt;char*\u0026gt;(\u0026amp;x); // variable address char* p2 = reinterpret_cast\u0026lt;char*\u0026gt;(a); // array address char* p3 = reinterpret_cast\u0026lt;char*\u0026gt;(\u0026amp;str); // object address The read Function (read成员函数) istream\u0026amp; read ( char* s, std::streamsize count );\n// 读字符串 fstream bio(\u0026#34;GreatWall.dat\u0026#34;, ios::in | ios::binary); char s[10]; bio.read(s, 5); s[5] = \u0026#39;\\0\u0026#39;; cout \u0026lt;\u0026lt; s; bio.close(); // 读其它类型数据（整数），需要使用 reinterpret_cast fstream bio(\u0026#34;temp.dat\u0026#34;, ios::in | ios::binary); int value; bio.read(reinterpret_cast\u0026lt;char *\u0026gt;(\u0026amp;value), sizeof(value)); cout \u0026lt;\u0026lt; value; File Positioner 文件位置指示器\nA file consists of a sequence of bytes.(文件由字节序列构成)\nFile positioner is a special marker that is positioned at one of these bytes. (一个特殊标记指向其中一个字节)\nA read or write operation takes place at the location of the file positioner. (读写操作都是从文件位置指示器所标记的位置开始)\nWhen a file is opened, the fp is set at the beginning. (打开文件，fp指向文件头)\nWhen you read or write data to the file, the file pointer moves forward to the next data item. (读写文件时，文件位置指示器会向后移动到下一个数据项)\nRandom Acess Random Access means one can read/write anywhere inside a file(随机访问意味着可以读写文件的任意位置)\nHow?\nWe are able to know where the file positioner is. (我们能知道文件定位器在什么位置)\nWe are able to move the file positioner inside the file (我们能在文件中移动文件定位器)\nMaybe we need two file positioners : one for reading, another for writing\n· For reading (读文件时用) For writing(写文件时用) 获知文件定位器指到哪里 tellg(); tell是获知，g是get表示读文件 tellp(); tell是获知，p是put表示写文件 移动文件定位器到指定位置 seekg(); seek是寻找，g是get表示读文件 seekp(); seek是寻找，p是put表示写文件 xxx_stream\u0026amp; seekg/seekp( pos_type pos );\nxxx_stream\u0026amp; seekg/seekp( off_type off, std::ios_base::seekdir dir);\nseekdir 文件定位方向类型 解释 std::ios_base::beg 流的开始；beg = begin std::ios_base::end 流的结尾 std::ios_base::cur 流位置指示器的当前位置；cur = current 例子 解释 seekg(42L); 将文件位置指示器移动到文件的第42字节处 seekg(10L, std::ios::beg); 将文件位置指示器移动到从文件开头算起的第10字节处 seekp(-20L, std::ios::end); 将文件位置指示器移动到从文件末尾开始，倒数第20字节处 seekp(-36L, std::ios::cur); 将文件位置指示器移动到从当前位置开始，倒数第36字节处 Left Value, Pure Right Value and eXpiring Value C++03 lvalue and rvalue (C++03的左值和右值) 通俗理解\n(1) 能放在等号左边的是lvalue\n(2) 只能放在等号右边的是rvalue\n(3) lvalue可以作为rvalue使用\nC++11: Left Value An lvalue designates a function or an object, which is an expression whose address can be taken (左值指定了一个函数或者对象，它是一个可以取地址的表达式)\nint lv1{ 42 }; // Object int main() { int\u0026amp; lv2{ lv1 }; // Lvalue reference to Object int* lv3{ \u0026amp;lv1 }; // Pointer to Object } int\u0026amp; lv4() { return lv1; } // Function returning Lvalue Reference C++11: Pure Right Value prvalue(Pure Right Value，纯右值)：是不和对象相关联的值(字面量)或者其求值结果是字面量或者一个匿名的临时对象\n(1) 除字符串字面量以外的字面量，比如 32, \u0026lsquo;a\u0026rsquo;\n(2) 返回非引用类型的函数调用 int f() { return 1;}\n(3) 后置自增/自减表达式i++/i\u0026ndash;\n(4) 算术/逻辑/关系表达式（a+b、a\u0026amp;b、a\u0026laquo;b）（a\u0026amp;\u0026amp;b、a||b、~a）（a==b、a\u0026gt;=b、a\u0026lt;b）\n(5) 取地址（\u0026amp;x）\n左值可以当成右值使用\nC++11: eXpiring Value xvalue(eXpiring Value，将亡值)：将亡值也指定了一个对象，是一个将纯右值转换为右值引用的表达式\nint prv(int x) { return 6 * x; } // pure rvalue int main() { const int\u0026amp; lvr5{ 21 }; // 常量左值引用可引用纯右值 int\u0026amp; lvr6{ 22 }; // 错！非常量左值引用不可引用纯右值 int\u0026amp;\u0026amp; rvr1{ 22 }; // 右值引用可以引用纯右值 int\u0026amp; lvr7{ prv(2) }; // 错！非常量左值引用不可引用纯右值 int\u0026amp;\u0026amp; rvr2{ prv(2) }; // 右值引用普通函数返回值 rvr1 = ++rvr2; // 右值引用做左值使用 } 运算符重载 不可重载的运算符\nOperator Name . 类属关系运算符 .* 成员指针运算符 :: 作用域运算符 ? : 条件运算符 ## 编译预处理符号 Restrictions for operator overloading (运算符重载的限制)\n(1) Precedence and Associativity are unchangeable (优先级和结合性不变)\n(2) NOT allowing to create new operator (不可创造新的运算符)\n重载的运算符必须和用户定义的class类型一起使用\n重载的运算符的参数至少应有一个是类对象(或类对象的引用)\n运算符函数的调用形式 表达式 作为成员函数 作为非成员函数 示例 @a (a).operator@ ( ) operator@ (a) 调用 std::cin.operator!() a@b (a).operator@ (b) operator@ (a, b) std::cout \u0026laquo; 42 调用 std::cout.operator\u0026laquo;(42) a=b (a).operator= (b) 不能是非成员 std::string s; s = \u0026ldquo;abc\u0026rdquo;; 调用 s.operator=(\u0026ldquo;abc\u0026rdquo;) a(b\u0026hellip;) (a).operator()(b\u0026hellip;) 不能是非成员 std::random_device r; auto n = r(); 调用 r.operator()() a[b] (a).operator 不能是非成员 std::map\u0026lt;int, int\u0026gt; m; m[1] = 2; 调用 m.operator a-\u0026gt; (a).operator-\u0026gt; ( ) 不能是非成员 auto p = std::make_unique(); p-\u0026gt;bar() 调用 p.operator-\u0026gt;() a@ (a).operator@ (0) operator@ (a, 0) std::vector::iterator i = v.begin(); i++ 调用 i.operator++(0) 此表中，@ 是表示所有匹配运算符的占位符：@a 为所有前缀运算符，a@ 为除 -\u0026gt; 以外的所有后缀运算符，a@b 为除 = 以外的所有其他运算符 二元 复合二元运算符\nVec2D Vec2D::operator +(const Vec2D\u0026amp; secondVec2D ) { *this = this-\u0026gt;add(secondVec2D); return (*this); } Vec2D Vec2D::add(const Vec2D\u0026amp; secondVec2D) { //prvalue double m = x_ + secondVec2D.getX() double n = y_ + secondVec2D.y_; return Vec2D(m, n); //临时的匿名对象 } How to make v2[] an Lvalue? (如何使r2[]成为左值)\ndeclare the [] operator to return a reference (使[]返回一个引用)\ndouble\u0026amp; Vec2D::operator[](const int \u0026amp;index) { //lvalue if (index == 0) return x_; //x_ can be modified //...... Now, the Vec2D class is mutable. } 一元 当编译器遇到 @obj; 时，\n若operator @是在obj的类中的成员，则调用\nobj.operator @()\n若operator @是obj的类的 friend 函数，则调用\noperator @(obj)\n- Vec2D Vec2D::operator-(){//无参数 return Vec2D(-this-\u0026gt;x_, -this-\u0026gt;y_); // 返回匿名临时对象 } 自增自减 前置++/\u0026ndash;重载无参数，返回引用类型\n后置++/\u0026ndash;重载带参数\u0026ndash;\u0026ldquo;dummy\u0026quot;参数 仅作占位\n运算符名 语法 可重载 原型示例（对于类 class T**）** 类内定义 类外定义 前自增 ++a 是 T\u0026amp; T::operator++(); T\u0026amp; operator++(T\u0026amp; a); 前自减 \u0026ndash;a 是 T\u0026amp; T::operator\u0026ndash;(); T\u0026amp; operator\u0026ndash;(T\u0026amp; a); 后自增 a++ 是 T T::operator++(int dummy); T operator++(T\u0026amp; a, int dummy); 后自减 a\u0026ndash; 是 T T::operator\u0026ndash;(int dummy); T operator\u0026ndash;(T\u0026amp; a, int dummy); 流运算符 运算符重载为类成员函数后,当调用该运算符时,左操作数必须是该类的实例。若\u0026laquo;和\u0026raquo;重载为成员函数，则只能用 v1\u0026laquo;cout;\n因此 \u0026laquo; (\u0026raquo;) should be overloaded as \u0026ldquo;friend function\u0026rdquo; (只能重载为友元函数)\nclass Vec2D { //重载为成员函数 public: ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;stream); istream \u0026amp;operator\u0026gt;\u0026gt;(istream \u0026amp;stream); }; Vec2D v1; v1 \u0026lt;\u0026lt; cout; //Vec2D对象只能作为第一个操作数 struct Vec2D { //重载为友元函数 friend ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;stream, Vec2D \u0026amp;v); friend istream \u0026amp;operator\u0026gt;\u0026gt;(istream \u0026amp;stream, Vec2D \u0026amp;v); }; Vec2D v1; cout \u0026lt;\u0026lt; v1; 对象转换运算符 Vec2D::operator double() {//无返回值类型 return magnitude(); } Vec2D v1(3, 4); double d = v1 + 5.1; // d: 10.1 double e = static_cast\u0026lt;double\u0026gt;(v1); // e: 5.0 赋值运算符 By default, the = operator performs a memberwise copy from one object to the other. (默认情况下，赋值运算符执行对象成员的一对一拷贝)\nTo change the way the default assignment operator = works, you need to overload the = operator. (重载赋值运算符，会改变其默认工作方式)\n一般情况下，如果拷贝构造函数需要执行深拷贝，那么赋值运算符需要重载\n异常处理 try { Code to try; throw an exception (1) with a throw statement (2) or from function; More code to try; } catch (type e) { Code to process the exception; } int quotient(int number1, int number2) { if (number2 == 0) throw number1; return number1 / number2; } int main() { try { int x = quotient(1, 0); } catch (int) { std::cout \u0026lt;\u0026lt; \u0026#34;除数为0！\u0026#34;; } } 异常匹配与异常类 catch ( ExceptionType\u0026amp; parameter ) { /* 处理异常 */ }\n若try{}中所抛异常类型与catch()的参数类型(ExceptionType)匹配，则进入catch块\n若对异常对象的内容不感兴趣，可省略catch参数，只保留类型\nBase Class of Exception in Standard Library(标准库中的异常基类)\n#include Class exception\nexception(); // 构造函数 virtual const char* what(); //返回解释性字符串 what()返回的指针指向拥有解释信息的空终止字符串的指针。该指针保证在获取它的异常对象被销毁前，或在调用该异常对象的非静态成员函数前合法\nc++11 noexcept C++03将throw(ExceptionType)放到函数后面，说明函数会抛出什么类型的异常，也被称为“异常规约”\njava用 throws关键字做同样的事情\nC++11后基本没人用“异常规约”\nC++11使用noexcept指明函数是否抛出异常\n若函数不抛异常，则可做编译优化\n即便函数抛异常，也不再说明所抛异常类型(简化)\nnoexcept不能用于区分重载函数 (对比：函数名后面的const可区分重载)\n重抛异常 When to rethrow an exception? (1) if the handler cannot process the exception (当它无法处理该异常)\n(2) the handler simply wants to let its caller be notified (或想通知它的调用者发生了一个异常)\ntry { // statements; } catch (TheException \u0026amp;ex) { // Do something; throw; } 模板 Programming(编程): Writing a program that creates, transforms, filters, aggregates and otherwise manipulates data. (写一个程序去处理数据)\nMetaprogramming(元编程): Writing a program that creates, transforms, filters, aggregates and otherwise manipulates programs.(写个程序去处理程序)\nC++ implements MetaProgramming with \u0026ldquo;template\u0026rdquo; to produce template instance, i.e. programs, in compiling time. (C++用模板实现元编程，由编译器在编译期根据模板生成模板实例，也就是程序)\nGeneric Programming(泛型编程): Writing a program that creates, transforms, filters, aggregates and otherwise manipulates data, but makes only the minimum assumptions about the structure of the data, thus maximizing reuse across a wide range of datatypes.(写个程序去处理数据，但是只对数据的结构做最小假设以使该程序能重用于处理广泛的数据类型)\nGeneric programming in C++ (i.e. compile-time polymorphism) is accomplished by metaprogramming (i.e. code generation from templated code).(C++的泛型编程，即编译时多态，是藉由元编程实现的，也就是由代码模板生成代码)\nTemplate 模板\ntemplate \u0026lt; typename T, typename S \u0026gt; auto add (T x1, S x2) { //C++14 return (x1 + x2); } A function template is just a blueprint, not a type, or a function. (函数模板只是蓝图，本身不是不是类型、函数)\n编译器扫描代码，遇到模版定义时，并不立即产生代码\nThe template arguments must be determined so that the compiler can generate an actual function (确定模板实参后，编译器生成实际函数代码)\n两种实例化方法 (确定模板实参的方法)\nExplicit instantiation (显式实例化)\ntemplate \u0026lt; typename T \u0026gt; void f( T s ){ std::cout \u0026lt;\u0026lt; s \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } template void f\u0026lt;double\u0026gt;(double); // 实例化，编译器生成代码 // void f(double s) { // T: double // std::cout \u0026lt;\u0026lt; s \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // } template void f\u0026lt;\u0026gt;(char); // 实例化 f\u0026lt;char\u0026gt;(char) ，推导出模板实参 template void f(int); // 实例化 f\u0026lt;int\u0026gt;(int) ，推导出模板实参 Implicit instantiation (隐式实例化)\n#include \u0026lt;iostream\u0026gt; template\u0026lt;typename T\u0026gt; void f(T s) { std::cout \u0026lt;\u0026lt; s \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } int main(){ f\u0026lt;double\u0026gt;(1); // 实例化并调用 f\u0026lt;double\u0026gt;(double) f\u0026lt;\u0026gt;(\u0026#39;a\u0026#39;); // 实例化并调用 f\u0026lt;char\u0026gt;(char) f(7); // 实例化并调用 f\u0026lt;int\u0026gt;(int) void (*ptr)(std::string) = f; // 实例化 f\u0026lt;string\u0026gt;(string) } A function instantiated from a function template is called an instantiated function. A class instantiated from a class template is called an instantiated class.(由函数模板实例化得到的函数叫做“实例函数”，由类模板实例化得到的类叫做“实例类”)\nDefault type parameter (默认类型参数) You can assign a default type for a type parameter in a class template. (可以为类模板的类型参数指定一个默认类型)\ntemplate\nYou can only use default type in class templates, NOT in function templates\nNon-type Parameters (非类型参数) Using nontype parameters in a template prefix. (在模板前缀中使用非类型参数)\nWhen instantiating a template, the nontype argument should be an object(实例化模板时，非类型实参应该是对象)\ntemplate\u0026lt;typename T, int capacity\u0026gt; class Stack{ ... private: T elements[capacity]; int size; }; Stack\u0026lt;char, 100\u0026gt; charStack; //对象作为非类型参数 template\u0026lt;typename T, Color c\u0026gt; class Label{ …… }; Color color(0, 0, 255); Label\u0026lt;char, color\u0026gt; label; Templates and inheritance 模板与继承\nA non-template class can be derived from a class template specialization(普通类可从类模板实例继承).\nA class template can be derived from a nontemplate class.(模板可从普通类继承)\nA class template can be derived from a class template.(类模板可继承类模板)\ntemplate class T1;\ntemplate class T2;\nclass C;\nSTL(Standard Template Library) containers\nstore a collection of data, often referred to as elements\nSequence container(顺序容器)\nrepresents linear data structures\nvector,list,deque\nAssociative containers（关联容器）\nnon-linear containers that can locate elements stored in the container quickly\nset,multiset,map and multimap\n关联容器和顺序容器统称为一级容器\nContainer adapters(容器适配器)\nconstrained versions of sequence containers, aiming at handling special cases\nstack,queue, and priority_queue\nIterators\nfacilitate traversing through the elements in a container\na generalization of pointers\nused extensively in the first-class containers for accessing and manipulating the elements(用于访问和处理一级容器中的元素)\nAlgorithm\nFunction Objects\nMemory Allocation\nIterator 规范 The prefix is should be used for boolean variables and methods. Type conversions must always be done explicitly. Never rely on implicit type conversion. Named constants (including enumeration values) must be all uppercase using underscore to separate words. Names representing types must be in mixed case starting with upper case. If the parameter of a member function has the same name as a private class variable, then the parameter should have underscore suffix. Class variables should never be declared public. ","date":"2020-12-06T00:00:00Z","permalink":"https://dyhes.github.io/p/cppc-%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1-%E5%B4%94%E6%AF%85%E4%B8%9C/","title":"【CPP】C++程序设计-崔毅东"},{"content":"Introduction C++ is a cross-platform language that can be used to create high-performance applications.\nC++ was developed by Bjarne Stroustrup, as an extension to the C language.\nC++ gives programmers a high level of control over system resources and memory.\nThe language was updated 4 major times in 2011, 2014, 2017, and 2020 to C++11, C++14, C++17, C++20.\nTo start using C++, you need two things:\nA text editor, like Notepad, to write C++ code A compiler, like GCC, to translate the C++ code into a language that the computer will understand Const the const keyword will declare the variable as \u0026ldquo;constant\u0026rdquo;, which means unchangeable and read-only)\nBasic Data Type The data type specifies the size and type of information the variable will store:\nData Type Size Description boolean 1 byte Stores true or false values char 1 byte Stores a single character/letter/number, or ASCII values int 2 or 4 bytes Stores whole numbers, without decimals float 4 bytes Stores fractional numbers, containing one or more decimals. Sufficient for storing 7 decimal digits double 8 bytes Stores fractional numbers, containing one or more decimals. Sufficient for storing 15 decimal digits Strings A string variable contains a collection of characters surrounded by double quotes\no use strings, you must include an additional header file in the source code, the \u0026lt;string\u0026gt; library\nIt is possible to use the extraction operator \u0026gt;\u0026gt; on cin to display a string entered by a user. However, cin considers a space (whitespace, tabs, etc) as a terminating character, which means that it can only display a single word . That\u0026rsquo;s why, when working with strings, we often use the getline() function to read a line of text. It takes cin as the first parameter, and the string variable as second.\ngetline (cin, fullName); ","date":"2020-12-01T00:00:00Z","permalink":"https://dyhes.github.io/p/cppw3c-c-/","title":"【CPP】W3C C++"}]