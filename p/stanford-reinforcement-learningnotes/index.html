<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Lecture 1 (intro) Element environment agent action observation and reward history History $h_t=(a_1,o_1,r_1,&mldr;,a_t,o_t,r_t)$\nagent choose action based on history\nstate state is information assumed to determine what happens next\nfunction of history $s_t=f(h_t)$\nThe true state of the world used to determine how world generates next observation and reward are often hidden or unknown to agent\nassume state used by the agent is sufficient statistic of history\nIn practice often assume most recent observation is sufficient statistic of history: $s_t=o_t$\n"><title>【Stanford Reinforcement Learning】Notes</title>
<link rel=canonical href=https://dyhes.github.io/p/stanford-reinforcement-learningnotes/><link rel=stylesheet href=/scss/style.min.6aa4d43a5cae1c51ef34b3f851ae7421f4b2f2d13827e2d975acbeb4f13c8710.css><meta property='og:title' content="【Stanford Reinforcement Learning】Notes"><meta property='og:description' content="Lecture 1 (intro) Element environment agent action observation and reward history History $h_t=(a_1,o_1,r_1,&mldr;,a_t,o_t,r_t)$\nagent choose action based on history\nstate state is information assumed to determine what happens next\nfunction of history $s_t=f(h_t)$\nThe true state of the world used to determine how world generates next observation and reward are often hidden or unknown to agent\nassume state used by the agent is sufficient statistic of history\nIn practice often assume most recent observation is sufficient statistic of history: $s_t=o_t$\n"><meta property='og:url' content='https://dyhes.github.io/p/stanford-reinforcement-learningnotes/'><meta property='og:site_name' content='飞鸿踏雪泥'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2023-06-06T00:00:00+00:00'><meta property='article:modified_time' content='2023-06-06T00:00:00+00:00'><meta name=twitter:title content="【Stanford Reinforcement Learning】Notes"><meta name=twitter:description content="Lecture 1 (intro) Element environment agent action observation and reward history History $h_t=(a_1,o_1,r_1,&mldr;,a_t,o_t,r_t)$\nagent choose action based on history\nstate state is information assumed to determine what happens next\nfunction of history $s_t=f(h_t)$\nThe true state of the world used to determine how world generates next observation and reward are often hidden or unknown to agent\nassume state used by the agent is sufficient statistic of history\nIn practice often assume most recent observation is sufficient statistic of history: $s_t=o_t$\n"><link rel="shortcut icon" href=/github.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu17834253352308399148.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>飞鸿踏雪泥</a></h1><h2 class=site-description>没有记录，就没有发生</h2></div></header><ol class=menu-social><li><a href=https://github.com/dyhes target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:dyheslin@gmail.com target=_blank title=Gmail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-gmail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16 20h3a1 1 0 001-1V5a1 1 0 00-1-1h-3v16z"/><path d="M5 20h3V4H5A1 1 0 004 5v14a1 1 0 001 1z"/><path d="M16 4l-4 4-4-4"/><path d="M4 6.5l8 7.5 8-7.5"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/categories/><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#lecture-1-intro>Lecture 1 (intro)</a><ol><li><a href=#element>Element</a></li><li><a href=#history>history</a></li><li><a href=#state>state</a></li><li><a href=#markov>Markov</a></li><li><a href=#rl-algorithm-component>RL Algorithm component</a></li><li><a href=#model>Model</a></li><li><a href=#policy>Policy</a></li><li><a href=#value>Value</a></li><li><a href=#types-of-rl-agents>Types of RL agents</a></li></ol></li><li><a href=#lecture-2-mdp>Lecture 2 (MDP)</a><ol><li><a href=#markov-process>Markov Process</a></li><li><a href=#markov-reward-processmrp>Markov Reward Process(MRP)</a><ol><li><a href=#horizon>horizon</a></li><li><a href=#return-g_t-for-a-mrp>Return $G_t$ (for a MRP)</a></li><li><a href=#state-value-function-vs-for-a-mrp>State Value Function, $V(s)$ (for a MRP)</a></li></ol></li><li><a href=#markov-decision-process-mdp>Markov Decision Process (MDP)</a></li><li><a href=#policy-1>Policy</a></li><li><a href=#control>control</a><ol><li><a href=#policy-iteration>Policy Iteration</a></li><li><a href=#value-iteration>Value Iteration</a></li><li><a href=#policy-iteration-as-bellman-operations>Policy iteration as Bellman Operations</a></li></ol></li></ol></li><li><a href=#lecture-3-model-free-evaluation>Lecture 3 (model-free evaluation)</a><ol><li><a href=#dynamic-programming-for-policy-evaluation>Dynamic programming for policy evaluation</a></li><li><a href=#monte-carlo-policy-evaluation>Monte Carlo policy evaluation</a><ol><li><a href=#every-visit-mc>Every-Visit MC</a></li><li><a href=#incremental-mc>Incremental MC</a></li></ol></li><li><a href=#temporal-differencetd>Temporal Difference(TD)</a></li></ol></li><li><a href=#lecture-4-model-free-control>Lecture 4 (model-free control)</a><ol><li><a href=#mc-for-on-policy-q-evaluation>MC for On Policy Q Evaluation</a></li><li><a href=#monotonic-epsilon-greedy-policy-improvement>Monotonic $\epsilon$-greedy policy improvement</a></li><li><a href=#greedy-in-the-limit-of-infinite-explorationglie>Greedy in the Limit of Infinite Exploration(GLIE)</a></li><li><a href=#monte-carlo-online-control-on-policy-improvement>Monte Carlo Online Control /On Policy Improvement</a></li><li><a href=#sarsa>SARSA</a></li><li><a href=#q-learning>Q-Learning</a></li><li><a href=#double-q-learning>Double Q-learning</a></li></ol></li><li><a href=#lecture-5-value-function-approximation>Lecture 5 (Value Function Approximation)</a><ol><li><a href=#motivation>motivation</a></li><li><a href=#function-approximators>function approximators</a></li><li><a href=#linear-value-function-approximation>Linear Value Function Approximation</a><ol><li><a href=#convergence-guarantees>convergence guarantees</a></li></ol></li><li><a href=#monte-carlo-value-function-approximation>Monte Carlo Value Function Approximation</a><ol><li><a href=#batch-monte-carlo-value-function-approximation>Batch Monte Carlo Value Function Approximation</a></li></ol></li><li><a href=#td-value-function-approximation>TD value Function Approximation</a></li><li><a href=#control-using-value-function-approximation>control using Value Function Approximation</a></li></ol></li><li><a href=#lecture-6-deep-q-learning>Lecture 6 (Deep Q Learning)</a><ol><li><a href=#deep-q-networksdqns>Deep Q-Networks(DQNs)</a><ol><li><a href=#experience-replay>Experience Replay</a></li><li><a href=#fixed-q-targets>Fixed Q-Targets</a></li><li><a href=#double-dqn>Double DQN</a></li><li><a href=#advantage-function>Advantage Function</a></li></ol></li></ol></li><li><a href=#lecture-7-imitation-learning>Lecture 7 (Imitation Learning)</a><ol><li><a href=#behavioral-cloning>Behavioral Cloning</a></li><li><a href=#inverse-reinforcement-learning>Inverse reinforcement learning</a><ol><li><a href=#linear-feature-reward-inverse-rl>Linear Feature Reward Inverse RL</a></li></ol></li><li><a href=#apprenticeship-learning>Apprenticeship Learning</a></li></ol></li><li><a href=#lecture-8-10-policy-gradient>Lecture 8-10 (Policy Gradient)</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/star/ style=background-color:#2e317c;color:>一天星</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/stanford-reinforcement-learningnotes/>【Stanford Reinforcement Learning】Notes</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jun 06, 2023</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>13 minute read</time></div></footer></div></header><section class=article-content><h2 id=lecture-1-intro>Lecture 1 (intro)</h2><h3 id=element>Element</h3><ul><li>environment</li><li>agent</li><li>action</li><li>observation and reward</li></ul><h3 id=history>history</h3><p>History $h_t=(a_1,o_1,r_1,&mldr;,a_t,o_t,r_t)$</p><p>agent choose action based on history</p><h3 id=state>state</h3><p>state is information assumed to determine what happens next</p><p>function of history $s_t=f(h_t)$</p><p>The true state of the world used to determine how world generates next observation and reward are often hidden or unknown to agent</p><p>assume state used by the agent is sufficient statistic of history</p><p>In practice often assume most recent observation is sufficient statistic of history: $s_t=o_t$</p><p>state representation has big implications for:</p><ul><li>computational complexity</li><li>data required</li><li>resulting performance</li></ul><h3 id=markov>Markov</h3><p>state $s_t$ is Markov if and only if:
$$
p(s_{t+1}|s_{t},a_{t})=p(s_{t+1}|h_t,a_{t})
$$
future is independent of past given present</p><p>setting state as history always Markov: $s_t=h_t$</p><h3 id=rl-algorithm-component>RL Algorithm component</h3><p>often include one or more of</p><ul><li><p>Model</p><p>representation of how the world changes in response to agent&rsquo;s action</p></li><li><p>Policy</p><p>function mapping agent&rsquo;s states to action</p></li><li><p>Value function</p><p>future rewards from being in a state and/or action when following a particular policy</p></li></ul><h3 id=model>Model</h3><p>mathematical models of dynamics and reward, including:</p><p><strong>Transition/dynamics model</strong> predicts next agent state
$$
p(s_{t+1}=s^{&rsquo;}|s_t=s,a_t=a)
$$
<strong>Reward model</strong> predicts <strong>immediate</strong> reward
$$
r(s_t=s,a_t=a)=E[r_t|s_t=s,a_t=a]
$$</p><h3 id=policy>Policy</h3><p>policy $\pi:S\to A$, mapping from states to actions, determines how the agent chooses actions</p><ul><li>deterministic: $\pi(s)=a$</li><li>stochastic: $\pi(a|s)=Pr(a_t=a|s_t=s)$</li></ul><h3 id=value>Value</h3><p>value function $V^\pi$ expected discounted sum of future rewards under a particular policy $\pi$
$$
V^\pi(s_t=s)=E_\pi[r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+&mldr;|s_t=s]
$$
discount factor $\gamma$ weighs immediate vs future rewards</p><p>can be used to quantify goodness/badness of states and actions and decide how to act by comparing policies</p><h3 id=types-of-rl-agents>Types of RL agents</h3><ul><li>Model-based<ul><li>Explicit: Model</li><li>May or may not have policy and/ or value function</li></ul></li><li>Model-free<ul><li>Explicit: Value function and /or policy function</li><li>No model</li></ul></li></ul><p><img src=https://i.ibb.co/SK1SPXp/image-20220928112645620.png loading=lazy alt=image-20220928112645620></p><h2 id=lecture-2-mdp>Lecture 2 (MDP)</h2><h3 id=markov-process>Markov Process</h3><ul><li><p>$S$ is a (finite) set of states ($s\in S$)</p></li><li><p>$P$ is dynamics/transition model that specifies $p(s_{t+1}=s{&rsquo;}|s_t=s)$</p></li><li><p>no reward, no actions</p></li><li><p>if finite number ($N$) of states, can express $P$ as a matrix
$$
P=\begin{pmatrix}
P(s_1|s_1) & P(s_2|s_1) & &mldr; &amp;P(s_N|s_1)\
P(s_1|s_2) & P(s_2|s_2) & &mldr; &amp;P(s_N|s_2)\
&mldr; & &mldr; & &mldr; & &mldr;\
P(s_1|s_N) & P(s_2|s_N) & &mldr; &amp;P(s_N|s_N)\
\end{pmatrix}
$$</p></li></ul><p>A Markov chain is the discrete-time version of a Markov process.</p><h3 id=markov-reward-processmrp>Markov Reward Process(MRP)</h3><ul><li>$S$ is a (finite) set of states ($s\in S$)</li><li>$P$ is dynamics/transition model that specifies $P(s_{t+1}=s{&rsquo;}|s_t=s)$</li><li>$R$ is a reward function $R(s_t=s)=E[r_t|s_t=s]$</li><li>Discount factor $\gamma\in [0,1]$</li><li>no actions</li><li>if finite number ($N$) of states, can express $R$ as a vector</li></ul><h4 id=horizon>horizon</h4><ul><li>number of time steps in each episode</li><li>can be infinite</li><li>otherwise called finite Markov reward process</li></ul><h4 id=return-g_t-for-a-mrp>Return $G_t$ (for a MRP)</h4><p>discounted sum of rewards from time step $t$ to horizon
$$
G_t=r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+&mldr;
$$</p><h4 id=state-value-function-vs-for-a-mrp>State Value Function, $V(s)$ (for a MRP)</h4><ul><li>expected return from starting in state $s$</li></ul><p>$$
\begin{split}
V(s) & = E[G_t|s_t=s] \
& = E[r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+&mldr;|s_t=s]
\end{split}
$$</p><ul><li><p>MRP value function satisfies
$$
V(s)=R(s)+\gamma\sum_{s^{&rsquo;}\in S}P(s^{&rsquo;}|s)V(s^{&rsquo;})
$$
where $R(s)$ is immediate reward and the later part is the discounted sum of future rewards</p><p>then:
$$
V=R+\gamma PV
$$
namely:
$$
V=(I-\gamma P)^{-1}R
$$</p></li></ul><h3 id=markov-decision-process-mdp>Markov Decision Process (MDP)</h3><ul><li>$S$ is a (finite) set of states ($s\in S$)</li><li>$A$ is a (finite) set of actions $a\in A$</li><li>$P$ is dynamics/transition model that specifies $P(s_{t+1}=s{&rsquo;}|s_t=s，a_t=a)$</li><li>$R$ is a reward function $R(s_t=s,a_t=a)=E[r_t|s_t=s,a_t=a]$</li><li>Discount factor $\gamma\in [0,1]$</li></ul><p>MDP is a tuple: $(S,A,P,R,\gamma)$</p><h3 id=policy-1>Policy</h3><ul><li><p>policy specifies what action to take in each state</p><p>can be deterministic or stochastic</p></li><li><p>for generality, consider as a conditional ditribution</p><p>given a state, specifies a distribution over actions</p></li><li><p>Policy: $\pi(a|s)=P(a_t=a|s_t=s)$</p></li></ul><p>MDP+$\pi(a|s)$=Markov Reward Process</p><p>precisely, it is the MRP $(S,R^\pi,P^\pi,\gamma)$, where
$$
R^\pi(s)=\sum_{a\in A}\pi(a|s)R(s,a)
\newline
P^\pi(s^{&rsquo;}|s)=\sum_{a\in A}\pi(a|s)P(s^{&rsquo;}|s,a)
$$
implies we can use same techniques to evaluate the value of a policy for a MDP as we could to compute the value of a MRP, by defining a MRP with $R^{\pi}$ and $P^{\pi}$</p><h3 id=control>control</h3><p>compute the optimal policy
$$
\pi^*(s)=arg,max_{\pi}V^{\pi}(s)
$$
there exists an unique optimal value function</p><p>but the optimal policy is not unique</p><p>optimal policy for a MDP in an infinite horizon problem (agent a acts forever) is:</p><ul><li>deterministic</li><li>stationary (does not depend on time step)</li><li>not necessarily unique</li></ul><h4 id=policy-iteration>Policy Iteration</h4><p>computes optimal value and policy</p><ul><li>set $i= 0$</li><li>initialize $\pi_0(s)$ randomly for all states $s$</li><li>while $i==0$ or $||\pi_i-\pi_{i-1}||_1>0$ (L1-norm, measures if the policy changed for any state):<ul><li>$V^{\pi_i}\leftarrow$ MDP V function policy evaluation of $\pi_i$</li><li>$\pi_{i+1}\leftarrow $ Policy improvement</li><li>$i=i+1$</li></ul></li></ul><h5 id=state-action-value-q>State-Action Value Q</h5><p>State-action value of a policy
$$
Q^{\pi}(s,a)=R(s,a)+\gamma\sum_{s^{&rsquo;}\in S}P(s^{&rsquo;}|s,a)V^{\pi}(s^{&rsquo;})
$$
take action a, then follow the policy $\pi$ later on</p><h5 id=policy-improvement>policy improvement</h5><ul><li><p>compute state-action value of a policy $\pi_i$</p><ul><li>for $s$ in $S$ and $a$ in $A$:
$$
Q^{\pi_i}(s,a)=R(s,a)+\gamma\sum_{s^{&rsquo;}\in S}P(s^{&rsquo;}|s,a)V^{\pi_i}(s^{&rsquo;})
$$</li></ul></li><li><p>conpute new policy $\pi_{i+1}$, for all $s\in S$
$$
\pi_{i+1}(s)=arg,max_a,Q^{\pi_i}(s,a),\forall s\in S
$$</p></li></ul><h5 id=definition>definition</h5><p>$$
V^{\pi_1}\ge V^{\pi_2}:V^{\pi_1}(s)\ge V^{\pi_2}(s),\forall s\in S
$$</p><h4 id=value-iteration>Value Iteration</h4><p>miantain optimal value of starting in a state $s$ if have a finite number of steps $k$ left in the episode</p><p>iterate to consider longer and longer episodes</p><ul><li><p>set $k=1$</p></li><li><p>initialize $V_0(s)=0$ all states $s$</p></li><li><p>loop until finite horizon or convergence:</p><ul><li><p>for each state $s$
$$
V_{k+1}(s)=max_aR(s,a)+\gamma\sum_{s^{&rsquo;}\in S}P^{\pi}(s^{&rsquo;}|s)V^{\pi}(s^{&rsquo;})
$$</p></li><li><p>view as bellman backup on value function
$$
V_{k+1}=BV_k
\newline
\pi_{k+1}(s)=arg,max_aR(s,a)+\gamma\sum_{s^{&rsquo;}\in S}P^{\pi}(s^{&rsquo;}|s)V^{\pi}(s^{&rsquo;})
$$</p></li></ul></li></ul><h5 id=bellman>Bellman</h5><p>value function of a policy must satisfy the Bellman equation
$$
V^{\pi}(s)=R^{\pi}(s)+\gamma\sum_{s^{&rsquo;}\in S}P^{\pi}(s^{&rsquo;}|s)V^{\pi}(s^{&rsquo;})
$$
bellman backup operator</p><ul><li>applied to a value function</li><li>returns a new value function</li><li>improves the value if possible</li></ul><p>$$
BV(s)={max}<em>aR(s,a)+\gamma\sum</em>{s^{&rsquo;}\in S}P^{\pi}(s^{&rsquo;}|s)V^{\pi}(s^{&rsquo;})
$$</p><ul><li>$BV$ yields a value function over all states $s$</li></ul><h4 id=policy-iteration-as-bellman-operations>Policy iteration as Bellman Operations</h4><p>Bellman backup operator $B^{\pi}$ for a particular policy is defined as
$$
B^{\pi}V(s)=R^{\pi}(s)+\gamma\sum_{s^{&rsquo;}\in S}P^{\pi}(s^{&rsquo;}|s)V^{\pi}(s^{&rsquo;})
$$
policy evaluation amounts to computing the fixed point of $B^{\pi}$</p><p>to do policy evaluation. repeatedly apply operator until $V$ stops chaning
$$
V^{\pi}=B^{\pi}&mldr;B^{\pi}V
$$</p><h2 id=lecture-3-model-free-evaluation>Lecture 3 (model-free evaluation)</h2><h3 id=dynamic-programming-for-policy-evaluation>Dynamic programming for policy evaluation</h3><p>given dynamics model $p$ and reward model $r$, namely the model is known</p><ul><li><p>initialize $V_0(s)=0$ all states $s$</p></li><li><p>for $k=1$ until convergence</p><ul><li><p>for all $s$ in $S$</p><p>$V_{k}^{\pi}(s)=r(s,\pi(s))+\gamma\sum_{s^{&rsquo;}\in S}P(s^{&rsquo;}|s,\pi(s))V^{\pi}_{k-1}(s^{&rsquo;})$</p></li></ul></li></ul><p>$V_{k}^{\pi}(s)$ is exact value of $k$-horizon value of state $s$ under policy $\pi$, and it&rsquo;s an estimate of infinite horizon value of state $s$ under policy $\pi$
$$
V_{k}^{\pi}(s)=E_{\pi}[G_t|s_t=s]\approx E_{\pi}[r_t+\gamma V_{k-1})|s_t=s]
$$</p><h3 id=monte-carlo-policy-evaluation>Monte Carlo policy evaluation</h3><p>does not require MDP dynamics/rewards so are used when model is unkown
$$
V^{\pi}(s)=E_{T\sim \pi}[G_t|s_t=s]
$$</p><ul><li><p>expectation of trajectories $T$ generated by following $\pi$</p></li><li><p>the idea is Value = mean return</p></li><li><p>does not assume state is Markov</p></li><li><p>can only be applied to episodic MDPs</p><ul><li>averaging over returns from a complete episode</li><li>requires each episode to terminate</li></ul></li><li><p>aim: estimate $V^\pi(s)$ given episodes generated under policy $\pi$</p><p>$s_1,a_1,r_1,s_2,a_2,r_2,&mldr;$ where the actions are sampled from $\pi$</p></li></ul><p>steps:</p><ul><li>initialize $N(s)=0,G(s)=0\forall s\in S$</li><li>loop<ul><li>sample episode $i=s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},&mldr;,s_{i,T_i},a_{i,T_i},r_{i,T_i}$</li><li>define $G_{i,t}=r_{i,t}+\gamma r_{i,t+1}+\gamma^2r_{i,t+2}+&mldr;\gamma^{T_i-1}r_{i,T_i}$ as return from time step $t$ onwards in $ith$ episode</li><li>for each state $s$ visited in episode $i$<ul><li>for first time $t$ that state $s$ is visited in episode $i$<ul><li>increment counter of total first visits: $N(s)=N(s)+1$</li><li>increment total return $G(s)=G(s)+G_{i,t}$</li><li>update estimate $V^{\pi}(s)=G(s)/N(s)$</li></ul></li></ul></li></ul></li></ul><h4 id=every-visit-mc>Every-Visit MC</h4><p>instead of for first time $t$ that state $s$ is visited in episode $i$, update $N(s),G(s)$ and $V^{\pi}(s)$ for every time $t$ that state $s$ is visited in episode $i$</p><p>this estimator is biased but is consistent and often has better MSE</p><h4 id=incremental-mc>Incremental MC</h4><ul><li><p>sample episode $i=s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},&mldr;,s_{i,T_i},a_{i,T_i},r_{i,T_i}$</p></li><li><p>define $G_{i,t}=r_{i,t}+\gamma r_{i,t+1}+\gamma^2r_{i,t+2}+&mldr;\gamma^{T_i-1}r_{i,T_i}$ as return from time step $t$ onwards in $ith$ episode</p></li><li><p>for state $s$ visited at time step $t$ in episode $i$</p><ul><li><p>increment counter of total visits: $N(s)=N(s)+1$</p></li><li><p>update estimate
$$
V^{\pi}(s)=V^{\pi}(s)\frac{N(s)-1}{N(s)}+\frac{G_{i,t}}{N(s)}=V^{\pi}(s)+\frac{1}{N(s)}(G_{i,t}-V^{\pi}(s))
$$</p></li></ul></li></ul><h5 id=running-mean>Running Mean</h5><p>estimation is updated as
$$
V^{\pi}(s)=V^{\pi}(s)+\alpha(G_{i,t}-V^{\pi}(s))
$$</p><ul><li>$\alpha=\frac{1}{N(s)}$: identical to every visit MC</li><li>$\alpha\gt\frac{1}{N(s)}$: forget older data, helpful for non-stationary domains</li></ul><h3 id=temporal-differencetd>Temporal Difference(TD)</h3><ul><li><p>combination of Monte Carlo & dynamic programming methods</p></li><li><p>Model-free</p></li><li><p>Bootstraps and samples</p></li><li><p>can be used in episodic or infinite-horizon non-episodic settings</p></li><li><p>immediately updates estimate of $V$ after each $(s,a,r,s^{&rsquo;})$ tuple</p></li><li><p>aim: estimate $v^{\pi}(s)$ given episodes generated under policy $\pi$</p></li><li><p>have an estimate of $V^{\pi}$, use to estimate expected return
$$
V^{\pi}(s)=V^{\pi}(s)+\alpha([r_t+\gamma V^{\pi}(s_{t+1})]-V^{\pi}(s))
$$</p></li></ul><p>TD learning</p><ul><li>Initialize $V^{\pi}(s)=0\forall s\in S$</li><li>loop<ul><li>sample tuple $(s_t,a_t,r_t,s_{t+1})$</li><li>$V^{\pi}(s_t)=V^{\pi}(s_t)+\alpha([r_t+\gamma V^{\pi}(s_{t+1})]-V^{\pi}(s_t))$</li></ul></li></ul><h2 id=lecture-4-model-free-control>Lecture 4 (model-free control)</h2><ul><li><p>on-policy learning</p><ul><li>direct experience</li><li>learn to estimate and evaluate a policy from experience obtained from following that policy</li></ul></li><li><p>off-policy learning</p><ul><li>learn to estimate and evaluate a policy using experience gathered from following a different policy</li></ul></li></ul><h3 id=mc-for-on-policy-q-evaluation>MC for On Policy Q Evaluation</h3><ul><li><p>initialize $N(s,a)=0,G(s,a)=0,Q^{\pi}(s,a)=0,\forall s\in S,\forall a\in A$</p></li><li><p>loop</p><ul><li>using policy $\pi$ sample episode $i=s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},&mldr;,s_{i,T_i},a_{i,T_i},r_{i,T_i}$</li><li>$G_{i,t}=r_{i,t}+\gamma r_{i,t+1}+\gamma^2r_{i,t+2}+&mldr;\gamma^{T_i-1}r_{i,T_i}$</li><li>for each state,action $(s,a)$ visited in episode $i$<ul><li>for first or every time $t$ that state $(s,a)$ is visited in episode $i$<ul><li>$N(s,a)=N(s,a)+1,G(s,a)=G(s,a)+G_{i,t}$</li><li>update estimate $Q^{\pi}(s,a)=G(s,a)/N(s,a)$</li></ul></li></ul></li></ul></li><li><p>given an estimate $Q^{\pi_i}(s,a)\forall s,a$</p></li><li><p>update new policy
$$
\pi_{i+1}(s)=arg,max_a,Q^{\pi_i}(s,a)
$$</p></li></ul><h3 id=monotonic-epsilon-greedy-policy-improvement>Monotonic $\epsilon$-greedy policy improvement</h3><p>for any $\epsilon$-greedy policy $\pi_i$, the ϵ-greedy policy w.r.t.(with respect to) $Q^{\pi_i}$, $\pi_{i+1}$ is a monotonic improvement $V^{\pi_{i+1}}\ge V^{\pi}$
$$
\begin{split}
Q^{\pi_i}(s,\pi_{i+1}(s)) & = \sum_{a\in A}\pi_{i+1}(a|s)Q^{\pi}(s,a) \
& = \frac{\epsilon}{|A|}\sum_{a\in A}Q^{\pi_i}(s,a)+(1-\epsilon)max_aQ^{\pi_i}(s,a)
\end{split}
$$</p><h3 id=greedy-in-the-limit-of-infinite-explorationglie>Greedy in the Limit of Infinite Exploration(GLIE)</h3><ul><li><p>all state-action pair are visited an infinite number of times
$$
{lim}_{i->\infty}N_i(s,a)\to\infty
$$</p></li><li><p>behavior policy converges to greedy policy</p></li><li><p>a simple GLIE strategy is $\epsilon$-greedy where $\epsilon$ is reduced to 0 with the following rate
$$
\epsilon_i=\frac{1}{i}
$$</p></li></ul><h3 id=monte-carlo-online-control-on-policy-improvement>Monte Carlo Online Control /On Policy Improvement</h3><ul><li>initialize $N(s,a)=0,G(s,a)=0,Q(s,a)=0,\forall s\in S,\forall a\in A,set,\epsilon=1,k=1$</li><li>$\pi_1= \epsilon-greedy(Q)$</li><li>loop<ul><li>sample $k-th$ episode $(s_{k,1},a_{k,1},r_{k,1},s_{k,2},a_{k,2},r_{k,2},&mldr;,s_{k,T},a_{k,T},r_{k,T})$</li><li>$G_{k,t}=r_{k,t}+\gamma r_{k,t+1}+\gamma^2r_{k,t+2}+&mldr;\gamma^{T-1}r_{k,T}$</li><li>for $t=1,&mldr;,T$ do<ul><li>if first(or every ) vistit to $(s,a)$ in episode $k$ then<ul><li>$N(s,a)=N(s,a)+1$</li><li>$Q(s_t,a_t)=Q(s_t,a_t)+\frac{1}{N(s,a)}(G_{k,t}-Q(s_t,a_t))$</li></ul></li></ul></li><li>$k=k+1,\epsilon=\frac{1}{k}$</li><li>$\pi_k= \epsilon-greedy(Q)$</li></ul></li></ul><h3 id=sarsa>SARSA</h3><ul><li>set initial $\epsilon$-greedy policy $\pi$ randomly, $t$=0, initial state $s_t=s_0$</li><li>take $a_t\sim\pi(s_t)$ //sample action from policy</li><li>observe $(r_t,s_{t+1})$</li><li>loop<ul><li>take action $a_{t+1}\sim\pi(s_{t+1})$</li><li>observe $(r_{t+1},s_{t+2})$</li><li>$Q(s_t,a_t)=Q(s_t,a_t)+\alpha(r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))$</li><li>$\pi(s_t)=arg,\max_aQ(s_t,a)$ w.prob $1-\epsilon$,else random</li><li>$t=t+1$</li></ul></li></ul><h3 id=q-learning>Q-Learning</h3><ul><li><p>initialize $Q(s,a),\forall s\in S,a\in A,t=0 $initial state $s_t=s_0$</p></li><li><p>set $\pi_b$ to be $\epsilon$-greedy w.r.t $Q$</p></li><li><p>loop</p><ul><li>take action $a_{t}\sim\pi_b(s_{t})$</li><li>observe $(r_{t},s_{t+1})$</li><li>$Q(s_t,a_t)=Q(s_t,a_t)+\alpha(r_t+\gamma max_aQ(s_{t+1},a)-Q(s_t,a_t))$</li><li>$\pi(s_t)=arg,\max_aQ(s_t,a)$ w.prob $1-\epsilon$,else random</li><li>$t=t+1$</li></ul></li></ul><h3 id=double-q-learning>Double Q-learning</h3><ul><li><p>initialize $Q_1(s,a)$ and $Q_2(s,a) ,\forall s\in S,a\in A,t=0 $ initial state $s_t=s_0$</p></li><li><p>loop</p><ul><li><p>select $a_t$ using $\epsilon$-greedy $\pi(s)=arg,max_aQ_1(s_t,a)+Q_2(s_t,a)$</p></li><li><p>observe $(r_{t},s_{t+1})$</p></li><li><p>if (with 0.5 probability then</p><p>$Q_1(s_t,a_t)=Q_1(s_t,a_t)+\alpha(r_t+\gamma max_aQ_1(s_{t+1},a)-Q_1(s_t,a_t))$</p></li><li><p>else</p><p>$Q_2(s_t,a_t)=Q_2(s_t,a_t)+\alpha(r_t+\gamma max_aQ_2(s_{t+1},a)-Q_2(s_t,a_t))$</p></li><li><p>$t=t+1$</p></li></ul></li></ul><h2 id=lecture-5-value-function-approximation>Lecture 5 (Value Function Approximation)</h2><p>represent a (state-action/state) value function with a parameterized function instead of a table
$$
\hat{V}(s;w)
\newline
\hat{Q}(s,a;w)
$$</p><h3 id=motivation>motivation</h3><ul><li>don&rsquo;t want to have to explicitly store or learn for every single state a<ul><li>dynamics or reward model</li><li>value</li><li>state-action value</li><li>policy</li></ul></li><li>want more compact representation that generalized acress state or states and actions</li></ul><h3 id=function-approximators>function approximators</h3><ul><li>linear combinations of features</li><li>neural networks</li><li>decision trees</li><li>nearest neighbors</li><li>fourier / wabelet bases</li></ul><h3 id=linear-value-function-approximation>Linear Value Function Approximation</h3><ul><li><p>represent a value function (or state-action value function) for a particular policy with a weighted linear combination of features
$$
\hat{V}(s;w)=\sum_{j=1}^nx_j(s)w_j=x(s)^Tw
$$</p></li><li><p>objective function is
$$
J(w)=E_{\pi}[(V^{\pi}(s)-\hat{V}(s;w))^2]
$$</p></li><li><p>recall weight update is
$$
\Delta w=-\frac{1}{2}\alpha \nabla_wJ(w)
$$</p></li></ul><h4 id=convergence-guarantees>convergence guarantees</h4><ul><li><p>the markov chain defined by a MDP with a paticular policy will eventually converge to a <strong>probability distribution over state</strong> $d(s)$</p></li><li><p>$d(s)$ is called the stationary distribution over state of $\pi$</p></li><li><p>$\sum_sd(s)=1$</p></li><li><p>$d(s)$ satisfies the following balance equation:
$$
d(s)=\sum_{s&rsquo;}\sum_a\pi(s&rsquo;|a)p(s&rsquo;|s,a)d(s&rsquo;)
$$</p></li><li><p>define the mean squared error of a linear value function approximation for a particular policy $\pi$ relative to the true value as
$$
MSVE(w)=\sum_{s\in S}d(s)(V^{\pi}(s)-\hat{V}^{\pi}(s;w))^2
$$</p></li></ul><h3 id=monte-carlo-value-function-approximation>Monte Carlo Value Function Approximation</h3><p>$$
J(w)=E_{\pi}[(G_t-\hat{V}(s;w))^2]
\newline
\Delta w=\alpha(G_t-x(s_t)^Tw)x(s_t)
$$</p><p>monte carlo policy evaluation with VFA converges to the weights $w_{MC}$ which has the minimum mean squared error possible:
$$
MSVE(w_{MC})=min_w\sum_{s\in S}d(s)(V^{\pi}(s)-\hat{V}^{\pi}(s;w))^2
$$</p><h4 id=batch-monte-carlo-value-function-approximation>Batch Monte Carlo Value Function Approximation</h4><p>let $G(s_i)$ be an unbiased sample of the true expected return $V^{\pi(s_i)}$
$$
arg,min_w\sum_{i=1}^N(G(s_i)-x(s_i)^Tw)^2
$$
take the derivative and set to 0
$$
w=(X^TX)^{-1}X^TG
$$
where $G$ is a vector of all N returns, and $X$ is a matrix of the features of each of the N states $x(s_)$</p><h3 id=td-value-function-approximation>TD value Function Approximation</h3><p>$$
J(w)=E_{\pi}[r+\gamma \hat{V}(s&rsquo;;w))^2]
\newline
\Delta w=\alpha(r+\gamma x(s&rsquo;)^Tw-x(s)^Tw)x(s)
$$</p><p>TD(0) policy evaluation with VFA converges to the weights $w_{TD}$ which is within a constant factor of the minimum mean squared error possible:
$$
MSVE(w_{TD})\le \frac{1}{1-\gamma}min_w\sum_{s\in S}d(s)(V^{\pi}(s)-\hat{V}^{\pi}(s;w))^2
$$</p><h3 id=control-using-value-function-approximation>control using Value Function Approximation</h3><ul><li>use value function approximation to represent state-ation values $\hat{Q}^{\pi}(s,a;w)\approx Q^{\pi}$</li></ul><p>$$
J(w)=E_{\pi}[(Q^{\pi}(s,a)-\hat{Q}(s,a;w))^2]
\newline
\Delta w=\alpha E[(Q^{\pi}(s,a)-\hat{Q}(s,a;w))\nabla_w\hat{Q}^{\pi}(s,a;w)]
$$</p><ul><li><p>use features to represent both the state and action
$$
x(s,a)=\begin{pmatrix}
x_1(s,a)\
x_2(s,a)\
&mldr;\
x_n(s,a)
\end{pmatrix}
$$</p></li><li><p>monte carlo
$$
\Delta w=\alpha (G_t-\hat{Q}(s,a;w))\nabla_w\hat{Q}^{\pi}(s,a;w)
$$</p></li><li><p>SARSA
$$
\Delta w=\alpha (r+\gamma\hat{Q}(s&rsquo;,a&rsquo;;w)-\hat{Q}(s,a;w))\nabla_w\hat{Q}^{\pi}(s,a;w)
$$</p></li><li><p>Q_learning
$$
\Delta w=\alpha (r+\gamma,max_{a&rsquo;}\hat{Q}(s&rsquo;,a&rsquo;;w)-\hat{Q}(s,a;w))\nabla_w\hat{Q}^{\pi}(s,a;w)
$$</p></li></ul><h2 id=lecture-6-deep-q-learning>Lecture 6 (Deep Q Learning)</h2><ul><li>Linear VFA often work well given the right set of features</li><li>But can require carefully hand designing that feature set</li><li>An alternative is to use a much richer function approximation class that is able to directly go from states without requiring an explicit specification of features</li></ul><h3 id=deep-q-networksdqns>Deep Q-Networks(DQNs)</h3><ul><li>represent state-action value function by Q-network with weights $w$
$$
\hat{Q}(s,a;w)\approx Q(s,a)
$$</li></ul><h4 id=experience-replay>Experience Replay</h4><ul><li>to help remove correlations, store dataset (called a replay buffer) $D$ from prior experience</li><li>to perform experience replay., repeat:<ul><li>$(s,a,r,s&rsquo;)\sim D$ :sample an experience tuple from the dataset</li><li>compute the target value for the sampled $s$：$r+\gamma,max_{a&rsquo;}\hat{Q}(s&rsquo;,a&rsquo;;w)$</li><li>use stochastic gradient descent to update the network weight</li></ul></li></ul><h5 id=prioritized-experience-replay>Prioritized Experience Replay</h5><ul><li><p>let $i$ be the index of the $i$-th tuple of experience $s_i,a_i,r_i,s_{i+1} $</p></li><li><p>sample tuples for update using priority function</p></li><li><p>priority of a tuple is proportional to DQN error
$$
p_i=|r+\gamma,max_{a&rsquo;}\hat{Q}(s&rsquo;,a&rsquo;;w)-\hat{Q}(s,a;w)|
$$</p></li><li><p>update $p_i$ every update</p></li><li><p>$p_i$ for new tuples is set to 0</p></li><li><p>one method:
$$
P(i)=\frac{p_i^{\alpha}}{\sum_kp_k^{\alpha n}}
$$</p></li></ul><h4 id=fixed-q-targets>Fixed Q-Targets</h4><ul><li>to help improve stability, fix the target weights used in the target calculation for multiple udpdates</li><li>use a different set of weights to compute target that is being updated</li><li>let parameters $w^-$ be the set of weights used in the target, and $``w$ be the weights that are being updated</li><li>slight change to computation of target value:<ul><li>sample $(s,a,r,s&rsquo;)\sim D$</li><li>compute $r+\gamma,max_{a&rsquo;}\hat{Q}(s&rsquo;,a&rsquo;;w^-)$</li><li>update $\Delta w=\alpha (r+\gamma,max_{a&rsquo;}\hat{Q}(s&rsquo;,a&rsquo;;w^-)-\hat{Q}(s,a;w))\nabla_w\hat{Q}(s,a;w)$</li></ul></li></ul><h4 id=double-dqn>Double DQN</h4><ul><li>current Q-network $w$ is used to select actions</li><li>older Q-network $w^-$ is used to evaluate actions</li></ul><h4 id=advantage-function>Advantage Function</h4><p>$$
A^{\pi}(s,a)=Q^{\pi}(s,a)-V^{\pi}(s)
$$</p><h2 id=lecture-7-imitation-learning>Lecture 7 (Imitation Learning)</h2><ul><li><p>Expert provides a set of demonstration trajectories: sequences of states and actions</p></li><li><p>Imitation learning is useful when it is easier for the expert to demonstrate the desired behavior rather than:</p><ul><li>specifying a reward that would generate such behavior</li><li>specifying the desired policy directly</li></ul></li><li><p>Input</p><ul><li>state space, action space</li><li>transition model $P(s&rsquo;|s,a)$</li><li>No reward function $R$</li><li>set of one or more teacher&rsquo;s demonstrations $(s_0,a_0,s_1,&mldr;)$ (actions drawn from teacher&rsquo;s policy $\pi^*$ )</li></ul></li><li><p>type</p><ul><li><p>Behavioral cloning</p><p>directly learn the teacher&rsquo;s policy</p></li><li><p>Inverse RL</p><p>recover $R$</p></li><li><p>Apprenticeship learning via Inverse RL</p><p>use $R$ to generate a good policy</p></li></ul></li></ul><h3 id=behavioral-cloning>Behavioral Cloning</h3><ul><li>formulate problem as a standard machine learning problem:<ul><li>fix a policy class (e.g. neural network, decision tree, etc.)</li><li>estimate a policy from training examples $(s_0,a_0),s(_1,a_1)&mldr;$</li></ul></li></ul><p>not a good choice</p><h3 id=inverse-reinforcement-learning>Inverse reinforcement learning</h3><ul><li>Goal: given input, infer the reward function R</li></ul><h4 id=linear-feature-reward-inverse-rl>Linear Feature Reward Inverse RL</h4><ul><li><p>consider reward is linear over features</p><p>$$R(s)=w^Tx(s)$$</p><p>where $w\in R^n,x:s\to R^n$</p></li><li><p>Goal: identify weight vector $w$ given a set of demonstrations</p></li><li><p>The resulting value function for a policy $\pi$ can be expressed as
$$
\begin{equation}
\begin{split}
V^{\pi}&=E[\sum_{t=0}^{\infty}\gamma^tR(s_t)|\pi]\
&=E[\sum_{t=0}^{\infty}\gamma^tw^Tx(s_t)|\pi]\
&=w^TE[\sum_{t=0}^{\infty}\gamma^tx(s_t)|\pi]\
&=w^T\mu(\pi)
\end{split}
\end{equation}
$$
where $\mu(\pi)(s)$ is defined as the discounted weighted frequency of state features under policy $\pi$</p></li></ul><h3 id=apprenticeship-learning>Apprenticeship Learning</h3><p>step</p><ul><li><p>assumption:$R(s)=w^Tx(s)$</p></li><li><p>Initialize policy $\pi_0$</p></li><li><p>loop</p><ul><li><p>find a reward function such that the teacher maximally outperforms all previous controllers:
$$
arg,max_wmax_{\gamma}s.t.w^T\mu(\pi^*)\ge w^T\mu(\pi)+\gamma,,\forall\pi\in{\pi_0,\pi_1,&mldr;,\pi_{i-1}}
$$</p></li><li><p>s.t. $||w||_2\le 1$</p></li><li><p>find optimal control policy $\pi_i$ for the current $w$</p></li><li><p>exit if $\gamma\le\epsilon/2$</p></li></ul></li></ul><p>summary</p><ul><li>if expert policy is suboptimal then the resulting policy is a mixture of somewhat arbitrary policies which have expert in the convex hull</li><li>in practice: pick the best one of this set and pick the corresponding reward function</li></ul><h2 id=lecture-8-10-policy-gradient>Lecture 8-10 (Policy Gradient)</h2><p>Directly parametrize the policy
$$
\pi_{\theta}(s,a)=P[a|s;\theta]
$$
Goal is to find a policy $\pi$ with the highest value function $V^{\pi}$</p><ul><li>Advantages<ul><li>better convergence properties</li><li>effective in high-dimensional or continuous action spaces</li><li>can learn stochastic policies</li></ul></li><li>Disadvantages<ul><li>Typically converge to a local rather than global optimum</li><li>Evaluating a policy is typically inefficient and</li></ul></li></ul></section><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Jun 06, 2023 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/software-architecturenotes/><div class=article-details><h2 class=article-title>【Software Architecture】Notes</h2></div></a></article><article><a href=/p/computer-architecturenotes/><div class=article-details><h2 class=article-title>【Computer Architecture】Notes</h2></div></a></article><article><a href=/p/computer-networknotes/><div class=article-details><h2 class=article-title>【Computer Network】Notes</h2></div></a></article><article><a href=/p/computer-visionnotes/><div class=article-details><h2 class=article-title>【Computer Vision】Notes</h2></div></a></article><article><a href=/p/stanford-compilersnotes/><div class=article-details><h2 class=article-title>【Stanford Compilers】Notes</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 飞鸿踏雪泥</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>