<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="History 1959 Hubel& Wiesel\n1963 Roberts\n1970s David Marr\n1979 Gen.Cylinders\n1986 Canny\n1997 Norm.Cuts\n199 SIFT\n2001 V&amp;J\none of the first successful applications of machine learning to vision\n2001 PASCAL\n2009 ImageNet\nOlympics of computer vision\n2012 AlexNet\ndeep learning\nAnother viewPoint 1958 perceptron\n1969 Minsky & Papert\nshowed that perceptrons could not learn the XOR function caused a lot of sidillusionment in the field\n1980 Neocognition: Fukushima\n"><title>【Computer Vision】Notes</title>
<link rel=canonical href=https://dyhes.github.io/p/computer-visionnotes/><link rel=stylesheet href=/scss/style.min.6aa4d43a5cae1c51ef34b3f851ae7421f4b2f2d13827e2d975acbeb4f13c8710.css><meta property='og:title' content="【Computer Vision】Notes"><meta property='og:description' content="History 1959 Hubel& Wiesel\n1963 Roberts\n1970s David Marr\n1979 Gen.Cylinders\n1986 Canny\n1997 Norm.Cuts\n199 SIFT\n2001 V&amp;J\none of the first successful applications of machine learning to vision\n2001 PASCAL\n2009 ImageNet\nOlympics of computer vision\n2012 AlexNet\ndeep learning\nAnother viewPoint 1958 perceptron\n1969 Minsky & Papert\nshowed that perceptrons could not learn the XOR function caused a lot of sidillusionment in the field\n1980 Neocognition: Fukushima\n"><meta property='og:url' content='https://dyhes.github.io/p/computer-visionnotes/'><meta property='og:site_name' content='飞鸿踏雪泥'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2023-06-06T00:00:00+00:00'><meta property='article:modified_time' content='2023-06-06T00:00:00+00:00'><meta name=twitter:title content="【Computer Vision】Notes"><meta name=twitter:description content="History 1959 Hubel& Wiesel\n1963 Roberts\n1970s David Marr\n1979 Gen.Cylinders\n1986 Canny\n1997 Norm.Cuts\n199 SIFT\n2001 V&amp;J\none of the first successful applications of machine learning to vision\n2001 PASCAL\n2009 ImageNet\nOlympics of computer vision\n2012 AlexNet\ndeep learning\nAnother viewPoint 1958 perceptron\n1969 Minsky & Papert\nshowed that perceptrons could not learn the XOR function caused a lot of sidillusionment in the field\n1980 Neocognition: Fukushima\n"><link rel="shortcut icon" href=/github.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu17834253352308399148.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>飞鸿踏雪泥</a></h1><h2 class=site-description>没有记录，就没有发生</h2></div></header><ol class=menu-social><li><a href=https://github.com/dyhes target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:dyheslin@gmail.com target=_blank title=Gmail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-gmail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16 20h3a1 1 0 001-1V5a1 1 0 00-1-1h-3v16z"/><path d="M5 20h3V4H5A1 1 0 004 5v14a1 1 0 001 1z"/><path d="M16 4l-4 4-4-4"/><path d="M4 6.5l8 7.5 8-7.5"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/categories/><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#history>History</a><ol><li><a href=#another-viewpoint>Another viewPoint</a></li><li><a href=#reason>Reason</a></li></ol></li><li><a href=#dataset>DataSet</a><ol><li><a href=#type>type</a></li><li><a href=#cross-validation>cross-validation</a></li></ol></li><li><a href=#k-nearest-neighbor>K-Nearest Neighbor</a></li><li><a href=#linear-classifier>Linear Classifier</a><ol><li><a href=#three-viewpoints>Three Viewpoints</a></li></ol></li><li><a href=#loss-function>Loss Function</a><ol><li><a href=#svm>SVM</a></li><li><a href=#cross-entropymultinomial-logistic-regression>Cross-Entropy(Multinomial Logistic Regression)</a></li><li><a href=#regularization>Regularization</a></li><li><a href=#purpose>Purpose</a></li></ol></li><li><a href=#optimization>Optimization</a><ol><li><a href=#sgd>SGD</a></li><li><a href=#sgdmomentum>SGD+Momentum</a></li><li><a href=#nesterov-momentum>Nesterov Momentum</a></li><li><a href=#adagrad>AdaGrad</a></li><li><a href=#rmsprop>RMSProp</a><ol><li><a href=#adam>Adam</a></li></ol></li></ol></li><li><a href=#neural-network>Neural Network</a><ol><li><a href=#activation-functions>Activation Functions</a></li></ol></li><li><a href=#back-propagation>Back propagation</a></li><li><a href=#convolutional-networks>Convolutional Networks</a><ol><li><a href=#convolutional-layer>Convolutional Layer</a><ol><li><a href=#evaporative>evaporative</a></li><li><a href=#receptive-fields>Receptive Fields</a></li><li><a href=#summary>summary</a></li></ol></li><li><a href=#pooling-layer>Pooling Layer</a></li><li><a href=#batch-normalization>Batch Normalization</a></li><li><a href=#layer-normalization>Layer Normalization</a><ol><li><a href=#instance-normalization>Instance Normalization</a></li></ol></li></ol></li><li><a href=#residual-networks>Residual Networks</a></li><li><a href=#programming-gpus>Programming GPUs</a></li><li><a href=#pytorch>Pytorch</a><ol><li><a href=#three-levels-of-abstraction>Three levels of abstraction</a></li></ol></li><li><a href=#training-neural-networks>Training Neural Networks</a><ol><li><a href=#activation-function>Activation Function</a><ol><li><a href=#sigmoid>Sigmoid</a></li><li><a href=#tanh>tanh</a></li><li><a href=#relu>ReLU</a></li><li><a href=#leaky-relu>Leaky ReLU</a></li><li><a href=#parametric-rectifierprelu>Parametric Rectifier(PReLU)</a></li><li><a href=#exponential-linear-unitelu>Exponential Linear Unit(ELU)</a></li><li><a href=#scaled-exponential-linear-unitselu>Scaled Exponential Linear Unit(SELU)</a></li><li><a href=#summary-1>summary</a></li></ol></li><li><a href=#data-preprocessing>Data Preprocessing</a></li><li><a href=#weight-initialization>Weight Initialization</a></li><li><a href=#regularization-1>Regularization</a></li><li><a href=#learning-rate-schedule>Learning rate schedule</a><ol><li><a href=#decay>Decay</a></li></ol></li><li><a href=#early-stopping>early stopping</a></li><li><a href=#choosing-hyperparameters>choosing hyperparameters</a></li><li><a href=#model-ensembles>Model Ensembles</a></li><li><a href=#transfer-learning>Transfer Learning</a></li></ol></li><li><a href=#recurrent-networks>Recurrent Networks</a><ol><li><a href=#vanilla>Vanilla</a></li><li><a href=#truncated-backpropagation-through-time>Truncated Backpropagation Through Time</a></li><li><a href=#lstm>LSTM</a></li><li><a href=#multilayer>Multilayer</a></li></ol></li><li><a href=#attention>Attention</a><ol><li><a href=#attention-layer>Attention Layer</a><ol><li><a href=#multiple-query>multiple query</a></li><li><a href=#seperate-key-and-value>seperate key and value</a></li></ol></li><li><a href=#self-attention-layer>Self-Attention Layer</a><ol><li><a href=#masked-self-attention-layer>masked self-attention layer</a></li></ol></li><li><a href=#multihead-self-attention-layer>Multihead self-attention layer</a></li><li><a href=#cnn-with-self-attention>CNN with self-attention</a></li><li><a href=#processing-sequences>processing sequences</a><ol><li><a href=#transformer-block>transformer block</a></li><li><a href=#transformer>Transformer</a></li></ol></li></ol></li><li><a href=#object-detection>Object Detection</a><ol><li><a href=#intersection-over-union-iou>Intersection over Union (IoU)</a></li><li><a href=#overlapping-boxes-non-max-suppression-nms>Overlapping Boxes: Non-Max Suppression (NMS)</a></li><li><a href=#evaluating-mean-average-precision-map>Evaluating: Mean Average Precision (mAP)</a></li><li><a href=#slow-r-cnn>&ldquo;slow&rdquo; R-CNN</a></li><li><a href=#fast-r-cnn>Fast R-CNN</a></li><li><a href=#faster-r-cnn--learnable-region-proposals>Faster R-CNN: learnable Region Proposals</a></li><li><a href=#semantic-segmentation>Semantic Segmentation</a></li><li><a href=#upsampling>Upsampling</a></li><li><a href=#instance-segmentation>Instance Segmentation</a></li><li><a href=#panoptic-segmentation>Panoptic Segmentation</a></li></ol></li><li><a href=#3d-vision>3D Vision</a><ol><li><a href=#3d-shape-representations>3D Shape Representations</a></li><li><a href=#graph-convolution>Graph Convolution</a></li><li><a href=#metrics>Metrics</a></li></ol></li><li><a href=#generative-model>Generative Model</a><ol><li><a href=#taxonomy>Taxonomy</a></li><li><a href=#autoregressive>Autoregressive</a><ol><li><a href=#pixelrnn>PixelRNN</a></li><li><a href=#pixelcnn>PixelCNN</a></li><li><a href=#pros-and-cons>Pros and Cons</a></li></ol></li><li><a href=#variational-autoencoder>Variational Autoencoder</a><ol><li><a href=#autoencoders>Autoencoders</a></li><li><a href=#variational-autoencoder-1>Variational Autoencoder</a></li><li><a href=#generate-new-data>generate new data</a></li><li><a href=#pros-and-cons-1>Pros and Cons</a></li></ol></li><li><a href=#gan>GAN</a></li></ol></li><li><a href=#assignments>Assignments</a><ol><li><a href=#pytorch-1>pytorch</a><ol><li><a href=#tensor-indexing>tensor indexing</a></li><li><a href=#reshape>Reshape</a></li><li><a href=#computation>Computation</a></li><li><a href=#vectorization>vectorization</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/star/ style=background-color:#2e317c;color:>一天星</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/computer-visionnotes/>【Computer Vision】Notes</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jun 06, 2023</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>18 minute read</time></div></footer></div></header><section class=article-content><h2 id=history>History</h2><ul><li><p>1959 Hubel& Wiesel</p></li><li><p>1963 Roberts</p></li><li><p>1970s David Marr</p></li><li><p>1979 Gen.Cylinders</p></li><li><p>1986 Canny</p></li><li><p>1997 Norm.Cuts</p></li><li><p>199 SIFT</p></li><li><p>2001 V&amp;J</p><p>one of the first successful applications of machine learning to vision</p></li><li><p>2001 PASCAL</p></li><li><p>2009 ImageNet</p><p>Olympics of computer vision</p></li><li><p>2012 AlexNet</p><p>deep learning</p></li></ul><h3 id=another-viewpoint>Another viewPoint</h3><ul><li><p>1958 perceptron</p></li><li><p>1969 Minsky & Papert</p><p>showed that perceptrons could not learn the XOR function caused a lot of sidillusionment in the field</p></li><li><p>1980 Neocognition: Fukushima</p><p>Computational model the visual system, directly inspired by Hubel and Wiesel&rsquo;s hierarchy of complex and simple cells</p><p>interleaved simple cells(convolution) and complex cells(pooling)</p><p>no practical training algorithm</p></li><li><p>Backprop: Rumelhart, Hinton, and williams, 1986</p><p>introduced backpropagation for computing gradients in neural networks</p><p>successfully trained perceptrons with multiple layers</p></li><li><p>Convolutional Networks:LeCun et al, 1998</p></li><li><p>deep learning 2006</p><p>people tried to train neural networks that were deeper and deeper</p><p>not a mainstream research topic at this time</p></li></ul><h3 id=reason>Reason</h3><ul><li>Algorithms</li><li>Data</li><li>Computation</li></ul><h2 id=dataset>DataSet</h2><ul><li>CIFAR10</li><li>CIFAR100</li><li>ImageNet</li><li>Places365</li></ul><h3 id=type>type</h3><ul><li>train: train the model</li><li>validation :choose hypterparameter</li><li>test: check out performance on new data</li></ul><h3 id=cross-validation>cross-validation</h3><p>split data into folds,try each fold as validation and average the results</p><h2 id=k-nearest-neighbor>K-Nearest Neighbor</h2><ul><li>very slow at test time</li><li>distance metrics on pixels are not infomative</li></ul><h2 id=linear-classifier>Linear Classifier</h2><h3 id=three-viewpoints>Three Viewpoints</h3><ul><li><p>Algebraic Viewpoint</p><p>$f(x,W)=Wx$</p></li><li><p>Visual Viewpoint</p><p>One template per class</p></li><li><p>Geometric Viewpoint</p><p>Hyperplanes cutting up space</p></li></ul><h2 id=loss-function>Loss Function</h2><p>a loss function tells how good our current classifier is</p><p>Also called: object function, cost function</p><p>Negative loss function sometimes called reward function, profit function, utility function, fitness function, etc</p><p>Given a dataset
$$
{(x_i,y_i)}^N_{i=1}
$$
where $x_i$ is image and $y_i$ is (integer) label</p><p>Loss function for it may be
$$
L_i(f(x_i,W),y_i)
$$
Loss for the dataset is average of per-example losses:
$$
L=\frac{1}{N}\sum_iL_i(f(x_i,W),y_i)
$$</p><h3 id=svm>SVM</h3><p>Let $s=f(x_i,W)$ be scores</p><p>Then the SVM loss has the form:
$$
L_i=\sum_{j\neq y_i}max(0,s_j-s_{y_i}+margin)
$$</p><h3 id=cross-entropymultinomial-logistic-regression>Cross-Entropy(Multinomial Logistic Regression)</h3><p>interpret raw classifier scores as probabilities
$$
s=f(x_i;W)
$$
softmax
$$
P(Y=k \lvert X=x_i)=\frac{e^{s_k}}{\sum_je^{ s_j}}
$$
then
$$
L_i=-logP(Y=y_i|X=x_i)
$$</p><h3 id=regularization>Regularization</h3><p>prevent the model from doing too well on training data
$$
L(W)=L=\frac{1}{N}\sum_iL_i(f(x_i,W),y_i)+\lambda R(W)
$$
where $\lambda$ is the regularization strength</p><ul><li><p>L2 Regularization</p><p>$R(W)=\sum_k\sum_lW^2_{k,l}$</p></li><li><p>L1 Regularization</p><p>$R(W)=\sum_k\sum_l\lvert W_{k,l}\lvert$</p></li><li><p>Elastic net(L1+L2)</p><p>$R(W)=\sum_k\sum_l\beta W^2_{k,l}+\lvert W_{k,l}\lvert$</p></li></ul><h3 id=purpose>Purpose</h3><ul><li>express preferences in among models beyond &ldquo;minimize training error&rdquo;</li><li>avoid overfitting: prefer simple models that generalize better</li><li>improve optimization by adding curvature</li></ul><h2 id=optimization>Optimization</h2><p>$$
w^*=arg,min_wL（w）
$$</p><h3 id=sgd>SGD</h3><p>stochastic gradient descent
$$
x_{t+1}=x_{t}-\alpha \nabla f(x_t)
$$</p><h3 id=sgdmomentum>SGD+Momentum</h3><p>$$
v_{t+1}=\rho v_{t}-\alpha \nabla f(x_t)
\newline
x_{t+1}=x_t+ v_{t+1}
$$</p><p>add velocity</p><h3 id=nesterov-momentum>Nesterov Momentum</h3><p>look ahead
$$
v_{t+1}=\rho v_t-\alpha\nabla f(x_t+\rho v_{t})
\newline
x_{t+1}=x_t+v_{t+1}
$$
let $\tilde{x_t}=x_t+\rho v_t$
$$
v_{t+1}=\rho v_t-\alpha \nabla f(\tilde{x_t})
\newline
\tilde{x_{t+1}}=\tilde{x_t}+v_{t+1}+\rho(v_{t+1}-v_t)
$$</p><h3 id=adagrad>AdaGrad</h3><p>$$
S_{t+1}=S_{t}+\nabla f(x_t)^2
\newline
x_{t+1}=x_t-\alpha \frac{\nabla f(x_t)}{\sqrt{S_{t}}+1e-7}
$$</p><p>might stop before converge</p><h3 id=rmsprop>RMSProp</h3><p>&ldquo;Leak Adagrad&rdquo;
$$
S_{t+1}=\rho S_{t}+(1-\rho)\nabla f(x_t)^2
\newline
x_{t+1}=x_t-\alpha \frac{\nabla f(x_t)}{\sqrt{S_{t}}+1e-7}
$$</p><h4 id=adam>Adam</h4><p>RMSProp+Momentum
$$
v_{t+1}=\rho v_{t}+\nabla f(x_t)
\newline
s_{t+1}=\beta s_{t}+(1-\beta)\nabla f(x_t)^2
\newline
v^{&rsquo;}<em>{t+1}=\frac{v</em>{t+1}}{1-\rho ^t}
\newline
s^{&rsquo;}<em>{t+1}=\frac{s</em>{t+1}}{1-\beta ^t}
\newline
x_{t+1}=x_t-\alpha \frac{v^{&rsquo;}<em>{t+1}}{\sqrt{s^{&rsquo;}</em>{t+1}}+1e-7}
$$</p><h2 id=neural-network>Neural Network</h2><h3 id=activation-functions>Activation Functions</h3><ul><li><p>Sigmoid</p><p>$\sigma(x)=\frac{1}{1+e^{-x}}$</p></li><li><p>tanh</p><p>$tanh(x)$</p></li><li><p>ReLU</p><p>$max(0,x)$</p></li><li><p>Leaky ReLU</p><p>$max(0.1x,x)$</p></li><li><p>Maxout</p><p>$max(w_1^Tx+b1,w_2^Tx+b2)$</p></li><li><p>ELU</p><p>$\left{
\begin{array}{lc}
x & x \geqslant 0 \
\alpha(e^x-1)&amp;x&lt;0\
\end{array}
\right.$</p></li></ul><h2 id=back-propagation>Back propagation</h2><p>$$
\frac{\partial L}{\partial y}=\frac{\partial g}{\partial y}\frac{\partial L}{\partial g}
$$</p><p>where</p><p>$\frac{\partial y}{\partial f}$ is the downstream gradient</p><p>$\frac{\partial y}{\partial g}$ is the local gradient</p><p>$\frac{\partial g}{\partial f}$ is the upstream gradient</p><h2 id=convolutional-networks>Convolutional Networks</h2><h3 id=convolutional-layer>Convolutional Layer</h3><h4 id=evaporative>evaporative</h4><ul><li>Input W</li><li>Filter: K</li><li>Output: W-K+1</li></ul><p>add Padding</p><ul><li>Padding: P</li><li>Output: W-K+1+2P</li></ul><p>same padding: Input equals Output</p><h4 id=receptive-fields>Receptive Fields</h4><p>each successive convolution adds K-1 to the receptive field size</p><p>With L layers the receptive field size is 1+L*(K-1)</p><p>to expand receptive fields</p><ul><li>stride: S</li><li>output: (W-K+2P)/S+1</li></ul><h4 id=summary>summary</h4><ul><li><p>Input: $C_{in}\times H \times W$</p></li><li><p>Hyperparameters:</p><ul><li>Kernel size: $K_H \times K_W$</li><li>Number of filters: $C_{out}$</li><li>Padding: $P$</li><li>stride: $S$</li></ul></li><li><p>Weight matrix: $C_{out} \times C_{in} \times K_H \times K_W$</p><p>Bias Vector: $C_{out}$</p></li><li><p>Ouput: $C_{out} \times H^{&rsquo;}\times W^{&rsquo;}$</p><p>where</p><p>$H^{&rsquo;}=(H-L+2P)/S+1$</p><p>$W^{&rsquo;}=(W-L+2P)/S+1$</p></li></ul><p>common settings:</p><ul><li>$K_H = K_W$</li><li>same padding</li><li>$C_{out} ,C_{in} =32,64,128,256$</li></ul><h3 id=pooling-layer>Pooling Layer</h3><p>to introduces invariance to small spatial shifts</p><p>Hyperparameters:</p><ul><li>kernel size</li><li>stride</li><li>pooling function</li></ul><h3 id=batch-normalization>Batch Normalization</h3><p>normalize the outputs of a layer so they have zero mean and unit variance</p><p>It helps reduce &ldquo;internal covariate shift&rdquo;, improves optimization
$$
\hat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}
$$
Input: $N\times D$ when in fully connected layers. convolutional layers are the same</p><ul><li><p>$\mu_j=\frac{1}{N}\sum_{i=1}^Nx_{i,j}$</p><p>Per-channel mean, shape is $1\times D$</p><p>average of values seen during training while testing</p></li><li><p>$\sigma_j^2=\frac{1}{N}\sum_{i=1}^N(x_{i,j}-\mu_j)^2$</p><p>per-channel std, shape is $1\times D$</p><p>average of values seen during training while testing</p></li><li><p>$\hat{x}<em>{i,j}=\frac{x</em>{i,j}-\mu_j}{\sqrt{\sigma_j^2+\epsilon}}$</p><p>normalized $x$, shape is $N\times D$</p></li><li><p>$y_{i,j}=\gamma_j\hat{x}_{i,j}+\beta_j$</p><p>output $y$, shape is $N\times D$</p><p>$\gamma$ and $\beta$ is learnable scale and shift parameter of shape $1\times D$</p></li></ul><h3 id=layer-normalization>Layer Normalization</h3><p>for fully connected layers</p><p>avoid different behavior between training and testing</p><p>the $\mu$ and $\sigma$ become shape of $N\times 1$</p><h4 id=instance-normalization>Instance Normalization</h4><p>for convolutional layers</p><p>the $\mu$ and $\sigma$ become shape of $N\times C\times 1\times 1$ rather than shape of $1\times C\times 1\times 1$ in batch normalization</p><p>the $\gamma$ and $\beta$ remain as shape of $1\times C\times 1\times 1$</p><h2 id=residual-networks>Residual Networks</h2><p>deep networks hard to train</p><p>a residual network is a stack of many residual blocks</p><p><img src=https://i.ibb.co/xJSzxph/image-20220924193218252.png loading=lazy alt=image-20220924193218252></p><h2 id=programming-gpus>Programming GPUs</h2><ul><li><p>CUDA(NVIDIA only)</p></li><li><p>OpenCL</p><p>similar to CUDA, but runs on anything</p><p>usually slower on NVIDIA hardware</p></li></ul><h2 id=pytorch>Pytorch</h2><h3 id=three-levels-of-abstraction>Three levels of abstraction</h3><ul><li><p>Tensor</p><p>like a numpy array, but can run on GPU</p></li><li><p>Autograd</p><p>Package for building computational graphs out of Tensors, and automatically computing gradients</p></li><li><p>Module</p><p>A neural network layer; may store state or learnable weights</p></li></ul><h2 id=training-neural-networks>Training Neural Networks</h2><h3 id=activation-function>Activation Function</h3><h4 id=sigmoid>Sigmoid</h4><ul><li>&lsquo;kill&rsquo; the gradient when saturated</li><li>outputs are not zero-centered</li><li>exp() is a bit compute expensive</li></ul><h4 id=tanh>tanh</h4><ul><li>zero centered</li><li>&lsquo;kill&rsquo; the gradient when saturated</li></ul><h4 id=relu>ReLU</h4><ul><li>Does not saturate(in position region)</li><li>very computationally efficient</li><li>converges much faster than sigmoid/tanh in practice</li><li>not zero-centered output</li><li>will never activate &lsquo;dead ReLU&rsquo;</li></ul><h4 id=leaky-relu>Leaky ReLU</h4><ul><li>Does not saturate(in position region)</li><li>very computationally efficient</li><li>converges much faster than sigmoid/tanh in practice</li><li>not zero-centered output</li><li>will not &lsquo;die&rsquo;</li></ul><h4 id=parametric-rectifierprelu>Parametric Rectifier(PReLU)</h4><p>$$
f(x)=max*\alpha x,x
$$</p><h4 id=exponential-linear-unitelu>Exponential Linear Unit(ELU)</h4><p>$$
\left{
\begin{array}{lc}
x & x \geqslant 0 \
\alpha(e^x-1)&amp;x&lt;0\
\end{array}
\right.
$$</p><ul><li>all benefits of ReLU</li><li>closer ti zero mean outputs</li><li>negative saturation regime compared with Leaky ReLU adds some robustness to noise</li></ul><h4 id=scaled-exponential-linear-unitselu>Scaled Exponential Linear Unit(SELU)</h4><p>$$
\left{
\begin{array}{lc}
\lambda x & x \geqslant 0 \
\lambda\alpha(e^x-1)&amp;x&lt;0\
\end{array}
\right.
$$</p><h4 id=summary-1>summary</h4><ul><li>don&rsquo;t think too much. Just use ReLU</li><li>Try out Leaky ReLU/ELU/SELU/GELU if you need to squeeze that last 0.1%</li><li>Don&rsquo;t use sigmoid or tanh</li></ul><h3 id=data-preprocessing>Data Preprocessing</h3><ul><li>normalization</li><li>PCA</li><li>whitening</li></ul><h3 id=weight-initialization>Weight Initialization</h3><ul><li><p>small random numbers</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W</span><span class=o>=</span><span class=mf>0.01</span><span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>Din</span><span class=p>,</span><span class=n>Dout</span><span class=p>)</span>
</span></span></code></pre></div></li><li><p>Xavier initialization</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>Din</span><span class=p>,</span><span class=n>Dout</span><span class=p>)</span><span class=o>/</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>Din</span><span class=p>)</span>
</span></span></code></pre></div></li><li><p>Kaiming/MSRA initialization</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>Din</span><span class=p>,</span><span class=n>Dout</span><span class=p>)</span><span class=o>/</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>2</span><span class=o>/</span><span class=n>Din</span><span class=p>)</span>
</span></span></code></pre></div></li></ul><h3 id=regularization-1>Regularization</h3><ul><li>L2</li><li>L1</li><li>Elastic net</li><li>dropout</li><li>data augmentation<ul><li>random crops and scales</li><li>translation</li><li>rotation</li><li>stretching</li><li>shearing</li><li>lens distortion</li></ul></li><li>fractional pooling</li><li>stochastic depth</li><li>cutout</li><li>mixup</li></ul><h3 id=learning-rate-schedule>Learning rate schedule</h3><h4 id=decay>Decay</h4><ul><li>cosine</li></ul><p>$$
\alpha_t=\frac{1}{2}\alpha_0(1+cos(\frac{t\pi}{T}))
$$</p><ul><li>linear</li></ul><p>$$
\alpha_t=\alpha_0(1-\frac{t}{T})
$$</p><ul><li>inverse sqrt
$$
\alpha_t=\frac{\alpha_0}{\sqrt{t}}
$$</li></ul><h3 id=early-stopping>early stopping</h3><p>stop training the model when accuracy on the validation set decreases or train for a long time, but always keep track of the model snapshot that worked best on val.</p><h3 id=choosing-hyperparameters>choosing hyperparameters</h3><p>random search is better than grid search</p><p>steps</p><ul><li>check initial loss</li><li>overfit a small sample</li><li>find LR that makes loss go down</li><li>coarse gird, train for 1-5 epochs</li><li>refine grid,train longer</li><li>look at loss curves</li></ul><h3 id=model-ensembles>Model Ensembles</h3><ul><li>use multiple independent models</li><li>use multiple snapshots of a single model during training</li></ul><h3 id=transfer-learning>Transfer Learning</h3><ul><li>train on imagenet</li><li>use cnn as a feature extractor</li><li>bigger dataset: Fine-Tuning</li></ul><h2 id=recurrent-networks>Recurrent Networks</h2><p>$$
h_t=f_W(h_{t-1},x_t)
$$</p><h3 id=vanilla>Vanilla</h3><p>$$
h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)
\newline
y_t=W_{hy}h_t
$$</p><ul><li><p>Exploding gradients</p><p>gradient clipping:scale gradient if its norm is too big</p></li><li><p>vanishing gradients</p><p>change RNN architecture</p></li></ul><h3 id=truncated-backpropagation-through-time>Truncated Backpropagation Through Time</h3><p>carry hidden states forward in time forever, but only backpropagate for some smaller number of steps</p><h3 id=lstm>LSTM</h3><p>compute four gates at each timestep
$$
\begin{pmatrix}
i\
f\
o\
g\
\end{pmatrix}=
\begin{pmatrix}
\sigma\
\sigma\
\sigma\
tanh\
\end{pmatrix}W
\begin{pmatrix}
h_{t-1}\
x_t\
\end{pmatrix}
$$</p><ul><li><p>i</p><p>input gate, whether to write cell</p></li><li><p>f</p><p>forget gate, whether to erase cell</p></li><li><p>o</p><p>output gate, how much to reveal cell</p></li><li><p>g</p><p>gate gate, how much to write to cell</p></li></ul><p>$$
c_t=f\bigodot c_{t-1}+i\bigodot g
\newline
h_t=o\bigodot tanh(c_t)
$$</p><p>where $c_t$ is the cell state and $h_t$ is the hidden state</p><h3 id=multilayer>Multilayer</h3><ul><li><p>RNN
$$
h_t^l=tanhW^l\begin{pmatrix}h_t^{l-1}\h_{t-1}^l\end{pmatrix}
$$
where $h\in R^n$ and $W^l [n\times 2n]$</p></li><li><p>LSTM
$$
\begin{pmatrix}
i\
f\
o\
g\
\end{pmatrix}=
\begin{pmatrix}
sigm\
sigm\
sigm\
tanh\
\end{pmatrix}W^l
\begin{pmatrix}
h_{t}^{l-1}\
h_{t-1}^l\
\end{pmatrix}
\newline
$$</p><p>$$
c_t^l=f\bigodot c^l_{t-1}+i\bigodot g
\newline
h_t^l=o\bigodot tanh(c_t^l)
$$</p></li></ul><h2 id=attention>Attention</h2><p><img src=https://i.ibb.co/WvVgRJP/image-20220929130002973.png loading=lazy alt=image-20220929130002973></p><p>above $\sum_ia_{t,i}$ should be 1</p><p>use a different context vector in each timestep of decoder</p><p><img src=https://i.ibb.co/BrPWrzy/image-20220929133844786.png loading=lazy alt=image-20220929133844786></p><h3 id=attention-layer>Attention Layer</h3><p>Input</p><ul><li><p>query vector: $q$ (shape: $D_Q$)</p></li><li><p>input vectors: $X$ (shape:$N_X \times D_X$)</p></li><li><p>similarity function: $f_{att}$</p><p>usually <strong>scaled dot product</strong></p><p>the reason is</p><p>large similarities will cause softmax to saturate and give vanishing gradients</p></li></ul><p>Computation</p><ul><li><p>similarities: $e$ (shape $N_X$)</p><p>$e_i=f_{att}(q,X_i)$</p><p>usually $e_i=q\cdot X_i/sqrt(D_Q)$</p></li><li><p>attention weights: $a=softmax(e)$ (shape $N_X$)</p></li><li><p>output vector: $y=\sum_ia_iX_i$ (shape $D_X$)</p></li></ul><h4 id=multiple-query>multiple query</h4><p>multiply query vectors</p><p>Input</p><ul><li>query vector: $Q$ (shape: $N_Q\times D_X$)</li><li>input vectors: $X$ (shape:$N_X \times D_Q$)</li></ul><p>Computation</p><ul><li><p>similarities: $E=QX^T$ (shape $N_Q\times N_X$)</p><p>$E_{i,j}=Q_i\cdot X_j/sqrt(D_Q)$</p></li><li><p>attention weights: $a=softmax(E,dim=1)$ (shape $N_Q\times N_X$)</p></li><li><p>output vector: $Y=AX$ (shape $N_Q\times D_X$)</p><p>$Y_i=\sum_jA_{i,j}X_j$</p></li></ul><h4 id=seperate-key-and-value>seperate key and value</h4><p>Input</p><ul><li>query vector: $Q$ (shape: $N_Q\times D_Q$)</li><li>input vectors: $X$ (shape:$N_X \times D_X$)</li><li>key matrix: $W_K$ (shape:$D_X \times D_Q$)</li><li>value matrix: $W_V$ (shape:$D_X \times D_V$)</li></ul><p>Computation</p><ul><li><p>key vectors: $K=XW_K$ (shape $N_X\times D_Q$)</p></li><li><p>value vectors: $V=XW_V$ (shape $N_X\times D_V$)</p></li><li><p>similarities: $E=QK^T$ (shape $N_Q\times N_X$)</p><p>$E_{i,j}=Q_i\cdot K_j/sqrt(D_Q)$</p></li><li><p>attention weights: $a=softmax(E,dim=1)$ (shape $N_Q\times N_X$)</p></li><li><p>output vector: $Y=AV$ (shape $N_Q\times D_V$)</p><p>$Y_i=\sum_jA_{i,j}V_j$</p></li></ul><h3 id=self-attention-layer>Self-Attention Layer</h3><p>one query per input vector</p><p>Input</p><ul><li>input vectors: $X$ (shape:$N_X \times D_X$)</li><li>key matrix: $W_K$ (shape:$D_X \times D_Q$)</li><li>value matrix: $W_V$ (shape:$D_X \times D_V$)</li><li>query matrix: $W_Q$ (shape:$D_X\times D_Q$)</li></ul><p>Computation</p><ul><li><p>query vectors:$Q=XW_Q$</p></li><li><p>key vectors: $K=XW_K$ (shape $N_X\times D_Q$)</p></li><li><p>value vectors: $V=XW_V$ (shape $N_X\times D_V$)</p></li><li><p>similarities: $E=QK^T$ (shape $N_Q\times N_X$)</p><p>$E_{i,j}=Q_i\cdot K_j/sqrt(D_Q)$</p></li><li><p>attention weights: $a=softmax(E,dim=1)$ (shape $N_Q\times N_X$)</p></li><li><p>output vector: $Y=AV$ (shape $N_Q\times D_V$)</p><p>$Y_i=\sum_jA_{i,j}V_j$</p></li></ul><p><img src=https://i.ibb.co/h2zHF6Z/image-20220929144511303.png loading=lazy alt=image-20220929144511303></p><h4 id=masked-self-attention-layer>masked self-attention layer</h4><p>don&rsquo;t let vectors &ldquo;look ahead&rdquo; in the sqeuence</p><p>used for language modeling (predict next word)</p><p><img src=https://i.ibb.co/7QdcSqc/image-20220929145019078.png loading=lazy alt=image-20220929145019078></p><h3 id=multihead-self-attention-layer>Multihead self-attention layer</h3><p>use $H$ independent &ldquo;attention heads&rdquo; in parallel</p><p><img src=https://i.ibb.co/MnwG1G1/image-20220929145319763.png loading=lazy alt=image-20220929145319763></p><h3 id=cnn-with-self-attention>CNN with self-attention</h3><p><img src=https://i.ibb.co/WnsTP0Y/image-20220929145552813.png loading=lazy alt=image-20220929145552813></p><h3 id=processing-sequences>processing sequences</h3><p><img src=https://i.ibb.co/pxvZHyC/image-20220929150003833.png loading=lazy alt=image-20220929150003833></p><h4 id=transformer-block>transformer block</h4><p><img src=https://i.ibb.co/vV4k7Rd/image-20220929150218418.png loading=lazy alt=image-20220929150218418></p><h4 id=transformer>Transformer</h4><p>a Transformer is a sequence of transformer blocks</p><p>can be used for transfer learning</p><ul><li><p>pretraining</p><p>download a lot of text from the internet</p><p>train a giant transformer model for language modeling</p></li><li><p>finetuning</p><p>fine-tune the transformer on your own NLP task</p></li></ul><h2 id=object-detection>Object Detection</h2><ul><li><p>Input:</p><p>Single RGB Image</p></li><li><p>Output:</p><p>A set of detected objects, for each object predict:</p><ul><li>Category label (from fixed,known set of categories)</li><li>Bounding box (four numbers: x,y,width,height)</li></ul></li><li><p>challenges</p><ul><li>multiple outputs</li><li>multiple types of output</li><li>large images</li></ul></li><li><p>method</p><ul><li>sliding window : too many boxes</li><li>region proposals:<ul><li>find a small set of boxes that are likely to cover all objects</li><li>often based on heuristics</li><li>relatively fast to run</li></ul></li></ul></li></ul><h3 id=intersection-over-union-iou>Intersection over Union (IoU)</h3><p>$$
\frac{area;of;Intersection}{Area;of;Union}
$$</p><h3 id=overlapping-boxes-non-max-suppression-nms>Overlapping Boxes: Non-Max Suppression (NMS)</h3><ul><li>problem: Object detectors often output many overlapping detections</li><li>solution: post-process raw detections using NMS<ol><li>select next highest-scoring box</li><li>eliminate lower-scoring boxes with $IoU\gt threshold$</li><li>if any boxes remain. goto 1.</li></ol></li></ul><h3 id=evaluating-mean-average-precision-map>Evaluating: Mean Average Precision (mAP)</h3><ul><li>run object detector on all test images</li><li>for each category, compute average precision(AP)=area under Precision vs Recall Curve<ul><li>for each detection( highest score to lowest score)<ul><li>if it matches some GT box with $IoU\gt 0.5$, mark it as positive and eliminate the GT</li><li>otherwise mark it as negetive</li><li>Plot a point on PR curve</li></ul></li><li>average precision(AP)= area under PR curve</li></ul></li><li>mean average precision(mAP)=average of AP for each category</li><li>for &lsquo;COCO mAP&rsquo;: compute mAP@thresh for each IoU threshold and take average</li></ul><h3 id=slow-r-cnn>&ldquo;slow&rdquo; R-CNN</h3><p>Region-Based CNN</p><ul><li>run region proposal method to compute ~2000 region proposals</li><li>resize each region to fixed size and run independently through CNN to predict class scores and bbox transform</li><li>use scores to select a subset of region proposals to output</li><li>compare with ground-truth boxes</li></ul><h3 id=fast-r-cnn>Fast R-CNN</h3><p><img src=https://i.ibb.co/Q91BDNF/image-20221013181339635.png loading=lazy alt=image-20221013181339635></p><h3 id=faster-r-cnn--learnable-region-proposals>Faster R-CNN: learnable Region Proposals</h3><p>insert RPN to predict proposals from features</p><p>otherwise same as Fast R-CNN</p><p>jointly train with 4 losses:</p><ul><li>RPN classification</li><li>RPN regression</li><li>Object classification</li><li>Object regression</li></ul><h3 id=semantic-segmentation>Semantic Segmentation</h3><p>Label each pixel in the image with a category label</p><p>Don&rsquo;t differentiate instances, only care about pixels</p><h3 id=upsampling>Upsampling</h3><p>unpooling</p><ul><li>bed of nails</li><li>nearest neighbor</li><li>bilinear interpolation</li><li>bicubic interpolation</li><li>max unpooling</li><li>learnable upsampling: Transposed Convolution</li></ul><h3 id=instance-segmentation>Instance Segmentation</h3><ul><li>things: object categories that can be separated into object instance</li><li>stuff: object categories that cannot be separated into instances</li></ul><p>Mask R-CNN</p><h3 id=panoptic-segmentation>Panoptic Segmentation</h3><p>Label all pixels in the image (both things and stuff)</p><p>for &ldquo;thing&rdquo; categories also separate into ins</p><h2 id=3d-vision>3D Vision</h2><p>two problems</p><ul><li>predicting 3D shapes from single image</li><li>Processing 3D input data</li></ul><p>more topics</p><ul><li>Computing correspondences</li><li>multi-view stereo</li><li>structure from motion</li><li>simultaneous localization and mapping (SLAM)</li><li>Self-supervised learning</li><li>View Synthesis</li><li>Differentiable graphics</li><li>3D Sensors</li></ul><p>Many non-Deep Learning methods alive and well in 3D</p><h3 id=3d-shape-representations>3D Shape Representations</h3><ul><li><p>Depth Map</p><p>Problem: Scale/Depth Ambiguity</p><p>Scale invariant loss</p></li><li><p>Voxel Grid</p><p>for each pixel, surface normals give a vector giving the normal vector to the object in the world for that pixel</p><ul><li>3D convolution</li><li>Voxel Tubes</li></ul><p>Problem: Memory Usage</p></li><li><p>Implicit Surface</p><p>learn a function to classify arbitrary 3D points as inside/outside the shape</p><p>$o:R^3->[0,1]$</p><p>The surface of the 3D object is the level set ${x:o(x)=\frac{1}{2}}$</p></li><li><p>PointCloud</p><p>represent shape as a set of P points in 3D space</p><ul><li>can represent fine structures without huge numbers of points</li><li>requires new architectures, losses, etc</li><li>doesn&rsquo;t explicitly represent the surface of the shape: extracting a mesh for rendering or other applications requires post-processing</li></ul></li><li><p>Mesh</p><p>represent a 3D shape as a set of triangles</p><p>Vertices: Set of V points in 3D space</p><p>Faces: Set triangles over the vertices</p></li></ul><h3 id=graph-convolution>Graph Convolution</h3><p>$$
f^{&rsquo;}<em>i=W_0f_i+\sum</em>{j\in N(i)}W_1f_j
$$</p><h3 id=metrics>Metrics</h3><ul><li><p>Voxel IoU</p></li><li><p>Chamfer Distance</p></li><li><p>F1 Score
$$
F1@t=2*\frac{Precision@t*Recall@t}{Precision@t+Reca}
$$</p></li></ul><h2 id=generative-model>Generative Model</h2><ul><li>Discriminative model<ul><li>learn a probability distribution p(y|x)</li><li>used to:<ul><li>assign labels to data</li><li>feature learning (with labels)</li></ul></li></ul></li><li>Generative Model<ul><li>distribution p(x)</li><li>used to:<ul><li>detect outliers</li><li>feature learning (without label)</li><li>sample to generate new data</li></ul></li></ul></li><li>Conditional Generative Model<ul><li>learn p(x|y)</li><li>used to:<ul><li>assign labels, while rejecting outliers. generate new data conditioned on input labels</li></ul></li></ul></li></ul><p><img src=https://i.ibb.co/8dXfpmR/image-20221005141347631.png loading=lazy alt=image-20221005141347631></p><h3 id=taxonomy>Taxonomy</h3><ul><li><p>explicit density</p><p>model can compute $p(x)$</p><ul><li><p>Tractable density</p><p>Autoregressive</p><p>NADE/MADE</p><p>NICE/RealNVP</p><p>Glow</p><p>Ffjord</p></li><li><p>Approximate density</p><ul><li><p>Variational</p><p>Variational Autoencoder</p></li><li><p>Markov Chain</p><p>Boltzmann Machine</p></li></ul></li></ul></li><li><p>implicit density</p><p>does not explicitly compute $p(x)$, but can sample from $p(x)$</p><ul><li><p>Markov Chain</p><p>GSN</p></li><li><p>Direct</p><p>Generative Adversarial Networks (GANs)</p></li></ul></li></ul><h3 id=autoregressive>Autoregressive</h3><ul><li><p>Goal: write down an explicit function for $p(x)=f(x,W)$</p></li><li><p>Given dataset $x^{(1)},x^{(2)}&mldr;x^{(N)}$, train the model by solving
$$
W^{*}=arg,max_W\Pi_ip(x^{i})
$$
log trick to exchange product for sum
$$
arg,max_W\sum_ilogp(x^{i})
$$
namely
$$
arg,max_W\sum_ilogf(x^{i},W)
$$
this is the loss function.</p></li><li><p>Assume x consists of multiple subparts
$$
x=(x_1,x_2,x_3,&mldr;,x_N)
$$
break down probability using the chain rule
$$
\begin{equation} \label{eq1}
\begin{split}
p(x) & =p(x_1,x_2,x_3,&mldr;,x_N) \
& = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)&mldr;\
&=\Pi_{t=1}^Tp(x_t|x_1,&mldr;,x_{t-1})
\end{split}
\end{equation}
$$</p></li></ul><h4 id=pixelrnn>PixelRNN</h4><p>Generate image pixels one at a time, starting at the upper left corner</p><p>Compute a hidden state for each pixel that depends on hidden states and RGB values from the left and from above
$$
h_{x,y}=f(h_{x-1,y},h_{x,y-1},W)
$$
<img src=https://i.ibb.co/rGgF84c/image-20221005145834282.png loading=lazy alt=image-20221005145834282></p><h4 id=pixelcnn>PixelCNN</h4><p>still generate image pixels starting from corner</p><p>dependency on previous pixels now modeled using a CNN over context region</p><p><img src=https://i.ibb.co/dWS9XJx/image-20221005170818945.png loading=lazy alt=image-20221005170818945></p><h4 id=pros-and-cons>Pros and Cons</h4><ul><li>Pros<ul><li>can explicitly compute likelihood $p(x)$</li><li>explicit likelihood of training data gives good evaluation metric</li><li>good samples</li></ul></li><li>Cons<ul><li>sequential generation => slow</li></ul></li></ul><h3 id=variational-autoencoder>Variational Autoencoder</h3><p>variational autoencoder define an intractable density that we cannot explicitly compute or optimize</p><p>But we will be able to directly optimize a lower bound on the density</p><h4 id=autoencoders>Autoencoders</h4><p>unsupervised method for learning feature vectors from raw data x, without any labels</p><p>features should extract useful information that we can use for downstream tasks</p><p><img src=https://i.ibb.co/gZVmwQJ/image-20221005172527253.png loading=lazy alt=image-20221005172527253></p><p>idea: use the features to reconstruct the input data with a decoder</p><p><img src=https://i.ibb.co/r3vdqrb/image-20221005172636415.png loading=lazy alt=image-20221005172636415></p><p>features need to be lower dimensional that the data</p><p>after training, throw away decoder and use encoder for a downstream task</p><h4 id=variational-autoencoder-1>Variational Autoencoder</h4><ul><li><p>assume training data ${x^{(i)}}_{i=1}^N$ is generated from unobserved (latent) representation $z$</p></li><li><p>assume simple prior $p(z)$, e.g. Gaussian</p></li><li><p>represent $p(x|z)$ with a neural network (similar to decoder from autoencoder)</p></li><li><p>decoder must be probabilistic:</p><p>Decoder input $z$, output mean $\mu_{x|z}$ and (diagonal) covariance $\sum_{x|z}$</p><p>sample $x$ from Gaussian with mean $\mu_{x|z}$ and (diagonal) covariance $\sum_{x|z}$</p></li></ul><p><img src=https://i.ibb.co/1Tv4GGP/image-20221005201326846.png loading=lazy alt=image-20221005201326846></p><p>so encoder and decoder be like</p><p><img src=https://i.ibb.co/qnDVmc2/image-20221005201508840.png loading=lazy alt=image-20221005201508840>
$$
\begin{equation} \label{eq2}
\begin{split}
logp_{\theta}(x) & =log\frac{p_{\theta}(x|z)p(z)}{p_{\theta}(z|x)}\
& = log\frac{p_{\theta}(x|z)p(z)q_{\phi}(z|x)}{p_{\theta}(z|x)q_{\phi}(z|x)}\
&=logp_{\theta}(x|z)-log\frac{q_{\phi}(z|x)}{p(z)}+log\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}\
&=E_z[logp_{\theta}(x|z)]-E_z[log\frac{q_{\phi}(z|x)}{p(z)}]+E_z[log\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}]\
&=E_{z\sim q_{\phi}(z|x)}[logp_{\theta}(x|z)]-D_{KL}(q_{\phi}(z|x),p(z))+D_{KL}(q_{\phi}(z|x),p(z))\
&\ge E_{z\sim q_{\phi}(z|x)}[logp_{\theta}(x|z)]-D_{KL}(q_{\phi}(z|x),p(z))
\end{split}
\end{equation}
$$
jointly train encoder q and decoder p to maximize the variational lower bound on the data likelihood</p><ul><li>run input data through encoder to get a distribution over latent codes (encoder output should match the prior $p(z)$)</li><li>sample $z$ from encoder output</li><li>run sampled $z$ through decoder to get a distribution over data samples</li><li>original input data should be likely under the distribution output</li></ul><p><img src=https://i.ibb.co/42NYv9H/image-20221005204831140.png loading=lazy alt=image-20221005204831140></p><h4 id=generate-new-data>generate new data</h4><ul><li>sample $z$ from prior $p(z)$</li><li>run sampled $z$ through decoder to get distribution over data $x$</li><li>sample from distribution obtained above to generate data</li></ul><h4 id=pros-and-cons-1>Pros and Cons</h4><ul><li>Pros<ul><li>principled approach to generative models</li><li>allows inference of $q(z|x)$, can be useful feature representation for other tasks</li></ul></li><li>Cons<ul><li>maximizes lower bound of likelihood: okay, but not as good evaluation as PixelRNN/PixelCNN</li><li>samples blurrier and lower quality compared to state-of-the-art (GANs)</li></ul></li></ul><h3 id=gan>GAN</h3><p><img src=https://i.ibb.co/XJ9YXSv/image-20221005212039207.png loading=lazy alt=image-20221005212039207></p><ul><li><p>Generative adversarial networks give up on modeling $p(x)$, but allow us to draw samples from $p(x)$</p></li><li><p>Assume we have data $x_i$ drawn from distribution $p_{data}(x)$. Want to sample from $p_{data}$</p></li><li><p>Introduce a latent variable $z$ with simple prior $p(z)$.</p></li><li><p>Sample $z\sim p(z)$ and pass to a Generator network $x=G(z)$</p></li><li><p>Then $x$ is a sample from the Generator distribution $p_G$. want $P_G=P_{data}$</p></li><li><p>Train Generator Network G to convert $z$ into fake data $x$ sampled from $p_G$</p><p>by &ldquo;fooling&rdquo; the discriminator D</p></li><li><p>Train Discriminator Network D to classify data as real or fake(1/0)</p></li></ul><p>jointly train generator G and discriminator D with a minmax game</p><p><img src=https://i.ibb.co/x2br6t3/image-20221005212133215.png loading=lazy alt=image-20221005212133215>
$$
\begin{equation}
\begin{split}
& min_G,max_D(E_{x\sim p_{data}}[logD(x)]+E_{z\sim p(z)}[log(1-D(G(z)))])\
&=min_G,max_DV(G,D)
\end{split}
\end{equation}
$$
update
$$
D=D+\alpha_D\frac{\partial V}{\partial D}
\newline
G=G-\alpha_G\frac{\partial V}{\partial G}
$$
this minimax game achieves its global minimum when $p_G=p_{data}$</p><p>=>some mathematical derivation</p><h2 id=assignments>Assignments</h2><h3 id=pytorch-1>pytorch</h3><p><a class=link href=https://pytorch.org/ target=_blank rel=noopener>PyTorch</a> is an open source machine learning framework. At its core, PyTorch provides a few key features:</p><ul><li>A multidimensional <strong>Tensor</strong> object, similar to <a class=link href=https://numpy.org/ target=_blank rel=noopener>numpy</a> but <strong>with GPU accelleration</strong>.</li><li>An optimized <strong>autograd</strong> engine for automatically computing derivatives</li><li>A clean, modular API for building and deploying <strong>deep learning models</strong></li></ul><p>A <code>torch</code> <strong>tensor</strong> is a multidimensional grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the <strong>rank</strong> of the tensor; the <strong>shape</strong> of a tensor is a tuple of integers giving the size of the array along each dimension.</p><p>Accessing an element from a PyTorch tensor returns a PyTorch scalar; we can convert this to a Python scalar using the <code>.item()</code> method</p><h4 id=tensor-indexing>tensor indexing</h4><ul><li><p>slice indexing</p><p>Slicing a tensor returns a <strong>view</strong> into the same data, so modifying it will also modify the original tensor. To avoid this, you can use the <code>clone()</code> method to make a copy of a tensor.</p></li><li><p>integer indexing</p><p>We can also use <strong>index arrays</strong> to index tensors; this lets us construct new tensors with a lot more flexibility than using slices.</p><pre tabindex=0><code>a[idx0, idx1]
is equi
torch.tensor([
  a[idx0[0], idx1[0]],
  a[idx0[1], idx1[1]],
  ...,
  a[idx0[N - 1], idx1[N - 1]]
])
</code></pre><p>A one-hot vector for an integer n is a vector that has a one in its nth slot, and zeros in all other slots. One-hot vectors are commonly used to represent categorical variables in machine learning models.</p></li><li><p>boolean indexing</p><p>Boolean tensor indexing lets you pick out arbitrary elements of a tensor according to a boolean mask. Frequently this type of indexing is used to select or modify the elements of a tensor that satisfy some condition.</p><p>In PyTorch, we use tensors of dtype <code>torch.bool</code> to hold boolean masks.</p></li></ul><h4 id=reshape>Reshape</h4><p>PyTorch provides many ways to manipulate the shapes of tensors. The simplest example is <a class=link href=https://pytorch.org/docs/stable/generated/torch.Tensor.view.html target=_blank rel=noopener><code>.view()</code></a>: This returns a new tensor with the same number of elements as its input, but with a different shape.</p><p>As its name implies, a tensor returned by <code>.view()</code> shares the same data as the input, so changes to one will affect the other and vice-versa</p><p>We can use <code>.view()</code> to flatten matrices into vectors, and to convert rank-1 vectors into rank-2 row or column matrices:</p><p>As a convenience, calls to <code>.view()</code> may include a single -1 argument; this puts enough elements on that dimension so that the output has the same number of elements as the input. This makes it easy to write some reshape operations in a way that is agnostic to the shape of the tensor</p><p>In general, you should only use <code>.view()</code> to add new dimensions to a tensor, or to collapse adjacent dimensions of a tensor.</p><p>For tensors with more than two dimensions, we can use the function <a class=link href=https://pytorch.org/docs/stable/generated/torch.transpose.html target=_blank rel=noopener><code>torch.transpose</code></a>) to swap arbitrary dimensions.</p><h4 id=computation>Computation</h4><h5 id=elementwise>Elementwise</h5><p>Basic mathematical functions operate elementwise on tensors, and are available as operator overloads, as functions in the <code>torch</code> module, and as instance methods on torch objects; all produce the same results:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>*</span> <span class=o>+</span> <span class=o>-</span> <span class=o>/</span> <span class=o>*</span>
</span></span></code></pre></div><h5 id=matrix>matrix</h5><p>Note that unlike MATLAB, * is elementwise multiplication, not matrix multiplication. PyTorch provides a number of linear algebra functions that compute different types of vector and matrix products. The most commonly used are:</p><ul><li><a class=link href=https://pytorch.org/docs/stable/generated/torch.dot.html target=_blank rel=noopener><code>torch.dot</code></a>: Computes inner product of vectors</li><li><a class=link href=https://pytorch.org/docs/stable/generated/torch.mm.html target=_blank rel=noopener><code>torch.mm</code></a>: Computes matrix-matrix products</li><li><a class=link href=https://pytorch.org/docs/stable/generated/torch.mv.html target=_blank rel=noopener><code>torch.mv</code></a>: Computes matrix-vector products</li><li><a class=link href=https://pytorch.org/docs/stable/generated/torch.addmm.html target=_blank rel=noopener><code>torch.addmm</code></a> / <a class=link href=https://pytorch.org/docs/stable/generated/torch.addmv.html target=_blank rel=noopener><code>torch.addmv</code></a>: Computes matrix-matrix and matrix-vector multiplications plus a bias</li><li><a class=link href=https://pytorch.org/docs/stable/generated/torch.bmm.html target=_blank rel=noopener><code>torch.bmm</code></a> / <a class=link href=https://pytorch.org/docs/stable/generated/torch.baddbmm.html target=_blank rel=noopener><code>torch.baddmm</code></a>: Batched versions of <code>torch.mm</code> and <code>torch.addmm</code>, respectively</li><li><a class=link href=https://pytorch.org/docs/stable/generated/torch.matmul.html target=_blank rel=noopener><code>torch.matmul</code></a>: General matrix product that performs different operations depending on the rank of the inputs. Confusingly, this is similar to <code>np.dot</code> in numpy.</li></ul><p>You can find a full list of the available linear algebra operators <a class=link href=https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations target=_blank rel=noopener>in the documentation</a>.
All of these functions are also available as Tensor instance methods, e.g. <a class=link href=https://pytorch.org/docs/stable/generated/torch.Tensor.dot.html target=_blank rel=noopener><code>Tensor.dot</code></a> instead of <code>torch.dot</code>.</p><p>Here is an example of using <code>torch.dot</code> to compute inner products. Like the other mathematical operators we&rsquo;ve seen, most linear algebra operators are available both as functions in the <code>torch</code> module and as instance methods of tensors:</p><h4 id=vectorization>vectorization</h4><p>In many cases, avoiding explicit Python loops in your code and instead using PyTorch operators to handle looping internally will cause your code to run a lot faster. This style of writing code, called <strong>vectorization</strong>, avoids overhead from the Python interpreter, and can also better parallelize the computation (e.g. across CPU cores, on on GPUs). Whenever possible you should strive to write vectorized code.</p><h5 id=gpu>GPU</h5><p>All PyTorch tensors also have a <code>device</code> attribute that specifies the device where the tensor is stored &ndash; either CPU, or CUDA (for NVIDA GPUs). A tensor on a CUDA device will automatically use that device to accelerate all of its operations.</p><p>Just as with datatypes, we can use the <a class=link href=https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to target=_blank rel=noopener><code>.to()</code></a> method to change the device of a tensor. We can also use the convenience methods <code>.cuda()</code> and <code>.cpu()</code> methods to move tensors between CPU and GPU.</p><p>squared_euclidean_distance(train_data,train_data.view(-1))</p></section><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Jun 06, 2023 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/software-architecturenotes/><div class=article-details><h2 class=article-title>【Software Architecture】Notes</h2></div></a></article><article><a href=/p/computer-architecturenotes/><div class=article-details><h2 class=article-title>【Computer Architecture】Notes</h2></div></a></article><article><a href=/p/computer-networknotes/><div class=article-details><h2 class=article-title>【Computer Network】Notes</h2></div></a></article><article><a href=/p/stanford-compilersnotes/><div class=article-details><h2 class=article-title>【Stanford Compilers】Notes</h2></div></a></article><article><a href=/p/stanford-reinforcement-learningnotes/><div class=article-details><h2 class=article-title>【Stanford Reinforcement Learning】Notes</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 飞鸿踏雪泥</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>