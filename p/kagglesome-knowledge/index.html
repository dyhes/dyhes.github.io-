<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="The first step in any machine learning project is familiarize yourself with the data. You&rsquo;ll use the Pandas library for this. Pandas is the primary tool data scientists use for exploring and manipulating data.\nThe most important part of the Pandas library is the DataFrame. A DataFrame holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database.\n"><title>【Kaggle】Some Knowledge</title>
<link rel=canonical href=https://dyhes.github.io/p/kagglesome-knowledge/><link rel=stylesheet href=/scss/style.min.6aa4d43a5cae1c51ef34b3f851ae7421f4b2f2d13827e2d975acbeb4f13c8710.css><meta property='og:title' content="【Kaggle】Some Knowledge"><meta property='og:description' content="The first step in any machine learning project is familiarize yourself with the data. You&rsquo;ll use the Pandas library for this. Pandas is the primary tool data scientists use for exploring and manipulating data.\nThe most important part of the Pandas library is the DataFrame. A DataFrame holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database.\n"><meta property='og:url' content='https://dyhes.github.io/p/kagglesome-knowledge/'><meta property='og:site_name' content='飞鸿踏雪泥'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='AI'><meta property='article:published_time' content='2022-08-09T00:00:00+00:00'><meta property='article:modified_time' content='2022-08-09T00:00:00+00:00'><meta name=twitter:title content="【Kaggle】Some Knowledge"><meta name=twitter:description content="The first step in any machine learning project is familiarize yourself with the data. You&rsquo;ll use the Pandas library for this. Pandas is the primary tool data scientists use for exploring and manipulating data.\nThe most important part of the Pandas library is the DataFrame. A DataFrame holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database.\n"><link rel="shortcut icon" href=/github.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu17834253352308399148.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>飞鸿踏雪泥</a></h1><h2 class=site-description>没有记录，就没有发生</h2></div></header><ol class=menu-social><li><a href=https://github.com/dyhes target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:dyheslin@gmail.com target=_blank title=Gmail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-gmail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16 20h3a1 1 0 001-1V5a1 1 0 00-1-1h-3v16z"/><path d="M5 20h3V4H5A1 1 0 004 5v14a1 1 0 001 1z"/><path d="M16 4l-4 4-4-4"/><path d="M4 6.5l8 7.5 8-7.5"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/categories/><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#building-your-model>Building Your Model</a></li><li><a href=#pipeline>pipeline</a></li><li><a href=#cross-validation>Cross-validation</a></li><li><a href=#deep-learning>Deep Learning</a></li><li><a href=#stochastic-gradient-descent>Stochastic Gradient Descent</a></li><li><a href=#overfitting-and-underfitting>overfitting and underfitting</a></li><li><a href=#other-kind-of-layers-beside-dense-layer>other kind of layers beside dense layer</a><ol><li><a href=#dropout-layer>dropout layer</a></li></ol></li><li><a href=#classification>Classification</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#computer-vision>Computer Vision</a></li><li><a href=#feature-extraction>Feature Extraction</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nutrition/ style=background-color:#93b5cf;color:>积雪粮</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/kagglesome-knowledge/>【Kaggle】Some Knowledge</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Aug 09, 2022</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>13 minute read</time></div></footer></div></header><section class=article-content><p>The first step in any machine learning project is familiarize yourself with the data. You&rsquo;ll use the Pandas library for this. Pandas is the primary tool data scientists use for exploring and manipulating data.</p><p>The most important part of the Pandas library is the DataFrame. A DataFrame holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database.</p><p>Pandas has powerful methods for most things you&rsquo;ll want to do with this type of data.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># save filepath to variable for easier access</span>
</span></span><span class=line><span class=cl><span class=n>melbourne_file_path</span> <span class=o>=</span> <span class=s1>&#39;../input/melbourne-housing-snapshot/melb_data.csv&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># read the data and store data in DataFrame titled melbourne_data</span>
</span></span><span class=line><span class=cl><span class=n>melbourne_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>melbourne_file_path</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=c1># print a summary of the data in Melbourne data</span>
</span></span><span class=line><span class=cl><span class=n>melbourne_data</span><span class=o>.</span><span class=n>describe</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=building-your-model>Building Your Model</h2><p>You will use the <strong>scikit-learn</strong> library to create your models. When coding, this library is written as <strong>sklearn</strong>, as you will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames.</p><p>The steps to building and using a model are:</p><ul><li><strong>Define:</strong> What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.</li><li><strong>Fit:</strong> Capture patterns from provided data. This is the heart of modeling.</li><li><strong>Predict:</strong> Just what it sounds like</li><li><strong>Evaluate</strong>: Determine how accurate the model&rsquo;s predictions are.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.tree</span> <span class=kn>import</span> <span class=n>DecisionTreeRegressor</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define model. Specify a number for random_state to ensure same results each run</span>
</span></span><span class=line><span class=cl><span class=n>melbourne_model</span> <span class=o>=</span> <span class=n>DecisionTreeRegressor</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Fit model</span>
</span></span><span class=line><span class=cl><span class=n>melbourne_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p>Many machine learning models allow some randomness in model training. Specifying a number for <code>random_state</code> ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won&rsquo;t depend meaningfully on exactly what value you choose.</p><p>We now have a fitted model that we can use to make predictions.</p><p>In practice, you&rsquo;ll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we&rsquo;ll make predictions for the first few rows of the training data to see how the predict function works.</p><p>The scikit-learn library has a function <code>train_test_split</code> to break up the data into two pieces. We&rsquo;ll use some of that data as training data to fit the model, and we&rsquo;ll use the other data as validation data to calculate <code>mean_absolute_error</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># split data into training and validation data, for both features and target</span>
</span></span><span class=line><span class=cl><span class=c1># The split is based on a random number generator. Supplying a numeric value to</span>
</span></span><span class=line><span class=cl><span class=c1># the random_state argument guarantees we get the same split every time we</span>
</span></span><span class=line><span class=cl><span class=c1># run this script.</span>
</span></span><span class=line><span class=cl><span class=n>train_X</span><span class=p>,</span> <span class=n>val_X</span><span class=p>,</span> <span class=n>train_y</span><span class=p>,</span> <span class=n>val_y</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>random_state</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Define model</span>
</span></span><span class=line><span class=cl><span class=n>melbourne_model</span> <span class=o>=</span> <span class=n>DecisionTreeRegressor</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># Fit model</span>
</span></span><span class=line><span class=cl><span class=n>melbourne_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_X</span><span class=p>,</span> <span class=n>train_y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># get predicted prices on validation data</span>
</span></span><span class=line><span class=cl><span class=n>val_predictions</span> <span class=o>=</span> <span class=n>melbourne_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>val_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>val_y</span><span class=p>,</span> <span class=n>val_predictions</span><span class=p>))</span>
</span></span></code></pre></div><p>Since we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the (red) validation curve in the figure below.</p><p><img src=C:%5cUsers%5cdyhes%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5cimage-20220201221001169.png loading=lazy alt=image-20220201221001169></p><h2 id=pipeline>pipeline</h2><p><strong>Pipelines</strong> are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.</p><h2 id=cross-validation>Cross-validation</h2><p>In <strong>cross-validation</strong>, we run our modeling process on different subsets of the data to get multiple measures of model quality.</p><p><img src=C:%5cUsers%5cdyhes%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5cimage-20220207221137378.png loading=lazy alt=image-20220207221137378></p><h2 id=deep-learning>Deep Learning</h2><p><strong>Deep learning</strong> is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow</span> <span class=kn>import</span> <span class=n>keras</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras</span> <span class=kn>import</span> <span class=n>layers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a network with 1 linear unit</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=n>units</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>[</span><span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span></code></pre></div><p>With the first argument, <code>units</code>, we define how many outputs we want.</p><p>With the second argument, <code>input_shape</code>, we tell Keras the dimensions of the inputs.</p><blockquote><p><strong>Many Kinds of Layers</strong>
A &ldquo;layer&rdquo; in Keras is a very general kind of thing. A layer can be, essentially, any kind of <em>data transformation</em>. Many layers, like the <a class=link href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D target=_blank rel=noopener>convolutional</a> and <a class=link href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN target=_blank rel=noopener>recurrent</a> layers, transform data through use of neurons and differ primarily in the pattern of connections they form. Others though are used for <a class=link href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding target=_blank rel=noopener>feature engineering</a> or just <a class=link href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add target=_blank rel=noopener>simple arithmetic</a>. There&rsquo;s a whole world of layers to discover &ndash; <a class=link href=https://www.tensorflow.org/api_docs/python/tf/keras/layers target=_blank rel=noopener>check them out</a>!</p></blockquote><p>It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something <em>nonlinear</em>. What we need are activation functions.</p><p><em>Without activation functions, neural networks can only learn linear relationships. In order to fit curves, we&rsquo;ll need to use activation functions.</em></p><p>An <strong>activation function</strong> is simply some function we apply to each of a layer&rsquo;s outputs (its <em>activations</em>). The most common is the <em>rectifier</em> function max(0,x)max(0,x).</p><h2 id=stochastic-gradient-descent>Stochastic Gradient Descent</h2><p>In addition to the training data, we need two more things:</p><ul><li>A &ldquo;loss function&rdquo; that measures how good the network&rsquo;s predictions are.</li><li>An &ldquo;optimizer&rdquo; that can tell the network how to change its weights.</li></ul><p>The optimizer is an algorithm that adjusts the weights to minimize the loss.</p><p>Virtually all of the optimization algorithms used in deep learning belong to a family called <strong>stochastic gradient descent</strong>. They are iterative algorithms that train a network in steps. One <strong>step</strong> of training goes like this:</p><ol><li>Sample some training data and run it through the network to make predictions.</li><li>Measure the loss between the predictions and the true values.</li><li>Finally, adjust the weights in a direction that makes the loss smaller.</li></ol><p>Then just do this over and over until the loss is as small as you like (or until it won&rsquo;t decrease any further.)</p><p>Each iteration&rsquo;s sample of training data is called a <strong>minibatch</strong> (or often just &ldquo;batch&rdquo;), while a complete round of the training data is called an <strong>epoch</strong>. The number of epochs you train for is how many times the network will see each training example.</p><p>After defining a model, you can add a loss function and optimizer with the model&rsquo;s <code>compile</code> method:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>=</span><span class=s2>&#34;adam&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>=</span><span class=s2>&#34;mae&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h2 id=overfitting-and-underfitting>overfitting and underfitting</h2><p>A model&rsquo;s <strong>capacity</strong> refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.</p><p>You can increase the capacity of a network either by making it <em>wider</em> (more units to existing layers) or by making it <em>deeper</em> (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.</p><h2 id=other-kind-of-layers-beside-dense-layer>other kind of layers beside dense layer</h2><h3 id=dropout-layer>dropout layer</h3><p>can help correct overfitting.</p><p>To break up these conspiracies, we randomly <em>drop out</em> some fraction of a layer&rsquo;s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.</p><p><img src=https://i.imgur.com/a86utxY.gif loading=lazy alt="An animation of a network cycling through various random dropout configurations."></p><p>In Keras, the dropout rate argument <code>rate</code> defines what percentage of the input units to shut off. Put the <code>Dropout</code> layer just before the layer you want the dropout applied to:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl>    <span class=n>layers</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>rate</span><span class=o>=</span><span class=mf>0.3</span><span class=p>),</span> <span class=c1># apply 30% dropout to the next layer</span>
</span></span><span class=line><span class=cl>    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>16</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span></code></pre></div><p>When adding dropout, you may need to increase the number of units in your <code>Dense</code> layers.</p><h2 id=classification>Classification</h2><p><strong>Accuracy</strong> is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: <code>accuracy = number_correct / total</code>. A model that always predicted correctly would have an accuracy score of <code>1.0</code>. All else being equal, accuracy is a reasonable metric to use whenever the classes in the dataset occur with about the same frequency.</p><p>The problem with accuracy (and most other classification metrics) is that it can&rsquo;t be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in &ldquo;jumps&rdquo;. So, we have to choose a substitute to act as the loss function. This substitute is the <em>cross-entropy</em> function.</p><p>Now, recall that the loss function defines the <em>objective</em> of the network during training. With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.</p><p>For classification, what we want instead is a distance between <em>probabilities</em>, and this is what cross-entropy provides. <strong>Cross-entropy</strong> is a sort of measure for the distance from one probability distribution to another.</p><h2 id=reinforcement-learning>Reinforcement Learning</h2><p>This idea of using reward to track the performance of an agent is a core idea in the field of reinforcement learning. Once we define the problem in this way, we can use any of a variety of reinforcement learning algorithms to produce an agent.</p><h2 id=computer-vision>Computer Vision</h2><p><img src=https://i.imgur.com/U0n5xjU.png loading=lazy alt="The parts of a convnet: image, base, head, class; input, extract, classify, output."></p><p>A convnet used for image classification consists of two parts: a <strong>convolutional base</strong> and a <strong>dense head</strong>.</p><p>The base is used to <strong>extract the features</strong> from an image. It is formed primarily of layers performing the convolution operation, but often includes other kinds of layers as well.</p><p>The head is used to <strong>determine the class</strong> of the image. It is formed primarily of dense layers, but might include other layers like dropout.</p><p>The goal of the network during training is to learn two things:</p><ol><li>which features to extract from an image (base),</li><li>which class goes with what features (head).</li></ol><p>These days, convnets are rarely trained from scratch. More often, we <strong>reuse the base of a pretrained model</strong>. To the pretrained base we then <strong>attach an untrained head</strong>. In other words, we reuse the part of a network that has already learned to do <em>1. Extract features</em>, and attach to it some fresh layers to learn <em>2. Classify</em></p><p>The most commonly used dataset for pretraining is <a class=link href=http://image-net.org/about-overview target=_blank rel=noopener><em>ImageNet</em></a>, a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its <a class=link href=https://www.tensorflow.org/api_docs/python/tf/keras/applications target=_blank rel=noopener><code>applications</code> module</a>.</p><h2 id=feature-extraction>Feature Extraction</h2><p>The <strong>feature extraction</strong> performed by the base consists of <strong>three basic operations</strong>:</p><ol><li><strong>Filter</strong> an image for a particular feature (convolution)</li><li><strong>Detect</strong> that feature within the filtered image (ReLU)</li><li><strong>Condense</strong> the image to enhance the features (maximum pooling)</li></ol><p><img src=https://i.imgur.com/IYO9lqp.png loading=lazy alt="An example of the feature extraction process."></p><p>The <strong>weights</strong> a convnet learns during training are primarily contained in its convolutional layers. These weights we call <strong>kernels</strong>. We can represent them as small arrays:</p><p><img src=https://i.imgur.com/uJfD9r9.png loading=lazy alt="A 3x3 kernel."></p><p>A kernel operates by scanning over an image and producing a <em>weighted sum</em> of pixel values. In this way, a kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information.</p><p><img src=https://i.imgur.com/j3lk26U.png loading=lazy alt="A kernel acts as a kind of lens.">A kernel acts as a kind of lens.</p><p>Kernels define how a convolutional layer is connected to the layer that follows. The kernel above will connect each neuron in the output to nine neurons in the input. By setting the dimensions of the kernels with <code>kernel_size</code>, you are telling the convnet how to form these connections. Most often, a kernel will have odd-numbered dimensions &ndash; like <code>kernel_size=(3, 3)</code> or <code>(5, 5)</code> &ndash; so that a single pixel sits at the center, but this is not a requirement.</p><p>The kernels in a convolutional layer determine what kinds of features it creates. During training, a convnet tries to learn what features it needs to solve the classification problem. This means finding the best values for its kernels.</p><p>The <strong>activations</strong> in the network we call <strong>feature maps</strong>. They are what result when we apply a filter to an image; they contain the visual features the kernel extracts. Here are a few kernels pictured with feature maps they produced.</p><p><img src=https://i.imgur.com/JxBwchH.png loading=lazy alt="Three kernels and the feature maps they produce.">Kernels and features.</p><p>From the pattern of numbers in the kernel, you can tell the kinds of feature maps it creates. Generally, what a convolution accentuates in its inputs will match the shape of the <em>positive</em> numbers in the kernel. The left and middle kernels above will both filter for horizontal shapes.</p><p>With the <code>filters</code> parameter, you tell the convolutional layer how many feature maps you want it to create as output.</p><p>Notice that after applying the ReLU function (<strong>Detect</strong>) the feature map ends up with a lot of &ldquo;dead space,&rdquo; that is, large areas containing only 0&rsquo;s (the black areas in the image). Having to carry these 0 activations through the entire network would increase the size of the model without adding much useful information. Instead, we would like to <em>condense</em> the feature map to retain only the most useful part &ndash; the feature itself.</p><p>This in fact is what <strong>maximum pooling</strong> does. Max pooling takes a patch of activations in the original feature map and replaces them with the maximum activation in that patch.</p><p>The pooling step increases the proportion of active pixels to zero pixels.</p><p>In fact, the zero-pixels carry <em>positional information</em>. The blank space still positions the feature within the image. When <code>MaxPool2D</code> removes some of these pixels, it removes some of the positional information in the feature map. This gives a convnet a property called <strong>translation invariance</strong>. This means that a convnet with maximum pooling will tend not to distinguish features by their <em>location</em> in the image.</p><p>We mentioned in the previous exercise that average pooling has largely been superceeded by maximum pooling within the convolutional base. There is, however, a kind of average pooling that is still widely used in the <em>head</em> of a convnet. This is <strong>global average pooling</strong>. A <code>GlobalAvgPool2D</code> layer is often used as an alternative to some or all of the hidden <code>Dense</code> layers in the head of the network</p><p>There are two additional parameters affecting both convolution and pooling layers &ndash; these are the <code>strides</code> of the window and whether to use <code>padding</code> at the image edges. The <code>strides</code> parameter says how far the window should move at each step, and the <code>padding</code> parameter describes how we handle the pixels at the edges of the input.</p><p>.</p><p>Increasing the stride means that we miss out on potentially valuble information in our summary. Maximum pooling layers, however, will almost always have stride values greater than 1, like <code>(2, 2)</code> or <code>(3, 3)</code>, but not larger than the window itself.</p><p>What the convolution does with these boundary values is determined by its <code>padding</code> parameter. In TensorFlow, you have two choices: either <code>padding='same'</code> or <code>padding='valid'</code>. There are trade-offs with each.</p><p>When we set <code>padding='valid'</code>, the convolution window will stay entirely inside the input. The drawback is that the output shrinks (loses pixels), and shrinks more for larger kernels. This will limit the number of layers the network can contain, especially when inputs are small in size.</p><p>The alternative is to use <code>padding='same'</code>. The trick here is to <strong>pad</strong> the input with 0&rsquo;s around its borders, using just enough 0&rsquo;s to make the size of the output the <em>same</em> as the size of the input. This can have the effect however of diluting the influence of pixels at the borders. The animation below shows a sliding window with <code>'same'</code> padding.</p><p><img src=https://i.imgur.com/RvGM2xb.gif loading=lazy alt="Illustration of zero (same) padding."></p><p>The VGG model we&rsquo;ve been looking at uses <code>same</code> padding for all of its convolutional layers. Most modern convnets will use some combination of the two.</p><p>. A single round of feature extraction can only extract relatively simple features from an image, things like simple lines or contrasts. These are too simple to solve most classification problems. Instead, convnets will repeat this extraction over and over, so that the features become more complex and refined as they travel deeper into the network.</p><p>It does this by passing them through long chains of <strong>convolutional blocks</strong> which perform this extraction.</p><p><img src=https://i.imgur.com/pr8VwCZ.png loading=lazy alt="Extraction as a sequence of blocks."></p><p>These convolutional blocks are stacks of <code>Conv2D</code> and <code>MaxPool2D</code> layers, whose role in feature extraction we learned about in the last few lessons.</p><p><img src=https://i.imgur.com/8D6IhEw.png loading=lazy alt="A kind of extraction block: convolution, ReLU, pooling."></p><p>Each block represents a round of extraction, and by composing these blocks the convnet can combine and recombine the features produced, growing them and shaping them to better fit the problem at hand. The deep structure of modern convnets is what allows this sophisticated feature engineering and has been largely responsible for their superior performance.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/ai/>AI</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Aug 09, 2022 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/mac.db_store-file/><div class=article-image><img src=/covers/cover20.jpg loading=lazy data-key data-hash=/covers/cover20.jpg></div><div class=article-details><h2 class=article-title>【Mac】.DB_Store file</h2></div></a></article><article><a href=/p/kaggleconcepts/><div class=article-details><h2 class=article-title>【Kaggle】Concepts</h2></div></a></article><article><a href=/p/pythonmatplotlib/><div class=article-details><h2 class=article-title>【Python】Matplotlib</h2></div></a></article><article><a href=/p/pythonnumpy/><div class=article-details><h2 class=article-title>【Python】Numpy</h2></div></a></article><article><a href=/p/pythonpandas/><div class=article-details><h2 class=article-title>【Python】Pandas</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 飞鸿踏雪泥</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>